# Standard continuous distributions {#ContinuousDistributions}

::: {.objectivesBox .objectives data-latex="{iconmonstr-target-4-240.png}"}
On completion of this module, you should be able to:

*  recognise the probability functions and underlying parameters of uniform, exponential, gamma, beta, and normal random variables.
* know the basic properties of the above continuous distributions.
* apply these distributions as appropriate to problem solving.
* approximate the binomial distribution by a normal distribution.
:::


In this chapter, some popular continuous distributions are discussed.
Properties such as definitions and applications are considered.


## Continuous uniform distribution {#ContinuousUniform}

The continuous uniform distribution has a constant pdf over a given range.

:::{.definition #ContinuousUniformDistribution name="Continuous uniform distribution"}
If a random variable $X$ with range space $[a, b]$ has the pdf
\[
   f_X(x) = \displaystyle\frac{1}{b - a}\quad\text{for $a\le x\le b$},
\]
then $X$ has a *continuous uniform distribution*.
We write $Y\sim U(a,b)$ or $X\sim\text{Unif}(a,b)$.
:::

The same notation is used to denote the discrete continuous uniform distribution; the context should make it clear which is meant). 
A plot of the pdf for a continuous uniform distribution is shown in Fig. \@ref(fig:ContinuousUniform).


```{r ContinuousUniform, echo=FALSE, fig.align="center", fig.cap="The pdf for a continuous uniform distribution $U(a,b)$.",fig.width=4,fig.height=3.5}
plot( x = c(1, 7),
      y = c(0, 1),
      type = "n",
      ylim = c(0, 1.1),
      xlim = c(-1, 9),
      xlab = expression(italic(x)),
      ylab = "Prob. fn",
      axes = FALSE,
      main = "Continuous uniform distribution")
abline(h = 0,
       col = "grey")
lines(x = c(0, 1),
      y = c(0, 0),
      lwd = 2)
lines(x = c(-1, 0),
      y = c(0, 0),
      lwd = 2,
      lty = 2)
lines(x = c(7, 8),
      y = c(0, 0),
      lwd = 2)
lines(x = c(8, 9),
      y = c(0, 0),
      lwd = 2,
      lty = 2)

axis(side = 1,
     at = c(1, 7),
     labels = c(expression(italic(a)), 
                expression(italic(b)) ) )
axis(side = 2,
     at = c(0, 1),
     label = c(0,
               expression(frac(1, (italic(b) - italic(a))) ) ),
     las = 1)
box()

lines( x = c(1, 7),
       y = c(1, 1),
       lwd = 2)
lines( x = c(1, 1),
       y = c(0, 1),
       col = "grey",
       lty = 3)
lines( x = c(7, 7),
       y = c(0, 1),
       col = "grey",
       lty = 3)
```


The following are the basic properties of the continuous uniform distribution.

:::{.theorem #ContUniformProperties name="Continuous uniform distribution properties"}
If $X\sim\text{Unif}(a,b)$ then

1. $\text{E}(X)  = (a + b)/2$.
2. $\text{var}(X) = (b - a)^2/12$.
3. $M_X(t) = \{ \exp(bt) - \exp(at) \} / [t(b - a)]$.
:::


:::{.proof}
These proofs are left as exercises.
:::


:::{.example #ContinuousUniform name="Continuous uniform"}
 If $X$ is uniformly distributed on $[-2, 2]$, find $\Pr(|X| > \frac{1}{2})$.
\begin{align*}
     \Pr\left(|X| > \frac{1}{2}\right) 
     &= \Pr\left(X > \frac{1}{2}\right) + \Pr\left(X < -\frac{1}{2}\right)\\
     &= \int^2_{\frac{1}{2}} f(x)\,dx  +  \int^{-\frac{1}{2}}_{-2} f(x)\,dx \text{ where $f(x) = \frac{1}{4}$}\\
     &= \frac{3}{4}.
\end{align*}
The probability could also be computed by finding the area of the appropriate rectangle.
:::


::: {.softwareBox .software data-latex="{iconmonstr-laptop-4-240.png}"}
In **R**, the continuous uniform distribution functions have the form `[dpqr]unif(min, max)`.
:::


## Normal distribution {#Normal}

The most well-known continuous distribution is probably the *normal distribution*, sometimes called the *bell-shaped* distribution.
The normal distribution has many applications (especially in [sampling](SamplingDistributions)), and many natural quantities (such as heights and weights of humans) follow normal distributions.


:::{.definition #NormalDistribution name="Normal distribution"}
If a random variable $X$ has the pdf
\[
   f_X(x) =
   \displaystyle \frac{1}{\sigma \sqrt{2\pi}}
                 \exp\left\{ -\frac{1}{2}\left( \frac{x-\mu}{\sigma}\right)^2 \right\}
\]
for $-\infty<x<\infty$, then $X$ has a *normal distribution*.
The two parameters are the mean $\mu$ such that $-\infty < \mu < \infty$; and the standard deviation $\sigma$ such that $\sigma > 0$.

We write $X\sim N(\mu, \sigma^2)$.
:::


Some authors use the notation $X\sim N(\mu,\sigma)$ so it is wise to check each article or book for the npotation used.
Some examples of normal distribution pdfs are shown in Fig. \@ref(fig:Normal).

```{r Normal, echo=FALSE, fig.align="center", fig.cap="Some examples of normal distributions. The solid lines correspond to $\\sigma = 0.5$ and the dashed lines to $\\sigma = 1$. For the left panel, $\\mu = -3$; for the right panel, $\\mu = 2$.", fig.height=3.5, fig.width=7, out.width='80%'}

par(mfrow = c(1, 2))

z <- seq(-3.5, 3.5,
         length = 100)

mu <- c(-3, 2)
sigma <- c(0.5, 1)

for (i in (1:2)){
   x <- mu[i] + z * max(sigma)
   muvalue <- mu[i]
   plot( x,
         dnorm(x, 
               mean = mu[i], 
               sd = sigma[1]),
         type = "l",
         xlab = expression(italic(x)),
         ylab = "Probability function",
         lwd = 2,
         las = 1,
         main = bquote("Normal distribution: " ~ mu == .(muvalue))
         )
   lines( x,
         dnorm(x, 
               mean = mu[i], 
               sd = sigma[2]),
         lty = 2,
         lwd = 2)
   
}
```


In drawing the graph of the normal pdf, note that

1. $f_X(x)$ is symmetrical about $\mu$: $f_X(\mu - x) = f_X(\mu + x)$.
2. $f_X(x) \to 0$ asymptotically as $x\to \pm \infty$.
3. $f_X'(x) = 0$  when $x = \mu$, and a maximum occurs there.
4. $f_X''(x) = 0$ when $x = \mu \pm \sigma$ (points of inflexion): PROVE THIS IN EXERCISE!???


The proof that $\displaystyle \int^\infty_{-\infty} f_X(x)\,dx = 1$ is not obvious so will not be given.
The proof relies on first squaring the integral and then changing to polar coordinates.

The following are the basic properties of the normal distribution.

:::{.theorem #NormalProperties name="Normal distribution properties"}
If $X\sim N(\mu,\sigma^2)$ then

1. $\text{E}(X) = \mu$.
2. $\text{var}(X) = \sigma^2$.
3. $M_X(t) = \displaystyle \exp\left(\mu t + \frac{t^2\sigma^2}{2}\right)$.
:::

:::{.proof}
The proof is delayed until after the Theorem \@ref(thm:StandardNormalProperties).
:::


::: {.softwareBox .software data-latex="{iconmonstr-laptop-4-240.png}"}
In **R**, the normal distribution functions have the form `[dpqr]norm(mean, sd)`.
:::


## The standard normal distribution {#StandardNormal}

A special case of the normal distribution is the *standard normal* distribution, when the normal distribution has mean zero and variance one.


:::{.definition #StandardNormal name="Standardard normal distribution"}
The pdf for a random variable $Z$ with a *standard normal distribution* is
\[
   f_Z(z) = \displaystyle \frac{1}{\sqrt{2\pi}}
                                \exp\left\{ -\frac{z^2}{2}\right\}
\]
where $-\infty < z < \infty$.
We write $Z\sim N(0, 1)$.
:::

Since $f_Z(z)$ is a pdf, then
\begin{equation}
   \frac 1{\sqrt{2\pi}}\int^\infty_{-\infty} e^{-\frac 12 z^2}\,dz = 1,
   (\#eq:ZDistribution)
\end{equation}
a result which proves useful in many contexts (as in the proof of \@ref(thm:GammaFunctionProperties), and in the proof below).

The following are the basic properties of the standard normal distribution.


:::{.theorem #StandardNormalProperties name="Standard normal distribution properties"}
If $Z\sim N(0, 1)$ then

1. $\text{E}(Z) = 0$.
2. $\text{var}(Z) = 1$.
3. $M_Z(t) = \displaystyle \exp\left(\frac{t^2}{2}\right)$.
:::

:::{.proof}
Part 3 could be proven first, and used to prove Parts 1 and 2.
However, proving Parts 1 and 2 directly is constructive.
For the expected value:
\begin{align*}
    \text{E}(Z)
    &= \frac {1}{\sqrt{2\pi}} 
       \int^\infty_{-\infty} z e^{-\frac{1}{2}z^2}\,dz\\
    &= \int^\infty_{-\infty} -\frac{d}{dz} \left(e^{-\frac{1}{2}z^2} \right)\, dz\\
    &= \left[ -e^{-\frac{1}{2}z^2}\right]^\infty_{-\infty} = 0,
\end{align*}
since the integrand is symmetric about $0$.

For the variance, first see that $\text{var}(Z) = \text{E}(Z^2) - \text{E}(Z)^2 = \text{E}(Z^2)$ since $\text{E}(Z) = 0$.
So:
\[
   \text{var}(X) = \text{E}(Z^2)
   = \frac {1}{\sqrt{2\pi}}\int^\infty_{-\infty}z^2e^{-\frac{1}{2}z^2}\,dz.
\]
To integrate by parts (i.e., $\int u\,dv = uv - \int v\, du$), set $u = z$ (so that $du = 1$) and set
\[
   dv = ze^{-\frac{1}{2}z^2}
   \quad\text{so that}\quad
   v = -e^{-\frac{1}{2}z^2}.
\]
Hence,
\begin{align*}
   \text{var}(X)
   &= \frac {1}{\sqrt{2\pi}} \left\{ -e^{-\frac{1}{2}z^2} - \int^\infty_{-\infty}-e^{-\frac{1}{2}z^2}\, dz \right \}\\
   &= \frac {1}{\sqrt{2\pi}}\left(\left. -z\,e^{-\frac{1}{2} z^2}\right|^\infty_{-\infty}\right) + \frac 1{\sqrt{2\pi}}\int^\infty_{-\infty} e^{-\frac 12 z^2}\,dz = 1
\end{align*}
since the first term is zero (due to symmetry), and the second term uses \@ref(eq:ZDistribution).

For the mgf:
\[ 
   M_Z(t) = \text{E}(e^{tZ}) =\int^\infty_{-\infty}e^{tz} \frac{1}{\sqrt{2\pi}}e^{-\frac{1}{2}z^2}\,dz.
\]
Collecting together the terms in the exponent and completing the square,
\begin{equation*}
     -\frac{1}{2}[z^2 -2tz] = -\frac{1}{2}(z - t)^2+\frac{1}{2} t^2.
\end{equation*}
Taking the constants outside the integral:
\[ 
   M_Z(t) = e^{\frac{1}{2}t^2}\int^\infty_{-\infty} \frac{1}{\sqrt{2\pi}}e^{-\frac{1}{2}[z - t]^2}\,dz. 
\]
The integral here is 1, since it is the area under an $N(t, 1)$ pdf. 
Hence
\[ 
   M_Z(t) = e^{\frac{1}{2} t^2}.
\]
:::

This distribution is important in practice since *any* normal distribution can be rescaled into a standard normal distribution using
\begin{equation}
   Z = \frac{X - \mu}{\sigma}.
   (\#eq:ConvertToZ)
\end{equation}
Since $Z = (X - \mu)/\sigma$, then $X = \mu + \sigma Z$, and so
\[
   \text{E}(X) = \text{E}(\mu + \sigma Z) = \mu + \sigma \text{E}(Z) = \mu
\]
because $\text{E}(Z) = 0$.
Also
\[
   \text{var}(X) = \text{var}(\mu  +\sigma Z) = \sigma^2\text{var}(Z) = \sigma^2
\]
because $\text{var}(Z) = 1$.
Finally
\[
   M_X(t) = \text{E}(e^{tX}) = \text{E}\big(\exp\{t(\mu + \sigma Z)\}\big) = \exp(\mu t)\text{E}\big(\exp(t\sigma Z)\big).
\]
However, $\text{E}\big(\exp(t\sigma Z)\big) = M_Z(t\sigma) = e^{\frac{1}{2}(\sigma t)^2}$ so 
\[
   M_Z(t) = \displaystyle \exp\left(\frac{t^2}{2}\right).
\]


::: {.softwareBox .software data-latex="{iconmonstr-laptop-4-240.png}"}
In **R**, the normal distribution functions have the form `[dpqr]norm(mean, sd)`.
By default, `mean = 0` and `sd = 1`, corresponding to the standard normal distribution.
:::



### Determining normal probabilities {#FindNormalProbs}

The probability $\Pr(a < X \le b)$ where $X\sim N(\mu, \sigma^2)$ can be written
\[
   \Pr(a < X \le b) = F_X(b) - F_X(a).
\]
where $F_X(x)$ is the distribution function
\[
   F_X(x) 
   = \Pr(X \le x) 
   = \frac1{\sqrt{2\pi}\sigma}\int_{-\infty}^x e^{-\frac{1}{2}(u - \mu )^2 / \sigma ^2}\, du.
\]
This integral cannot be written in terms of standard functions and in general must be evaluated for a particular $x$ numerically (e.g., using Simpson's rule or similar).
However, all statistical packages have a built-in procedure that evaluates $F_X(x)$ for any $z$ (such as `pnorm()` in **R**).

Also, since any normal distribution can be transformed into a standard normal distribution, we can write
\[
   z_1 = (a - \mu)/\sigma
   \quad\text{and}\quad
   z_2 = (b - \mu)/\sigma
\]
and hence the probability is 
\[
   \Pr( z_1 < Z \le z_2) = \Phi(z_2) - \Phi(z_1),
\]
where $\Phi(z)$ is the distribution function for the standard normal distribution
\[
   \Phi(z) = \int_{-\infty}^z \frac{1}{\sqrt{2\pi}}
                                \exp\left\{ -\frac{t^2}{2}\right\}\, dt.
\]
The process of converting a value $x$ into $z$ using $z = (x - \mu)/\sigma$ is called *standardising*.
Tables of $\Phi(z)$ (or sometimes $1 - \Phi(z)$) are commonly available.
These tables can be used to compute any probabilities associated with the normal distributions (see the examples below).

In addition, the tables are often used in the reverse sense, where the probability of an event is given and the value of the random variable is sought.
In these case, the tables are used 'backwards'; the appropriate area is found in the body of the table and the corresponding $z$-value found in the table margins.
In **R**, the function `qnorm()` is used.
This $z$-value is then converted to a value of the original random variable using
\begin{equation}
   x = \mu + z\sigma.
   (\#eq:Unstandardising)
\end{equation}
This process is sometimes referred to as *unstandardising*.

The following examples illustrate the use of the standard normal table.
Drawing rough graphs showing the relevant areas is encouraged.



:::{.example #NormalWalkingSpeeds name="Walking speeds"}
A study of stadium evacuations used a simulation to compare scenarios [@xie2017improved].
The walking speed of people was modelled using a $N(1.15, 0.2^2)$ distribution; that is, $\mu = 1.15$ m/s with a standard deviation of 0.2m/s.
The situation can be shown in Fig. \@ref(fig:NormalWalkingSpeed), top left panel.
Many questions can be asked about the walking speeds, and **R** used to compute answers.
Using this model:

1. What is the probability that a person walks faster than 1.5m/s?
1. What is the probability that a person walks slower than 1.0m/s?
1. What is the probability that a person walks between 1.0m/s and 1.5m/s?
1. At what speed do the quickest 15% of people walk?
1. At what speed do the slowest 5% of people walk?

Sketches of each of these situations are shown in Fig. \@ref(fig:NormalWalkingSpeed).
Using **R**:

```{r}
# Part 1
1 - pnorm(1.5, mean = 1.15, sd = 0.2)

# Part 2
pnorm(1, mean = 1.15, sd = 0.2)

# Part 3
pnorm(1.5, mean = 1.15, sd = 0.2) - 
   pnorm(1, mean = 1.15, sd = 0.2)

# Part 4
qnorm(0.15, mean= 1.15, sd = 0.2)

# Part 5
qnorm(0.95, mean = 1.15, sd = 0.2)
```
So the answers are, respectively:

1. `r round( 1 - pnorm(1.5, mean = 1.15, sd = 0.2), 3) * 100`%;
1. `r round( pnorm(1, mean = 1.15, sd = 0.2), 3) * 100`%;
1. `r round( pnorm(1.5, mean = 1.15, sd = 0.2) - pnorm(1, mean = 1.15, sd = 0.2), 3) * 100`%;
1. `r round( qnorm(0.10, mean= 1.15, sd = 0.2), 3)` m/s;
1. `r round( qnorm(0.95, mean = 1.15, sd = 0.2), 3)`m/s.
:::



```{r NormalWalkingSpeed, echo=FALSE, fig.align="center", fig.cap="The normal distribution used for modelling walking speeds. The vertical dotted lines are at the mean and $\\pm1$, $\\pm2$, $\\pm3$ and $\\pm4$ standard deviations from the mean.", fig.width=7, fig.height=4, out.width='100%'}
plotWalking <- function(){
  mu <- 1.15
  sigma <- 0.2
  zseq <- seq(-4, 4,
              length = 500)
  yy <- dnorm(zseq)
  
  xx <- mu + sigma*zseq
  
  z <- seq(-4, 4, 
           by = 1)
  plot(yy ~ xx,
       type = "n",
       las = 1,
       axes = FALSE,
       xlab = "Walking speed (m/s)",
       ylab = "")
  axis(side = 1,
       at = mu + z * sigma)
  abline(h = 0,
         lwd = 2,
         col = "grey")
  abline(v =  mu + z * sigma,
         lwd = 1,
         lty = 2,
         col = "grey")
  lines(yy ~ xx,
       lwd = 2)
  
  return(list( x = xx, 
               y = yy) )
}


par( mfrow = c(2, 3) )
out <- plotWalking()
title(main = "Walking speeds")

out <- plotWalking()
title(main = "Faster than 1.5m/s")
shadeNormal(out$x, 
            out$y,
            lo = 1.5, 
            hi = 10,
            col = plotColour1)

out <- plotWalking()
title(main = "Slower than 1.0m/s")
shadeNormal(out$x, 
            out$y,
            lo = -100, 
            hi = 1,
            col = plotColour1)

out <- plotWalking()
title(main = "Between 1.0m/s and 1.5m/s")
shadeNormal(out$x, 
            out$y,
            lo = 1, 
            hi = 1.5,
            col = plotColour1)

out <- plotWalking()
title(main = "Quickest 15%")
shadeNormal(out$x, 
            out$y,
            hi = qnorm(0.15, mean= 1.15, sd = 0.2),
            lo = -100, 
            col = plotColour1)

out <- plotWalking()
title(main = "Slowest 5%")
shadeNormal(out$x, 
            out$y,
            hi = 100, 
            lo = qnorm(0.90, mean= 1.15, sd = 0.2),
            col = plotColour1)

```



:::{.example #Exam name="Using normal distributions"}
An examination has mean score of 500 and standard deviation 100.
The top 75% of candidates taking this examination are to be passed.
Assuming the score has a normal distribution, what is the lowest passing score?

We have $X\sim N(500, 100^2)$. 
Let $x_1$ be such that
\begin{align*}
   \Pr(X > x_1) 
   &= 0.75\\
   \Pr\left(Z > \frac{x_1 - 500}{100}\right) 
   &= 0.75\\
   \Pr\left(Z < \frac{500 - x_1}{100}\right) 
   & = 0.75.
\end{align*}
From tables or **R**, $0.75 = \Phi(0.675)$ so $(500 - x_1)/100 = 0.675$ and $x_1 = 432.5$.
:::




### Normal approximation to the binomial {#NormalApproxBinomial}

In Sect. \@ref(BinomialDistribution), the binomial distribution was considered.
Sometines using the binomial distribution is tedious; consider a binomial random variable $X$ where $n = 1000$, $p = 0.45$ and $\Pr(X > 650)$ is sought.

However, sometimes the normal distribution can be used to approximate binomial probabilities
For certain parameter values, the binomial pf starts to take on a normal distribution shape (Fig. \@ref(fig:NormalApprox)).

When is the binomial pf close enough to use the normal approximation?
There is no definitive answer; a common guideline suggests that if *both* $np \ge 5$ *and* $n(1 - p) \ge 5$ the approximation is satisfactory.
(These are only guidelines, and other texts may suggest different guidelines.)

Figure \@ref(fig:NormalApprox) shows some picture of various binomial pfs overlaid with the corresponding normal distribution; the approximation is visibly better as the guidelines given above are satisfied.


```{r NormalApprox, echo=FALSE, , fig.height = 7, fig.width=7, out.width='80%', fig.align="center", fig.cap="The normal distribution approximating a binomial distribution. The guidelines suggest the approximation should be good when $np \\ge 5$ and $n (1 - p) \\ge 5$; this is evident from the pictures. In the top row, a significant amount of the approximating normal distribution even appears when $Y < 0$."}
par( mfrow = c(2, 2))

n <- c(5, 10, 
         20, 50)
p <- c(0.2, 0.2,
       0.2, 0.2)
ylim <- c(0.5, 0.35,
          0.25, 0.14)

for (i in (1:4)){
  ni <- n[i]
  pi <- p[i]
  
  mn <- ni * pi
  sd <- sqrt( ni * pi * (1 - pi) )
  
  xBinomial <- 1:ni
  xNormal <- seq(0, ni,
                 length = 100)
    
  yBinomial <- dbinom(xBinomial,
                      size = ni,
                      prob = pi)
  yNormal <- dnorm(xNormal, 
                   mean = mn,
                   sd = sd) 
  plot( x = xBinomial,
        y = yBinomial,
        xlab = expression(italic(x)),
        ylab = "Probability function",
        type = "h",
        ylim = c(0, ylim[i]),
        las = 1,
        lty = 2,
        col = "grey",
        main = bquote(italic(n) == .(ni) ~ "and" ~ italic(p) == .(pi)))
  points( x = xBinomial,
          y = yBinomial,
          pch = 19)
  lines( x = xNormal,
         y = yNormal,
         col = plotColour1,
         lwd = 2)
}


```



The normal distribution can be used to approximate probabilities in situation that are actually binomial.
A fundamental difficulty with this approach is that discrete distribution is being modelled with a continuous distribution.
This is best explained through an example.
The example explains the *principle*; the idea extends to all situations where the normal distribution is used to approximate a binomial distribution.


::: {.linkBox .link data-latex="{iconmonstr-link-1-240.png}"}
If the random variable $X$ has the binomial distribution $X \sim \text{Bin}(n, p)$, the probability function can be approximated by the normnal distribution $Y \sim \text{Norm}(\mu, \sigma^2)$, where $\mu = np$ and $\sigma^2 = np(1 - p)$.
\smallskip

The approximation is good if *both* $np \ge 5$ *and* $n(1 - p) \ge 5$.
:::



:::{.example #NormalApproxMice name="Normal approximation to binomial"}
Consider *mdx* mice (which have a strain of muscular dystrophy) from a particular source for which 30% of the mice survive for at least 40 weeks.
One particular experiment requires at least 35 of the 100 mice to live beyond 40 weeks.
What is the probability that 35 or more of the group will survive beyond 40 weeks?

First note that the situation is binomial; if $X$ is the number of mice from the group of 100 that survive, then $X \sim \text{Bin}(100, 0.3)$.
This could be *approximated* by the normal distribution $Y\sim N(30, 21)$, where the variance is $np(1 - p) = 100\times 0.3\times 0.7 = 21$.
Both $np = 30$ and $n(1 - p) = 70$ are much larger than 5, so this approximation is expected to adequate.
Figure \@ref(fig:ContCorrBoxes) shows the upper tail of the distribution near $X = 35$:
using the normal approximation from $Y = 35$, only *half* of the original bar in the binomial pf is included.
However, since the number of mice is discrete, we want the *entire* bar corresponding to $X = 35$.
So to compute the correct answer, the normal distribution must be evaluated for $\Pr(Y > 34.5)$.
This change from $X \ge 34.5$ to $Y > 34.5$ is called using the *continuity correction*.

The exact answer (using the binomial distribution) is $0.1629$ (rounded to four decimal places).
Using the normal distribution *with* the continuity correction gives the answer as $0.1631$; using the normal distribution *without* the continuity correction, the answer is $0.1376$.
The solution is more accurate, as expected, using the continuity correction.
:::


```{r BinomialCC, echo=TRUE, fig.width=4, fig.height=3.5, fig.cap="The continuity correction used with the binomial distribution"}
# Exact
ExactP <- sum( dbinom(x = 35:100,
                      size = 100,
                      prob = 0.30))
# Normal approx
NormalP <- 1 - pnorm(35, 
                     mean = 30,
                     sd = sqrt(21))
# Normal approx with continuity correction
ContCorrP <- 1 - pnorm(34.5, 
                       mean = 30,
                       sd = sqrt(21))
c("Exact:" = round(ExactP, 6), 
  "Normal approx:" = round(NormalP, 6),
  "With correction:" = round(ContCorrP, 6))
```


```{r ContCorrBoxes, echo=FALSE, fig.width=7, fig.height=4, out.width='70%', fig.align="center", fig.cap="The normal distribution approximating the binomial distribution when $n = 100$, $p = 0.3$ and finding the probability that $X > 35$."}
n <- 100
p <- 0.3

xx <- 33 : 37
pdf.binomial <- dbinom(x = xx,
                       size = n,
                       prob = p)


par( mfrow = c(1, 2))

barplot(pdf.binomial,
        space = 0, #Put bars together to make histogram
        las = 1,
        names = as.character(xx),
        col = "white",
        ylim = c(0, 0.08),
        main = "Without continuity correction",
        xlab = expression(italic(X)),
        ylab = "Prob. function")
# Now, this is plotted with x = 1, 2, 3, 4, 5... so make such normal pdf matches


xx.normal <- seq( min(xx) - 0.5, # Go to left of box
                  max(xx) + 1.0, # Go to right of box
                  by = 0.01)
pdf.normal <- dnorm(x = xx.normal,
                    mean = n * p,
                    sd = sqrt(n * p * (1 - p) ) )

# SO: 0.5 on the barplot is equivalent to 33 
# SO: 4.5 on the barplot is equivalent to 37
# NOTE: Diff as alway 32.5
x.normal.rescaled <- xx.normal - 32.5 # Rescale to barchart scale
shading <- (x.normal.rescaled > 2.4999)

lines( x =  x.normal.rescaled,
       y = pdf.normal,
       lwd = 2)
# Shade
polygon( x = c(2.5, 
               x.normal.rescaled[shading], 
               5.5),
         y = c(0, 
               pdf.normal[shading], 
               0),
         col = plotColour)





barplot(pdf.binomial,
        space = 0, #Put bars together to make histogram
        las = 1,
        names = as.character(xx),
        col = "white",
        ylim = c(0, 0.08),
        main = "With continuity correction",
        xlab = expression(italic(X)),
        ylab = "Prob. function")
# Now, this is plotted with x = 1, 2, 3, 4, 5... so make such normal pdf matches


xx.normal <- seq( min(xx) - 0.5, # Go to left of box
                  max(xx) + 1.0, # Go to right of box
                  by = 0.01)
pdf.normal <- dnorm(x = xx.normal,
                    mean = n * p,
                    sd = sqrt(n * p * (1 - p) ) )

# SO: 0.5 on the barplot is equivalent to 33 
# SO: 4.5 on the barplot is equivalent to 37
# NOTE: Diff as alway 32.5
x.normal.rescaled <- xx.normal - 32.5 # Rescale to barchart scale
shading <- (x.normal.rescaled > 1.9999)

lines( x =  x.normal.rescaled,
       y = pdf.normal,
       lwd = 2)
# Shade
polygon( x = c(2, 
               x.normal.rescaled[shading], 
               5.5),
         y = c(0, 
               pdf.normal[shading], 
               0),
         col = plotColour)


```

:::{.example #ContCorrectionDice name="Continuity correction"}
Consider rolling a standard die 100 times, and counting the number of times a `r knitr::include_graphics("Dice/die1.png", dpi=2000)` appear uppermost.
The random variable $X$ is the number of times a `r knitr::include_graphics("Dice/die1.png", dpi=2000)` appears; then, $X\sim \text{Bin}(n = 100, p = 1/6)$.
Since $np = 16.667$ and $n(1 - p) = 83.333$ are both greater than $5$, a normal approximation should be accurate, so define $Y\sim N(16.6667, 13.889)$ (where the variance is $np(1 - p) = 13.889$).
Various probabilities (Table \@ref(tab:ContCorrectionTable)) show the accuracy of the approximation, and the way in which the continuity correction has been used.

You should understand how the concept of the continuity correction has been applied in each situation, and be able to compute the probabilities for the normal approximation.
:::

```{r ContCorrectionTable, echo=FALSE}
CCTable.caption <- "Some events and their probabilities for the die-rolling example, computed using the binomial distribution (exact) and the normal approximation using the continuity correction. The accuracy of the approximation is generally very good."

prob <- 1/6
n <- 100
mn <- n * prob
stdev <- sqrt( n * prob * (1 - prob) )

CCTable <- array( "",
                  dim = c(6, 5) )
colnames(CCTable) <- c("Event (binomial)",
                       "Prob (binomial)",
                       " ",  # Spacer
                       "Event (normal)",
                       "Prob (normal)")

CCTable[1, ] <- c("$\\Pr(X \\lt 10)$",
                  round(pbinom(9, prob = prob, size = n), 4),
                  "",
                  "$\\Pr(Y \\lt 9.5)$",
                  round( pnorm(10, mean = mn, sd = stdev), 4) )
CCTable[2, ] <- c("$\\Pr(X \\le 15)$",
                  round(pbinom(15, prob = prob, size = n), 4),
                  "",
                  "$\\Pr(Y \\lt 15.5)$",
                  round( pnorm(15.5, mean = mn, sd = stdev), 4) )

CCTable[3, ] <- c("$\\Pr(X \\gt 17)$",
                  round(1 - pbinom(17, prob = prob, size = n), 4),
                  "",
                  "$\\Pr(Y \\gt 17.5)$",
                  round( 1 - pnorm(17.5, mean = mn, sd = stdev), 4) )

CCTable[4, ] <- c("$\\Pr(X \\ge 21)$",
                  round(1 - pbinom(20, prob = prob, size = n), 4),
                  "",
                  "$\\Pr(Y \\gt 20.5)$",
                  round( 1 - pnorm(20.5, mean = mn, sd = stdev), 4) )

CCTable[5, ] <- c("$\\Pr(15 \\lt X \\le 17)$",
                  round(pbinom(17, prob = prob, size = n) - pbinom(16, prob = prob, size = n), 4),
                  "",
                  "$\\Pr(15.5 \\lt Y \\lt 16.5)$",
                  round( pnorm(16.5, mean = mn, sd = stdev) - pnorm(15.5, mean = mn, sd = stdev), 4) )

if( knitr::is_latex_output() ) {
CCTable[1, ] <- c("$\\Pr(X < 10)$",
                  round(pbinom(9, prob = prob, size = n), 4),
                  "",
                  "$\\Pr(Y < 9.5)$",
                  round( pnorm(10, mean = mn, sd = stdev), 4) )
CCTable[2, ] <- c("$\\Pr(X \\le 15)$",
                  round(pbinom(15, prob = prob, size = n), 4),
                  "",
                  "$\\Pr(Y < 15.5)$",
                  round( pnorm(15.5, mean = mn, sd = stdev), 4) )

CCTable[3, ] <- c("$\\Pr(X > 17)$",
                  round(1 - pbinom(17, prob = prob, size = n), 4),
                  "",
                  "$\\Pr(Y > 17.5)$",
                  round( 1 - pnorm(17.5, mean = mn, sd = stdev), 4) )

CCTable[4, ] <- c("$\\Pr(X \\ge 21)$",
                  round(1 - pbinom(20, prob = prob, size = n), 4),
                  "",
                  "$\\Pr(Y > 20.5)$",
                  round( 1 - pnorm(20.5, mean = mn, sd = stdev), 4) )

CCTable[5, ] <- c("$\\Pr(15 < X \\le 17)$",
                  round(pbinom(17, prob = prob, size = n) - pbinom(16, prob = prob, size = n), 4),
                  "",
                  "$\\Pr(15.5 < Y < 16.5)$",
                  round( pnorm(16.5, mean = mn, sd = stdev) - pnorm(15.5, mean = mn, sd = stdev), 4) )

  knitr::kable(CCTable,
               format = "latex",
               booktabs = TRUE,
               align = "r",
               longtable = FALSE,                
               escape = FALSE,
               linesep = c("", "", "\\addlinespace"),
               caption = CCTable.caption) %>%
    kable_styling(font_size = 10) %>%
    row_spec(0, bold = TRUE) 
}
if( knitr::is_html_output() ) {
CCTable[1, ] <- c("$\\Pr(X \\lt 10)$",
                  round(pbinom(9, prob = prob, size = n), 4),
                  "",
                  "$\\Pr(Y \\lt 9.5)$",
                  round( pnorm(10, mean = mn, sd = stdev), 4) )
CCTable[2, ] <- c("$\\Pr(X \\le 15)$",
                  round(pbinom(15, prob = prob, size = n), 4),
                  "",
                  "$\\Pr(Y \\lt 15.5)$",
                  round( pnorm(15.5, mean = mn, sd = stdev), 4) )

CCTable[3, ] <- c("$\\Pr(X \\gt 17)$",
                  round(1 - pbinom(17, prob = prob, size = n), 4),
                  "",
                  "$\\Pr(Y \\gt 17.5)$",
                  round( 1 - pnorm(17.5, mean = mn, sd = stdev), 4) )

CCTable[4, ] <- c("$\\Pr(X \\ge 21)$",
                  round(1 - pbinom(20, prob = prob, size = n), 4),
                  "",
                  "$\\Pr(Y \\gt 20.5)$",
                  round( 1 - pnorm(20.5, mean = mn, sd = stdev), 4) )

CCTable[5, ] <- c("$\\Pr(15 \\lt X \\le 17)$",
                  round(pbinom(17, prob = prob, size = n) - pbinom(16, prob = prob, size = n), 4),
                  "",
                  "$\\Pr(15.5 \\lt Y \\lt 16.5)$",
                  round( pnorm(16.5, mean = mn, sd = stdev) - pnorm(15.5, mean = mn, sd = stdev), 4) )

  knitr::kable(CCTable,
               escape = TRUE,
               format = "html",
               booktabs = TRUE,
               align = "r",
               longtable = FALSE,                
               caption = CCTable.caption) %>%
    row_spec(0, bold = TRUE) 
}
```





## Log normal???


## Exponential distribution {#ExponentialDistribution}

One shortcoming of using the normal distribution in modelling is that it is defined for all real values, but many quantities are not defined on the positive real numbers only.
This is not always a problem, especially when the observations are far from zero, such as heights.
However, many random variables have observations close to zero, and the data are often skewed to the right (positively skewed).

Many distributions can be used for modelling right skewed data defined on the positive real numbers; one is the exponential distribution.


:::{.definition #ExponentialDistribution name="Exponential distribution"}
If a random variable $X$ has the pdf
\[
   f_X(x) = \displaystyle \frac{1}{\beta}  \exp(-x/\beta)
\]
then $X$ has an *exponential distribution* with parameter $\beta > 0$. 
We write $X\sim\text{Exp}(\beta)$.
:::


The parameter $\lambda = 1/\beta$ (or $\theta = 1/\beta$) is often used in place of $\beta$, and is called the *rate* parameter.
Plots of the pdf for various exponential distributions are given in Fig. \@ref(fig:ExponentialDistributions)}.


```{r ExponentialDistributions, echo=FALSE, fig.align="center", fig.cap="Exponential distributions",fig.width=4,fig.height=3.5}
x <- seq(0, 12,
         length = 100)

lambda <- c(0.5, 1, 2)

for (i in (1:length(lambda))){
  lambdai <- lambda[i]
  if (i == 1) {
     plot(x = x,
          y = dexp(x,
                   rate = 1/lambda[i]),
          xlab = expression(italic(x)),
          ylab = "Probability function",
          lwd = 2,
          las = 1,
          ylim = c(0, 1.5),
          type = "l",
          main = "Exponential distributions")
  } else {
    lines(x = x,
          y = dexp(x,
                   rate = 1/lambda[i]),
          lty = i,
          lwd = 2)
  }
}

legend( "topright",
        lty = 1:3,
        lwd = 2,
        legend = c(bquote(lambda == 0.5),
                   bquote(lambda == 1),
                   bquote(lambda == 2) )
        )
```


The following are the basic properties of the exponential distribution.

:::{.theorem #ExponentialProperties name="Exponential distribution properties"}
If $X\sim\text{Exp}(\beta)$ then 

1. $\text{E}(X) = \beta$.
2. $\text{var}(X) = \beta^2$.
3. $M_X(t) = (1 - \beta t)^{-1}$ for $t < 1/\beta$.
:::

:::{.proof}
The proofs are left as an exercise.
:::


The parameter $\beta$ represents the *mean* of the exponential distribution.
The alternative parameter $\lambda = 1/\beta$ represents the mean *rate* at which events occur.
Like the [Poisson distribution](#PoissonDistribution), once the mean is defined the variance is defined.


::: {.softwareBox .software data-latex="{iconmonstr-laptop-4-240.png}"}
In **R**, the exponential distribution functions have the form `[dpqr]exp(rate)` where `rate`${}= \lambda$.
:::


:::{.example #ExpRainFall name="Exponential distributions"}
@allen1975stochastic use the exponential distribution to model rainfall.
WHAT SCALE? WHERE?
:::


:::{.example #ExprainFall2 name="Exponential distributions"}
@cox1966statistical give data collected by Fatt and Katz concerning the time intervals between successive nerve pulses along a nerve fibre.
There are 799 observations which we do not give here.
The mean time between pulses is $\beta = 0.2186$ seconds. 
An exponential distribution might be expected to model the data well.
This is indeed the case (Fig. \@ref(fig:ExpNerves)).
What proportion of time intervals can be expected to be longer than 1 second?

If $X$ is the time between successive nerve pulses (in seconds), then $X\sim \text{Exp}(\beta = 0.2186)$.
The solution will then be
\begin{align*}
   \Pr(X>1)
   &=  \int_1^\infty \frac{1}{0.2186}\exp(-x/0.2186)\, dx \\
   &= -\exp(-x/0.2186)\Big|_1^\infty \\
   &= (-0) + (\exp\{-1/0.2186\})\\
   &=  0.01031.
\end{align*}
There is about a 1% chance of a nerve pulse exceeding one second.
:::


```{r ExpNerves, echo=FALSE, fig.align="center", fig.width=4, fig.height=3.5, fig.cap="The time between successive nerve pulses. An exponential distribution fits well."}
NP <- read.csv("Data/NervePulses.csv")

hist(NP$WaitTime,
     xlab = "Time between pulses (seconds)",
     ylab = "Density",
     main = "Nerve pulse wait times",
     col = "white",
     freq = FALSE,
     ylim = c(0, 4),
     las = 1,
     breaks = seq(0, 1.4, by = 0.1) )

beta <- mean(NP$WaitTime)
xx <- seq(0, 1.4, 
          by = 0.01)

lines( x = xx,
       y = dexp(xx, rate = 1/beta),
       lwd = 2,
       col = plotColour)


```



The Poisson distribution and the exponential distribution are related.
If the number of events in a process follows a Poisson distribution, the space or time between two consecutive events has an exponential distribution (Theorem \@ref(thm:PoissonProcess)).
Hence the exponential distribution is used to describe the interval between consecutive randomly occurring events.


:::{.theorem #PoissonProcess name="Poisson process"}
Consider a Poisson process at rate $\lambda$ and suppose observation starts at an arbitrary time point. 
Then the time $T$ to the first event has an exponential distribution with mean $\text{E}(T) = 1/\lambda$; i.e.,
\[
   f_T(t) = \lambda e^{-\lambda t},\quad t > 0.
\]
:::

:::{.proof}
Let $t = 0$ be the arbitrary time at which observation of the Poisson process starts. 
Consider an interval $(0, t]$ for some fixed value $t > 0$. 
Now $T$ is the elapsed time after observations starts until the first event occurs.

Clearly, if the first event takes longer than $t$ to occur, then $T > t$ and the number of events in $(0, t]$ is zero.
Conversely, if the first event occurs at time $t$ or earlier then $T \le t$ and the number of events in $(0, t]$ is greater than zero.
Hence, the events $\{T > t\}$ and $\{N(t) = 0\}$ are equivalent where $N(t)$ is the number of events occurring in time $(0, t]$.
Therefore
\[
   \Pr(T > t) = \Pr(N(t) = 0).
\]
But $N(t) \sim \text{Pois}(\lambda)$ with mean $\text{E}(N(t)) = \lambda t$.
Hence
\begin{equation}
   \Pr(T > t) 
   = \frac{(\lambda t)^0 e^{-\lambda t}}{0!}
   = e^{-\lambda t}.
   (\#eq:ExpSurvivor)
\end{equation}
This shows that the df of $T$ is given by
\[
   F_T(t)
   = \Pr(T \le t) 
   = 1 - \Pr(T > t)
   = 1 - e^{-\lambda t}.
\]
Differentiating with respect to $t$ yields the pdf
\[
   f_T(t)
   = \frac{d}{dt}F_T(t)
   = \lambda e^{-\lambda t}
\]
which we recognise as the exponential distribution with mean $\beta = 1/\lambda$.
:::


Although the theorem refers to 'time', the variable of interest may be distance or any other continuous variable measuring the interval between events.


::: {.linkBox .link data-latex="{iconmonstr-link-1-240.png}"}
If events occur according to a Poisson distribution, then the "time" *between* the Poisson events can be modelled using an exponential distribution.
:::


:::{.example #ExpPoisson name="Relationship between exponential and Poisson distributions"}
A Poisson process occurs at the mean rate of 5 events per hour.

Let $N$ represent the number of events in one day and $T$ the time between consecutive events. 
Describe the distribution of the time between consecutive events and the distribution of the number of events in one day (24 hours).
We are given that events occur at the mean rate of $\lambda = 5$ events per hour. 
It follows that the mean time between consecutive events is $\beta = 1/\lambda = 0.2$ hours. 
Hence, the mean number of events in one day is $\mu = 24\times 5 = 120$.

Consequently, $N\sim\text{Pois}(\mu = 120)$ and  $X\sim\text{Exp}(\beta = 0.2)$ (or, equivalently, $X\sim\text{Exp}(\lambda = 5)$).
:::


An important feature of a Poisson process, and hence of the exponential distribution, is the *memoryless* or *Markov property*: the future of the process at any time point does not depend on the history of the process.
This property is captured in the following theorem.


:::{.theorem #MemorylessProperty name="Memoryless property"}
If $T \sim \text{Exp}(\lambda)$, then for $s > 0$ and $t > 0$, 
\[
    \Pr(T > s + t \mid T > s) = \Pr(T > t)
\]
:::

:::{.proof}
Using Definition \@ref(def:ConditionalProb),
\[
   \Pr(T > s + t \mid T > s) 
   = \frac{ \Pr( \{T > s + t\} \cap \{T > s\})} {\Pr(T > s)}
\]
But if $T > s + t$, then $T > s$. 
Consequently $\Pr( \{T > s + t \}\cap \{T > s \} ) = \Pr(T > s + t)$ and so
\begin{align*}
   \Pr(T > s + t \mid T > s )
   &= \frac{\Pr(T > s +t )}{\Pr(T > s)}\\
   &= \frac{e^{-\lambda(s + t)}}{e^{-\lambda s}}\\
   &= e^{-\lambda t}\\
   &= \Pr(T > t)
\end{align*}
where we have used \@ref(eq:ExpSurvivor).
:::

This theorem states that the probability that the time to the next event is greater than $t$ does not depend on the time $s$ back to the previous event.
This is called the *memoryless property* of the exponential distribution.


:::{.example #Memoryless name="Memoryless property of exponential distribution"}
Suppose the lifespan of component $A$ is modelled by an exponential distribution with mean 12 months.
Then, $T \sim \text{Exp}(\beta = 6)$.

The probability that component $A$ fails in less than 6 months is
\[
   \Pr(T < 6) = 1 - \exp(-6/12) = 0.3935.
\]

Now, suppose component $A$ has been in place for 12 months. 
The probability that it will fail in less than a further 6 months is
\[
   \Pr(T < 18 | T > 12) = \Pr(T < 6) = 1 - \exp(-6/12) = 0.3935
\]
by the memoryless property.
:::


Example \@ref(exm:Memoryless) highlights the notion that an exponential process is ageless: the risk of 'mortality' remains constant with age.
In other words, the probability of such an event occurring in the next small interval, whether it be the failure of a component or the occurrence of an accident, remains constant regardless of the age of the component or the length of time since the last accident. 
In this sense an exponential lifetime is different from a human lifetime or the lifetime of many man-made objects where the risk of 'death' in the next small interval increases with age.


## Gamma distribution {#GammaDistribution}

Once the mean of an exponential distribution is defined, the variance is defined.
More flexibility is sometimes needed, which is provided by the gamma distribution.


:::{.definition #GammaDistribution  name="Gamma distribution"}
If a random variable $X$ has the pdf
\[
   f_X(x) 
   = \frac{1}{\beta^\alpha \Gamma(\alpha)} x^{\alpha - 1} \exp(-x/\beta)
\]
then $X$ has a *gamma distribution*, where $\Gamma(\cdot)$ is the *gamma function* (see Sect. \@ref(def:GammaFunction)) and $\alpha, \beta > 0$.
We write $X \sim \text{Gam}(\alpha, \beta)$.
:::


The parameter $\alpha$ is called the *shape parameter* and $\beta$ is called the *scale parameter*.
Some texts use different notation for the shape and scale parameters.
In broad terms, the *shape* parameter dictates the general shape of the distribution; the *scale* parameter dictates how 'stretched out' the distribution is.


::: {.softwareBox .software data-latex="{iconmonstr-laptop-4-240.png}"}
In **R**, the gamma function $\Gamma(x)$ is evaluated using `gamma(x)`.

The gamma distribution functions have the form `[dpqr]gamma(shape, rate, scale = 1/rate)`, where `shape`${}= \alpha$ and `scale`${}= \beta$.
:::


Plots of the gamma pdf for various values of the parameters are given in Fig. \@ref(fig:GammaDistribution).


::: {.linkBox .link data-latex="{iconmonstr-link-1-240.png}"}
The exponential distribution is a special case of the gamma distribution with $\alpha = 1$.
This means that properties of the exponential distribution can be obtained by substituting $\alpha = 1$ into the formulae for the gamma distribution.
:::



```{r GammaDistribution, echo=FALSE, fig.align="center", fig.cap="The pdf of a gamma distribution for various values of $\\alpha$ and $\\beta$", fig.width=7, fig.height=7, out.width='80%'}

par( mfrow = c(2, 2))

alpha <- c(0.9, 1,
           2, 3)
beta <- c(0.5, 1, 2)

x <- seq(0, 10,
         length = 100)

for (i in (1:length(alpha))){
  alphai <- alpha[i]
  
  plot(x = x,
       y = dgamma(x,
                  shape = alphai,
                  scale = beta[1]),
       xlab = expression(italic(x)),
       ylab = "Prob. density",
       las = 1,
       lwd = 2,
       type = "l",
       main = bquote("Gamma distributions:" ~ alpha == .(alphai) ) ) 

  lines( x = x,
         y = dgamma(x,
                  shape = alphai,
                  scale = beta[2]),
         lty = 2,
         lwd = 2)
  lines( x = x,
         y = dgamma(x,
                  shape = alphai,
                  scale = beta[3]),
         lty = 3,
         lwd = 2)
  legend("topright",
         lwd = 2,
         lty = 1:3,
         bty = "n",
         col = plotColour1,
         legend = c(bquote( beta == .(beta[1])),
                    bquote( beta == .(beta[2])),
                    bquote( beta == .(beta[3]))))

}
```


Notice that
\begin{align}
   \int_0^\infty f_X(x)\,dx
   &= \int_0^\infty\frac{e^{-x/\beta}x^{\alpha - 1}}{\beta^\alpha\Gamma(\alpha)}\,dx\nonumber\\
   &= \frac{1}{\Gamma(\alpha)} \int_0^\infty e^{-y} y^{\alpha-1}\,dy  \quad \text{(on putting $y = x/\beta$)}\nonumber \\
   &= 1\quad \text{(because $\int_0^\infty e^{-y} y^{\alpha-1}\,dy = \Gamma(\alpha)$)}
   (\#eq:GammaStd)
\end{align}
as it must.


The following are the basic properties of the gamma distribution.


:::{.theorem #GammaProperties name="Gamma distribution properties"}
If $X\sim\text{Gam}(\alpha,\beta)$ then

1. $\text{E}(X) = \alpha\beta$.
2. $\text{var}(X) = \alpha\beta^2$.
3. $M_X(t) = (1-\beta t)^{-\alpha}$ for $t < 1/\beta$.
:::

:::{.proof}
\[
   \text{E}(X)
   = \int_0^{\infty}x f_X(x)\,dx 
   = \beta \frac{\Gamma(\alpha + 1)}{\Gamma(\alpha)}
     \underbrace{\int_0^{\infty}\frac{e^{-x/\beta} x^{(\alpha + 1) - 1}}{\beta^{\alpha + 1}\Gamma(\alpha + 1)} \, dx}_{= 1}
   = \alpha\beta.
\]
This result follows from using \@ref(eq:GammaStd) and Theorem \@ref(thm:GammaProperties).

\[
   \text{E}(X^2)
   = \int_0^{\infty}x^2  f_X(x) \, dx 
   = \beta^2\frac{\Gamma(\alpha + 2)}{\Gamma(\alpha)}
     \underbrace{\int_0^{\infty}\frac{e^{-x/\beta} x^{(\alpha + 2) - 1}}{\beta^{\alpha + 2}\Gamma(\alpha + 2)}\,dx}_{= 1}
   = \alpha(\alpha + 1)\beta^2
\]
where the result follows by writing $\Gamma(\alpha + 2) = (\alpha + 1)\alpha\Gamma(\alpha)$.
Hence
\[
   \text{var}(X)
   = \text{E}(X^2) - [\text{E}(X)]^2
   = \alpha(\alpha + 1)\beta^2 - (\alpha\beta)^2
   = \alpha\beta^2
\]
Also:
\begin{align*}
   M_X(t)
    = \text{E}(e^{Xt})
   &= \int_0^{\infty}e^{tx} \frac{e^{-x/\beta}x^{\alpha - 1}}{\beta^{\alpha} \Gamma(\alpha)} \, dx\\
   &= \int_0^{\infty} \frac{e^{-x(1 - \beta t)/\beta}x^{\alpha - 1}}{\beta^{\alpha}\Gamma(\alpha)} \, dx\\
   &= \int_0^{\infty} \frac{e^{-z}z^{\alpha - 1}}{\Gamma(\alpha)(1 - \beta t)^{\alpha - 1}} \, \frac{dz} {1 - \beta t}, 
      \text{putting $z = x(1 - \beta t)/\beta$}\\ 
   &= (1 - \beta t)^{-\alpha},
\end{align*}
since the integral remaining is 1.
:::

As usual the moments can be found by expanding $M_X(t)$ as a series.
That is,
\[
   M_X(t) = 1 \ + \ \alpha\beta t \ + \ \frac{\alpha(\alpha+1)\beta^2}{2!}  t^2 \ + \ \cdots
\]
from which
\begin{align*}
   E(X)   &= \text{coefficient of }t =\alpha\beta,\\
   E(Y^2) &= \text{coefficient of }t^2/2!=\alpha(\alpha+1)\beta^2,
\end{align*}
as found earlier.
 
As for the normal distribution, the distribution function of the gamma cannot, in general, be computed without using numerical integration, tables (although see Example \@ref(exm:ElectricalComponent)) or software.


:::{.example #Rainfall name="Rainfall"}
@climate:das:1955 used a (truncated) gamma distribution for modelling precipitation.
A similar approach is adopted by [@climate:wilks:1990].

WHERE???? WHAT SCALE???
:::


:::{.example #Rainfall2 name="Rainfall"}
@larsen1986introduction (Case Study 4.6.1) use the gamma distribution to model daily rainfall in Sydney, Australia using the parameter estimates $\alpha = 0.105$ and $\beta = 76.9$.
(Their example is based on @climate:das:1955.)
The comparison between the data and the model (Table \@ref(tab:GammaRain)) indicates a good agreement between the data and the theoretical distribution.
:::


```{r GammaRain, echo=FALSE}
RainTable <- array(dim = c(9, 7))

RainTable[1, ] <- c("0--5",   1631, 1639, "", "46--50",   18, 12)
RainTable[2, ] <- c("6--10",  115,   106, "", "51--60",   18, 20)
RainTable[3, ] <- c("11--15",  67,    62, "", "61--70",   13, 15)
RainTable[4, ] <- c("16--20",  42,    44, "", "71--80",   13, 12)
RainTable[5, ] <- c("21--25",  27,    32, "", "81--90",    8, 9)
RainTable[6, ] <- c("26--30",  26,    26, "", "91--100",   8, 7)
RainTable[7, ] <- c("31--35",  19,    21, "", "101--125", 16, 12)
RainTable[8, ] <- c("36--40",  14,    17, "", "126--150",  7, 7)
RainTable[9, ] <- c("41--45",  12,    14, "", "151--425", 14, 13)



if( knitr::is_latex_output() ) {
  knitr::kable(RainTable,
               format = "latex",
               booktabs = TRUE,
               align = c("c", "r", "r", "c", "c", "r", "r"),
               col.names = c("Rainfall (mm)",
                             "Observed",
                             "Modelled",
                             " ",
                             "Rainfall (mm)",
                             "Observed",
                             "Modelled"),
               longtable = FALSE,                
               escape = FALSE,
               caption = "The gamma distribution used to model Sydney daily rainfall") %>%
    kable_styling(font_size = 10) %>%
    row_spec(0, bold = TRUE) 
}
if( knitr::is_html_output() ) {
  knitr::kable(RainTable,
               escape = TRUE,
               format = "html",
               booktabs = TRUE,
               align = c("c", "r", "r", "c", "c", "r", "r"),
               col.names = c("Rainfall (mm)",
                             "Observed",
                             "Modelled",
                             " ",
                             "Rainfall (mm)",
                             "Observed",
                             "Modelled"),
               longtable = FALSE,                
               caption = "The gamma distribution used to model Sydney daily rainfall") %>%
    row_spec(0, bold = TRUE) 
}

```



:::{.example #ElectricalComponent name="Electrical components"}
The lifetime of an electrical component in hours, say $T$, can be well modelled by the distribution $\text{Gam}(2, 1)$.
What is the probability that a component will last for more than three hours?

From the information, $T\sim \text{Gam}(\alpha = 2, \beta = 1)$.
The required probability is therefore
\begin{align*}
   \Pr(T > 3)
   &= \int_3^\infty \frac{1}{1^2 \Gamma(2)}t^{2 - 1} \exp(-t/1)\,dt \\
   &= \int_3^\infty t \exp(-t)\,dt \\
\end{align*}
since $\Gamma(2) = 1! = 1$.
This expression can be integrated using integration by parts:
\begin{align*}
   \Pr(T > 3)
   &= \int_3^\infty t \exp(-t)\,dt \\
   &= \left\{ -t \exp(-t)\right\}\Big|_3^\infty - \int_3^\infty -\exp(-t)\, dt \\
   &= [ (0) - \{-3\exp(-3)\}] - \left\{ \exp(-t)\Big|_3^\infty\right\} \\
   &= 3\exp(-3) + \exp(-3)\\
   &= 0.1991
\end{align*}

Integration by parts is only possible since $\alpha$ is integer.
If $\alpha$ was, for example $\alpha = 2.5$, integration by parts would not be possible.

A more general approach is to use tables of the incomplete gamma function to evaluate the integral, numerical integration of software.
To use **R**:


```{r echo=TRUE}
# Integrate
integrate(dgamma, # The gamma distribution prob. fn
          lower = 3,
          upper = Inf,  # "Inf" means "infinity"
          shape = 2,
          scale = 1)

# Directly
1 - pgamma(3,
           shape = 2,
           scale = 1)
```

Using any method, the probability is about 20%.
:::


:::{.example #Gamma name="Gamma"}
If $X\sim \text{Gam}(\alpha, \beta)$, find the distribution of $Y = kX$ for some constant $k$.

One way to approach this question is to use moment generating functions.
Since $X$ has a gamma distribution, $M_X(t) = (1-\beta t)^{-\alpha}$.
Now,
\begin{align*}
   M_Y(t)
   &= \text{E}(\exp(tY)) \qquad\text{by definition of the mgf}\\
   &= \text{E}(\exp(t kX)) \qquad\text{since $X = kX$}\\
   &= \text{E}(\exp(s X)) \qquad\text{by letting $s = kt$}\\
   &= M_X(s) \qquad\text{by definition of the mgf}\\
   &= M_X(kt) \\
   &= (1 - \beta kt)^{-\alpha}.
\end{align*}
This is just the mgf for random variable $X$ with $k\beta$ in place of $\beta$, so the distribution of $Y$ is $\text{Gam}(\alpha, k\beta)$.
:::



## Beta distribution {#BetaDistribution}

Apart from the uniform distribution, all the continuous distributions considered have a range space on the positive real numbers.
However, some phenomena are constrained to a finite interval. 
The beta distribution is useful in these situations.


:::{.definition #BetaDistribution name="Beta distribution"}
A random variable $X$ with probability density function
\begin{equation*}
   f_X(x)
   = \frac{x ^ {m - 1}(1 - x)^{n - 1}}{B(m,n)}, \ 0 \leq x \leq 1, \ m>0, \ n>0
\end{equation*}
where
\begin{equation}
   B(m, n)
   = \int_0^1  x^{m - 1}(1 - x)^{n - 1} \, dx, \quad m > 0, \ n > 0
   = \frac{\Gamma(m) \Gamma(n)}{\Gamma(m + n)}
   (\#eq:BetaFunction)
\end{equation}
is said to have a *beta distribution* with parameters $m$, $n$.
We write $X \sim \text{Beta}(m, n)$.
:::


$B(m, n)$ defined by \@ref(eq:BetaFunction) is known as the *beta function* with parameters $m$ and $n$.
Since $\int_0^1 f_X(x)\,dx = 1$ then
\begin{equation}
   \int_0^1  \frac{x^{m - 1}(1 - x)^{n - 1}}{B(m, n)}\,dx = 1.
\end{equation}


::: {.softwareBox .software data-latex="{iconmonstr-laptop-4-240.png}"}
In **R**, the beta function is evaluated using `beta(a, b)`.

The beta distribution functions have the form `[dpqr]beta(shape1, shape2)`, where `shape1`$= n$ and `shape2`$= m$.
:::


Some properties of the beta function follow.


:::{.theorem #BetaFunctionProperties name="Beta function properties"}
The beta function \@ref(eq:BetaFunction) satisfies the following:

1. The beta function is symmetric in $m$ and $n$.
   That is, if $m$ and $n$ are interchanged, the function remains unaltered; i.e., $B(m, n) = B (n, m)$.
2. $B(1, 1) = 1$
3. $\displaystyle B\left(\frac{1}{2}, \frac{1}{2}\right) = \pi$
4. For positive constants $m$, $n$ (not necessarily integers) $\displaystyle B(m, n) = \frac{\Gamma(m) \Gamma(n)}{\Gamma(m + n)}$.
:::

:::{.proof}
To prove the first, put $z = 1 - x$ and hence $dz = -dx$ in \@ref(eq:BetaFunction). 
Then
\begin{align*}
   B(m, n)
   &= -\int_1^0(1 - z)^{m - 1}  z^{n - 1} \, dz\\
   &= \int_0^1 z^{n - 1}(1 - z)^{m - 1} \, dz\\
   &= B(n, m)
\end{align*}

For the second, put $x = \sin^2\theta$, and so $dx = 2\sin\theta\cos\theta\,d\theta$, in \@ref(eq:BetaFunction).
We have
\[
   B(m, n) = 2 \int_0^{\pi/2} \sin^{2m - 1}\theta \cos ^{2n - 1}\theta\,d\theta.
\]
So, for $m = n = \frac{1}{2}$,
\[
   B\left(\frac{1}{2}, \frac{1}{2}\right) = 2\int_0^{\pi/2} d\theta = \pi.
\]

For the third, note that $\Gamma(1/2) = \sqrt{\pi}$ from Theorem \@ref(thm:GammaProperties).
Further, since $\Gamma(1) = 0! = 1$, and the results follows.
:::


Typical graphs for the beta pdf are given below. 
When $m = n$, the distribution is symmetric about $x = \frac{1}{2}$.



::: {.linkBox .link data-latex="{iconmonstr-link-1-240.png}"}
When $m = n = 1$, the beta distribution becomes the uniform distribution on $(0, 1)$.
:::


```{r BetaDistributions, echo=FALSE, fig.align="center", fig.cap="Various beta-distribution PDFs", fig.width=7, fig.height=3.5, out.width='80%'}

par(mfrow = c(1, 2))

x <- seq(0, 1, 
         length = 100)

m <- c(0.5, 0.1, 4, 6)
n <- c(0.5, 2, 4, 2)


plot( x = x,
      y = dbeta(x = x,
                shape1 = m[1],
                shape2 = n[1]),
      xlab = expression(italic(x)),
      ylab = "Probability function",
      lwd = 2,
      type = "l",
      las = 1,
      ylim = c(0, 3),
      main = "Beta distributions")
lines( x = x,
       y = dbeta(x = x,
                 shape1 = m[2],
                 shape2 = n[2]),
       lwd = 2,
       lty = 2)
lines( x = x,
       y = dbeta(x = x,
                 shape1 = m[3],
                 shape2 = n[3]),
       lwd = 2,
       lty = 3)
lines( x = x,
       y = dbeta(x = x,
                 shape1 = m[4],
                 shape2 = n[4]),
       lwd = 2,
       lty = 4)

# Now have a panel just for the legend!
plot( x = c(1, 5),
      y = c(1, 5),
      axes = FALSE,
      xlab = "",
      ylab = "",
      type = "n")

legend("right",
       lty = 1:4,
       lwd = 2,
       bty = "n",
       legend = c( bquote(italic(m)== .(m[1])~and~italic(n)==.(n[1])),
                   bquote(italic(m)== .(m[2])~and~italic(n)==.(n[2])),
                   bquote(italic(m)== .(m[3])~and~italic(n)==.(n[3])),
                   bquote(italic(m)== .(m[4])~and~italic(n)==.(n[4])) )
)



```


Some basic properties of the beta distribution follow.


:::{.theorem #BetaDistributionProperties name="Beta distribution properties"}
If $X \sim \text{Beta}(m, n)$ then

1. $\text{E}(X) = m/(m + n)$.
2. $\text{var}(X) = \frac{mn}{(m + n)^2(m + n + 1)}$.
3. A mode occurs at $x = \frac{m - 1}{m + n - 2}$ for $m, n > 1$.
:::


:::{.proof}
Assume $X \sim \text{Beta}(m, n)$, then
\begin{align*}
   \mu_r'
   =\text{E}(X^r)
   &= \int_0^1  \frac{x^r  x^{m - 1}(1 - x)^{n - 1}} {B(m, n)}\,dx\\
   &= \frac{B(m + r, n)}{B(m, n)}  \int_0^1 \frac{x^{m + r - 1}(1 - x)^{n - 1}}{B(m + r, n)}\,dx\\
   &= \frac{\Gamma(m + r)\Gamma(n)}{\Gamma(m + r + n)}\frac{\Gamma(m + n)}{\Gamma(m)\Gamma(n)}\\
   &= \frac{\Gamma(m + r)\Gamma(m + n)}{\Gamma(m + r + n)\Gamma(m)}
\end{align*}
Putting $r = 1$ and $r = 2$ into the above expression, and using sthat $\text{var}(X) = \text{E}(X^2) - \text{E}(X)^2$, the mean and variance are $\text{E}(X) = m/(m + n)$ and $\text{var}(X) = \frac{mn}{(m + n)^2(m + n + 1)}$.

Any mode $\theta$ of the distribution for which $0 < \theta < 1$ will satisfy $f'_X(\theta) = 0$. 
From the definition we see that for $m, n>1$ and $0 < x < 1$,
\[
   f'_X(x) = (m - 1)x^{m - 2} (1 - x)^{n - 1} - (n - 1)x^{m - 1}(1 - x)^{n - 1}/B(m, n) = 0
\]
implying $(m - 1)(1 - x) = (n - 1)x$ which is satisfied by $x = (m - 1) / (m + n - 2)$.
:::


The mgf of the beta distribution cannot be written in terms of standard functions.
The distribution function of the beta must be evaluated numerically in general, except when $m$ and $n$ are integers as shown in the example below.


If $X\sim \text{Beta}(m, n)$, then $X$ is defined on $[0, 1]$.
Sometimes then, the beta distribution is used to model proportions, though not proportions out of a total number (when the [binomial distribution](#BinomialDistribution) is used).
Instead, the beta distribution is used in situations like the percentage cloud cover (and also see Example \@ref(exm:BetaTanks)).

For a random variable $Y$ defined on a different closed interval $[a, b]$, define $Y = X(b - a) + a$.


:::{.example #BetaTanks name="Beta distributions"}
The bulk storage tanks of a fuel retailed are filled each Monday.
The retailer has observed that over many weeks the proportion of the available fuel supply sold is well modelled by a beta distribution with $m = 4$ and $n = 2$.

If $X$ denotes the proportion of the total supply sold in a given week, the mean proportion of fuel sold each week is
\[
   \text{E}(X) = m/(m + n) = 4/6 = 2/3.
\]

What is the probability that at least 90% of the supply will sell in a given week?
Compute:
\begin{align*}
   \Pr(X > 0.9)
   &= \int_{0.9}^1\frac{\Gamma(4 + 2)}{\Gamma(4)\Gamma(2)}x^3(1 - x)dx\\
   &= 20\int_{0.9}^1 (x^3 - x^4))\,dx\\
   &= 20(0.004)\\
   &= 0.08
\end{align*}
It is unlikely that 90% of the supply will be sold in a given week.
:::


## Exercises

:::{.exercise #BetaRepara}
Write the beta distribution density function in terms of the mean and variance.
:::


:::{.exercise #SpeedSimUnif}
In a study modelling pedestrian road-crossing behaviour [@shaaban2017agent], the uniform distribution is used in the simulation to model the vehicle speeds.
The speeds have a minimum value of 30 km/h, and a maximum value of 72 km/h.

1. Using this model, compute the mean and standard deviation of vehicle speeds.
1. Plot the pdf and df.
1. If $X$ is the vehicle speed, compute $\Pr(X > 60)$, where 60km/h is the posted speed limit.
1. Compute $\Pr(X > 65\mid X > 60)$.
:::


:::{.exercise #SpeedSimNorm}
In a study modelling pedestrian road-crossing behaviour [@shaaban2017agent], the normal  distribution is used in the simulation to model the vehicle speeds.
The normal model has a mean of 48km/h, with a standard deviation^[Although the article calls it the 'variance'...] of 8.8km/h.

1. The article ignores all vehicles travelling slower than 30 km/h and faster than 72 km/h.
   Write the pdf for this truncated normal distribution.
1. Plot the pdf and df for this truncated normal distribution.
1. Compute the mean and variance for this truncated normal distribution.
1. Suppose a vehicle is caught speeding (i.e., exceeding 60km/h); what is the probability that the vehicle is exceeding 65 km/h?
:::


:::{.exercise #ConcreteDiffusion}
A study of the service life of concrete in various conditions [@liu2012stochastic] modelled the diffusion coefficients using a gamma distribution, with $\alpha = 27.05$ and $\beta = 1.42$ (units not given).

1. Plot the pdf and df.
1. Determine the mean and standard deviation of the chloride diffusion coefficients used in the simulation.
1. If $C$ represents the chloride diffusion coefficients, compute $\Pr(C > 30\mid C < 50)$.
:::


:::{.exercise #ConcreteSurfaceCholride}
A study of the service life of concrete in various conditions [@liu2012stochastic] used normal distributions to model the surface chloride concentrations.
In one model, the mean was set as $\mu = 2$kg/m^2^ and the standard deviation as $\sigma = 0.2$kg/m^3^.

1. Plot the pdf and df.
2. Let $Y$ be the surface chloride concentration.
   Compute $\Pr(Y > 2.4)$.
3. 80% of the time, surface chloride concentrations are below what value?
3. 15% of the time, surface chloride concentrations exceed what value?
:::


:::{.exercise #BetaHospital}
In a study modelling waiting times at a hospital [@khadem2008evaluating], patients are classified into one of three categories:

* Red: Critically ill or injured patients.
* Yellow: Moderately ill or
injured patients.
* Green: Minimally injured or
uninjured patients.

For 'Green' patients, the service time $S$ was modelled as $S = 4.5 + 11V$, where $V \sim \text{Beta}(0.287, 0.926)$.

1. Find the mean and standard deviation of the service times $S$.
1. Plot the pdf and df.
1. What proportion of patients have a service time exceeding 15 minutes?
1. The quickest 20% of patients are serviced within what time?
:::



:::{.exercise #ExpHospital}
In a study modelling waiting times at a hospital [@khadem2008evaluating], patients are classified into one of three categories:

* Red: Critically ill or injured patients.
* Yellow: Moderately ill or
injured patients.
* Green: Minimally injured or
uninjured patients.

The time (in minutes) spent in the reception are for 'Yellow' patients, say $T$, is modelled as $T = 0.5 + W$, where $W\sim \text{Exp}(16.5)$.

1. Find the mean and standard deviation of the waiting times $T$.
1. Plot the pdf and df.
1. What proportion of patients waits more than 20 minutes, if they have already been waiting for 10 minutes?
1. How long to the slowest 10% of patients need to wait?
:::


:::{.exercise #NormalHospital}
In a study modelling waiting times at a hospital [@khadem2008evaluating], patients are classified into one of three categories:

* Red: Critically ill or injured patients.
* Yellow: Moderately ill or
injured patients.
* Green: Minimally injured or
uninjured patients.

The time (in minutes) spent in the reception are for 'Green' patients, say $T$, is modelled as a normal distribution with mean $\mu = 45.4$ min and standard deviation $\sigma = 23.4$ min.


1. Find the mean and standard deviation of the waiting times $T$.
1. Plot the pdf and df.
1. What proportion of 'Green' patients wait longer than an hour?
1. How long to the slowest 10% of patients need to wait?
:::


:::{.exercise #GammaRain}
In a study of rainfall [@watterson2003simulated], rainfall *on wet days* in SE Australia (37^$\circ$^S; 146^$\circ$^E) was simulated using a gamma distribution with $\alpha = 0.62$ and $\beta = 7.1$mm.

1. Rainfall below $0.0017$mm cannot be recorded by the equipment used.
   What proportion of days does this represent?
1. Plot the pdf and df.
1. What is the probability that more than 3mm falls *on a wet day*?
1. The proportion of wet days is $0.43$.
   What proportion *of all days* receive more than 3mm?
:::

:::{.exercise #DisaggregationDuration}
In a study of rainfall disaggregation [@connolly1998daily] (extracting small-scale rainfall features from large-scale measurements), the duration of non-overlapping rainfall events per day at Katherine was modelled using a Gamma distribution with $\alpha = 2$, and $\beta = 0.04$ in summer and $\beta = 0.03$ in winter.

Denote the number of rainfall events in summer as $S$, and in winter as $W$.

1. Plot the rainfall event duration distributions, and compare summer and winter.
1. What is the probability of more than 3 rainfall events per day in winter? 
1. What is the probability of more than 3 rainfall events per day in summer? 
1. Describe what is meant by the statement $\Pr(S > 3 \mid S > 1)$, and compute the probability.
1. Describe what is meant by the statement $\Pr(W > 2 \mid W > 1)$, and compute the probability.
::: 



:::{.exercise #DisaggregationDuration}
In a study of rainfall disaggregation [@connolly1998daily] (extracting small-scale rainfall features from large-scale measurements), the starting time of the first rainfall event at Katherine each day (scaled from 0 to 1) was modelled using a beta  distribution with parameters $(1.16, 1.50)$ in summer and $(0.44, 0.56)$ in winter.

Denote the number of rainfall events in summer as $S$, and in winter as $W$.

1. Plot the two starting-time distributions, and compare summer and winter.
1. Compute the mean and standard deviation of starting *times* for both seasons.
1. What is the probability that the first rain event in winter is after 6am? 
1. What is the probability that the first rain event in summer is after 6am?  
1. Describe what is meant by the statement $\Pr(S > 3 \mid S > 1)$, and compute the probability.
1. Describe what is meant by the statement $\Pr(W > 2 \mid W > 1)$, and compute the probability.
::: 


:::{.exercise #Diarrhoea}
A study of the impact of diarrhoea [@schmidt2009simulation] used a gamma distribution to model the duration of symptoms.
In Guatemala, duration was modelled using a gamma distribution with $\alpha = 1.11$ and $\beta = 3.39$ days.
Let $X$ refer to the duration of symptoms.

1. Compute the mean and standard deviation of the duration.
1. The value of $\alpha$ is close to one.
   Plot the pdf of the gamma distribution and the near-equivalent exponential distribution, and comment.
1. Compute $\Pr(X > 4)$ using both the gamma and exponential distributions, and comment.
1. The patients in the 5% with symptoms the longest had symptoms for how long?
   Again, compare both models, and comment.
:::



:::{.exercise #NormalLeaveTime}
In a study of office occupancy at a university [@luo2017performance], the leaving-times of professors were modelled as a normal distribution, with a mean leaving time of 6pm, with a standard deviation of 1.5h.

1. Using this model, what proportion of professors leave before 5pm?
1. Suppose a professor is still at work at 5pm; what is the probability that the professor leaves after 7pm?
1. The latest-leaving 15% of professors leave after what time?
:::


:::{.exercise #BetaClay}
The percentage of clay content in soil has been modelled using a beta distribution.
In one study [@haskett1995use], two counties in Iowa were modelled with beta-distributions: County A with parameters $m = 11.52$ and $n = 4.75$, and County B with parameters $m = 3.85$ and $n = 3.65$. 

1. Plot the two distributions, and comment (in context).
1. Compute the mean and standard deviation of each county.
1. What percentage of soil samples exceed 50% clay in the two counties?
   Comment.
:::


:::{.exercise}
A study of water uptake in plants in grasssland and savanna ecosystems [@nippert2015challenging] used a beta distribution to model the distribution of root depth $D$.
The MRD (maximum root density) was 70cm, so the beta distribution used was defined over the interval $[0, 70]$.
For one simulation, the beta-distribution parameters were $m = 1$ and $n = 5$.

1. Plot the pdf, and explain what it means in this context.
2. Determine the probability that the root depth was deeper than 50cm.
3. Determine the root depth of the plants with the deepest 20% of roots.
:::
