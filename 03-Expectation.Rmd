# Mathematical expectation {#Expectation}

::: {.objectivesBox .objectives data-latex="{iconmonstr-target-4-240.png}"}
Upon completion of this module students should be able to:

* understand the concept and definition of mathematical expectation
* compute the expectations of a random variable, functions of a random variable and linear functions of a random variable
* know how to compute the variance and other higher moments of a random variable
* derive the moment generating function of a random variable and linear functions of a random variable
* find the moments of a random variable from the moment generating function
* state and make use of Tchebysheff's inequality
:::


## The expected value {#ExpectedValue}

In discussing random variables, it is important to be able to identify a typical (or 'average') value of a random variable. 
To do this, we need to understand *expectation*.

The expectation is a mathematical expression of the *mean* of the probability distribution of the rv. 
Note that the expectation is a mathematical operator. 
The definition looks different in detail for discrete and continuous random variables, but the intention is the same.


:::{.definition #Expectation name="Expectation"}
The *expectation* or *expected value* or *mean* of a random variable $X$ is defined as
\[
   \text{E}(X) = 
   \begin{cases}
        \displaystyle
        \sum_{x\in R_X} x p_X(x) & \text{if $X$ is discrete with pf $p_X(x)$}\\[10pt]
        \displaystyle
        \int_{-\infty}^\infty x f_X(x) & \text{if $X$ is continuous with pdf $f_X(x)$}
        \end{cases}
\]
:::

Effectively $\text{E}(X)$ is a weighted average of the points in $R_X$, the weights being the probabilities in the discrete case and probability densities in the continuous case.

Typically the expectation is denoted by $\mu$, or $\mu_X$ if there is a need to distinguish between rvs.

:::{.example #ExpectationContinuous name="Expectation for continuous variables"}
Consider a continuous random variable $X$ with pdf
\[
   f_X(x) = \begin{cases}
               x/4 & \text{for $1<x<3$}\\
               0 & \text{elsewhere}.
            \end{cases}
\]
The expected value of $X$ is,
by definition,
\begin{align*}
   \text{E}(X)
   &= \int_{-\infty}^\infty x f_X(x) \, dx \\
   &= \int_1^3 x(x/4)\, dx\\
   &= \frac{1}{12} x^3\Big|_1^3 = 13/6.
\end{align*}
The expected value of $X$ is $13/6$.
:::


:::{.example #ExpectationDiscrete name="Expectation for discrete variables"}
Consider the discrete random variable $U$ with probability function
\[
   p_U(u) = \begin{cases}
               (u^2+1)/5 & \text{for $u = -1, 0, 1$}\\
               0 & \text{elsewhere}.
           \end{cases}
\]
The expected value of $U$ is, by definition,
\begin{align*}
   \text{E}(U)
   &= \sum_{u=-1, 0, 1} up_U(u) \\
   &= \left( -1 \times \frac{(-1)^2 +1}{5} \right ) +
       \left( 0 \times \frac{(0)^2 +1}{5} \right ) +
       \left( 1 \times \frac{(1)^2 +1}{5} \right ) \\
   &= -2/15 + 0 + 2/15 = 0.
\end{align*}
The expected value of $U$ is $0$.
:::


:::{.example #ExpectationCoinOnce name="Expectation for a coin toss"}
Consider tossing a coin *once* and counting the number of tails.
Let this random variable be $T$.
The probability function is
\[
   p_T(t) = \begin{cases}
               0.5 & \text{for $x=0$ or $x=1$}\\
               0   & \text{otherwise}
            \end{cases}
\]
The expected value of $T$ is, by definition
\begin{align*}
   \text{E}(T) &= \sum_{i=1}^2 t p_T(t)\\
         &= \Pr{T=0}\times 0 + \Pr{T=1}\times 1\\
         &= (0.5 \times 0) + (0.5 \times 1) =0.5.
\end{align*}
Note that $0.5$ tails can never actually be observed in practice.
But it would be silly to round up (or down) and say that the expected number of tails in one toss of a coin is one (or zero).
The expected value of $0.5$ simply means that over a large number of repeats of this experiment, we expect a tail to occur in half of those repeats.
:::


### Expectation of a function of a random variable {#ExpectationFunction}

Let $X$ be a discrete random variable with a probability function $p_X(x)$, or a continuous random variable with pdf $f_X(x)$.
Then if $g(X)$ is a real-valued function of $X$, the expected value of $g(X)$ is defined as follows.

:::{.definition #ExpectationFunction name="Expectation for function of a random variable"}
\[
   \text{E}(g(X)) = \begin{cases}
              \displaystyle
               \sum_{x\in R_X} g(x) p_X(x) & \text{if $X$ is discrete with pf $p_X(x)$}\\[10pt]
               \displaystyle
               \int_{-\infty}^\infty g(x) f_X(x)\,dx & \text{if $X$ is continuous with pdf $f_X(x)$}
              \end{cases}
\]
:::

If $Y = g(X)$ we would naturally write $\mu_Y = \text{E}(Y) = \text{E}(g(X))$.

Before considering a particular function of interest we prove an important property of the expectation operator.

:::{.theorem}
For any rv $X$ and constants $a$ and~$b$.
\[
   \text{E}(aX + b) = a\text{E}(X) + b
\]
:::

:::{.proof}
We assume $X$ is a discrete rv with probability function $p_X(x)$.
(An analogous proof applies in the continuous case by replacing the probability function with a pdf and summations with integrals.)
Then by Definition~\ref{DF:3.2} with $g(X) = aX + b$,
\[
   \text{E}(aX + b) = \sum_x (ax + b) p_X(x) = a\sum_x p_X(x) + \sum_x b p_X(x) = a\text{E}(X) + b,
\]
where we have used the fact that $\sum_x p_X(x) = 1$.
:::


:::{.example #ExpectationFunction name="Expectation of a function of a random variable"}
Consider the random variable $Y = 2x$ where $X$ is defined in Example~\ref{EG:expect:contX}.
Using Theorem~\ref{TM:3.1} with $a = 2$ and $b = 0$, the value of $\text{E}(Y)$ can be written
\[
      \text{E}(2X) = 2\text{E}(X) = 2 \times 13/6 = 13/3.
\]
:::


## The variance and standard deviation {#Variance}

An important function of a random variable gives rise to the variance of a random variable (or, if you like, of the distribution of the rv).

The variance is a measure of how spread out the values of a random variable are.
A small variance means the observations are nearly the same; a large variance means they are quite different.
The variance of a random variable $X$ can be defined using the expectation operator.

:::{.definition #Variance name="Variance"}
The *variance* of a rv $X$ (or of the distribution of $X$)
\[
   \text{var}(X)  = \text{E}((X - \mu)^2)
\]
where $\mu = \text{E}(X)$.
:::

The variance of $X$ is commonly denoted by $\sigma^2$, or $\sigma^2_X$ if it's necessary to distinguish amongst variables.

The unit of measurement for variance is the original ($x$) unit *squared*. 
It is often more meaningful to describe the 'spread' in terms of the original units by taking the square root of the variance.

:::{.definition #StandardDeviation name="Standard deviation"}
The *standard deviation* of a rv $X$ is defined as the *positive* square root of the variance (and is thus denoted by $\sigma$); i.e.,
\[
   \text{sd}(X) = +\sqrt{\text{var}(X)}
\]
:::

:::{.example #VarianceDice name="Variance for a die toss"}
Suppose a fair die is tossed and $X$ denotes the number of points showing. 
Then $\Pr(X = x) =  1/6$ for $x = 1, 2, 3, 4, 5, 6$ and $\mu = \text{E}(X) = \sum x\Pr{X=x} =(1+2+3+4+5+6)/6 = 7/2$.

The variance of $X$ is then
\begin{align*}
   \sigma^2 
   &= \text{var}(X) = \sum (X-\mu)^2 \Pr{X=x}\\
   &= \frac{1}{6}\left[ \left(1-\frac{7}{2}\right)^2 + \left(2-\frac{7}{2}\right)^2 +\dots+\left(6-\frac{7}{2}\right)^2\right]\\
   &=\frac{70}{24}.
\end{align*}

The standard deviation is then $\sigma = \sqrt{\frac{70}{24}} = 1.71$.
:::


An important result is known as the  *computational formula for variance*.

:::{.theorem #VarianceComputational name="Computational formula for variance"}
For any rv $X$,
\[
   \text{var}(X) = \text{E}(X^2) - [\text{E}(X)]^2
\]
:::

:::{.proof}
Let $\text{E}(X) = \mu$, then
\begin{align*}
\text{var}(X)&=\text{E}((X-\mu)^2)\\
   &= \text{E}( X^2 - 2X\mu + \mu^2) \\
   &= \text{E}(X^2) - \text{E}(2X\mu) + \text{E}(\mu^2)\\
   &= \text{E}(X^2) - 2\mu\text{E}(X) + \mu^2\\
   &= \text{E}(X^2) - 2\mu^2 + \mu^2 \\
   &= \text{E}(X^2) - \mu^2 \\
   &= \text{E}(X^2) - (\text{E}(X))^2.
\end{align*}
:::
This formula is often easier to use to compute $\text{var}(X)$ than using the definition.


:::{.example #VarianceComputational name="Variance using computational formula"}
Consider the continuous random variable $X$ with pdf
\[
   f_X(x) = \begin{cases}
            -x^2 + 2x - 1/6 & \text{for $0<x<2$}\\
            0 & \text{elsewhere}
            \end{cases}
\]
The variance of $X$ can be computed in two ways: directly using $\text{var}(X) = \text{E}((X-\mu)^2)$ or using the computational formula.

The expected value of $X$ is
\begin{align*}
   \text{E}(X)
   &= \int_0^2 x(-x^2 + 2x - 1/6)\, dx\\
   &= -\frac{x^4}{4} + \frac{2x^3}{3} - \frac{x^2}{12}\Big|_0^2 = 1.
\end{align*}
To use the computational formula,
\begin{align*}
   \text{E}(X^2)
   &= \int_0^2 x^2(-x^2 + 2x - 1/6)\, dx \\
   &= -\frac{x^5}{5} + \frac{x^4}{2} - \frac{x^3}{18}\Big|_0^2 = \frac{52}{45},
\end{align*}
and so
\[
   \text{var}(X) = \text{E}(X^2) - (\text{E}(X))^2 = 7/45.
\]
Using the definition,
\begin{align*}
   \text{var}(X)
   &= \text{E}((X-\text{E}(X))^2) \\
   &= \text{E}((X-1)^2)\\
   &= \int_0^2 (x-1)^2(-x^2+2x-1/6)^2\,dx \\
   &= \int_0^2 -x^4 + 4x^3 - \frac{31}{6}x^2 + \frac{7}{3}x - \frac{1}{6}\, dx\\
   &= -\frac{x^5}{5} + x^4 - \frac{31}{18}x^3 + \frac{7}{6}x^2 - \frac{1}{6}x\Big|_0^2 = 7/45.
\end{align*}

Both methods give the same answer of course,
and both methods require initial computation of $\text{E}(X)$.
The computational formula requires less work.
:::

NOTES ABOUT USE IN COMUTING AMD ROUND OFF

The variance represents the mean squared distance of the values of the rv from the mean.

The variance can never be negative. 
It is only ever zero in the uninteresting case where all the values of the random variable are identical (that is, there *is* no variation).

SOME REPETAED:

The units of the variance are the square of the units of $X$.
That is, if $X$ is measured in metres the variance of $X$ is in $\text{metres}^2$.
Because of this, the variance is less popular than the standard deviation, which has the same units as the rv, in describing spread numerically.
In theoretical work however, because of the square root function, variance is easier to work with than standard deviation and you will therefore find it rather than standard deviation featuring in many results throughout the study of statistics.




## Features of distributions {#DistributionFeatures}

Instead of describing the distribution of a rv in terms of a probability function of pdf or df, it is often sufficient to describe a distribution in terms of certain quantities that can be obtained from the distribution function.

Two general features of a distribution are its

* location or centre; and
* dispersion or spread.
  

### Measures of location {#FeaturesLocation}

The most important measure of location is the mean $\mu = \text{E}(X)$, which we have already defined.
In physical terms this measure is the *balance point* of the distribution in the sense that the mean deviation of the rv from $\mu$ is zero; i.e., $\text{E}(X - \mu) = 0$. (Why is this?)

The mean is not always a suitable measure of centre since the balance point can be unduly affected by distributions that have a relatively higher concentration of probability at one end of the distribution than the other.
Other measure of centre may then be more suitable.
The mode and median are two commonly-used alternatives.

USEFUL ANYWAY: CLT

:::{.definition #Mode name="Mode"}
The *mode(s)* is/are the value(s) $x\in R_X$ for the discrete case at which $p_X(x)$ attains its maximum or the value(s) of $x$ for the continuous case at which $f_X(x)$ attains its maximum.
:::

:::{.definition #Median name="Median"}
The *median* of a random variable $X$ (or of a distribution) is a number $\nu$ such that
\begin{equation}
   \Pr{X\leq \nu }\geq \frac 12\text{ and }\Pr{X \geq \nu }\geq \frac 12.
\end{equation}
:::

For $X$ discrete, the median is not necessarily unique.


:::{.example #ModeDiscrete name="Mode for discrete distributions"}
If $X$ is a discrete random variable with probability function BELOW.
We see this distribution is bimodal with modes at 3 and 4.

The median, $\nu$, is any value of $X$ in the range $3 < x < 4$ since in this range $\Pr{X \le x} = 0.5$ and $\Pr{X \ge x} = 0.5)$. 
That is the median is not unique.
The probability function and its df are shown in Fig. \@ref(fig:DiscreteMedian)}. NEED TO SHOW WHERE MEDIAN IS
:::

$x$ | 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7
$\Pr(X = x)$ | 1/128 | 7/128 | 21/128 | 35/128 | 35/128 | 21/128 | 7/128 | 1/128



```{r DiscreteMedian, echo=FALSE, fig.cap="CAPTION", fig.align="center"}
par( mfrow = c(1, 2))

x <- 0:7
fx <- c(1, 7, 21, 35, 35, 21, 7, 1) / 128

plot(x = x,
     y = fx,
     las = 1,
     type = "h",
     col = "grey",
     xlab = "X",
     ylab = "PX",
     main = "Probability mass function")
points( x = x,
        y = fx,
        pch = 19)




plot( x = x,
      y = cumsum(fx),
      las = 1,
      xlim = c(-2, 9),
      xlab = "x",
      ylab = "FX",
      main = "Distribution function",
      type = "n")
abline( h = c(0, 1),
        col = "grey")
points( x = x,
        y = cumsum(fx),
        pch = 19)
points( x = x[2: length(x)],
        y = cumsum(fx)[1:(length(x) - 1)],
        pch = 1)

for (i in (1 : (length(x) - 1)) ){
   lines( x = x[i:(i + 1)],
          y = c(cumsum(fx)[i],
                cumsum(fx)[i]),
          lwd = 2)
}

# Now add edge cases
lines( x = c(-1, 0),
       y = c(0, 0),
      lwd = 2)
lines(x = c(-2, -1),
      y = c(0, 0),
      lty = 2,
      lwd = 2)
points(0, 0, 
       pch = 1)
lines(x = c(7, 8),
      y = c(1, 1),
      lwd = 2)
lines(x = c(8, 9),
      y = c(1, 1),
      lty = 2,
      lwd = 2)

```


:::{.example #ExpectationContinuous name="Expectation for continuous variables"}
Let $X$ be a discrete random variable with probability function (BELOW).

The mode of this distribution is 3.

The median is $\nu = 3$ since this is the only value of $X$ that will satisfy Definition \@ref(def:Median).
The probability function and it's df are shown in  Fig. \@ref(fig:DiscreteMedian2).
:::

$x$ | 0 | 1 |  2 | 3 | 4 | 5 | 6
$\Pr(X = x)$ | 1/64 | 6/64 | 15/64 | 20/64 | 15/64 | 6/64 | 1/64


```{r DiscreteMedian2, echo=FALSE, fig.align="CAPTION", fig.align="center"}
par( mfrow = c(1, 2))

x <- 0:6
fx <- c(1, 6, 15, 20, 15, 6, 1) / 64

plot(x = x,
     y = fx,
     las = 1,
     type = "h",
     col = "grey",
     xlab = "X",
     ylab = "PX",
     main = "Probability mass function")
points( x = x,
        y = fx,
        pch = 19)




plot( x = x,
      y = cumsum(fx),
      las = 1,
      xlim = c(-2, 8),
      xlab = "x",
      ylab = "FX",
      main = "Distribution function CHECK!!",
      type = "n")
abline( h = c(0, 1),
        col = "grey")
points( x = x,
        y = cumsum(fx),
        pch = 19)
points( x = x[2: length(x)],
        y = cumsum(fx)[1:(length(x) - 1)],
        pch = 1)

for (i in (1 : (length(x) - 1)) ){
   lines( x = x[i:(i + 1)],
          y = c(cumsum(fx)[i],
                cumsum(fx)[i]),
          lwd = 2)
}

# Now add edge cases
lines( x = c(-1, 0),
       y = c(0, 0),
      lwd = 2)
lines(x = c(-2, -1),
      y = c(0, 0),
      lty = 2,
      lwd = 2)
points(0, 0, 
       pch = 1)
lines(x = c(7, 8),
      y = c(1, 1),
      lwd = 2)
lines(x = c(8, 9),
      y = c(1, 1),
      lty = 2,
      lwd = 2)

```


## Measures of dispersion (OR VARIATION?)  {#Dispersion}

If most of the probability lies near the mean, the dispersion will be small; if the probability is spread out over a considerable range the dispersion will be large.
We want a measure of the 'spread' irrespective of the location or centre.

We have already seen variance and standard deviation are suitable measures of dispersion.
Another alternative is the following.

:::{.definition #MAD name="Mean absolute diffeence (OR: deviation??)"}
The *mean absolute difference* (MAD) is defined as
\[
      \text{E}(|X - \mu|) 
      = 
      \begin{cases} 
         \sum_x |x-\mu| p_X(x)&\text{for discrete $X$}\\
         \int_{-\infty}^\infty |x-\mu| f_X(x)\,dx&\text{for continuous $X$}
      \end{cases}
\]
:::
Like the standard deviation, the MAD is difficult to use in theoretical work.

The most-used measure of location is the mean while for dispersion it is the variance (or standard deviation).

:::{.example #VarianceDie name="Variance of die toss"}
Consider the fair die described in Example~\ref{EG:3:die}.

Then $\mu = \text{E}(X) = 7/2$ and thus
\begin{align*}
   \sigma^2 
   &= \text{var}(X) \\
   &=\sum(x-\mu)^2 \Pr{X=x}\\
   &= \frac{1}{6}\left[ \left(1-\frac{7}{2}\right)^2+ \left(2-\frac{7}{2}\right)^2         +\dots+\left(6-\frac{7}{2}\right)^2\right]\\
   &=\frac{70}{24}.
\end{align*}
The standard deviation is then given by $\sigma = \sqrt{\frac{70}{24}} = 1.71$.

The MAD of $X$ is
\begin{align*}
   \text{E}(|X-\mu|) &=\sum |x-\mu| \Pr{X=x}\\
   &=\frac{1}{6}\left[ \left|1-\frac{7}{2}\right| + \left|2-\frac{7}{2}\right|+\dots+\left|6-\frac{7}{2}\right|\right]\\
   &=1.5
\end{align*}
:::


## Symmetry {#Symmetry}

:::{.definition #Symmetry name="Symmetry"}
The distribution of $X$ is said to be *symmetric* if for all $x\in R_X$,
\begin{align*}
   \Pr{X = \mu + x}
   &=\Pr{X=\mu -x}\text{ for all $x\in R_X$ in the discrete case}\\
   f_X(\mu +x)&=f_X(\mu -x)\text{ for all $x$ in the continuous case}
\end{align*}
:::

It can easily be seen that for a symmetric distribution the mean is also a median of the distribution.



## Higher moments {#HigherMoments}

The idea of a mean and variance are generalised in the following definitions.

:::{.definition #RawMoments name="Raw moments"}
The *$r$th moment about the origin*, or *$r$th raw moment*, of a random variable $X$ is defined as
\[
   \mu'_r = \text{E}(X^r) =
      \begin{cases}
      \displaystyle \sum_X x^r p_X(x) & \text{for $X$ discrete with pf $p_X(x)$} \\[10pt]
      \displaystyle \int_{-\infty}^\infty x^r f_X(x) & \text{for $X$ continuous with pdf $f_X(x)$}
      \end{cases}
\]
where $r$ is a positive integer.
:::


:::{.definition #CentralMoments name="Central moments"}
The *$r$th central moment*, or *$r$th moment about the mean*, is defined as
\[
   \mu_r = \text{E}((X-\mu)^r) =
   \begin{cases}
   \displaystyle \sum_x (x-\mu)^r p_X(x) & \text{for $X$ discrete with pf $p_x(x)$}\\[10pt]
   \displaystyle \int_{-\infty}^\infty (x-\mu)^r f_X(x) & \text{for $X$ continuous with pdf $f_X(x)$}
   \end{cases}
\]
where $r$ is a positive integer.
:::

From these definitions we see the mean $\mu'_1 = \mu$ is the *first moment about the origin*, or the *first raw moment* and the variance $\mu_2 = \sigma^2$ is the *second moment about the mean* or the *second central moment*.
Higher moments also exist that describe other features of a rv.

It can be shown that for a symmetric distribution the odd central moments are zero (EASY TO SHOW IT HERE??).
This suggests that odd central moments can be used to measure the *asymmetry* of a distribution.
It is convenient to use, instead of central moments themselves, expressions that are unaffected by a linear transformation of the type $Y = AX + b$.
Now the ratio $(\mu_r)^p / (\mu_p)^r$ is such an expression and the simplest form of it is $\mu^2_3/\mu_2^3$.

:::{.definition #Skewness name="Skewness"}
The *skewness* of a distribution is defined as
\begin{equation*}
     \gamma_1 = \frac{\mu_3}{\mu_2^{3/2}}
\end{equation*}
:::


If $\gamma_1 >0 \ (<0)$ we say the distribution is positively (negatively) skewed, and it is 'drawn out' in the positive (negative) direction.

Another ratio gives a measure of *kurtosis* (or peakedness).

:::{.definition #Kurtosis name="Kurtosis"}
The *kurtosis* of a distribution is defined as
\begin{equation*}
     \gamma_2 = \frac{\mu_4}{\mu^2_2} - 3
\end{equation*}
:::


%     The interpretation of 'peakedness' is somewhat vague.

:::{.example #SkewWind name="Uses of skewness and kurtosis"}
Monypenny and Middleton~\cite{BIB:Monypenny:winds,BIB:Monypenny:gust} use the skewness and kurtosis to analyse wind gusts at Sydney airport.
:::

:::{.example #SkewPricing name="Uses of skewness and kurtosis"}
Galagedera, Henry and Silvapulle~\cite{BIB:Galagedera:stocks} used higher moments in a capital analysis pricing model for Australian stock returns.
:::

:::{.example #SkewDiscrete name="Skewness for discrete distributions"}
Consider the discrete random variable $U$ from Example~\ref{EG:expect:discreteU}.
The raw moments are
\begin{align*}
   \mu'_r = \text{E}(U^r)
   &= \sum_{u=-1, 0, 1} u^r \frac{u^2+1}{5} \\
   &= (-1)^r \frac{ (-1)^2 +1}{5} +
       (0)^r \frac{ (0)^2 +1}{5} +
       (1)^r \frac{ (1)^2 +1}{5} \\
   &= \frac{2(-1)^r}{5} + 0 + \frac{2}{5} \\
   &= \frac{2}{5}[ (-1)^r + 1]
\end{align*}
for the $r$th raw moment.
Then,
\begin{align*}
   \text{E}(X) &= \mu'_1 = \frac{2}{5}[ (-1)^1 + 1 ] = 0,\\
   \text{E}(X^2) &= \mu'_2 = \frac{2}{5}[ (-1)^2 + 1 ] = 4/5,
\end{align*}
so that $\text{var}(X) = \text{E}(X^2) - \text{E}(X)^2 = (4/5) - 0^2 = 4/5$.
Notice that once the initial computations to find $\mu'_r$
have been done,
the evaluation of any raw moment is simple.
:::


By themselves, the mean, variance and skewness do not completely describe a distribution; many different distributions can be found having a given mean, variance and skewness. 
However, it turns out that in general *all* the moments of a distribution together define the distribution.


## Moment generating function {#MGF}

So far, the distribution of a rv has been described using a probability function, pdf or a distribution function.
Sometimes, however, it is convenient to work with a different representation. 
In this section, the *moment generating function* is used to represent the distribution of the probabilities of a random variable.
In addition, this function can be used to generate *any* moment of a distribution.
There are other uses of the moment generating function that are seen later (see Section~\ref{SEC:transforms:mgf}).

:::{.definition #MGF name="Moment generating function (MGF)"}
The \emph{moment generating function} $M_X(t)$ of the $X$ is defined as
\[
   M_X(t)  = \text{E}(\exp(tX)) =
   \begin{cases}
   \displaystyle \sum_x \exp(tx) p_X(x) & \text{for $X$ discrete with pf $p_Y(y)$}\\[10pt]
   \displaystyle \int_{-\infty}^\infty \exp(tx) f_X(x) &
   \text{for $X$ continuous with pdf $f_X(x)$}
   \end{cases}
\]
:::

The mgf is defined as an infinite series or an infinite integral.
Such an expression may not always exist (that is, converge to a finite value) for all values of $t$.
Hence it may happen that the mgf is not defined for all values of $t$.
Note that the mgf always exists for $t = 0$; in fact $M_X(0) = 1$.

It turns out that provided the mgf is defined for some values of $t$ other than zero it uniquely defines a probability distribution and we can use it to generate the moments of the distribution as described in Theorem~\ref{TM:mgfmoments} below.


:::{.example #MGF name="MGF"}
Consider the random variable $Y$ with pdf
\[
   f_Y(y) =
   \begin{cases}
      \exp(-y) & \text{for $y>0$} \\
      0   & \text{elsewhere.}
   \end{cases}
\]
The mgf is given by
\begin{align*}
   M_Y(t)
   &= \text{E}[\exp(tY)] \\
   &= \int_0^\infty \exp(ty)\exp(-y)\, dy \\
   &= \int_0^\infty \exp\{ y(t-1) \}\, dy \\
   &= \frac{1}{t-1} \exp\{y(t-1)\}\Big|_{y = 0}^{y = \infty}\\
   &= (1-t)^{-1}
\end{align*}
provided $t - 1 < 0$; that is, $t<1$.
(If $t>1$, the integral does not converge. 
For example, if $t = 2$, we would have
\[
   \frac{1}{2-1} \exp(y)\Big|_{y=0}^{y=\infty} = \exp(0) - \exp(\infty)
\]
which does not converge.)
:::



:::{.example #MGFDice name="MGF for die rolls"}
Consider the pf of $X$, the outcome in tossing a fair die (see Example~\ref{EG:3:die}).
The mgf of $X$ can be expressed
\begin{align*}
   M_X(t)
   &= \text{E}(\exp(tX)) =\sum_x \exp(tx) p_X(x)\\
   &= \frac16\left(1+e^{2t}+e^{3t}+e^{4t}+e^{5t}+e^{6t}\right)
\end{align*}
which exists for all values of $t$.
:::

### Using the mgf to generate moments {#MGFMoments}

Replacing $e^{xt}$ by its series expansion (see Section~\ref{SC:series})
in the definition of the mgf gives
\begin{align*}
     M_X(t) 
     & = \ds{\sum_x} (1+xt+\frac{x^2t^2}{2!} + \dots)\Pr{X=x}\\
     & = 1+\mu'_1t + \mu'_2 \frac{t^2}{2!} +\mu'_3 \frac{t^3}{3!} + \dots
\end{align*}
Note that to obtain the result the order of summations has been interchanged, and in cases where this interchange is permitted, the $r$th moment of a
distribution about the origin is the coefficient of $t^r/r!$ in the series expansion of $M_X(t)$.
\begin{align*}
     M_X'(t) 
     & = \sum_x x\,e^{xt}\Pr{X=x}\\
     M''_X(t) 
     & = \sum_x x^2\,e^{xt}\Pr{X=x},
\end{align*}
and for each positive integer $r$,
\[ 
   M_X^{(r)}(t) = \sum_x x^re^{xt}\Pr{X=x}. 
\]
On setting $t = 0$ we find that $M'_X(0) = \text{E}(X)$, $M''_X(0)=\text{E}(X^2)$, and
for each positive integer $r$,
\begin{equation}
     M^{(r)}_X (0) = \text{E}(X^r).
\end{equation}
This result is summarised in the following theorem.


:::{.theorem #Moments name="Moments???"}
The $r$th moment $\mu'_r$ of the distribution of the rv $X$ about the origin is given by either

1. the coefficient of $t^r/r!,  r = 1, 2, 3,\dots$ in the power series expansion of $M_X(t)$; or
2. $\displaystyle{ \mu'_r=M^{(r)}(0) = \left[\frac{d^rM(t)}{dt^r}\right]_{t=0}}$ where $M_X(t)$ is the mgf of $X$.
:::


:::{.example #MeanVarMGF name="Mean and variance from a MF"}
Continuing Example~\ref{EG:3.9}, the mean and variance of $Y$ can be found from the mgf.
To find the mean, first find
\[
   \frac{d}{dt}M_Y(t) = (1 - t)^{-2}.
\]
Setting $t = 0$ gives the mean as $\text{E}(Y) = 1$. 
Likewise,
\[
   \frac{d^2}{dt^2}M_Y(t) = 2(1-t)^{-3}.
\]
Setting $t = 0$ gives $\text{E}(Y^2) = 2$.
The variance is therefore $\text{var}[Y] = 2-1^2 = 1$.

Once the moment generating function has been computed, raw moments can be computed using
\[
   \text{E}(Y^r) = \mu'_r = \frac{d^r}{dt^r} M_Y(t)\Big|_{t=0}.
\]
:::



### Some useful results

The moment generating function can be used to derive the distribution of a function of a rv (see Section~\ref{SEC:transforms:mgf}).
The following theorems are valuable for this task.

:::{.theorem #MGFLinear name="MGF of linear combinations"}
If the rv $X$ has mgf $M_X(t)$ and $Y = aX + b$ where $a$ and $b$ are constants, then the mgf of $Y$ is
\[
   M_Y(t) = \text{E}(\exp\{t(aX + b)\}) = \exp(bt) M_X(at)
\]
:::



:::{.theorem #MGFIndependent name="MGF of independent rvs"}
If $X_1$, $X_2$, $\dots$, $X_n$ are $n$ independent random variables, where $X_i$ has mgf $M_{X_i}(t)$, then the mgf of $Y = X_1 + X_2 + \cdots X_n$ is
\[
   M_Y(t) = \prod_{i = 1}^n M_{X_i}(t).
\]
:::


The proofs are left as an exercise.

Note that in the special case when all the rvs are independently and identically distributed in Theorem~\ref{TM:3:mgf2},
\[
   M_Y(t) = [M_{X_i}(t)]^n.
\]


:::{.example #MGFLinearCombinations name="MGF of linear combinations"}
Consider the random variable $X$ with pf
\[
   p_X(x) = 2(1/3)^x\qquad\text{for $x=1, 2, 3, \dots$}
\]
and zero elsewhere.

The mgf of $X$ is
\begin{align*}
   M_X(t)
   &= \sum_{x:p(x)>0} \exp(tx) p_X(x) \\
   &= \sum_{x=1}^\infty \exp(tx) 2(1/3)^x \\
   &= 2\sum_{x=1}^\infty (\exp(t)/3)^x \\
   &= 2\left\{ \frac{\exp(t)}{3} + \left(\frac{\exp(t)}{3}\right)^2
   + \left(\frac{\exp(t)}{3}\right)^3 + \dots\right\} \\
   &= 2 \frac{ \exp(t)/3}{1-\exp(t)/3}\\
   &= \frac{2\exp(t)}{3-\exp(t)}
\end{align*}
where $\sum_{y=1}^\infty a^y = \frac{a}{1-a}$ for $a<1$ has been used
(see (\ref{EQN:series4})); here $a=\exp(t)/3$.

Next consider finding the mgf of $Y = (X - 2)/3$.
From Theorem~\ref{TM:3:mgf1} with $a = 1/3$ and $b = -2/3$ we have 
\[
   M_Y(t) 
   = \exp(-2t/3) M_X(t/3)
   = \frac{2\exp\{(-t)/3\}}{3 - \exp(t/3)}.
\]
In practice, rather than identify $a$ and $b$ and remember Theorem~\ref{TM:3:mgf1}, problems such as this one are best solved directly from the definition of the mgf; viz
\begin{align*}
   M_Y(t) =\text{E}(\exp(tY))&=\text{E}(\exp\{t(X-2)/3\})\\
   &= \text{E}(\exp\{tX/3-2t/3\})\\
   &= \exp(-2t/3) M_X(t/3) \\
   &= \frac{2\exp\{(-t)/3\}}{3-\exp(t/3)}
\end{align*}
:::


### Determining the distribution from the mgf {#DistributionFromMGF}

It can be proven that the mgf (if it exists) completely determines the distribution of a rv.
Hence it should be possible, given a mgf, to deduce the probability function.
In the discrete case, this is reasonably easy; it is very difficult in the continuous case which is not presented here.

In the discrete case, the mgf is defined as
\[
   M_X(t)  = \text{E}(\exp(tX)) =
   \sum_X e^{tx} p_X(x)
\]
for $X$ discrete with pf $p_X(x)$.
This can be expressed as
\begin{align*}
   M_X(t)
   &= \exp(t x_1) p_X(x_1) + \exp(t x_2)p_X(x_2) + \dots\\
   &= \exp(t x_1) \Pr{X=x_1} + \exp(t x_2)\Pr{X=x_2} + \dots\\
\end{align*}
and so the probability function of $Y$ can be deduced
from the mgf.

:::{.example #DistFromMGF name="Distribution from the mgf"}
Suppose a discrete random variable $D$ has the mgf
\[
   M_D(t) = \frac{1}{3} \exp(2t) + \frac{1}{6}\exp(3t) + \frac{1}{12}\exp(6t)
   + \frac{5}{12}\exp(7t).
\]
Then, by the definition of the mgf in the discrete case given above, the coefficient of $t$ in the exponential indicates values of $D$, and the coefficient indicates the probability of that value of $y$.
\begin{align*}
   M_D(t)
   &= \overbrace{\frac{1}{3} \exp(2t)}^{D=2} + \overbrace{\frac{1}{6}\exp(3t)}^{D=3} +
       \overbrace{\frac{1}{12}\exp(6t)}^{D=6} + \overbrace{\frac{5}{12}\exp(7t)}^{D=7}\\
   &= \Pr{D=2}\exp(2t) + \Pr{D=3}\exp(3t) + \\
   & \quad \Pr{D=6}\exp(6t) + \Pr{D=7}\exp(7t),
\end{align*}
so the pf is
\[
   p_D(d) =
   \begin{cases}
      1/3 & \text{for $d=2$}\\
      1/6 & \text{for $d=3$}\\
      1/12 & \text{for $d=6$}\\
      5/12 & \text{for $d=7$}\\
      0 & \text{otherwise}
   \end{cases}
\]
(Of course, it is easy to check by computing the mgf for $D$ from the pf found above; you should get the original mgf.)
:::


## Tchebysheff's inequality {#Tchebysheff}

An inequality that applies to any probability distribution is sometimes useful in theoretical work or to provide bounds on probabilities.

:::{.theorem #Tchebysheff name="Tchebysheff's theorem"}
Let $X$ be a rv with finite mean $\mu$ and variance $\sigma^2$.
Then for any positive $k$ we have
\begin{equation}
     \Pr{|X-\mu| \geq k\sigma }\leq \frac{1}{k^2}
\end{equation}
or, equivalently
\begin{equation}
     \Pr{|X-\mu| < k\sigma }\geq 1 - \frac{1}{k^2}
\end{equation}
:::

:::{.proof}
The proof for the continuous case only is given here.
Let $X$ be continuous with pdf $f(x)$.
For some $c > 0$, consider
\begin{align*}
     \sigma^2 & = \int^\infty_{-\infty} (x-\mu )^2f(x)\,dx\\
     & = \int^{\mu -\sqrt{c}}_{-\infty} (x-\mu )^2f(x)\, dx+\int^{\mu
+\sqrt{c}}_{\mu-\sqrt{c}}(x-\mu )^2f(x)\,dx +\int^\infty_{\mu+ \sqrt{c}}(x-
\mu)^2f(x)\,dx\\
     & \geq \int^{\mu -\sqrt{c}}_{-\infty} (x-\mu )^2f(x)\,dx +
\int^\infty_{\mu +\sqrt{c}}(x-\mu )^2f(x)\,dx
\end{align*}
since the second integral is non-negative.
Now $(x - \mu )^2\geq c$ if $x\leq \mu -\sqrt{c}$ or $x\geq \mu + \sqrt{c}$.
So in both the above remaining integrals we may replace $(x-\mu )^2$ by $c$ without altering the direction of the inequality.
Thus,
\begin{align*}
     \sigma^2 & \geq  c \int^{\mu -\sqrt{c}}_{-\infty} f(x)\,dx
+c\int^\infty_{\mu + \sqrt{c}}f(x)\,dx\\
     & =  c\,\Pr{X\leq \mu - \sqrt{c}}+c\,\Pr{X\geq \mu + \sqrt{c}}\\
     & =  c\,\Pr{|X-\mu| \geq \sqrt{c}}.
     \end{align*}
Putting $\sqrt{c} = k\sigma$, we obtain (\ref{EQN:tchebyheff}).
:::

Note that if we have the probability function or pdf of a random variable $X$ we can find $\text{E}(X)$ and $\text{var}(X)$ but the converse is not true. 
That is, from a knowledge of $\text{E}(X)$ and $\text{var}(X)$ we cannot reconstruct the probability distribution of $X$ and hence cannot compute probabilities such as $\Pr{|X-\mu |\geq k\sigma}$.
But using Tchebysheff's inequality we can find a useful bound to either the probability outside or inside of $\mu \pm k\sigma$.



