# Mathematical expectation and distributions{#Expectation}

::: {.objectivesBox .objectives data-latex="{iconmonstr-target-4-240.png}"}
Upon completion of this chapter, you should be able to:

* understand the concept and definition of mathematical expectation.
* compute the expectations of a random variable, functions of a random variable and linear functions of a random variable.
* compute the variance and other higher moments of a random variable.
* derive the moment generating function of a random variable and linear functions of a random variable.
* find the moments of a random variable from the moment generating function.
* state and use Tchebysheff's inequality.
:::


## Mathematical expectation {#ExpectedValue}

Because random variables are *random*, knowing the outcome on any one realisation of the random process is not possible.
Instead, we can talk about what we might *expect* to happen, or what might happen *on average*..

This is the idea of *mathematical expectation*. 
In more usual terms, the mathematical expression of the probability distribution of a random variable is the *mean* of the random variable.
Mathematical expectation goes far beyond just computing means, but we begin here as the idea of a *mean* is easily understood.

The definition looks different in detail for discrete and continuous random variables, but the intention is the same.


:::{.definition #Expectation name="Expectation"}
The *expectation* or *expected value* (or *mean*) of a random variable $X$ is defined as

* $\displaystyle
   \text{E}(X) = 
        \sum_{x\in R_X} x p_X(x)$
  for a discrete rv $X$ with pmf $p_X(x)$;

* $\displaystyle
   \text{E}(X) = 
        \int_{-\infty}^\infty x f_X(x)$
  for a continuous rv $X$ with pdf $f_X(x)$.

Often we write $\mu = \text{E}(X)$, or $\mu_X$ to distinguish between random variables.
:::


Effectively $\text{E}(X)$ is a weighted average of the points in $R_X$, the weights being the probabilities in the discrete case and probability densities in the continuous case.






:::{.example #ExpectationDiscrete name="Expectation for discrete variables"}
Consider the discrete random variable $U$ with probability function
\[
   p_U(u) = \begin{cases}
               (u^2 + 1)/5 & \text{for $u = -1, 0, 1$};\\
               0 & \text{elsewhere}.
           \end{cases}
\]
The expected value of $U$ is, by definition,
\begin{align*}
   \text{E}(U)
   &= \sum_{u = -1, 0, 1} up_U(u) \\
   &= \left( -1 \times \frac{(-1)^2 + 1}{5} \right ) +
       \left( 0 \times \frac{(0)^2 + 1}{5} \right ) +
       \left( 1 \times \frac{(1)^2 + 1}{5} \right ) \\
   &= -2/15 + 0 + 2/15 = 0.
\end{align*}
The expected value of $U$ is $0$.
:::


:::{.example #ExpectationContinuousX name="Expectation for continuous variables"}
Consider a continuous random variable $X$ with pdf
\[
   f_X(x) = \begin{cases}
               x/4 & \text{for $1 < x < 3$};\\
               0 & \text{elsewhere}.
            \end{cases}
\]
The expected value of $X$ is,
by definition,
\begin{align*}
   \text{E}(X)
   &= \int_{-\infty}^\infty x f_X(x) \, dx \\
   &= \int_1^3 x(x/4)\, dx\\
   &= \left.\frac{1}{12} x^3\right|_1^3 = 13/6.
\end{align*}
The expected value of $X$ is $13/6$.
:::


:::{.example #ExpectationCoinOnce name="Expectation for a coin toss"}
Consider tossing a coin *once* and counting the number of tails.
Let this random variable be $T$.
The probability function is
\[
   p_T(t) = \begin{cases}
               0.5 & \text{for $t = 0$ or $t = 1$};\\
               0   & \text{otherwise.}
            \end{cases}
\]
The expected value of $T$ is, by definition
\begin{align*}
   \text{E}(T) 
   &= \sum_{i = 1}^2 t p_T(t)\\
   &= \Pr(T = 0) \times 0 \quad + \quad \Pr(T = 1) \times 1\\
   &= (0.5 \times 0) \qquad + \qquad (0.5 \times 1) = 0.5.
\end{align*}
Of course, $0.5$ tails can never actually be observed in practice on one toss.
But it would be silly to round up (or down) and say that the expected number of tails on one toss of a coin is one (or zero).
The expected value of $0.5$ simply means that over a large number of repeats of this random process, we expect a tail to occur in half of those repeats.
:::


:::{.example #InfiniteMean name="Mean not defined"}
Consider the distribution of $Z$, with the probability density function
\[
   f_Z(z) = 
   \begin{cases}
      z^{-2} & \text{for $z \ge 1$};\\
      0      & \text{elsewhere}
   \end{cases}
\]
as in Fig. \@ref(fig:NoMean).
The expected value of $Z$ is
\[
   \text{E}[Z] = \int_1^{-\infty} z \frac{1}{z^2}\, dz = \int_1^\infty \frac{1}{z} = -\log z \Big|_1^\infty.
\]
However, $\displaystyle\lim_{z\to\infty} -\log z \to \infty$.
The expected value of $Z$ is undefined.
:::



```{r NoMean, echo=FALSE, fig.align="center", fig.cap="The probability function for the random variable $Z$. The mean is not defined.", fig.height = 3.5, fig.width=4}
z <- seq(1, 6, 
         length = 100)
plot(x = z,
     y = z^(-2),
     type = "l",
     lwd = 2,
     xlim = c(0, 6),
     las = 1,
     xlab = expression( italic(z)),
     ylab = "Probability density",
     main = expression(paste("The probability function for"~italic(Z))))
lines( x = c(-1, 1),
       y = c(0, 0),
       lwd = 2)
abline(v = 1,
       lwd = 1,
       col = "grey",
       lty = 2)
```



## Expectation of a function of a random variable {#ExpectationFunction}

While the mean can be expressed in terms of mathematical expectation, mathematical expectation is a more general concept.

Let $X$ be a discrete random variable with a probability function $p_X(x)$, or a continuous random variable with pdf $f_X(x)$.
Also assume $g(X)$ is a real-valued function of $X$.
We can then define the expected value of $g(X)$.


:::{.definition #ExpectationFunction name="Expectation for function of a random variable"}
The *expected value* of some function $g(\cdot)$ of a random variable $X$ is:

* $\displaystyle\text{E}\big(g(X)\big) = \sum_{x\in R_X} g(x) p_X(x)$ for a discrete random variable $X$ with pmf $p_X(x)$;

* $\displaystyle\text{E}\big(g(X)\big) = \int_{-\infty}^\infty g(x) f_X(x)\,dx$ for a continuous random variable $X$ with pdf $f_X(x)$.
:::

If $Y = g(X)$, write $\mu_Y = \text{E}(Y) = \text{E}\big(g(X)\big)$.
Importantly, the expectation operator is a *linear operator*, as stated below.


:::{.theorem #ExpectationLinear name="Expectation properties"}
For any random variable $X$ and constants $a$ and $b$,
\[
   \text{E}(aX + b) = a\text{E}(X) + b.
\]
:::

:::{.proof}
Assume $X$ is a discrete random variable with probability function $p_X(x)$.
By Definition \@ref(def:ExpectationFunction) with $g(X) = aX + b$,
\[
   \text{E}(aX + b) = \sum_x (ax + b) p_X(x) = a\sum_x p_X(x) + \sum_x b p_X(x) = a\text{E}(X) + b,
\]
using that $\sum_x p_X(x) = 1$.
(The proof in the continuous case uses the probability function with a pdf, and replaces summations with integrals.)
:::


:::{.example #ExpectationFunctionY name="Expectation of a function of a random variable"}
Consider the random variable $Y = 2x$ where $X$ is defined in Example \@ref(exm:ExpectationContinuousX).
Using Theorem \@ref(thm:ExpectationLinear) with $a = 2$ and $b = 0$, the value of $\text{E}(Y)$ is
\[
      \text{E}(2X) = 2\text{E}(X) = 2 \times 13/6 = 13/3.
\]
:::



## Measures of location {#FeaturesLocation}

Having introduced the idea of mathematical expectation, other descriptions of a random variable can now be introduced.

Two general features of a distribution are the location or centre, and the variability.
We have already met the mean (or expected value) as one measure of location.


### The mean {#Mean}

The most important measure of location is the mean $\mu = \text{E}(X)$, which has [already been defined](#def:Expectation).
The mean is the *balance point* of the distribution:  the expected mean deviation of the random variable from $\mu$ is zero.
In other words, $\text{E}(X - \mu) = 0$.

While the mean is commonly used, it is not always a suitable measure of centre: the balance point can be unduly affected by outliers and extreme skewness.
We have also seen a situation where the mean is undefined (Example \@ref(exm:InfiniteMean)).


### The mode and median {#ModeMedian}

Other measures of centre sometimes may be more appropriate.
The mode and median are two common alternatives.
The mean, when it is defined, is more useful in theoretical work due to nicer mathematical properties,


:::{.definition #Mode name="Mode"}
The *mode(s)* is/are the value(s) $x\in R_X$ such that:

* $p_X(x)$ attains its maximum (for the discrete case); or 
* the value(s) of $x$ at which $f_X(x)$ attains its maximum (for the continuous case).
:::


The mode is not necessarily unique.


:::{.definition #Median name="Median"}
The *median* of a random variable $X$ (or of a distribution) is a number $\nu$ such that
\begin{equation}
   \Pr(X\leq \nu )\geq \frac{1}{2}\text{ and }\Pr(X \geq \nu )\geq \frac{1}{2}.
\end{equation}
:::

For $X$ discrete, the median is not necessarily unique.


:::{.example #ModeDiscrete name="Mode and median for discrete distributions"}
If $X$ is a discrete random variable with probability function defined in Table \@ref(tab:ProbDeg1r) (and shown in Fig. \@ref(fig:DiscreteMedian)), the distribution is bimodal (i.e., has two modes), with modes at 3 and 4.

The median, $\nu$, is any value of $X$ in the range $3 < x < 4$, since *any* number in this range satisfies $\Pr(X \le x) = 0.5$ and $\Pr(X \ge x) = 0.5$. 
The median is not unique.
:::

```{r ProbDeg1r, echo=FALSE}
eg1Table <- array( dim = c(2, 8))

eg1Table[1, ] <- 0:7
eg1Table[2, ] <- paste0(c("1", "7", "21", "35", "35", "21", "7", "1"), 
                        "/128")

rownames(eg1Table) <- c("$x$",
                        "$\\Pr(X = x)$")


if( knitr::is_latex_output() ) {
  knitr::kable(eg1Table,
               format = "latex",
               booktabs = TRUE,
               align = "r",
               longtable = FALSE,                
               escape = FALSE,
               caption = "A probability distribution") %>%
    kable_styling(font_size = 10) %>%
    row_spec(0, bold = TRUE) 
}
if( knitr::is_html_output() ) {
  knitr::kable(eg1Table,
               escape = TRUE,
               format = "html",
               booktabs = TRUE,
               align = "r",
               longtable = FALSE,                
               caption = "A probability distribution") %>%
    row_spec(1, bold = TRUE) 
}


```


```{r DiscreteMedian, echo=FALSE, fig.cap="Finding the median, when it is not unique", fig.align="center", fig.height = 8,out.width='70%'}
par( mfrow = c(2, 1))

x <- 0:7
fx <- c(1, 7, 21, 35, 35, 21, 7, 1) / 128

plot(x = x,
     y = fx,
     las = 1,
     lty = 3,
     ylim = c(0, 0.30 ),
     type = "h",
     col = "grey",
     sub = "Each half contains a probability of 0.5",
     xlab = expression( italic(X) ),
     ylab = expression(
       paste(italic(p)[italic(X)](italic(x)))
     ),
     main = "Probability mass function")
polygon( x = c(-10, -10, 3.5, 3.5),
         y = c(0, 1, 1, 0),
         col = plotColour)
         #col = gray(0.9) )

polygon( x = c(3.5, 3.5,10, 10),
         y = c(0, 1, 1, 0),
         col = gray(0.95) )

points( x = x,
        y = fx,
        pch = 19)
points(x = x,
     y = fx,
     type = "h",
     lty = 3,
     col = grey(0.2))
box()


plot( x = x,
      y = cumsum(fx),
      las = 1,
      axes = FALSE,
      xlim = c(-2, 9),
      xlab = expression( italic(X) ),
      ylab = expression(
       paste(italic(F)[italic(X)](italic(x)))
      ),
      main = "Distribution function",
      sub = "Any number between 3 and 4 has a cumulative probability of 0.5",
      type = "n")
polygon( x = c(3, 3, 4, 4),
         y = c(0, 0.5, 0.5, 0),
      col = plotColour)
#         col = gray(0.9))
abline( h = c(0, 1),
        col = "grey")
axis(side = 1,
     at = c(-2, 0, 2, 3, 4, 6, 8))
axis(side = 2,
     at = c(0, 0.2, 0.5, 0.8, 1),
     las = 1)
box()
points( x = x,
        y = cumsum(fx),
        pch = 19)
points( x = x[2: length(x)],
        y = cumsum(fx)[1:(length(x) - 1)],
        pch = 1)

for (i in (1 : (length(x) - 1)) ){
   lines( x = x[i:(i + 1)],
          y = c(cumsum(fx)[i],
                cumsum(fx)[i]),
          lwd = 2)
}

# Now add edge cases
lines( x = c(-1, 0),
       y = c(0, 0),
      lwd = 2)
lines(x = c(-2, -1),
      y = c(0, 0),
      lty = 2,
      lwd = 2)
points(0, 0, 
       pch = 1)
lines(x = c(7, 8),
      y = c(1, 1),
      lwd = 2)
lines(x = c(8, 9),
      y = c(1, 1),
      lty = 2,
      lwd = 2)

lines( x = c(-10, 3),
       y = c(0.5, 0.5),
       lty = 3)
```


:::{.example #ExpectationDiscrete2 name="Expectation for discrete variables"}
Let $X$ be a discrete random variable with the probability function shown in Table \@ref(tab:ProbDeg2r) (and Fig. \@ref(fig:DiscreteMedian2)).

The *mode* of this distribution is 3.
The *median* is $\nu = 3$ since this is the only value of $X$ that will satisfy Definition \@ref(def:Median).
:::



```{r ProbDeg2r, echo=FALSE}
eg2Table <- array( dim = c(2, 7))

eg2Table[1, ] <- 0:6
eg2Table[2, ] <- paste0(c("1", "6", "15", "20", "15", "6", "1"), 
                        "/64")

rownames(eg2Table) <- c("$x$",
                        "$\\Pr(X = x)$")
eg2Table.caption <- "A probability distribution"


if( knitr::is_latex_output() ) {
  knitr::kable(eg2Table,
               format = "latex",
               booktabs = TRUE,
               align = "r",
               longtable = FALSE,                
               escape = FALSE,
               caption = eg2Table.caption ) %>%
    kable_styling(font_size = 10) %>%
    row_spec(0, bold = TRUE) 
}
if( knitr::is_html_output() ) {
  knitr::kable(eg2Table,
               escape = TRUE,
               format = "html",
               booktabs = TRUE,
               align = "r",
               longtable = FALSE,                
               caption = eg2Table.caption) %>%
    row_spec(1, bold = TRUE) 
}
```

```{r DiscreteMedian2, echo=FALSE, fig.align="center", fig.cap="The probability function and distribution function for $H$", fig.height = 8, out.width='70%'}
par( mfrow = c(2, 1))

x <- 0:6
fx <- c(1, 6, 15, 20, 15, 6, 1) / 64

plot(x = x,
     y = fx,
     las = 1,
     xlim = c(-2, 8),
     type = "h",
     col = "grey",
     xlab = expression(italic(X)),
     ylab = expression(
       paste(italic(p)[italic(X)](italic(x)))
       ),
     main = "Probability mass function")
points( x = x,
        y = fx,
        pch = 19)




plot( x = x,
      y = cumsum(fx),
      las = 1,
      xlim = c(-2, 8),
      axes = FALSE,
      xlab = expression(italic(X)),
      ylab = expression(
       paste(italic(F)[italic(X)](italic(x)))
       ),
      main = "Distribution function",
      type = "n")
axis(side = 1,
     at = c(-2, 0, 2, 3, 4, 6, 8))
axis(side = 2,
     at = c(0, 0.2, 0.5, 0.8, 1),
     las = 1)
box()
#polygon( x = c(2.75, 2.75, 3.25, 3.25),
#         y = c(22/64, 42/64, 42/64, 22/64),
#         col = grey(0.9),
#         border = NA) # Omits borders
lines( x = c(-10, 3),
       y = c(0.5, 0.5),
       lty = 3)
lines( x = c(3, 3),
       y = c(42/64, 0),
       lty = 3)

abline( h = c(0, 1),
        col = "grey")
points( x = x,
        y = cumsum(fx),
        pch = 19)
points( x = x[2: length(x)],
        y = cumsum(fx)[1:(length(x) - 1)],
        pch = 1)

for (i in (1 : (length(x) - 1)) ){
   lines( x = x[i:(i + 1)],
          y = c(cumsum(fx)[i],
                cumsum(fx)[i]),
          lwd = 2)
}

# Now add edge cases
lines( x = c(-1, 0),
       y = c(0, 0),
       lwd = 2)
lines(x = c(-2, -1),
      y = c(0, 0),
      lty = 2,
      lwd = 2)
points(0, 0, 
       pch = 1)
lines(x = c(6, 7),
      y = c(1, 1),
      lwd = 2)
lines(x = c(7, 8),
      y = c(1, 1),
      lty = 2,
      lwd = 2)

```


## Measures of variability  {#Dispersion}

Apart from the mean, the most important description of a random variable is the *variability*: quantifying how the values of the random variable are dispersed.
The most important measure of variability is the *variance*.


### The variance and standard deviation {#VarianceStdDev}

The *variance* can be expressed as a function of a random variable.
(More correctly, we should say "the variance of the distribution of the random variable", but our language is very common.)

The variance is a measure of the variability of a random variable.
A small variance means the observations are nearly the same (i.e., small variation); a large variance means they are quite different.


:::{.definition #Variance name="Variance"}
The *variance* of a random variable $X$ (or of the distribution of $X$) is
\[
   \text{var}(X)  = \text{E}\big((X - \mu)^2\big)
\]
where $\mu = \text{E}(X)$.
The variance of $X$ is commonly denoted by $\sigma^2$, or $\sigma^2_X$ if distinguishing among variables is needed.
:::


The variance is the *expected value* of the squared distance of the values of the random variable from the mean, weighted by the probability function.
The unit of measurement for variance is the original unit of measurement *squared*.
That is, if $X$ is measured in metres, the variance of $X$ is in $\text{metres}^2$.

Describing the variability in terms of the original units is more natural, by taking the square root of the variance.


:::{.definition #StandardDeviation name="Standard deviation"}
The *standard deviation* of a random variable $X$ is defined as the *positive* square root of the variance (denoted by $\sigma$); i.e.,
\[
   \text{sd}(X) = \sigma = +\sqrt{\text{var}(X)}
\]
:::

The variance is less popular than the standard deviation in practice to describe variability.
In theoretical work, however, the variance is easier to work with than standard deviation (due to the square root), and the variance, rather than standard deviation, features in many results in theoretical statistics.



:::{.example #VarianceDice name="Variance for a die toss"}
Suppose a fair die is tossed, and $X$ denotes the number of points showing. 
Then $\Pr(X = x) =  1/6$ for $x = 1, 2, 3, 4, 5, 6$ and 
\[
   \mu = \text{E}(X) = \sum_S x\Pr(X = x) = (1 + 2 + 3 + 4 + 5 + 6 )/6 = 7/2.
\]
The variance of $X$ is then
\begin{align*}
   \sigma^2 
   &= \text{var}(X) = \sum (X - \mu)^2 \Pr(X = x)\\
   &= \frac{1}{6}\left[ \left(1 - \frac{7}{2}\right)^2 + \left(2 - \frac{7}{2}\right)^2 + \dots + \left(6 - \frac{7}{2}\right)^2 \right] = \frac{70}{24}.
\end{align*}
The standard deviation is then $\sigma = \sqrt{70/24} = 1.71$.
:::


An important result is known as the  *computational formula for variance*.

:::{.theorem #VarianceComputational name="Computational formula for variance"}
For any rv $X$,
\[
   \text{var}(X) = \text{E}(X^2) - [\text{E}(X)]^2.
\]
:::

:::{.proof}
Let $\text{E}(X) = \mu$, then (using the properties of expectation in Theorem \@ref(thm:ExpectationLinear)):
\begin{align*}
\text{var}(X)
   = \text{E}((X - \mu)^2)
   &= \text{E}( X^2 - 2X\mu + \mu^2) \\
   &= \text{E}(X^2) - \text{E}(2X\mu) + \text{E}(\mu^2)\\
   &= \text{E}(X^2) - 2\mu\text{E}(X) + \mu^2\\
   &= \text{E}(X^2) - 2\mu^2 + \mu^2 \\
   &= \text{E}(X^2) - \mu^2 \\
   &= \text{E}(X^2) - \big(\text{E}(X)\big)^2.
\end{align*}
:::
This formula is often easier to use to compute $\text{var}(X)$ than using the definition directly.


:::{.example #VarianceDice2 name="Variance for a die toss"}
Consider Example \@ref(exm:VarianceDice2) again.
Then
\begin{align*}
  \text{E}(X^2) = \sum_S x^2 \Pr(X = x) 
  &= \frac{1}{6}[1^2 + 2^2 + 3^2 + 4^2 = 5^2 + 6^2]\\
  &= 91/6,
\end{align*}
and so $\text{var}(X) = 91/6 - (7/2)^2 = 70/24$, as before.
:::



:::{.example #VarianceComputational name="Variance using computational formula"}
Consider the continuous random variable $X$ with pdf
\[
   f_X(x) = \begin{cases}
            3x(2 - x)/4  & \text{for $0 < x < 2$};\\
            0 & \text{elsewhere}.
            \end{cases}
\]
The variance of $X$ can be computed in two ways: using $\text{var}(X) = \text{E}((X - \mu)^2)$ or using the computational formula.
The expected value of $X$ is
\[
   \text{E}(X) = \int_0^2 x.3x(2 - x)4\, dx = 1.
\]
To use the computational formula,
\[
   \text{E}(X^2) = \frac{6}{5},
\]
and so $\text{var}(X) = \text{E}(X^2) - \big(\text{E}(X)\big)^2 = 1/5$.

Using the definition,
\begin{align*}
   \text{var}(X)
   = \text{E}\big((X - \text{E}(X))^2\big)
   &= \text{E}\big((X - 1)^2\big)\\
   &= \int_0^2 (x - 1)^2 3x(2 - x)/4\,dx = 1/5.
\end{align*}

Both methods give the same answer of course, and both methods require initial computation of $\text{E}(X)$.
The computational formula requires less work.
:::


The variance represents the expected value of the squared distance of the values of the random variable from the mean.
The variance is never negative, and is only zero when all the values of the random variable are identical (that is, there *is* no variation).



:::{.example #InfiniteVar name="Infinite variance"}
In Example \@ref(exm:InfiniteMean), $\text{E}(X)$ was not defined.
For that reason, the variance is also undefined, since computing the variance relies on having a finite value for $\text{E}(X)$.
:::


If most of the probability lies near the mean, the dispersion will be small; if the probability is spread out over a considerable range the dispersion will be large.
We want a measure of the 'spread' irrespective of the location or centre.

We have already seen variance and standard deviation are suitable measures of dispersion.
Another alternative is the following.


### The mean absolute deviation {#Variance}


:::{.definition #MAD name="Mean absolute deviation"}
The *mean absolute deviation* (MAD) is defined as:

* $\displaystyle 
   \text{E}(|X - \mu|) 
      = \sum_x |x - \mu| p_X(x)$ for a discrete rv $X$;
* $\displaystyle 
    \text{E}(|X - \mu|) 
      = 
         \int_{-\infty}^\infty |x - \mu| f_X(x)\,dx$ for a continuous rv $X$.
:::


Like the median, the MAD is difficult to use in theoretical work.
The most-used measure of dispersion is the variance (or standard deviation).


:::{.example #MADDie name="MAD of die toss"}
Consider the fair die described in Example \@ref(exm:VarianceDice).
Then $\mu = \text{E}(X) = 7/2$ and thus the MAD of $X$ is
\begin{align*}
   \text{E}(|X - \mu|) 
   &= \sum |x - \mu| \Pr(X = x)\\
   &= \frac{1}{6}\left[ \left|1 - \frac{7}{2}\right| +\left|2 - \frac{7}{2}\right| + \dots + \left|6 - \frac{7}{2}\right|\right]\\
   &= 1.5.
\end{align*}
:::


## Measuring symmetry {#Symmetry}

:::{.definition #Symmetry name="Symmetry"}
The distribution of $X$ is said to be *symmetric* if, for all $x\in R_X$,

* $\Pr(X = \mu + x) = \Pr(X = \mu - x)$ for the discrete case;
* $\Pr(X = \mu + x) = f_X(\mu - x)$ for the continuous case.
:::

For a symmetric distribution, the mean is also a median of the distribution, as these results show.
A formal definition for computing skewness is given below (Def. \@ref(def:Skewness)).


## Higher moments {#HigherMoments}

The idea of a mean and variance are generalised in the following definitions.

:::{.definition #RawMoments name="Raw moments"}
The *$r$th moment about the origin*, or *$r$th raw moment*, of a random variable $X$ (where $r$ is a positive integer) is defined as:

* $\displaystyle\mu'_r = \text{E}(X^r) = \sum_X x^r p_X(x)$ for the discrete case;\\[6pt]
* $\displaystyle\mu'_r = \text{E}(X^r) =  \int_{-\infty}^\infty x^r f_X(x)$ for the continuous case.
:::


:::{.definition #CentralMoments name="Central moments"}
The *$r$th central moment*, or *$r$th moment about the mean* (where $r$ is a positive integer), is defined as

* $\displaystyle\mu_r = \text{E}((X-\mu)^r) = \sum_x (x-\mu)^r p_X(x)$ in the discrete case;
* $\displaystyle\mu_r = \text{E}\big((X-\mu)^r\big) = \int_{-\infty}^{\infty} (x-\mu)^r f_X(x)$ in the continuous case.
:::

From these definitions:

* the mean $\mu'_1 = \mu$ is the *first moment about the origin*, or the *first raw moment*;
* $\mu'_2 = \text{E}(X^2)$ is the *second moment about the origin*, or the *second raw moment*; and
* the variance $\mu_2 = \sigma^2$ is the *second moment about the mean* or the *second central moment*.

Higher moments also exist that describe other features of a random variable.

For a symmetric distribution, the odd central moments are zero (Exercise \@ref(exr:SkewDiscrete)).
This suggests that odd central moments can be used to measure the *asymmetry* of a distribution.

Instead of central moments themselves, using expressions unaffected by a linear transformation of the type $Y = AX + b$ are convenient.
The ratio $(\mu_r)^p / (\mu_p)^r$ is such an expression, and the simplest form is a function of $\mu^2_3/\mu_2^3$.


:::{.definition #Skewness name="Skewness"}
The *skewness* of a distribution is defined as
\begin{equation}
     \gamma_1 = \frac{\mu_3}{\mu_2^{3/2}}.
     (\#eq:Skewness)
\end{equation}
:::


If $\gamma_1 >0$ we say the distribution is positively (or right) skewed, and it is 'stretched' in the positive (negative) direction.
Similarly, if $\gamma_1 <0$ we say the distribution is negatively (or left) skewed.


:::{.example #SkewnessCont name="Skewness"}
Consider the random variable $X$ in Example \@ref(exm:VarianceComputational), where $f_X(x) = x(2 - x)$ for $0 < x < 2$.
In that example, we found $\text{E}(X) = \mu'_1 = 1$ and $\text{E}(X^2) = \mu_2 = 6/5$.
Then,
\[
   \mu_3 = \int_0^2 (x - 1)^3 3x(2 - x)/4 \,dx = 0,
\]
so that the skewness in \@ref(eq:Skewness) will be zero.
This is expected, since the distribution is symmetric (Fig. \@ref(fig:VarianceComputationalPDF)).
:::


```{r, VarianceComputationalPDF, echo=FALSE, fig.align="center", fig.cap="The probability density function for $X$", fig.height=4, fig.width=5, out.width='65%'}
xx <- seq(-1, 3, 
          length = 500)

fx <- function(x) {
  fx <- 3*x*(2 - x)/4
  fx <- ifelse( x <= 0, 0, fx)
  fx <- ifelse( x >= 2, 0, fx)
  fx
  
}

y <- fx(xx)

plot(x = xx,
     y = y,
     lwd = 2,
     type = "l",
     las = 1,
     main = expression("The probability function for"~italic(X)),
     xlab = expression(italic(X)),
     ylab = "Prob. function")
```



:::{.example #SkewnessDiscrete name="Skewness"}
Consider the random variable $Y$ with pmf
\[
   p_Y(y) = 
   \begin{cases}
      0.2 & \text{for $y = 5$};\\
      0.3 & \text{for $y = 6$};\\
      0.5 & \text{for $y = 7$};\\
      0   & \text{elsewhere}.
    \end{cases}
\]
Then $\mu'_1 = \text{E}(Y) = (5\times 0.2) + (6\times 0.3) + (7\times 0.5) = 6.3$.
Likewise, $\mu_2 = \text{E}\big((y - 6.3)^2 p_Y(y) \big) = 0.1982$ and
          $\mu_3 = \text{E}\big((y - 6.3)^3 p_Y(y) \big) = 0.02457$.
Hence, the skewness is 
\[
   \gamma_1 = \frac{\mu_3}{\mu_2^{3/2}} = \frac{0.02457}{0.1982^(3/2)} =  0.02785,
\]
so the distribution has positive skewness.
:::


Another ratio gives a measure of *kurtosis*, which measures the heaviness of the tails in a distribution.


:::{.definition #Kurtosis name="Kurtosis"}
The *kurtosis* of a distribution is defined as
\begin{equation*}
     \frac{\mu_4}{\mu^2_2}.
\end{equation*}
The *excess kurtosis* of a distribution is defined as
\begin{equation*}
     \gamma_2 = \frac{\mu_4}{\mu^2_2} - 3.
\end{equation*}
:::


:::: {.importantBox .important data-latex="{iconmonstr-warning-8-240.png}"}
Excess kurtosis is commonly used, and sometimes is just called 'kurtosis'.
:::


Large values of kurtosis corresponds to greater proportion of the distribution in the tails.
The *excess kurtosis* is defined so that the normal, or bell-shaped, distribution (Sect. \@ref(Normal)), which has a kurtosis of 3, has a excess kurtosis of zero.
Then:

* distributions with *negative* excess kurtosis are called *platykurtic*.
  These distribution have fewer, or less extreme, observations in the tail compared to the normal distribution ('thinner tails').
  Examples include the [Bernoulli distribution](BernoulliTrials).
* distributions with *positive* excess kurtosis are called *leptokurtic*.
  These distribution have more, or more extreme, observations in the tail compared to the normal distribution ('fatter tails').
  Examples include the [exponential](ExponentialDistribution) and [Poisson distributions](#PoissonDistribution).
* distributions with *zero* excess kurtosis are called *mesokurtic*.
  The [normal distribution](#Normal) is the obvious example.



:::{.example #SkewWind name="Uses of skewness and kurtosis"}
@monypenny1998analysiswinds and @monypenny1998analysisgust use the skewness and kurtosis to analyse wind gusts at Sydney airport.
:::


:::{.example #SkewPricing name="Uses of skewness and kurtosis"}
@galagedera2002conditional used higher moments in a capital analysis pricing model for Australian stock returns.
:::


:::{.example #SkewDiscrete name="Skewness and kurtosis"}
Consider the discrete random variable $U$ from Example \@ref(exm:ExpectationDiscrete).
The raw moments are
\begin{align*}
   \mu'_r = \text{E}(U^r)
   &= \sum_{u = -1, 0, 1} u^r \frac{u^2 + 1}{5} \\
   &= (-1)^r \frac{ (-1)^2 + 1}{5} +
       (0)^r \frac{ (0)^2 + 1}{5} +
       (1)^r \frac{ (1)^2 + 1}{5} \\
   &= \frac{2(-1)^r}{5} + 0 + \frac{2}{5} \\
   &= \frac{2}{5}[ (-1)^r + 1]
\end{align*}
for the $r$th raw moment.
Then,
\begin{align*}
   \text{E}(X)   &= \mu'_1 = \frac{2}{5}[ (-1)^1 + 1 ] = 0;\\
   \text{E}(X^2) &= \mu'_2 = \frac{2}{5}[ (-1)^2 + 1 ] = 4/5;\\
   \text{E}(X^3) &= \mu'_1 = \frac{2}{5}[ (-1)^3 + 1 ] = 0;\\
   \text{E}(X^4) &= \mu'_2 = \frac{2}{5}[ (-1)^4 + 1 ] = 4/5.
\end{align*}
Since $\text{E}(U) = 0$, then the $r$th central and raw moments are the same: $\mu'_r = \mu_r$.
Notice that once the initial computations to find $\mu'_r$ are complete, the evaluation of any raw moment is simple.

The skewness is
\[
   \gamma_1 = \frac{\mu_3}{\mu_2^{3/2}} = \frac{0}{(4/5)^{3/2}} = 0, 
\]
so the distribution is symmetric.
The excess kurtosis is
\[
   \gamma_2 = \frac{\mu_4}{\mu_2^2} -3 = \frac{4/5}{(4/5)^2} -3 = -7/4, 
\]
so the distribution is platykurtic.
:::


By themselves, the mean, variance, skewness and kurtosis do not completely describe a distribution; many different distributions can be found having a given mean, variance, skewness and kurtosis. 
However, in general, *all* the moments of a distribution together define the distribution.
This leads to the idea of a *moment generating function.


## Moment generating function {#MGF}

So far, the distribution of a random variable has been described using a probability function or a distribution function.
Sometimes, however, working with a different representation is useful.

In this section, the *moment generating function* is used to represent the distribution of the probabilities of a random variable.
As the name suggests, this function can be used to generate *any* moment of a distribution.
Other uses of the moment generating function are seen later (see Sect. \@ref(TransformationMoments)).


:::{.definition #MGF name="Moment generating function (mgf)"}
The *moment generating function* (or mgf) $M_X(t)$ of the random variable $X$ is defined as:

* $\displaystyle
  M_X(t)  = \text{E}\big(\exp(tX)\big) = \sum_x \exp(tx) p_X(x)$ in the discrete case;
* $\displaystyle
   M_X(t)  = \text{E}\big(\exp(tX)\big) = \int_{-\infty}^\infty \exp(tx) f_X(x)$ in the continuous case.
:::

The mgf is defined as an infinite series or an infinite integral.
Such an expression may not always exist (that is, converge to a finite value) for all values of $t$, so the mgf may not be defined for all values of $t$.
Note that the mgf always exists for $t = 0$; in fact $M_X(0) = 1$.

Provided the mgf is defined for some values of $t$ *other* than zero, it *uniquely* defines a probability distribution, and we can use it to generate the moments of the distribution as described in Theorem \@ref(thm:Moments).


:::{.example #MGF name="Moment generating function"}
Consider the random variable $Y$ with pdf
\[
   f_Y(y) =
   \begin{cases}
      \exp(-y) & \text{for $y > 0$};\\
      0        & \text{elsewhere.}
   \end{cases}
\]
The mgf is
\begin{align*}
   M_Y(t)
    = \text{E}[\exp(tY)] 
   &= \int_0^\infty \exp(ty)\exp(-y)\, dy \\
   &= \int_0^\infty \exp\{ y(t-1) \}\, dy \\
   &= \left.\frac{1}{t-1} \exp\{y(t-1)\}\right|_{y = 0}^{y = \infty}\\
   &= (1 - t)^{-1}
\end{align*}
provided $t - 1 < 0$; that is, for $t < 1$.
If $t > 1$, the integral does not converge. 
For example, if $t = 2$,
\[
   \left. \frac{1}{2 - 1} \exp(y)\right|_{y = 0}^{y = \infty} = \exp(0) - \exp(\infty)
\]
which does not converge.
:::



:::{.example #MGFDice name="Mgf for die rolls"}
Consider the pf of $X$, the outcome of tossing a fair die (Example \@ref(exm:VarianceDice)).
The mgf of $X$ is
\begin{align*}
   M_X(t)
   &= \text{E}(\exp(tX)) = \sum_x \exp(tx) p_X(x)\\
   &= \frac16\left(1 + e^{2t} + e^{3t} + e^{4t} + e^{5t} + e^{6t}\right),
\end{align*}
which exists for all values of $t$.
:::


### Using the mgf to generate moments {#MGFMoments}

Replacing $\exp(xt)$ by its series expansion (App. \@ref(UsefulSeries)) in the definition of the mgf gives
\begin{align*}
     M_X(t) 
     & = {\sum_x} \left(1 + xt + \frac{x^2t^2}{2!} + \dots\right) \Pr(X = x)\\
     & = 1 + \mu'_1t + \mu'_2 \frac{t^2}{2!} +\mu'_3 \frac{t^3}{3!} + \dots
\end{align*}
Then,  the $r$th moment of a distribution about the origin is seen to be the coefficient of $t^r/r!$ in the series expansion of $M_X(t)$:
\begin{align*}
     \frac{d M_X(t)}{dt} 
     & = \sum_x x\,e^{xt}\Pr(X = x)\\
     \frac{d^2 M_X(t)}{dt^2} 
     & = \sum_x x^2\,e^{xt} \Pr(X = x),
\end{align*}
and so, for each positive integer $r$,
\[ 
   \frac{d^r M_X(t)}{dt^r} = \sum_x x^re^{xt}\Pr(X = x). 
\]
On setting $t = 0$, $d M_X(t)/dt\Big|_{t = 0} = \text{E}(X)$, $d^2M_X(t)/dt^2 \Big|_{t = 0} = \text{E}(X^2)$, and for each positive integer $r$,
\begin{equation}
     \frac{d^r M_X(t)}{dt^r}\Big|_{t = 0} = \text{E}(X^r).
\end{equation}
(Sometimes, $d^r M_X(t)/dt^r\Big|_{t = 0}$ is written as $M^{(r)}(0)$ for brevity.)
This result is summarised in the following theorem.


:::{.theorem #Moments name="Moments"}
The $r$th moment $\mu'_r$ of the distribution of the rv $X$ about the origin is given by either

1. the coefficient of $t^r/r!,  r = 1, 2, 3,\dots$ in the power series expansion of $M_X(t)$; or
2. $\displaystyle \mu'_r = \left.\frac{d^rM(t)}{dt^r}\right|_{t = 0}$ where $M_X(t)$ is the mgf of $X$.
:::


:::{.example #MeanVarMGF name="Mean and variance from a MF"}
Continuing Example \@ref(exm:MGF), the mean and variance of $Y$ can be found from the mgf.
To find the mean, first find
\[
   \frac{d}{dt}M_Y(t) = (1 - t)^{-2}.
\]
Setting $t = 0$ gives the mean as $\text{E}(Y) = 1$. 
Likewise,
\[
   \frac{d^2}{dt^2}M_Y(t) = 2(1 - t)^{-3}.
\]
Setting $t = 0$ gives $\text{E}(Y^2) = 2$.
The variance is therefore $\text{var}[Y] = 2 - 1^2 = 1$.

Once the moment generating function has been computed, raw moments can be computed using
\[
   \text{E}(Y^r) = \mu'_r = \left.\frac{d^r}{dt^r} M_Y(t)\right|_{t = 0}.
\]
:::



### Some useful results

The moment generating function can be used to derive the distribution of a function of a random variable (see Sect. \@ref(TransformationMoments)).
The following theorems are valuable for this task.


:::{.theorem #MGFLinear name="Mgf of linear combinations"}
If the random variable $X$ has mgf $M_X(t)$ and $Y = aX + b$ where $a$ and $b$ are constants, then the mgf of $Y$ is
\[
   M_Y(t) = \text{E}\big(\exp\{t(aX + b)\}\big) = \exp(bt) M_X(at).
\]
:::



:::{.theorem #MGFIndependent name="Mgf of independent rvs"}
If $X_1$, $X_2$, $\dots$, $X_n$ are $n$ independent random variables, where $X_i$ has mgf $M_{X_i}(t)$, then the mgf of $Y = X_1 + X_2 + \cdots X_n$ is
\[
   M_Y(t) = \prod_{i = 1}^n M_{X_i}(t).
\]
:::

:::{.proof}
The proofs are left as an exercise.
:::


Note that in the special case when all the random variables are independently and identically distributed in Theorem \@ref(thm:MGFLinear),
\[
   M_Y(t) = [M_{X_i}(t)]^n.
\]


:::{.example #MGFLinearCombinations name="Mgf of linear combinations"}
Consider the random variable $X$ with pf
\[
   p_X(x) = 2(1/3)^x \qquad \text{for $x = 1, 2, 3, \dots$}
\]
and zero elsewhere.

The mgf of $X$ is
\begin{align*}
   M_X(t)
   &= \sum_{x: p(x)>0} \exp(tx) p_X(x) \\
   &= \sum_{x = 1}^\infty \exp(tx) 2(1/3)^x \\
   &= 2\sum_{x = 1}^\infty (\exp(t)/3)^x \\
   &= 2\left\{ \frac{\exp(t)}{3} + \left(\frac{\exp(t)}{3}\right)^2
   + \left(\frac{\exp(t)}{3}\right)^3 + \dots\right\} \\
   &= \frac{2\exp(t)}{3 - \exp(t)}
\end{align*}
where $\sum_{y = 1}^\infty a^y = a/(1 - a)$ for $a < 1$ has been used (App. \@ref(UsefulSeries)); here $a = \exp(t)/3$.

Next consider finding the mgf of $Y = (X - 2)/3$.
From Theorem \@ref(thm:MGFLinear) with $a = 1/3$ and $b = -2/3$, 
\[
   M_Y(t) 
   = \exp(-2t/3) M_X(t/3)
   = \frac{2\exp\{(-t)/3\}}{3 - \exp(t/3)}.
\]
In practice, rather than identify $a$ and $b$ and remember Theorem \@ref(thm:MGFLinear), problems like this are best solved directly from the definition of the mgf:
\begin{align*}
   M_Y(t) 
    = \text{E}(\exp(tY))
   &= \text{E}(\exp\{t(X - 2)/3\})\\
   &= \text{E}(\exp\{tX/3 - 2t/3\})\\
   &= \exp(-2t/3) M_X(t/3) \\
   &= \frac{2\exp\{(-t)/3\}}{3 - \exp(t/3)}.
\end{align*}
:::


### Determining the distribution from the mgf {#DistributionFromMGF}

The mgf (if it exists) completely determines the distribution of a random variable hence, given a mgf, deducing the probability function should be possible.
For some distributions, the pdf cannot be written in closed form, but the mgf is relatively simple to write down.

In the discrete case, the mgf is defined as
\[
   M_X(t)  
   = \text{E}(\exp(tX)) 
   = \sum_X e^{tx} p_X(x)
\]
for $X$ discrete with pf $p_X(x)$.
This can be expressed as
\begin{align*}
   M_X(t)
   &= \exp(t x_1) p_X(x_1) + \exp(t x_2)p_X(x_2) + \dots\\
   &= \exp(t x_1) \Pr(X = x_1) + \exp(t x_2)\Pr(X = x_2) + \dots\\
\end{align*}
and so the probability function of $Y$ can be deduced from the mgf.


:::{.example #DistFromMGF name="Distribution from the mgf"}
Suppose a discrete random variable $D$ has the mgf
\[
   M_D(t) = \frac{1}{3} \exp(2t) + \frac{1}{6}\exp(3t) + \frac{1}{12}\exp(6t)
   + \frac{5}{12}\exp(7t).
\]
Then, by the definition of the mgf in the discrete case given above, the coefficient of $t$ in the exponential indicates values of $D$, and the coefficient indicates the probability of that value of $y$:
\begin{align*}
   M_D(t)
   &= \overbrace{\frac{1}{3} \exp(2t)}^{D = 2} + \overbrace{\frac{1}{6}\exp(3t)}^{D = 3} +
       \overbrace{\frac{1}{12}\exp(6t)}^{D = 6} + \overbrace{\frac{5}{12}\exp(7t)}^{D = 7}\\
   &= \Pr(D = 2)\exp(2t) + \Pr(D = 3)\exp(3t) + \\
   & \quad \Pr(D = 6)\exp(6t) + \Pr(D = 7)\exp(7t).
\end{align*}
So the pf is
\[
   p_D(d) =
   \begin{cases}
      1/3 & \text{for $d=2$}\\
      1/6 & \text{for $d=3$}\\
      1/12 & \text{for $d=6$}\\
      5/12 & \text{for $d=7$}\\
      0 & \text{otherwise}
   \end{cases}
\]
(Of course, it is easy to check by computing the mgf for $D$ from the pf found above; you should get the original mgf.)
:::


Sometimes, using the results in App. \@ref(UsefulSeries) can be helpful.


:::{.exercise #DistFromMGF2 name="Distribution from the mgf"}
Consider the mgf
\[
   M_X(t) = \frac{\exp(t)}{3 - 2\exp(t)}.
\]
To find the corresponding probability function, one approach is to write the mgf as
\[
   M_X(t) = \frac{\exp(t)/3}{1 - 2\exp(t)/3}.
\]
This is the sum of a geometric series \@ref(eq:SumGeometricInfinite):
\[
   a + ar + ar^2 + \ldots + ar^{n - 1}
   \rightarrow \frac{a}{1 - r} \text{ as $n  \rightarrow  \infty$},
\]
where $a = \exp(t)/3$ and $r = 2\exp(t)/3$.
Hence the mgf can be expressed as 
\[
   \frac{1}{3}\exp(t) + 
   \frac{1}{3}\left(\frac{2}{3}\right) \exp(2t) + 
   \frac{1}{3}\left(\frac{2}{3}\right)^2 \exp(3t) + \dots
\]
so that the probability function can be deduced as 
\begin{align*}
   \Pr(X = 1) &= \frac{1}{3};\\
   \Pr(X = 2) &= \frac{1}{3}\left(\frac{2}{3}\right);\\
   \Pr(X = 3) &= \frac{1}{3}\left(\frac{2}{3}\right)^2,
\end{align*}
or, in general,
\[
   p_x(x) = \frac{1}{3}\left( \frac{2}{3}\right)^{x - 1}\quad\text{for $x = 1, 2, 3, \dots$}.
\]
(Later, this will be identified as a [geometric distribution](#GeometricDistribution).)
::: 


The continuous case is not always easy.


CONTINUOUS.
TWEEDIE ONLY KOWN BY MGF


## Tchebysheff's inequality {#Tchebysheff}

Tchebysheff's inequality applies to any probability distribution, and is sometimes useful in theoretical work or to provide bounds on probabilities.


:::{.theorem #Tchebysheff name="Tchebysheff's theorem"}
Let $X$ be a random variable with finite mean $\mu$ and variance $\sigma^2$.
Then for any positive $k$,
\begin{equation}
   \Pr\big(|X - \mu| \geq k\sigma \big)\leq \frac{1}{k^2}
   (\#eq:Tchebysheff)
\end{equation}
or, equivalently
\begin{equation}
   \Pr\big(|X - \mu| < k\sigma \big)\geq 1 - \frac{1}{k^2}.
\end{equation}
:::

:::{.proof}
The proof for the continuous case only is given.
Let $X$ be continuous with pdf $f(x)$.
For some $c > 0$, then
\begin{align*}
     \sigma^2 
     & = \int^\infty_{-\infty} (x - \mu )^2f(x)\,dx\\
     & = \int^{\mu -\sqrt{c}}_{-\infty} (x - \mu )^2f(x)\, dx +
         \int^{\mu + \sqrt{c}}_{\mu-\sqrt{c}}(x - \mu )^2f(x)\,dx +
         \int^\infty_{\mu + \sqrt{c}}(x - \mu)^2f(x)\,dx\\
     & \geq \int^{\mu -\sqrt{c}}_{-\infty} (x - \mu )^2f(x)\,dx + 
       \int^\infty_{\mu + \sqrt{c}}(x - \mu )^2f(x)\,dx,
\end{align*}
since the second integral is non-negative.
Now $(x - \mu )^2 \geq c$ if $x \leq \mu -\sqrt{c}$ or $x\geq \mu + \sqrt{c}$.
So in both the remaining integrals above, replace $(x - \mu )^2$ by $c$ without altering the direction of the inequality:
\begin{align*}
     \sigma^2 
     &\geq  c \int^{\mu -\sqrt{c}}_{-\infty} f(x)\,dx + c\int^\infty_{\mu + \sqrt{c}}f(x)\,dx\\
     &=  c\,\Pr(X \leq \mu - \sqrt{c}\,) + c\,\Pr(X \geq \mu + \sqrt{c}\,)\\
     &=  c\,\Pr(|X - \mu| \geq \sqrt{c}\,).
\end{align*}
Putting $\sqrt{c} = k\sigma$, \@ref(eq:Tchebysheff) is obtained.
:::

With the probability function or pdf of a random variable $X$, then $\text{E}(X)$ and $\text{var}(X)$ can be found, but the converse is not true. 
That is, from a knowledge of $\text{E}(X)$ and $\text{var}(X)$ we cannot reconstruct the probability distribution of $X$ and hence cannot compute probabilities such as $\Pr(|X - \mu| \geq k\sigma)$.
Nonetheless, using Tchebysheff's inequality we can find a useful *bound* to either the probability outside or inside of $\mu \pm k\sigma$.



## Exercises


:::{.exercise #C3ContA}
The random variable $Y$ is defined as
\[
   f_Y(y) = \begin{cases}
               2y + k & \text{for $1\le y \le 2$};\\
               0      & \text{elsewhere}.
            \end{cases}
\]

1. Find a value for $k$.
2. Plot the pdf of $Y$.
3. Compute $\text{E}(Y)$.
4. Compute $\text{var}(Y)$.
5. Compute $\Pr(X > 1.5)$.
:::



:::{.exercise #C3DiscreteA}
The random variable $D$ is defined as
\[
   p_D(d) =
   \begin{cases}
      1/2 & \text{for $d = 1$};\\
      1/4 & \text{for $d = 2$};\\
      k   & \text{for $d = 3$}.\\
      0 & \text{otherwise}
   \end{cases}
\]

1. Find the value of $k$.
2. Compute the mean and variance of $D$.
3. Find the mgf for $D$.
4. Compute the mean and variance of $D$ from the mgf.
:::




:::{.exercise #C3DiscreteB}
The mgf of the discrete rv $Z$ is
\[
   M_Z(t) = [0.3\exp(t) + 0.7]^2.
\]

1. Compute the mean and variance of $Z$.
2. Find the pf of $Z$.
:::


:::{.exercise #C3MGFA}
The mgf of $G$ is $M_G(t) = (1 - \beta t)^{-\alpha}$. 
Find the mean and variance of $G$.
:::


:::{.exercise #C3Moments}
Suppose that the pdf of $X$ is
\[
   f_X(x) = \begin{cases}
               2(1 - x) & \text{for $0 < x < 1$};\\
               0 & \text{otherwise}.
            \end{cases}
\]

1. Find the $r$th raw moment of $X$.
2. Find $\text{E}\big((X + 3)^2\big)$ using the previous answer.
3. Find the variance of $X$.
:::



:::{.exercise #C3NoMean}
Consider the distribution
\[
  f_Y(y) = \frac{2}{y^2}\qquad y\ge 2.
\]

1. Show that the mean of the distribution is not defined.
2. Show that the variance does not exist.
3. Plot the probability density function over a suitable range.
4. Plot the distribution function over a suitable range.
5. Determine the median of the distribution.
6. Determine the interquartile range of the distribution.  
  (The interquartile range is a measure of spread, and is calculated as the difference between the third quartile and the first quartile. 
   The first quartile is the value below which 25% of the data lie; the third quartile is the value below which 75% of the data lie.)
7. Find $\Pr(Y > 4 \mid Y > 3)$.
:::



:::{.exercise #C3Cauchy}
The Cauchy distribution has the pdf
\[
   f = \frac{1}{\pi(1 + x^2)}\quad\text{for $x\in\mathbb{R}$}.
\]

1. Use **R** to draw the probability density function.
1. Compute the distribution function for $X$.
   Again, use **R** to draw the function.
1. Show that the mean of the Cauchy distribution is not defined.
1. Find the mean and the mode of the Cauchy distribution.
:::



:::{.exercise #C3Exponential}
The exponential distribution has the probability density function
\[
   f_Y(y) = \frac{1}{\lambda}\exp( -y/\lambda)
\]
(for $\lambda > 0$) for $y > 0$ and is zero elsewhere.

1. Determine the moment generating function of $Y$.
2. Use the moment generating function to compute the mean and variance of the exponential distribution.
:::


:::{.exercise #C3MGFSymmetric}
Prove that for a continuous random variable $X$ which has a distribution that is symmetric about 0 then $M_X(t) = M_{-X}(t)$.
Hence prove that for such a random variable, all odd moments about the origin are zero.
:::


:::{.exercise}
INFINITE MEAN?

* Cauchy: $f = \frac{1}{\pi(1+ x^2}$ for $x\in\mathbb{R}$
* Cont: $f = x^{-2}$. USED ABOVE
* Pareto distn: $f = \frac{\alpha}{x^{\alpha + 1}}$ for $\alpha > 0$. $E[X]$ DNE when $) < \alpha < 1$.
* Discrete $p = \frac{6}{\pi^2}\frac{1}{(n + 1)^2}$ for $n\ge 0$)
* Or St Peterburg paradox (https://math.stackexchange.com/questions/239288/infinite-expected-value-of-a-random-variable): 
  * Throw a coin until it lands tails.
  * You win $2n$ dollars, where $n$ is the number of heads.
  * The expected value funcion of your payment (let's name it $X$): $E[X] = \infty$

:::

