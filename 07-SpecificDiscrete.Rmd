# (PART) Standard univariate probability distributions {-}

# Standard discrete distributions {#DiscreteDistributions}


::: {.objectivesBox .objectives data-latex="{iconmonstr-target-4-240.png}"}
Upon completion of this chapter, you should be able to:

* recognise the probability functions and underlying parameters of uniform, Bernoulli, binomial, geometric, negative binomial, Poisson, and hypergeometric random variables.
* know the basic properties of the above discrete distributions.
* apply these discrete distributions as appropriate to problem solving.
:::


 


## Introduction

In this chapter, some popular discrete distributions are discussed.
Properties such as definitions and applications are considered.


## Discrete uniform distribution {#DiscreteUniform}

If a discrete random variable\ $X$ can assume\ $k$ different and distinct values with *equal* probability, then\ $X$ is said to have a *discrete uniform distribution*. 
This is one of the simplest discrete distributions.


:::{.definition #DiscreteUniform name="Discrete uniform distribution"}
If a random variable\ $X$ with range space $\{a, a + 1, a + 2, \dots, b\}$, where\ $a$ and\ $b$ ($a < b$) are integers, has the probability function
\begin{equation}
   p_X(x; a, b) = \frac{1}{b - a + 1}\text{ for $x = a, a + 1, \dots, b$}
   (\#eq:DiscreteUniform)
\end{equation}
then\ $X$ has a *discrete uniform distribution*.
We write $X\sim U(a, b)$ or $X\sim \text{Unif}(a, b)$.
:::


::: {.importantBox .important data-latex="{iconmonstr-warning-8-240.png}"}
The notation $p_X(x; a, b)$ means that the probability function for\ $X$ depends on the parameters\ $a$ and\ $b$.

The symbol $\sim$ means 'is distributed with'; hence, '$X\sim U(a, b)$' means '$X$\ is distributed with a discrete uniform distribution with parameters\ $a$ and\ $b$'.
:::


A plot of the probability function for a discrete uniform distribution is shown in Fig. \@ref(fig:DiscreteUnform).




```{r DiscreteUnform, echo=FALSE, fig.align="center", fig.cap="The probability function for the discrete uniform distribution $\\text{Unif}(a, b)$.", fig.width=5, fig.height=3.5, out.width='50%'}
x <- c(1:4, 7)
y <- rep(1, length(x))

plot(x = x,
     y = y,
     ylim = c(0, 1.3),
     type = "h",
     lwd = 2,
     axes = FALSE,
     lty = 3,
     col = "grey",
     main = "A discrete uniform distribution",
     xlab = expression(italic(x)),
     ylab = "Probability function",
     las = 1)
points(x = x,
       y = y,
       pch = 19)
       
# Ellipses
points( x = c(5.3, 5.5, 5.7),
        y = c(0.4, 0.4, 0.4),
        col = plotColour1,
        cex = 0.4,
        pch = 20) # Smaller dots than  pch = 19
points( x = c(5.3, 5.5, 5.7),
        y = c(0.8, 0.8, 0.8),
        col = plotColour1,
        cex = 0.4,
        pch = 20) # Smaller dots than  pch = 19



axis(side = 2,
     at = c(0, 1),
     labels = expression(0, 1/(italic(b) - italic(a) + 1) ) )
axis(side = 1,
     at = c(1, 2, 3, 4, 5.5, 7),
     labels = expression(
                 italic(a),
                 italic(a) + 1,
                 italic(a) + 2,
                 italic(a) + 3,
                 cdots,
                 italic(b) )
                 )
box()
```



:::{.definition #DiscreteUniformDF name="Discrete uniform distribution: distribution function"}
For a random variable\ $X$ with the uniform distribution given by the probability function in \@ref(eq:DiscreteUniform), the *distribution function* is
$$
   F_X(x; a, b) = 
   \begin{cases}
      0                                                        & \text{for $x < a$}\\
      \displaystyle \frac{\lfloor x\rfloor - a + 1}{b - a + 1} & \text{for $x = a, a + 1, \dots, b$}\\
      1                                                        & \text{for $x > b$}
   \end{cases}
$$
where $\lfloor z \rfloor$ is the *floor function*\index{Floor function} (i.e., round\ $z$ to the nearest integer in the direction of\ $-\infty$).
:::


:::{.example #DiscreteUniform name="Discrete uniform"}
Let\ $X$ be the number showing after a single throw of a fair die.
Then $X \sim \text{Unif}(1, 6)$.

To select a single-digit number from a table of random digits, the number chosen,\ $X$, has probability distribution $\text{Unif}(0, 9)$.
:::

The following are the basic properties of the discrete uniform distribution.

:::{.theorem #DiscreteUniformProperties name="Discrete uniform properties"}
If $X\sim \text{Unif}(a, b)$ then

1. $\displaystyle \operatorname{E}[X] = (a + b)/2$.
2. $\displaystyle \operatorname{var}[X] = \frac{(b - a)(b - a + 2)}{12}$.
3. $\displaystyle M_X(t) = \frac {e^{at} - e^{(b + 1)t}}{(b - a + 1)(1 - e^{t})}$.
:::

:::{.proof}
The mean and variance are easier to find by working with  $Y = X - a$ rather than\ $X$ itself (as\ $Y$ is defined on $0 < y < (b - a)$), using that $\operatorname{E}[X] = \operatorname{E}[Y] + a$ and $\operatorname{var}[Y] = \operatorname{var}[X]$.
Since $Y\sim\text{Unif}(0, b - a)$:
\begin{align*}
  \operatorname{E}[Y]
  &= \sum_{y = 0}^{b - a} i\frac{1}{b - a + 1}\\
  &= \frac{1}{b - a + 1}\big(0 + 1 + 2 + \dots + (b - a)\big)\\
  &= \frac{(b - a)(b - a + 1)}{2(b - a + 1)} = (b - a)/2
\end{align*}
using \@ref(eq:SumNaturalNumbers).
Therefore,
$$
   \operatorname{E}[X]= \operatorname{E}[Y] + a = \frac{b - a}{2} + a = \frac{a + b}{2}.
$$
The variance of\ $Y$ can be found similarly (Exercise \@ref(exr:DiscreteUniformVar)).

To find the MGF:
\begin{align*}
  M_X(t)
  &= \sum_{x = a}^{b} \exp(xt) \frac{1}{b - a + 1}\\
  &= \frac{1}{b - a + 1} \sum_{x = a}^{b} \exp(xt)\\
  &= \frac{1}{b - a + 1} \left( \exp\{at\} + \exp\{(a + 1)t\} + \exp\{(a + 2)t\} + \dots + \exp\{bt\} \right) \\
  &= \frac{\exp(at)}{b - a + 1} \left( 1 + \exp\{t\} + \exp\{2t\} + \dots + \exp\{(b - 1)t\} \right) \\
  &= \frac{\exp(at)}{b - a + 1} \left( \frac{1 - \exp\{(b - a + 1)t\}}{1 - \exp(t)} \right) 
\end{align*}
using \@ref(eq:SumGeometricFinite).
:::


## Bernoulli distribution {#BernoulliDistribution}

\index{Bernoulli distribution|(}
A Bernoulli distribution is used in a situation where a single trial of a random process has two possible outcomes.
A simple example is tossing a coin and observing if a head falls. 

The probability function is simple:
$$
   p_X(x) = 
   \begin{cases}
      1 - p & \text{if $x = 0$};\\
      p     & \text{if $x = 1$},
    \end{cases}
$$
so that\ $p$ represents the probability of $x = 1$, called a 'success' (while $x = 0$ is called a 'failure').
More succinctly:
\begin{equation}
   p_X(x; p) = p^x (1 - p)^{1 - x} \quad\text{for $x = 0, 1$}.
   (\#eq:BernoulliPMF)
\end{equation}


:::{.definition #BrnoulliDistribution name="Bernoulli distribution"}
Let\ $X$ be the number of successes in a single trial with $\Pr(\text{Success}) = p$ ($0\le p\le 1$).
Then\ $X$ has a *Bernoulli probability distribution* with parameter\ $p$ and probability function given by \@ref(eq:BernoulliPMF).
We write $X\sim\text{Bern}(p)$.
:::


:::{.definition #BrnoulliDistributionCDF name="Bernoulli distribution: distribution function"}
For a random variable\ $X$ with the Bernoulli distribution given in \@ref(eq:BernoulliPMF), the *distribution function* is
$$
  F_X(x; p) = 
  \begin{cases}
    0     & \text{if $x < 0$}\\
    1 - p & \text{if $0\leq x < 1$}\\
    1     & \text{if $x\geq 1$}.
  \end{cases}
$$
:::




::: {.importantBox .important data-latex="{iconmonstr-warning-8-240.png}"}
The terms 'success' and 'failure' are not literal.
'Success' simply refers to the event of interest.
If the event of interest is whether a cyclone causes damage, this is still called a 'success'.
:::


These ideas also introduces a common idea of a *Bernoulli trial*.


:::{.definition #BernoulliTrials name="Bernoulli trials"}
A *Bernoulli trial* is a random process with only two possible outcomes, usually labelled 'success'\ $s$ and 'failure'\ $f$.
The sample space can be denoted by $S = \{ s, f\}$.
:::


The following are the basic properties of the Bernoulli distribution.

:::{.theorem #BernoulliProperties name="Bernoulli distribution properties"}
If $X\sim\text{Bern}(p)$ then

1. $\operatorname{E}[X] = p$.
2. $\operatorname{var}[X] = p(1 - p) = pq$ where $q = 1 - p$.
3. $M_X(t) = pe^t + q$.
:::

:::{.proof}
By definition:
$$
     \operatorname{E}[X] = \sum^1_{x = 0} x\, p_X(x) = 0\times (1 - p) + 1\times p = p.
$$
To find the variance, use the computational formula $\operatorname{var}[X] = \operatorname{E}[X^2] - \operatorname{E}[X]^2$.
Then,
$$
     \operatorname{E}[X^2] = \sum^1_{x = 0} x^2\, p_X(x) = 0^2\times (1 - p) + 1^2\times p = p,
$$
and so
$$ 
   \operatorname{var}[X] 
   = \operatorname{E}[X^2] - \operatorname{E}[X]^2 
   = p - p^2
   = p (1- p). 
$$

The MGF of\ $X$ is
\begin{align*}
   M_X(t)
   &= \operatorname{E}\big[\exp(tX)\big]\\
   &= \sum^1_{x = 0} e^{tx} p^x q^{1 - x}\\
   &= \sum^1_{x = 0}(pe^t)^x q^{1 - x}
    = pe^t + q.
\end{align*}
Proving the third result first, and then using it to prove the others, is easier (using of the methods in Sect.\ \@ref(MGFMoments)---try this as an exercise.)
:::
\index{Bernoulli distribution|)}



## Binomial distribution {#BinomialDistribution}

\index{Binomial distribution|(}
A binomial distribution is used to model the number of successes in\ $n$ independent Bernoulli trials (Def. \@ref(def:BernoulliTrials)).
A simple example is tossing a coin ten times and observing how often a head falls. 
The same random process is repeated (tossing the coin), only two outcomes are possible on each trial (a head or a tail), and the probability of a head remains constant on each trial (i.e., the tosses are independent).

### Derivation of a binomial distribution {#BinomialDerivation}

Consider tossing a die five times and observing the number of times a `r knitr::include_graphics("Dice/die2.png", dpi=1750)` is rolled.
The probability of observing a `r knitr::include_graphics("Dice/die2.png", dpi=1750)` three times can be found as follows:
In the five tosses, a `r knitr::include_graphics("Dice/die2.png", dpi=1750)` must appear three times; there are $\binom{5}{3}$ ways of allocating on which of the five rolls they will appear.
In the five rolls, `r knitr::include_graphics("Dice/die2.png", dpi=1750)` must appear three times with probability\ $1/6$; the other two rolls must produce another number, with probability\ $5/6$.
So the probability is
$$
   \Pr(\text{3 ones}) = \binom{5}{3} (1/6)^3 (5/6)^2 = 0.032,
$$
assuming independence of the events.
Using this approach, the PMF for the binomial distribution can be developed.


A binomial situation arises if a *sequence* of Bernoulli trials is observed, in each of which $\Pr(\{ s\} ) = p$ and $\Pr(\{ f\} ) = q = 1 - p$.
For\ $n$ such trials, consider the random variable\ $X$, where\ $X$ is the number of successes in $n$\ trials. 
Now\ $X$ will have value set $\mathcal{R}_X = \{ 0, 1, 2, \dots, n\}$.
$p$\ must be constant from trial to trial, and the $n$\ trials must be independent.

Consider the event $X = r$ (where $0\leq r\leq n$). 
This could correspond to the sample point
$$
   \underbrace{s \quad s \quad s \dots s \quad s \quad s \quad s}_r\quad
   \underbrace{f \quad f \dots f \quad f}_{n - r} 
$$
which is the intersection of\ $n$ independent events comprising $r$\ successes and $n - r$ failures, and hence the probability is $p^r q^{n - r}$.

Every other sample point in the event $X = r$ will appear as a rearrangement of these\ $s$'s and\ $f$'s in the sample point described above and will therefore have the same probability. 

Now the number of distinct arrangements of the\ $r$ successes\ $s$ and $(n - r)$ failures\ $f$ is $\binom{n}{r}$, so
$$
     \Pr(X = r) = \binom{n}{r} p^r q^{n - r}
$$
for $r = 0, 1, \dots, n$.
This is the *binomial distribution*.

Note that the sum of the probabilities is\ $1$, as the binomial expansion of $(p + q)^n$ (using \@ref(eq:BinomialSeries)) is
\begin{equation}
   \sum_{r = 0}^n \binom{n}{r} p^r q^{n - r} = (p + q)^n = 1\label{EQN:sumbin}
\end{equation}
since $p + q = 1$. 


### Definition and properties {#BinomialDefinition}

The definition of the binomial distribution can now be given.

:::{.definition #BinomialDistribution name="Binomial distribution"}
Let $X$ be the number of successes in $n$\ independent Bernoulli trials with $\Pr(\text{Success}) = p$ (for $0\le p\le 1$) constant for each trial.
Then\ $X$ has a *binomial probability distribution* with parameters\ $n$, $p$ and probability function given by
\begin{equation}
   p_X(x; n, p) = \binom{n}{x} p^x q^{n - x} \quad\text{for $x = 0, 1, \dots, n$}.
   (\#eq:BinomialPMF)
\end{equation}
We write $X\sim\text{Bin}(n, p)$.
:::


The distribution function is complicated and is not given.
Fig. \@ref(fig:BinomialExamples) shows the probability function for the binomial distribution for various parameter values.


```{r BinomialExamples, echo=FALSE, fig.align="center", fig.cap="The probability function for the binomial distribution for various values of\\ $p$ and\\ $n$.", fig.height = 7, fig.width=7, out.width='80%'}
par( mfrow = c(2, 2))

n <- c(8, 8, 
       15, 15)
prob <- c(0.2, 0.6, 
          0.2, 0.6)

for (i in (1:4)) {
  ni <- n[i]
  probi <- prob[i]
  x <-  0:ni
  y <- dbinom(x, 
              size = ni,
              prob = probi)
  
   plot( x = x,
         y = y,
         xlab = expression(italic(x)),
         ylab = "Probability function",
         main = bquote("Binomial: " ~ italic(n) == .(ni) ~ " and " ~ italic(p) == .(probi)),
         las = 1,
         lty = 3,
         lwd = 2,
         col = "grey",
         type = "h")
  points(x = x,
         y = y,
         pch = 19)
}
```


The following are the basic properties of the binomial distribution.


:::{.theorem #BinomialProperties name="Binomial distribution properties"}
If $X\sim\text{Bin}(n,p)$ then

1. $\operatorname{E}[X] = np$.
2. $\operatorname{var}[X] = np(1 - p) = npq$.
3. $M_X(t) = (pe^t + q)^n$.
:::

:::{.proof}
Using \@ref(eq:BinomialSeries):
\begin{align*}
     \operatorname{E}[X] 
     & = \sum^n_{x = 0} x\binom{n}{x} p^x q^{n - x}\\
     & = \sum^n_{x = 1} x \frac{n}{x} \binom{n - 1}{x - 1} p^x q^{n - x} 
          \quad\text{(note the lower summation-index change)}\\
     & = np\sum^n_{x = 1} \binom{n - 1}{x - 1} p^{x - 1} q^{n - x}\\
     & = np \sum^{n - 1}_{y = 0} \binom{n - 1}{y}p^y q^{n - 1 - y}\quad \text{putting $y = x - 1$}.
\end{align*}
The summation is\ $1$, since it is equivalent to summing over all values of\ $y$ for the binomial probability function with $y$\ successes in $(n - 1)$ Bernoulli trials, and hence represents a probability function with a sum of one.
In the second line, the sum is over\ $x$ from\ $1$ to\ $n$ because, for $x = 0$, the probability is multiplied by $x = 0$ and makes no contribution to the summation. 
Thus,
$$
   \operatorname{E}[X] = np.
$$

To find the variance, use the computational formula $\operatorname{var}[X] = \operatorname{E}[X^2] - \operatorname{E}[X]^2$.
Firstly, to find $\operatorname{E}[X^2]$, write $\operatorname{E}[X^2]$ as $\operatorname{E}[X(X - 1) + X]$ so that $\operatorname{E}[X(X - 1)] + \operatorname{E}[X]$; then:
\begin{align*}
     \operatorname{E}[X^2] 
     &= \sum^n_{x = 0} x(x - 1)\Pr(X = x) + np\\
     &= np + \sum^n_{x = 2} x(x  -1)\frac{n(n - 1)}{x(x - 1)} \binom{n - 2}{x - 2} p^x q^{n - x}\\
     &= np + \sum^n_{x = 2} n(n - 1)\binom{n - 2}{x - 2} p^x q^{n - x}\\
     &= np + n(n - 1)p^2 \sum^{n - 2}_{y = 0} \binom{n - 2}{y} p^yq^{n - 2 - y},
\end{align*}
putting $y = x - 2$.
For the same reason as before, the summation is\ $1$, so
$$
   \operatorname{E}[X^2] = np + n^2 p^2 - np^2 
$$
and hence
$$ 
   \operatorname{var}[X]
   = \operatorname{E}[X^2] - ([\operatorname{E}[X])^2 
   = np + n^2p^2 - np^2 - n^2 p^2
   = np (1 - p). 
$$

The MGF of\ $X$ is
\begin{align*}
   M_X(t)
   &= \operatorname{E}\big(\exp(tX)\big)\\
   &= \sum^n_{x = 0} e^{tx}\binom{n}{x} p^x q^{n - x}\\
   &= \sum^n_{x = 0}\binom{n}{x} (pe^t)^x q^{n - x}
    = (pe^t + q)^n.
\end{align*}
Proving the third result first, and then using it to prove the others, is easier (using of the methods in Sect. \@ref(MGFMoments)---try this as an exercise.)
:::


Tables of binomial probabilities are commonly available, but computers (e.g., using **R**) can also be used to generate the probabilities.
If the number of 'successes' has a binomial distribution, so does the number of 'failures'.
Specifically if $X\sim \text{Bin}(n,p)$, then $Y = (n - X) \sim \text{Bin}(n, 1 - p)$.


::: {.softwareBox .software data-latex="{iconmonstr-laptop-4-240.png}"}
In **R**, many functions are built-in for computing the probability function, distribution function and other quantities for common distributions (see Appendix\ \@ref(UseRDistributions)).

The four **R** functions for working with the binomial distribution have the form `[dpqr]binom(..., size, prob)`, where `prob`${} = p$ and `size`${} = n$.
For example:

* The function `dbinom(x, size, prob)` computes the probability function for the binomial distribution at $X = {}$`x`;
* The function `pbinom(q, size, prob)` computes the distribution function for the binomial distribution at $X = {}$`q`;
* The function `qbinom(p, size, prob)` computes the quantiles of the binomial distribution function for cumulative probability `p`; and
* The function `rbinom(n, size, prob)` generates `n` random numbers from the given binomial distribution.
:::



:::{.example #ThrowDice name="Throwing dice"}
A die is thrown $4$\ times. 
To find the probability of rolling exactly $2$\ sixes, see that there are $n = 4$ Bernoulli trials with $p = 1/6$.
Let the random variable\ $X$ be the number of 6's in $4$\ tosses. 
Then
$$
  \Pr(X = 2) = 
  \binom{4}{2} \left(\frac{1}{6}\right)^2\left(\frac{5}{6}\right)^2 = 150/1296 \approx 0.1157.
$$
In **R**:

```{r}
dbinom(2,
       prob = 1/6,
       size = 4)
```
:::



::: {.importantBox .important data-latex="{iconmonstr-warning-8-240.png}"}
A binomial situation requires the trials to be *independent*, and the probability of success $p$ to be *constant* throughout the trials.

For example, drawing cards from a pack *without* replacing them is **not** a binomial situation; after drawing one card, the probabilities will then change for the drawing of the next card.
In this case, the *hypergeometric distribution* should be used (Sect. \@ref(HypergeometricDistribution)).
:::


:::{.example name="Freezing lake"}
Based on @BIB:Wilks:statistical (p.\ 68), the probability that Cayuga Lake freezes can be modelled by a binomial distribution with $p = 0.05$.

Using this information, the number of times the lake will *not* freeze in $n = 10$ randomly chosen years in given by the random variable\ $X$, where $X \sim \text{Bin}(10, 0.95)$.
The probability that the lake will not freeze in any of these ten years is $\Pr(X = 10) = \binom{10}{10} 0.95^{10} 0.05^{0} \approx 0.599$, or about 60%.

In **R**:
```{r}
dbinom(0,
       size = 10,
       prob = 0.05)
```

Note we could define the random variable\ $Y$ as the number of times the lake *will* freeze in the ten randomly chosen years.
Then, $Y\sim\text{Bin}(10, 0.05)$ and we would compute $\Pr(Y = 0)$ and get the same answer.
```{r}
dbinom(10,
       size = 10,
       prob = 0.95)
```
:::


Binomial probabilities can sometimes be approximated using the normal distribution (Sect. \@ref(NormalApproxBinomial)) or the Poisson distribution (Sect. \@ref(PoissonBinomial)).
\index{Binomial distribution|)}


## Geometric distribution {#GeometricDistribution}

### Derivation of a geometric distribution {#GeometricDerivation}

\index{Geometric distribution|(}
Consider now a random process where independent Bernoulli trials  (Def.\ \@ref(def:BernoulliTrials)) are repeated until the *first* success occurs. 
What is the distribution of the number of *failures* until the first success is observed?

Let the random variable\ $X$ be the number of failures before the first success is observed.
Since the first success may occur on the first trial, or second trial or third trial, and so on, $X$\ is a random variable with range space $\{0, 1, 2, 3, \dots\}$ with no (theoretical) upper limit.

Since the probability of failure is $q = 1 - p$ and the probability of success is\ $p$, the probability of $x$\ failures before the first success is
\begin{align*}
   \Pr(\text{$x$ failures})          &\times \Pr(\text{first success}) \\
   q^x                               &\times  p.
\end{align*}
This derivation assumes the events are independent.


### Definition and properties {#GeometricDefinition}

The definition of the geometric distribution can now be given.

:::{.definition #GeometricDistribution name="Geometric distribution"}
A random variable\ $X$ has a *geometric distribution* if the probability function of\ $X$ is
\begin{equation}
   p_X(x; p) =  (1 - p)^x p = q^x p\quad\text{for $x = 0, 1, 2, \dots$}
   (\#eq:GeometricPMF)
\end{equation}
where $q = 1 - p$ and $0 < p < 1$ is the parameter of the distribution.
We write $X\sim\text{Geom}(p)$.
:::


:::{.definition #GeometricDistributionDF name="Geometric distribution: distribution function"}
For a random variable\ $X$ with the geometric distribution given in \@ref(eq:GeometricPMF), the distribution function is
$$
  F_X(x; p) = 
  \begin{cases}
    0                                  & \text{for $x < 0$}\\
    1 - (1 - p)^{\lfloor x\rfloor + 1} & \text{for $x\ge 0$},
  \end{cases}
$$
where $\lfloor z \rfloor$ is the *floor function*.\index{Floor function}
:::



This is the parameterisation used by **R**.
The probability function for a geometric distribution for various values of\ $p$ is shown in Fig.\ \@ref(fig:Geometric).
The following are the basic properties of the geometric distribution.

:::{.theorem #GeometricProperties name="Geometric distribution properties"}
If $X\sim\text{Geom}(p)$ as defined in Eq.\ \@ref(eq:GeometricPMF), then

1. $\operatorname{E}[X] = (1 - p)/p$.
2. $\operatorname{var}[X] = (1 - p)/p^2$.
3. $M_X(t) = p/\{1 - (1 - p)e^t\}$ for $t < -\log(1 - p)$.
:::

:::{.proof}
The first two result can be proven directly but proving the third result, and using the MGF to prove the first two, is easier.
This is left as an exercise (Ex.\ \@ref(exr:GeometricPropertiesProof)).
:::



::: {.softwareBox .software data-latex="{iconmonstr-laptop-4-240.png}"}
The four **R** functions for working with the geometric distribution have the form `[dpqr]geom(prob)`, where `prob`${} = p$ (see Appendix \@ref(UseRDistributions)).
:::






```{r Geometric, echo=FALSE, fig.align="center", fig.cap="The probability function for the geometric distribution for $p = 0.1$, $0.3$, $0.5$ and $0.8$.", fig.height = 7, fig.width=7, out.width='80%'}
par( mfrow = c(2, 2))

xmax <- c(15, 15, 15, 15)
p <- c(0.1, 0.3, 
       0.5, 0.8)

for (i in (1:4)) {
  pValue <- p[i]
  x <-  0:xmax[i]
  y <- dgeom(x,
             prob = pValue)
  
   plot( x = x,
         y = y,
         xlim = c(0, xmax[i]),
         xlab = expression(italic(x)),
         ylab = "Probability function",
         main = bquote("Geometric: " ~ italic(p) == .(pValue) ),
         las = 1,
         ylim = c(0, max(y)),
         col = "grey",
         lty = 3,
         lwd = 2,
         type = "h")
  points(x = x,
         y = y,
         pch = 19)
  
}

```



:::{.example #GeometricNetball name="Netball shooting"}
Suppose a netball goal shooter has a probability of $p = 0.2$ of *missing* a goal.
In this case a 'success' refers to *missing* the shot with $p = 0.2$

Let\ $X$ be the number of shots made till the first miss.
Then, $X = \{1, 2, 3, \dots \}$, since the first miss may occur on the first shot, or any subsequent shot.
*This is **not** the parameterisation used by **R***.

Instead, let\ $Y$ be the number of goals *scored* until the shooter first *misses*.
Notice that $Y = \{0, 1, 2, \dots \}$ so this parameterisation *does* correspond to that used by **R**, and $Y\sim \text{Geom}(0.2)$.

The probability that the shooter makes\ $4$ goals before the first miss is:
```{r}
dgeom(4, prob = 0.2)
```
The probability that the shooter makes\ $4$ or fewer goals before the first miss is $\Pr(Y = 0, 1, 2, 3, \text{or } 4)$:
```{r}
pgeom(4, prob = 0.2)
```
The expected number of shots until the first miss is $\operatorname{E}[Y] = (1 - p)/p  = 4$.
:::


The parameterisation above is for the *number of failures* needed for the first success, so that $x = 0, 1, 2, 3, \dots$.
An alternative parameterisation is for the *number of trials* $X$ before observing a success, when $x = 1, 2, \dots$.
The only way to distinguish which parameterisation is being used is to check the range space or the probability function.


:::{.definition #GeometricDistributionALT name="Geometric distribution: Alternative parameterisation"}
A random variable\ $X$ has a *geometric distribution* if the probability function of\ $X$ is
\begin{equation}
   p_X(x) =  (1 - p)^{x - 1}p = q^{x - 1} p\quad\text{for $x = 1, 2, \dots$}
   (\#eq:GeometricPMFAlternative)
\end{equation}
where $q = 1 - p$ and $0 < p < 1$ is the parameter of the distribution.
We write $X\sim\text{Geom}(p)$.
:::


:::{.theorem #GeometricPropertiesALT name="Geometric distribution properties"}
If $X\sim\text{Geom}(p)$ as defined in Eq.\ \@ref(eq:GeometricPMFAlternative), then

1. $\operatorname{E}[X] = 1/p$.
2. $\operatorname{var}[X] = (1 - p)/p^2$.
3. $M_X(t) = p\exp(t) /\{ 1 - (1 - p)e^t \}$ for $t < -\log(1 - p)$.
:::

:::{.proof}
The first two result can be proven from Theorem \@ref(thm:GeometricProperties).
:::
\index{Geometric distribution|)}


## Negative binomial distribution {#NegativeBinomialDistribution}

### Derivation: standard parameterisation {#NegBinStandard}

\index{Negative binomial distribution|(}
Consider a random process where independent Bernoulli trials are repeated until the $r$th\ success occurs. 
Let the random variable\ $X$ be the number of *failures* before the $r$th success is observed, so that $X = 0, 1, 2, \dots$.

To observe the $r$th\ success after $x$\ failures, $x$\ failures and $r - 1$ successes are observed first in these $x + r - 1$ trials.
There are $\binom{x + r - 1}{r - 1}$ ways in which to allocate these successes to the first $x + r - 1$ trials.
Each of the $r - 1$ successes occur with probability $p$, and the $x$ failures with probability $1 - p$ (assuming events are independent).
Hence the probability of observing the $r$th\ success in trial $x$ is
\begin{align*}
   \text{No. ways}
   &\times \Pr\big(\text{$x$ failures}\big) \times \Pr(\text{$r - 1$ successes}) \\
   \binom{x + r - 1}{r - 1} 
   &\times (1 - p)^{x} \times p^{r - 1}.
\end{align*}


### Definition and properties: standard parameterisation {#NegBinAlternative}

The standard definition of the negative binomial distribution can now be given.

:::{.definition #NegativeBinomialDistribution name="Negative binomial distribution"}
A random variable\ $X$ with probability function
\begin{equation}
   p_X(x; p, r) = \binom{x + r - 1}{r - 1}(1 - p)^{x} p^{r - 1}
 \quad\text{for $x = 0, 1, 2, \dots$}
   (\#eq:NegativeBinomialPMF)
\end{equation}
has a *negative binomial distribution* with parameters\ $r$ (an positive integer) and\ $p$ ($0\le p\le 1$).
We write $X\sim\text{NB}(r, p)$.
:::

The distribution function is complicated and is not given.
The following are the basic properties of the negative binomial distribution.


:::{.theorem #NegBinomialProperties name="Negative binomial properties"}
If $X\sim \text{NB}(r, p)$ with the probability function in Eq.\ \@ref(eq:NegativeBinomialPMF) then

1. $\operatorname{E}[X] = \{r(1 - p)\}/p$.
2. $\operatorname{var}[X] = r(1 - p)/p^2$.
3. $M_X(t) = \left[ p / \{1 - (1 - p)\exp(t)\} \right]^r$ for $t < -\log p$.
:::

:::{.proof}
Proving the third statement is left as an exercise.
Then the first two are then derived from the MGF.
:::


:::{.example #Demonstration name="Negative binomial"}
 A telephone marketer invites customers, over the phone, to a product demonstration. 
Ten people are needed for the demonstration. 
The probability that a randomly-chosen person accepts the invitation is only\ $0.15$.

Here, a ‘success’ is an acceptance to attend the demonstration. 
Let\ $Y$ be the number of *failed* calls necessary to secure ten acceptances.
Then\ $Y$ has a negative binomial distribution with $p = 0.15$ and $r = 10$.
The mean number of failures made will is $\operatorname{E}[Y] = \{r(1 - p)\}/p = \{10 \times (1 - 0.15)\}/0.15 \approx  56.66667$.

Hence, including the ten calls that leads to a person accepting the invitation, the mean number of calls to be made will is $10 + \operatorname{E}[Y] = 66.66\dots$.
:::


Since the parameterisation in \@ref(eq:NegativeBinomialPMF) is defined over the non-negative integers, it is often used to model count data.


::: {.softwareBox .software data-latex="{iconmonstr-laptop-4-240.png}"}
The four **R** functions for working with the negative binomial distribution are based on the paramaterisation in Eq.\ \@ref(eq:NegativeBinomialPMF), and have the form `[dpqr]nbinom(size, prob)`, where `prob` is\ $p$ and `size` is\ $r$ (see Appendix \@ref(UseRDistributions)).
:::



::: {.linkBox .link data-latex="{iconmonstr-link-1-240.png}"}
When $r = 1$, the negative binomial distribution is the same as the geometric distribution: the geometric distribution is a special case of the negative binomial.
:::


The probability function for the negative binomial distribution for various values of\ $p$ and\ $r$ is shown in Fig. \@ref(fig:NegativeBinomial).


```{r NegativeBinomial, echo=FALSE, fig.align="center", fig.cap="The probability function for the negative binomial distribution for $p = 0.2$ and $0.7$ and $r = 1$ and $r = 3$.", fig.height = 7, fig.width=7, out.width='80%'}
par( mfrow = c(2, 2))
xmax <- 15

r <- c(1, 3, 
       1, 3)
p <- c(0.2, 0.2, 
       0.6, 0.6)

for (i in (1:4)) {
  rValue <- r[i]
  pValue <- p[i]
  
  x <-  0:xmax
  y <- dnbinom(x, 
              size = r[i],
              prob = p[i])
  
   plot( x = x,
         y = y,
         xlim = c(0, xmax),
         xlab = expression(italic(x)),
         ylab = "Probability",
         main = bquote("Negative binomial: " ~ italic(r) == ~ .(rValue) ~ ";" ~ italic(p) == .(pValue) ),
         las = 1,
         lty = 3,
         lwd = 2,
         col = "grey",
         type = "h")
  points(x = x,
         y = y,
         pch = 19)
  
}

```



:::{.example #DemonstrationNB name="Negative binomial"}
Consider Example \@ref(exm:Demonstration), concerning a telephone marketer inviting customers, over the phone, to a product demonstration.
Ten people are needed for the demonstration. 
The probability that a randomly chosen person accepts the invitation is only\ $0.15$.

Consider finding the probability that the marketer will need to make more than $100$\ calls to secure ten acceptances.
In this situation, a 'success' is an acceptance to attend the demonstration.
Let\ $Y$ be the number of failed calls before securing ten acceptances.
Then\ $Y$ has a negative binomial distribution such that $Y\sim\text{NBin}(p = 0.15, r = 10)$.

To determine $\Pr(Y > 100) = 1 - \Pr(Y\le 100)$, using a computer is the easiest approach. 
In **R**, the command `dnbinom()` returns probabilities from the probability function of the negative binomial distribution, and `pnbinom()` returns the cumulative distribution probabilities:


```{r}
x.values <- seq(1, 100, 
                by = 1)
1 - sum(dnbinom( x.values, 
                 size = 10, 
                 prob = 0.15))
# Alternatively:
1 - pnbinom(100, 
            size = 10, 
            prob = 0.15)
```

The probability is about\ $0.0244$.

Assuming each call take an average of\ $5$ minutes, we can determine how long is the marketer expected to be calling to find ten acceptances.
Let\ $T$ be the time to make the calls in minutes.
Then $T = 5Y$.
Hence, $\operatorname{E}[T] = 5\operatorname{E}[Y] = 5 \times 66.7 = 333.5$, or about $5.56\hs$.

Assume that each call costs $25$\ cents, and that the company pays the marketer \$$30$\ per hour.
To determine the total cost, let\ $C$ be the total cost in dollars.
The cost of employing the marketer is, on average, $30\times 5.56 = \$166.75$.
Then $C = 0.25 Y + 166.75$, so $\operatorname{E}[C] = 0.25 \operatorname{E}[Y] + 166.75 = C = 0.25 \times 66.7 + 166.75 = \$183.43$.
:::


### Alternative parameterisations

\index{Negative binomial distribution!alternative parameterisation}
Often, a different parameterisation for the negative binomial distribution; see Exercise \@ref(exr:NegativeBinomialALT).
However, the parameterisation presented in Sect.\ \@ref(NegBinStandard) is used by **R**.

The negative binomial distribution can be extended so that\ $r$ can be *any* positive number, not just an integer.
When\ $r$ is non-integer, the above interpretations are lost, but the distribution is more flexible.
Relaxing the restriction on\ $r$ gives the probability function as
\begin{equation}
   p_X(x; p, r) = \frac{\Gamma(x + r)}{\Gamma(r)\, x!} p^r (1 - p)^x,
   (\#eq:NegativeBinomialPMFAlternative2)
\end{equation}
for $x = 0, 1, 2 \dots$ and $r > 0$.
In this expression, $\Gamma(r)$ is the *gamma function* (Def.\ \@ref(def:GammaFunction)), and is related to factorials when\ $r$ is integer: $\Gamma(r) = (r - 1)!$ if\ $r$ is a positive integer.


:::{.definition #GammaFunction name="Gamma function"}
The function $\Gamma(\cdot)$ is called the *gamma function* and is defined as\index{Gamma function}
$$
   \Gamma(r) = \int_0^\infty x^{r - 1}\exp(-x)\, dx
$$
for $r > 0$.
:::

The gamma function has the property that
\begin{equation}
   \Gamma(r) = (r - 1)!
   (\#eq:gammaprop)
\end{equation}
if\ $r$ is a positive integer (Fig. \@ref(fig:GammaFunction)).
Important properties of the gamma function are given below.


:::{.theorem #GammaFunctionProperties name="Gamma function properties"}
For the gamma function $\Gamma(\cdot)$,

1. $\Gamma(r) = (r - 1)\Gamma(r - 1)$ if $r > 0$.
1. $\Gamma(1/2) = \sqrt{\pi}$.
1. $\lim_{r\to 0} \to \infty$.
1. $\Gamma(1) = \Gamma(2) = 1$.
1. $\Gamma(n) = (n - 1)$ for all positive integers $n$.
:::

:::{.proof}
For the first property, integration by parts gives
\begin{align*}
   \Gamma(r)
   &= \left. -\exp(-x) \frac{x^2}{r}\right|_0^\infty + \int_0^\infty \exp(-x) (r - 1) x^{r - 2}\,dx\\
   &= 0 + \ (r - 1) \int_0^{\infty}  e^{-x}  x^{r - 2} \, dx\\
   &= (r - 1) \Gamma(r - 1).
\end{align*}
:::


```{r GammaFunction, echo=FALSE, fig.cap="The gamma function is like the factorial function but has a continuous argument. The line corresponds to the gamma function $\\Gamma(z)$; the solid points correspond to the factorial $(z - 1)! = \\Gamma(z)$ for integer $z$.", fig.align="center", fig.width=6.5, fig.height=3.75, out.width='75%'}

xPoints <- 1:5
xCont <- seq(0.1, 5.1, 
             length = 100)

plot( x = c(0, 5.3),
      y = c(0, 25),
      type = "n",
      las = 1,
      ylim = c(0, 30),
      xlab = expression(italic(z)),
      ylab = bquote("Gamma function" ~ Gamma(z)),
      main = expression(paste("The Gamma function"~Gamma(italic(z)))))
lines( x = xCont,
       y = gamma(xCont),
       lwd = 2)
points(x = xPoints,
       y = gamma(xPoints),
       pch = 19)
legend("top",
       pch = c(NA, 19),
       lty = c(1, NA),
       bty = "n",
       lwd = c(2, NA),
       legend = c(bquote("Gamma function" ~ Gamma(italic(z))),
                  expression(paste( group("(", list(italic(z) - 1),")"), "!" ) ) ) )

text(x = xPoints,
     y = gamma(xPoints),
     pos = c(3, 3, 3, 3, 2),
     labels = c("0!",
                "1!",
                "2!",
                "3!",
                "4!") )
```

:::{.example #Computing name="Negative binomial"}
Consider a computer system that fails with probability $p = 0.02$ on any given day (so that the system failure is a 'success').
Suppose after five failures, the system is upgraded.

To find the probability that an upgrade will happen within one year, let\ $D$ be the number of days before the fifth failure.
So, we seek $\Pr(X < 360)$.
Using **R**:

```{r echo=TRUE}
pnbinom(360, 
        size = 5, 
        prob = 0.02)
```
So the probability of upgrading within one year is about $85$%.
:::


:::{.example #NBMites name="Mites"}
@BIB:Bliss:negbin gives data from the counts of adult European red mites on leaves selected at random from six similar apple trees (Table\ \@ref(tab:MitesTable)).
The mean number of mites per leaf is\ $1.14$ with a variance of\ $3.57$; since the Poisson distribution has an equal mean and variance, the Poisson distribution may not model these count data well.
However, the mean number of mites per leaf can be modelled using a negative binomial distribution with $r = 1.18$ and $p = 0.5$.

The estimated probability function for both the Poisson and negative binomial distributions are given in Table\ \@ref(tab:MitesTable); the negative binomial distribution fits better as expected.
:::


```{r MitesTable, echo=FALSE}
MitesTab <- array( dim = c(9, 5))

MitesTab[1, ] <- c(0 ,  70 ,  0.467 ,  0.318 ,  0.449)
MitesTab[2, ] <- c(1 ,  38 ,  0.253 ,  0.364 ,  0.261)
MitesTab[3, ] <- c(2 ,  17 ,  0.113 ,  0.209 ,  "0.140")
MitesTab[4, ] <- c(3 ,  10 ,  0.067 ,  "0.080" ,  0.073)
MitesTab[5, ] <- c(4 ,  9 ,  "0.060" ,  0.023 ,  0.038)
MitesTab[6, ] <- c(5 ,  3 ,  "0.020" ,  0.005 ,  0.019)
MitesTab[7, ] <- c(6 ,  2 ,  0.013 ,  0.001 ,  "0.010")
MitesTab[8, ] <- c(7 ,  1 ,  0.007 ,  "0.000" ,  0.005)
MitesTab[9, ] <- c("8+" ,  0 ,  "0.000" ,  "0.000" ,  "0.000")

MitesTab.caption <- "Counts of mites on leaves selected at random from six similar apple trees."

if( knitr::is_latex_output() ) {
  knitr::kable(MitesTab,
               format = "latex",
               booktabs = TRUE,
               longtable = FALSE,
               align = "c",
               linesep = c("", "", "\\addlinespace"),
               col.names = c("mites per leaf", 
                             "with that many mites",
                             "probability",
                             "probability",
                             "probability"),
               caption = MitesTab.caption) %>%
    kable_styling(font_size = 10) %>%
    add_header_above( c("Number of" = 1,
                        "Number of leaves" = 1,
                        "Empirical" = 1,
                        "Poisson" = 1,
                        "Negative binomial" = 1),
                      bold = TRUE,
                      line = FALSE) %>%
    row_spec(0, bold = TRUE) 
}
if( knitr::is_html_output() ) {
  knitr::kable(MitesTab,
               format = "html",
               booktabs = TRUE,
               longtable = FALSE,
               align = "c",
               linesep = c("", "", "\\addlinespace"),
               col.names = c("mites per leaf", 
                             "with that many mites",
                             "probability",
                             "probability",
                             "probability"),
               caption = MitesTab.caption) %>%
    add_header_above( c("Number of" = 1,
                        "Number of leaves" = 1,
                        "Empirical" = 1,
                        "Poisson" = 1,
                        "Negative binomial" = 1),
                      bold = TRUE,
                      line = FALSE) %>%
    row_spec(1, bold = TRUE) 
}
```
\index{Negative binomial distribution|)}



## Poisson distribution {#PoissonDistribution}

\index{Poisson distribution|(}
The Poisson distribution is very commonly used to model the number of occurrences of an event which occurs randomly in time or space.
The Poisson distribution arises as a result of assumptions made about a random process:

* Events that occur in one time-interval (or region) are independent of those occurring in any other non-overlapping time-interval (or region).
* The probability that an event occurs in a small time-interval is proportional to the length of the interval.
* The probability that\ $2$ or more events occur in a very small time-interval is so small that it is negligible.

Whenever these assumptions are valid, or approximately so, the Poisson distribution is appropriate.
Many natural phenomena fall into this category.


### Derivation of Poisson distribution {#PoissonDerivation}

The binomial distribution (Sect.\ \@ref(BinomialDistribution)) applies in situations where an event may occur a certain number of times in a fixed number of trials, say\ $n$.
What if\ $n$ gets increasingly large though, so effectively there is no upper limit?
Mathematically, we might say: What happens if $n\to\infty$?

Let's find out.
Begin with the binomial probability function
\begin{equation}
   p_X(x; n, p) = \binom{n}{x} p^x (1 - p)^{n - x} \quad\text{for $x = 0, 1, \dots, n$}
   (\#eq:BinomialPMF2)
\end{equation}
which has a mean of $\lambda = np$. 
First, re-writing \@ref(eq:BinomialPMF2) in terms of the mean\ $\lambda$:
\begin{equation}
   p_X(x; n, \lambda) 
   = 
   \binom{n}{x} \left(\frac{\lambda}{n}\right)^x \left(1 - \frac{\lambda}{n}\right)^{n - x} \quad\text{for $x = 0, 1, \dots, n$}.
   (\#eq:BinomialPMF2Mean)
\end{equation}
Now consider the case $n \to \infty$ (for $x = 0, 1, \dots$):
\begin{align}
   \lim_{n\to\infty}
   p_X(x; n, \lambda) 
   &= 
   \lim_{n\to\infty} \binom{n}{x} 
   \left(\frac{\lambda}{n}\right)^x \left(1 - \frac{\lambda}{n}\right)^{n - x}\\
   &= \frac{\lambda^x}{x!} 
   \lim_{n\to\infty} \frac{n!}{(n - x)!} \frac{1}{n^x} \left(1 - \frac{\lambda}{n}\right)^{n}  \left(1 - \frac{\lambda}{n}\right)^{-x}.
\end{align}
Now, since *the limit of product of functions is equal to product of their limits*, each component can be considered in turn.

Looking at the first component, see that
\begin{align}
  \lim_{n\to\infty} \frac{n!}{(n - x)!} \times \frac{1}{n^x}
  &= \lim_{n\to\infty} \frac{n \times (n - 1) \times (n - 2) \times\cdots\times 2 \times 1}
                            {(n - x)\times (n - x - 1) \times \cdots \times 2 \times 1} \times \frac{1}{n^x}\\
  &= \lim_{n\to\infty}\frac{ \overbrace{n \times (n - 1) \times \cdots\times (n - x + 1)}^{\text{$x$ terms}}}
                           { \underbrace{n \times n \times\cdots \times n\times n}_{\text{$x$ terms}}}\\
  &= \lim_{n\to\infty} \frac{n}{n} \times \frac{n - 1}{n} \times \frac{n - 2}{n} \times\cdots\times \frac{n - x + 1}{n}\\
  &= 1 \times 1\times\cdots\times 1 = 1.
\end{align}
For the next component, see that (using \@ref(eq:ExponentialLimit)):
$$
  \lim_{n\to\infty} \left(1 - \frac{\lambda}{n}\right)^{n} = \exp(-\lambda).
$$
For the last term:
$$
  \lim_{n\to\infty} \left(1 - \frac{\lambda}{n}\right)^{-x} = 1^{-x} = 1.
$$
Putting the pieces together:
\begin{align}
   \lim_{n\to\infty}
   p_X(x; n, \lambda) 
   &= 
   \frac{\lambda^x}{x!} 
   \lim_{n\to\infty} \frac{n!}{(n - x)!} \frac{1}{n^x} 
   \left(1 - \frac{\lambda}{n}\right)^{n}  \left(1 - \frac{\lambda}{n}\right)^{-x}\\
   p_X(x; \lambda) 
   &= \frac{\lambda^k}{x!}
   1 \times \exp(-\lambda) \times 1\\
   &= \frac{\exp(-\lambda) \lambda^x}{x!},
\end{align}
for $x = 0, 1, 2, \dots$.
This is the probability function for the *Poisson distribution*.

A *Poisson process* refers to events that occur at a *rate*\ $\lambda$, but occur at random.


### Definition and properties {#PoissonDefinition}

The definition of the Poisson distribution can now be given.

:::{.definition #PoissonDistribution name="Poisson distribution"}
A random variable\ $X$ is said to have a *Poisson distribution* if its probability function is
\begin{equation}
   p_X(x; \lambda) = \frac{\exp(-\lambda) \lambda^x}{x!}\quad \text{for $x = 0, 1, 2, \dots$}
   (\#eq:PoissonPMF)
\end{equation}
where the parameter is $\lambda > 0$.
We write $X\sim\text{Pois}(\lambda)$.
:::


:::{.definition #PoissonDistributionDF name="Poisson distribution: distribution function"}
For a random variable\ $X$ with the Poisson distribution given in \@ref(eq:PoissonPMF), the distribution function is
$$
  F_X(x; \lambda) = 
  \begin{cases}
    0                                  & \text{for $x < 0$}\\
    \displaystyle \exp(-\lambda) \sum _{i = 0}^{\lfloor x \rfloor }{\frac {\lambda ^{i}}{i!}} & \text{for $x\ge 0$};\\
  \end{cases}
$$
where $\lfloor z \rfloor$ is the *floor function*.\index{Floor function}
:::


The PMF for a Poisson distribution for different values of\ $\lambda$ is shown in Fig.\ \@ref(fig:PoissonExamples).




```{r PoissonExamples, echo=FALSE, fig.align="center", fig.cap="The probability function for the Poisson distribution for various values of $\\lambda$.", fig.height = 7, fig.width=7, out.width='80%'}
par( mfrow = c(2, 2))

xmax <- 10
mu <- c(0.5, 1, 2, 5)

for (i in (1:4)) {
  muvalue <- mu[i]
  x <-  0:xmax
  y <- dpois(x, 
             lambda = muvalue )
  
   plot( x = x,
         y = y,
         xlim = c(0, xmax),
         xlab = expression(italic(x)),
         ylab = "Probability function",
         main = bquote("Poisson mean:" ~ lambda == .(muvalue)),
         las = 1,
         lty = 3,
         lwd = 2,
         col = "grey",
         type = "h")
  points(x = x,
         y = y,
         pch = 19)
  
}
```


The following are the basic properties of the Poisson distribution.

::: {.theorem #PoissonProperties name="Poisson distribution properties"}
If $X\sim\text{Pois}(\lambda)$ then

1. $\operatorname{E}[X] = \lambda$.
2. $\operatorname{var}[X] = \lambda$.
3. $M_X(t) = \exp[ \lambda\{\exp(t) - 1\}]$.
:::

:::{.proof}
The third result is proven, then the other results follow:
\begin{align*}
   M_X(t) = \operatorname{E}[e^{tX}] 
     &= \sum^\infty_{x = 0} e^{xt} e^{-\lambda} \lambda^x / x!\\
     &= e^{-\lambda} \sum^\infty_{x = 0} \frac{(\lambda\, e^t)^x}{x!} \\
     &= e^{-\lambda} \left[ 1 + \lambda\, e^t + \frac{(\lambda\,e^t)^2}{2!} + \dots \right]\\
     &= e^{-\lambda} e^{\lambda\, e^t}\\
     &= e^{-\lambda (1 - e^t)},
\end{align*}
using \@ref(eq:Exponential).
The first two results follow from differentiating the MGF.
:::


Notice that the Poisson distribution has the variance equal to the mean.
The negative binomial distribution has two parameters and the Poison only one, so often the negative binomial distribution produces a better fit to data.


::: {.softwareBox .software data-latex="{iconmonstr-laptop-4-240.png}"}
The four **R** functions for working with the Poisson distribution have the form `[dpqr]poiss(lambda)` (see Appendix \@ref(UseRDistributions)).
:::


:::{.example #Queues name="Queuing"}
Customers enter a service line 'at random' at a rate of 4 per minute.
Assume that the number entering the line in any given time interval has a Poisson distribution.
To determine the probability that at least one customer enters the line in a given $\frac{1}{2}$-minute interval, use (since $\lambda = 2$ over half-hour):
$$
   \Pr(X\geq 1) = 1 - \Pr(X = 0) = 1 - e^{-2} = 0.865.
$$
In **R**:
```{r}
1 - dpois(0,
          lambda = 2)
```
:::


:::{.example #Bombs name="Bomb hits"}
@clarke1946application (quoted in @data:hand:handbook, Dataset 289) discusses the number of flying bomb hits on London during World War II in a $36$\ square kilometre area of South London.
The area was gridded into $0.25$\ km squares and the number of bombs falling in each grid was counted (Table\ \@ref(tab:BombTable)).
Assuming random hits, the Poisson distribution can be used to model the data using $\lambda = 0.93$.
The probabilityfunction for the Poisson distribution can be compared to the empirical probabilities computed above; see Table\ \@ref(tab:BombTable).
For example, the probability of zero hits is
$$
   \frac{\exp(-0.93) (0.93)^0}{0!} \approx 0.39.
$$
The two probabilities are very close; the Poisson distribution fits the data very well.
:::


```{r BombTable, echo=FALSE}
BombTable <- array( dim = c(8, 4))
colnames(BombTable) <- c("Hits",
                         "Number of bombs",
                         "Proportion of bombs",
                         "Poisson proportion")
BombTable[, 1] <- 0:7
BombTable[, 2] <- c(229,
                    211,
                    93,
                    35,
                    7,
                    0,
                    0,
                    1)
BombTable[, 3] <- round(BombTable[, 2] / sum(BombTable[, 2]), 4)
lambda <- 0.93 # sum( BombTable[, 1] * BombTable[, 3])

BombTable[, 4] <- dpois(0:7,
                        lambda = lambda )
BombTable[, 4] <- round( BombTable[, 4], 4)
BombTable[ 8, 4] <- 1 - sum( BombTable[1:7, 4])

BombTable.caption <- "The number of flying bomb hits on London during World War II in a $36$\\ square kilometre area of South London. The proportion of the $576$\\ grid squares receiving $0, 1, ...$ hits was also computed. The observed and empirical probabilities are shown. The Poisson distribution fits the data well."

if( knitr::is_latex_output() ) {
  knitr::kable( BombTable,
                format = "latex",
                booktabs = TRUE,
                longtable = FALSE,
                linesep = c("", "", "\\addlinespace"),
                col.names = c("Hits",
                              "Number bombs", 
                              "Proportion bombs", 
                              "Poisson proportion"),
                caption = BombTable.caption) %>%
    kable_styling(font_size = 10) %>%
    row_spec(0, bold = TRUE)
}
if( knitr::is_html_output() ) {
  knitr::kable( BombTable,
                format = "html",
                booktabs = TRUE,
                longtable = FALSE,
                linesep = c("", "", "\\addlinespace"),
                col.names = c("Hits",
                              "Number bombs", 
                              "Proportion bombs", 
                              "Poisson proportion"),
                caption = BombTable.caption) %>%
    row_spec(1, bold = TRUE)
}

```



### Relationship to the binomial distribution {#PoissonBinomial}

Given the derivation of the Poisson distribution (Sect.\ \@ref(PoissonDerivation)), a relationship between the Poisson and binomial distributions should come as no surprise.

Computing binomial probabilities is tedious if the number of trials\ $n$ is very large and the probability of success in a single trial is very small. 
For example, consider $X\sim \text{Bin}(n = 2000, p = 0.005)$; the probability function is
$$
   p_X(x) = \binom{2000}{x}(0.005)^x 0.995^{2000 - x}.
$$
Using the PMF, computing some probabilities, such as $\Pr(X > 101)$, is tedious.
(Try computing $\binom{2000}{102}$ on your calculator, for example.)
However, the Poisson distribution can be used to *approximate* this probability.

Set the Poisson mean to equal the binomial mean (that is, $\lambda = \mu = np$).
Since $\operatorname{E}[Y] = \operatorname{var}[Y] = \lambda$ for the Poisson distribution, this means the variance is also set to $\sigma^2 = np$.
Of course, the binomial mean is $np(1 - p)$, so this can only be (approximately) true if\ $p$ is close to zero (and so $1 - p\approx 1$).

A general guideline is that the Poisson distribution can be used to approximate the binomial when $n$\ is large (recall, the derivation was for the case $n\to\infty$), $p$\ is small and $np \le 7$.


::: {.linkBox .link data-latex="{iconmonstr-link-1-240.png}"}
If the random variable\ $X$ has the binomial distribution $X \sim \text{Bin}(n, p)$, the probability function can be approximated by the Poisson distribution $X \sim \text{Pois}(\lambda)$, where $\lambda = np$.
The approximation is good when\ $n$ is large, $p$\ is small, and $np\le 7$.
:::


```{r BinomialPoisson, echo=FALSE, fig.align="center", fig.cap="The Poisson distribution is an excellent approximation to the binomial distribution when $p$ is small and $n$ is large. The binomial probability function is shown using empty circles; the Poisson probability function using crosses.", fig.height = 7, fig.width=7, out.width='80%'}
par( mfrow = c(2, 2))

n <- c(10, 50, 
       100, 1000)
p <- c(0.2, 0.1, 
       0.010, 0.005)
xmax <- c(10, 20, 
          10, 15)

for (i in (1:4)) {
   pValue <- p[i]
   nValue <- n[i]
   
  x <-  0:n[i]
  y <- dbinom(x, 
              size = n[i],
              prob = p[i])
  
   plot( x = x,
         y = y,
         xlim = c(0, xmax[i]),
         xlab = expression(italic(x)),
         ylab = "Probability",
         main = bquote("Binomial: " ~ italic(n) == ~ .(nValue) ~ "and" ~ italic(p) == .(pValue) ),
         lwd = 2,
         las = 1,
         lty = 3,
         col = "grey",
         type = "h")
  points(x = x,
         y = y,
         pch = 19)

  muPoisson <- nValue * pValue
  xValues <- 0:xmax[i]
  points( x = xValues,
          y = dpois(xValues,
                    lambda = muPoisson),
          pch = 4,
          cex = 1.5)
  
  if (i == 4 ){
    legend("topright",
           pch = c(4, 19),
           pt.cex = c(1.5, 1),
           legend = c("Binomial",
                      "Poisson approx."))
  }
  
}

```


### Extensions {#PoissonExtensions}

The Poisson distribution is commonly used to model independent counts.
However, sometimes these counts explicitly *exclude* a count of zero.
For example, consider modelling the number of nights patients spend in hospital; patients must spend at least one night to be in the data. 
Then, the probability functions can be expressed as
$$
   p_X(x; \lambda) = \frac{\exp(-\lambda) \lambda^{x - 1}}{(x - 1)!}\quad   \text{for $x = 1, 2, 3, \dots$}
$$
where the parameter is $\lambda > 0$.
This is called the *zero-truncated Poisson distribution*.\index{Poisson distribution!zero truncated}
In this case, $\operatorname{E}[X] = \lambda + 1$ and $\operatorname{var}[X] = \lambda$ (Ex. \@ref(exr:PoissonZeroTruncated)).

In some cases, the random variable is a count, but has an upper limit on the possible number of counts.
This is the *truncated Poisson distribution*.

Other situations exist where the the proportions of zeros exceed the proportions expected by the Poisson distribution (e.g., $\exp(-\lambda)$), but the Poisson distributions seems to otherwise be suitable.
In these situations, the *zero-inflated Poisson distribution* may be suitable.\index{Poisson distribution!zero inflated}
\index{Poisson distribution|)}


## Hypergeometric distribution {#HypergeometricDistribution}

### Derivation of a hypergeometric distribution {#HypergeometricDerivation}

\index{Hypergeometric distribution|(}
When the selection of items a fixed number of times is done *with* replacement, the probability of an item being selected stays the same and the binomial distribution can be used.
However, when the selection of items is done *without* replacement, the trials are not independent, making the binomial model unsuitable.
In these situations, a hypergeometric distribution is appropriate.

Consider a simple example.
A bag contains six light-coloured balls and four dark-coloured balls (Fig.\ \@ref(fig:HypergeometricHelp)).
The variable of interest, say\ $X$, is the number of light-coloured balls drawn in three random selections from the bag, without replacing the balls.
Since the balls are not replaced, $\Pr(\text{draw a light-coloured ball})$ is not constant, and so the binomial distribution cannot be used.


```{r HypergeometricHelp, echo=FALSE, fig.align="center", fig.cap="Drawing balls from a bag.", fig.width=7, out.width='75%'}
drawBall <- function(x, y, diameter = 1, col = "white", ...){
  t <- seq(0, 2 * pi, 
           length = 100)
  tx <- cos(t) * diameter
  ty <- sin(t) * diameter
  polygon( x = x + tx,
           y = y + ty,
           col = col,
         ...)
  
}



#############
### "Bag"


plot(x = c(-1, 9),
     y = c(-2, 2),
     asp = 1,
     axes = FALSE,
     type = "n",
     xlab = "",
     ylab = ""
     )
drawBall(x = 0.2,
         y = 1.4,
         diameter = 0.15,
         col = grey(0.8))
drawBall(x = 0.6,
         y = 1.4,
         diameter = 0.15,
         col = grey(0.8))
drawBall(x = 1.0,
         y = 1.4,
         diameter = 0.15,
         col = grey(0.8))
drawBall(x = 1.4,
         y = 1.4,
         diameter = 0.15,
         col = grey(0.8))
drawBall(x = 1.8,
         y = 1.4,
         diameter = 0.15,
         col = grey(0.8))
drawBall(x = 2.2,
         y = 1.4,
         diameter = 0.15,
         col = grey(0.8))




drawBall(x = 0.6,
         y = 0.4,
         diameter = 0.15,
         col = grey(0.4))
drawBall(x = 1.0,
         y = 0.4,
         diameter = 0.15,
         col = grey(0.4))
drawBall(x = 1.4,
         y = 0.4,
         diameter = 0.15,
         col = grey(0.4))
drawBall(x = 1.8,
         y = 0.4,
         diameter = 0.15,
         col = grey(0.4))

polygon( x = c(-0.1, -0.1, 2.5, 2.5),
         y = c(0.1, 1.7, 1.7, 0.1),
         lwd = 2)
text(x = 1,
     y = -0.5,
     #font = 2, # BOLD
     labels = expression(paste(Balls~"in"~the~bag:~italic(N)==10*":")))
text(x = 1,
     y = -1.35,
     labels = expression( atop( italic(m)==6~"light-coloured balls",
                                italic(n)==4~"dark-coloured balls") ))



#############
### Selection



drawBall(x = 1.0 + 6,
         y = 1.4,
         diameter = 0.15,
         col = grey(0.8))





drawBall(x = 0.6 + 6,
         y = 0.4,
         diameter = 0.15,
         col = grey(0.4))
drawBall(x = 1.0 + 6,
         y = 0.4,
         diameter = 0.15,
         col = grey(0.4))


#polygon( x = c(-0.1 + 6, -0.1 + 6, 2.5, 2.5),
#         y = c(0.1, 1.7, 1.7, 0.1),
#         lwd = 2)
text(x = 1 + 6,
     y = -0.5,
     #font = 2, # BOLD
     labels = expression(paste(Balls~selected:~italic(k)==3)) )
text(x = 1 + 6,
     y = -1.35,
     labels = expression( atop( italic(x)==1~"light-coloured ball",
                                italic(k)-italic(x)==2~"dark-coloured balls") ) )

arrows( x0 = 2.6,
        x1 = 6.2,
        y0 = 0.9, 
        y1 = 0.9,
        lwd = 2,
        length = 0.15,
        angle = 15)
text(x = 4.4,
     y = 1.1,
     labels = expression( paste(Select~italic(k)==3~balls)))

#axis(side = 1); axis(side = 2); box()
```



The probabilities can be computed, however, using the counting ideas from Chap.\ \@ref(Probability).
There are a total of $\binom{10}{3}$ ways of selecting a sample of size $k = 3$ from the bag.
Consider the case $X = 0$.
The number of ways of drawing no light-coloured balls is $\binom{6}{0}$ and the number of ways of drawing the three dark-coloured balls in $\binom{4}{3}$, so the probability is
$$
   \Pr(X = 0) = \frac{\binom{6}{0}\binom{4}{3}}{\binom{10}{3}} \approx 0.00833.
$$
Likewise, the number of ways to draw one light-coloured ball (and hence two dark-coloured balls; Fig.\ \@ref(fig:HypergeometricHelp)) is $\binom{6}{1}\times \binom{4}{2}$, so
$$
   \Pr(X = 1) = \frac{ \binom{6}{1}\times \binom{4}{2} }{\binom{10}{3}}.
$$
Similarly,
$$
   \Pr(X = 2) =\frac{ \binom{6}{2}\times \binom{4}{1} }{\binom{10}{3}}
   \quad\text{and}\quad
   \Pr(X = 3) =\frac{ \binom{6}{3}\times \binom{4}{0} }{\binom{10}{3}}.
$$



### Definition and properties {#HypergeometricDefinition}

In general, suppose a bag contains $N$\ balls, and\ $m$ of them are light-coloured and\ $n$ and *not* light-coloured; then $N = m + n$.
Suppose a sample of size\ $k$ is drawn from the bag *without replacement*; then the probability of finding\ $x$ light-coloured balls in the sample of size\ $k$ is
$$
   \Pr(X = x) 
   = \frac{ \binom{m}{x}{ \binom{n}{k - x}}}{\binom{N}{k}}
   = \frac{ \binom{m}{x}{ \binom{n}{k - x}}}{\binom{m + n}{k}}
$$
where\ $X$ is the number of light-coloured balls in sample of size\ $k$.
(In the example, $k = 3$, $m = 6$ and $N = 10$.)

In the formula, $\binom{m}{x}$ is the number of ways of selecting\ $x$ light-coloured balls from the\ $m$ light-coloured balls in the bag; $\binom{n}{k - x}$ is the number of ways of selecting all the remaining $k - x$ to be the other colour (and there are $n = N - m$ of those in the bag); and $\binom{N}{k}$ is the number of ways of selecting a sample of size\ $k$ if there are $N$\ balls in the bag in total.


:::{.definition #HypergeometricDistribution name="Hypergeometric distribution"}
Consider a set of $N = m + n$ items of which\ $m$ are of one kind (call them 'successes') and other $n = N - m$ are of another kind (call them 'failures').
We are interested in the probability of $x$\ successes in $k$\ trials, when the selection (or drawing) is made *without* replacement. 
Then the random variable\ $X$ is said to have a *hypergeometric distribution* with probability function
\begin{equation}
   p_X(x; n, m, k) = \frac{ \binom{m}{x}\binom{n}{k - x}}{\binom{m + n}{k}}
   (\#eq:HypergeometricPMF)
\end{equation}
where $\max(0, k - n) \le x \le \min(n, m)$.
:::

The distribution function is complicated and is not given.
The following are the basic properties of the hypergeometric distribution.


:::{.theorem #HypergeomatricDistributionProperties name="Hypergeometric distribution properties"}
If\ $X$ has a hypergeometric distribution with probability function\ \@ref(eq:HypergeometricPMF), then (writing $N = m + n$)

1. $\operatorname{E}[X] = km/N$.
2. $\displaystyle
    \operatorname{var}[X] = k \left(\frac{m}{N}\right)\left(\frac{N - k}{N - 1}\right)\left(1 - \frac{m}{N}\right)$.
:::
The moment-generating function is difficult and will not be considered.


::: {.softwareBox .software data-latex="{iconmonstr-laptop-4-240.png}"}
The four **R** functions for working with the hypergeometric distribution functions have the form `[dpqr]hyper(m, n, k)`, where `k`${} = k$, `m`${} = m$ and `m`${} + {}$`n`${} = N$, so that `n`${}={}$`N`${}-{}$`m` (see Appendix \@ref(UseRDistributions)).
:::


::: {.linkBox .link data-latex="{iconmonstr-link-1-240.png}"}
If the population is much larger than the sample size (that is, $N$\ is much larger than\ $k$), then the probability of a success will be approximately constant, and the binomial distribution can be used to give approximate probabilities.
:::


Consider the example at the start of this section.
The probability of drawing a light-coloured ball initially is $6/10 = 0.6$, and the probability that the next ball is light-coloured is $5/9 = 0.556$.
But suppose there are $10\,000$\ balls in the bag, of which $6\,000 $are light-coloured.
The probability of drawing a light-coloured ball initially is $6000 / 10,000 = 0.6$, and the probability that the next ball is light-coloured becomes $5999/9999 = 0.59996$; the probability is almost the same.
In this case, we might consider using the binomial distribution with $p\approx 0.6$.

In general, if\ $N$ is much larger than $k$, the population proportion then will be approximately $p\approx m/N$, and so $1 - p \approx (N - m)/N$.
Using this information,
$$
   \operatorname{E}[X] = k \times (m/N) \approx kp
$$
and
\begin{align*}
   \operatorname{var}[X]
   &= k\left(\frac{m}{N}\right)\left(1 - \frac{m}{N}\right)\left(\frac{N - k}{N - 1}\right)\\
   &\approx k\left( p \right ) \left( 1 - p \right) \left(1 \right ) \\
   &= k p (1 - p),
\end{align*}
which correspond to the mean and variance of the binomial distribution.


:::{.example #Mice name="Mice"}
Twenty mice are available to be used in an experiment; seven of the mice are female and\ $13$ are male.
Five mice are required and will be sacrificed.
What is the probability that more than three of the mice are males?

Let\ $X$ be the number of male mice chosen in a sample of size $k = 5$.
Then\ $X$ has  a hypergeometric distribution (since mice are chosen without replacement) where $N = 20$, $k = 5$, $m = 13$ and $n = 20 - 13 = 7$, and we seek 
\begin{align*}
   \Pr(X > 3)
   &= \Pr(X = 4) + \Pr(X = 5) \\
   &= \frac{ \binom{13}{4} \binom{7}{1}}{ \binom{20}{5} } +
       \frac{ \binom{13}{5} \binom{7}{0}}{ \binom{20}{5} } \\
   & \approx  0.3228 + 0.0830 = 0.4058.
\end{align*}
The probability is about\ $41$%:

```{r}
dhyper(x = 4, m = 13, n = 7, k = 5) + 
  dhyper(x = 5, m = 13, n = 7, k = 5)
```
:::
\index{Hypergeometric distribution|)}




## Multinomial distribution {#MultinomialDistribution}

\index{Multinomial distribution|(}
A specific example of a *discrete* multivariate distribution is the *multinomial distribution*, a generalization of the binomial distribution. 


:::{.definition #MultinomialDistribution name="Multinomial distribution"}
Consider an experiment with the the sample space partitioned as $S = \{B_1, B_2, \ldots, B_k\}$. 
Let $p_i = \Pr(B_i), \ i = 1, 2,\ldots k$ where $\sum_{i = 1}^k p_i = 1$. 
Suppose there are $n$\ repetitions of the experiment in which\ $p_i$ is constant. 
Let the random variable\ $X_i$ be the number of times (in the $n$\ repetitions) that the event\ $B_i$ occurs. 
In this situation, the random vector $(X_1, X_2, \dots, X_k)$ is said to have a *multinomial distribution* with probability function
\begin{equation}
   \Pr(X_1 = x_1, X_2 = x_2, \ldots, X_k = x_k)
   = \frac{n!}{x_1! \, x_2! \ldots x_k!} 
      p_1^{x_1}\, p_2^{x_2} \ldots p_k^{x_k},
   (\#eq:Multinomial)
\end{equation}
where $\mathcal{R}_X = \{(x_1, \ldots x_k) : x_i = 0,1,\ldots,n, \, i = 1, 2, \ldots k, \, \sum_{i = 1}^k x_i = n\}$.
:::

The part of Eq.\ \@ref(eq:Multinomial) involving factorials arises as the number of ways of arranging $n$\ objects, $x_1$\ of which are of the first kind, $x_2$\ of which are of the second kind, etc. 
The above distribution is $(k - 1)$-variate since $x_k = n-\sum_{i = 1}^{k - 1}x_i$.
In particular if $k = 2$, the multinomial distribution reduces to the binomial distribution which is a univariate distribution.

$X_i$\ is the number of times (out of\ $n$) that the event\ $B_i$, which has probability\ $p_i$, occurs. 
So the random variable\ $X_i$ clearly has a binomial distribution with parameters\ $n$ and\ $p_i$.
We see then that the marginal probability distribution of one of the components of a multinomial distribution is a binomial distribution.

Notice that the distribution in Example\ \@ref(exm:BVDiscreteTwoDice) is an example of a *trinomial distribution*. 
The probabilities shown in Table\ \@ref(tab:BVTwoDice) can be expressed algebraically as
$$
   \Pr(X = x, Y = y)
   = \frac{2!}{x! \, y!(2 - x - y)!}
      \left(\frac{1}{6}\right)^x\left(\frac{1}{6}\right)^y\left(\frac{2}{3}\right)^ {2 - x - y}
$$
for $x, y = 0 , 1 , 2$; $x + y \leq 2$.

The following are the basic properties of the multinomial distribution.


:::{.theorem #MultinomialProperties name="Multinomial distribution properties"}
Suppose $(X_1, X_2, \ldots, X_k)$ has the multinomial distribution given in Def.\ \@ref(def:MultinomialDistribution). 
Then for $i = 1, 2, \ldots, k$:

* $\operatorname{E}[X_i] = np_i$.
* $\operatorname{var}[X_i] = n p_i(1 - p_i)$.
* $\operatorname{Cov}(X_i, X_j) = -n p_i p_j$ for $i \neq j$.
:::


:::{.proof}
The first two results follow from the fact that $X_i \sim \text{Bin}(n, p_i)$.

We will use\ $x$ for\ $x_1$ and\ $y$ for\ $x_2$ in the third for convenience.
Consider only the case $k = 3$, and note that
$$
   \sum_{(x, y) \in R} 
   \frac{n!}{x! \, y! (n - x - y)!} p_1^x p_2^y (1 - p_1 - p_2)^{n - x - y} = 1.
$$
Then, putting $p_3 = 1 - p_1 - p_2$,
\begin{align*}
  E(XY)
  &= \sum_{(x, y)}xy \Pr(X = x, Y = y)\\
  &= \sum_{(x, y)}\frac{n!}{(x - 1)!(y - 1)!(n - x - y)!} p_1^x  p_2^y p_3^{n - x - y}\\
  &= n(n - 1) p_1 p_2\underbrace{\sum_{(x,y)}\frac{(n - 2)!}{(x - 1)!(y - 1)!(n - x - y)!} p_1^{x - 1} p_2^{y - 1}p_3^{n - x - y}}_{ = 1}.
\end{align*}
So $\operatorname{Cov}(X, Y) = n^2 p_1 p_2 - n p_1 p_2 - (n p_1)(n p_2) = -n p_1 p_2$.
:::

::: {.softwareBox .software data-latex="{iconmonstr-laptop-4-240.png}"}
The two **R** functions for working with the multinomial distribution functions have the form `[dr]multinom(size, prob)` where `size`${}= n$ and `prob`${} = (p_1, p_2, \dots, p_k)$ for $k$ categories (see App.\ \@ref(UseRDistributions)).
Note that the functions `qmultinom()` and `pmultinom()` are not defined.
:::


:::{.example #Multinomial name="Multinomial distribution"}
Suppose that the four basic blood groups\ O, A, B and\ AB are known to occur in the proportions $9:8:2:1$. 
Given a random sample of $8$\ individuals, what is the probability that there will be $3$\ each of Types\ O and\ A and $1$\ each of Types\ B and\ AB?

The probabilities are $p_1 = 0.45$, $p_2 = 0.4$, $p_3 = 0.1$, $p_4 = 0.05$, and
\begin{align*}
   \Pr(X_O = 3, X_A = 3, X_B = 1, X_{AB} = 1)
   &= \frac{8!}{3!\,3!\,1!\,1!}(0.45)^3 (0.4)^3 (0.1)(.05)\\
   &= 0.033.
\end{align*}
In **R**:

```{r echo=TRUE}
dmultinom(x = c(3, 3, 1, 1), 
          size = 8, 
          prob = c(0.45, 0.4, 0.1, 0.05))
```
:::
\index{Multinomial distribution|)}

 



## Other notable discrete distributions {#OtherDiscreteDistributions}

```{r echo=FALSE}
discrete_Table <- array( dim = c(10, 3) )
discrete_Table[, 1] <- c("Bernoulli",
                         "Binomial",
                         "Geometric",
                         "Negative Binomial",
                         "Poisson",
                         "Hypergeometric",
                         "Discrete Uniform",
                         "Multinomial",
                         "Zero-truncated Poisson",
                         "Zero-inflated Poisson")
discrete_Table[, 2] <- c("Single success/failure trial",
                         "Number of successes in $n$ trials",
                         "Trials until first success",
                         "Trials until $k$ successes",
                         "Count in fixed time/space interval",
                         "Successes in draws w/o replacement",
                         "Equally likely outcomes",
                         "Counts of outcomes in $n$ trials",
                         "Skewed frequency of discrete events",
                         "??")
discrete_Table[, 3] <- c("Coin toss (Heads?)",
                         "Number of heads in 10",
                         "Rolls until first 6",
                         "Throws until 3 baskets",
                         "Emails per hour", 
                         "Aces in 5-card draw",
                         "Fair die roll",
                         "Die rolled 10 times",
                         "Word counts in text",
                         "??")

if (knitr::is_latex_output()) {
  kable(discrete_Table,
        format = "latex",
        booktabs = TRUE,
        align = "l",
        escape = FALSE,
        longtable = FALSE,
        caption = "Commonly-used discrete distributions.") %>%
    kable_styling(font_size = 10) %>%
    row_spec(0, bold = TRUE) 
  
}
```



## Simulation {#SimulationDiscrete}
\index{Simulation}

Simulation was introduced in Sect.\ \@ref(ProbStatisticalComputing) as a computer-based tool for helping understand and model specific scenarios.
Probability distributions are usually crucial in simulations for modelling phenomena.
Simulation effectively connects theory to real-world modelling.
Many application (finance, epidemiology, engineering, AI) use simulation to study uncertainty in complex situations.
This book provides the building blocks for those more complex situations.

Distributions can be used to *simulate* practical situations, using random numbers generated from the distributions.
In **R**, these function start with the letter `r`; for example, to generate three random numbers from a Poisson distribution, use `rpois()`:

```{r echo=FALSE}
set.seed(107283)
```

```{r echo=TRUE}
rpois(3, # Generate three random numbers...
      lambda = 4) # ... with mean = 4
```


Consider a study of insects where a females lays $E$\ eggs, only some of which are fertile (based on @ito1977model).
Suppose $E \sim \text{Pois}(8.5)$, and the probability that any given egg is fertile is $p = 0.6$ (and assume the fertility of the eggs from any one female are independent).

The fertility for one females can be modelled in **R**:

```{r echo=FALSE}
set.seed(17283)
```

```{r}
# One female
NumberOfEggs <- rpois(1, 
                      lambda = 8.5)

# How many eggs are fertile?
NumberFertileEggs <- rbinom(1, # Only one random number for this one female
                            size = NumberOfEggs,
                            prob = 0.6)
cat("Number of eggs:", NumberOfEggs,
    "| Number of fertile eggs:", NumberFertileEggs, "\n")
```

Since random numbers are being generated, each simulation will produce different results, so we repeat this over many simulations:


```{r echo=FALSE}
set.seed(109883)
```

```{r}
# Many simulations

numberSims <- 5000

NumberOfEggs <- array( dim = numberSims )
NumberFertileEggs <- array( dim = numberSims )

for (i in (1:numberSims)){ 
  NumberOfEggs[i] <- rpois(1, lambda = 8.5)
  
  # How many fertile?
  NumberFertileEggs[i] <- rbinom(1, # Only one random number/female
                                 size = NumberOfEggs[i],
                                 prob = 0.6)
}
```

We can then draw a histogram of the number of fertile eggs laid by females (Fig. \@ref(fig:EggsSimulation)):

```{r EggsSimulation, fig.cap="The number of fertile eggs laid, over $5\\,000$\\ simulations.", fig.height=4, fig.width=5, out.width='56%', fig.align="center"}
hist(NumberFertileEggs,
     las = 1, # Horizontal tick mark labels
     xlab = "Number of fertile eggs",
     ylab = "Count",
     main = "Number of fertile eggs:\n5000 simulations")
```

Using this information, questions can be answered about the number of fertile eggs layed.
For example:

```{r echo=TRUE}
sum( NumberFertileEggs > 6) / numberSims
```

The chance that a female lays more than\ $6$ fertile eggs is about `r round(sum( NumberFertileEggs > 6) / numberSims * 100, 1)`%.
The mean and variance of the number of fertile eggs laid is:
```{r echo=TRUE}
mean( NumberFertileEggs )
var( NumberFertileEggs )
```


## Exercises {#DiscreteExercises}

Selected answers appear in Sect.\ \@ref(AnswersChapDiscreteDistributions).

:::{.exercise #BinChange}
If $X \sim \text{Bin}(n, 1 - p)$, show that $(n - X) \sim \text{Bin}(n, p)$.
:::


:::{.exercise #SaltIntake}
A study by @data:Sutherland:SaltIntake found that about\ $30$% of Britons 'generally added salt' at the dinner table.

1. In a group of $25$\ Britons, what is the probability that at least\ $10$ generally added salt?
1. In a group of $25$\ Britons, what is the probability that no more than\ $9$ generally added salt?
1. In a group of $25$\ Britons, what is the probability that between\ $5$ and\ $10$ (inclusive) generally added salt?
1. What is the probability that $6$\ Britons would need to be selected to find one that generally adds salt?
1. What is the probability that at least $8$\ Britons would need to be selected to find the first that generally add salt?
1. What is the probability that at least $8$\ Britons would need to be selected to find three that generally add salt?
1. What assumptions are being made in the above calculations?
:::


:::{.exercise #Placebos}
A study by @loyeung2018experimental examined whether people could identify potential placebos.
The $81$\ subjects were each presented with five different supplements, and asked to select which *one* was the legitimate herbal supplement based on the taste (the rest were placebos).

1. What is the probability that more than\ $15$ will correctly identify the legitimate supplement?
1. What is the probability that at least\ $12$ correctly identify the legitimate supplement?
1. What is the probability that the first person to identify the legitimate supplement is the third person tested?
1. What is the probability that the fifth person to identify the legitimate supplement is the $10$th person tested?
1. In the study, $50$\ people correctly selected the true herbal supplement.
   What does this suggest?
:::


:::{.exercise #NoisyMiners}
A study by @data:Maron:eucthreshold used statistical modelling to show that the mean number of noisy miners (a type of bird) in sites with about\ $15$ eucalypts per two hectares was about\ $3$.

1. In a two hectare site with $15$\ eucalypts, what is the probability of observing no noisy miners?
1. In a two hectare site with $15$\ eucalypts, what is the probability of observing more than\ $5$ noisy miners?
1. In a four hectare site with $30$\ eucalypts, what is the probability of observing two noisy miners?
:::


:::{.exercise #DisaggregationPoisson}
In a study of rainfall disaggregation (extracting small-scale rainfall features from large-scale measurements), the number of non-overlapping rainfall events per day at Katherine was modelled using a Poisson distribution\index{Poisson distribution} (for $x = 0, 1, 2, 3, \dots$) with $\lambda = 2.5$ in summer and $\lambda = 1.9$ in winter [@connolly1998daily].

Denote the number of rainfall events in summer as\ $S$, and in winter as\ $W$.

1. Plot the two distributions, and compare summer and winter (on the same graph).
   Compare and comment.
1. What is the probability of more than\ $3$ rainfall events per day in winter? 
1. What is the probability of more than\ $3$ rainfall events per day in summer? 
1. Describe what is meant by the statement $\Pr(S > 3 \mid S > 1)$, and compute the probability.
1. Describe what is meant by the statement $\Pr(W > 2 \mid W > 1)$, and compute the probability.
::: 


:::{.exercise #UnknownSize}
Consider a population of animals of a certain species of unknown size\ $N$ [@romesburg1979fitting].
A certain number of animals in an area are trapped and tagged, then released.
At a later point, more animals are trapped, and the number tagged is noted.

Define\ $p$ as the probability that an animal is captured zero times during the study, and\ $n_x$ as the number of animals captured $x$\ times (for $x = 0, 1, 2, \dots$).
($n_0$, which we write as\ $N$, is unknown.)
The study consist of $s$\ trapping events, so we have $n_1$, $n_2, \dots n_s$ where\ $s$ is sufficiently 'large' that the truncation is negligible.
Then,
\begin{equation}
   n_x = N p (1 - p)^x \quad \text{for $x = 0, 1, 2, \dots$}.
   (\#eq:GeomCapture)
\end{equation}
While\ $p$ and\ $N$ are both unknown, the value of\ $N$ (effectively, the population size) is of interest.

1. Explain *how* \@ref(eq:GeomCapture) arises from understanding the problem.
1. Take logarithms of both sides of of \@ref(eq:GeomCapture), and hence write this equation in the form of a linear regression equation $\hat{y} = \beta_0 + \beta_1 x$.
1. Using the regression equation, identify *how* to estimate\ $p$ from knowing an estimate of the slope of the regression line ($\beta_1$), and then how to estimate\ $N$ from the $y$-intercept ($\beta_0$).
1. Use the data in Table \@ref(tab:RabbitPop), from a study of rabbits in an area Michigan [@eberhardt1963problems], to estimate the rabbit population.

(**Hint:** To fit a regression line in\ **R**, use `lm()`; for example, to fit the regression line $\hat{y} = \beta_0 + \beta_1 x$, use `lm(y ~ x)`.)


```{r RabbitPop, echo=FALSE}
Rabbits <- array( dim = c(2, 7) )

Rabbits[1, ] <- c("$x$", 1:6)
Rabbits[2, ] <- c("$n_x$", 247, 63, 20, 4, 2, 1)


if( knitr::is_latex_output() ) {
  knitr::kable(Rabbits, 
               format = "latex",
               booktabs = TRUE,
               align = "r",
               longtable = FALSE,                
               escape = FALSE,
               caption = "The number of rabbits $n_x$ at each trapping who had been previously trapped.") %>%
    kable_styling(font_size = 10) %>%
    row_spec(0, bold = TRUE) 
}
if( knitr::is_html_output() ) {
  knitr::kable(Rabbits, 
               format = "html",
               booktabs = TRUE,
               align = "r",
               longtable = FALSE,                
               escape = FALSE,
               caption = "The number of rabbits $n_x$ at each trapping who had been previously trapped.") 
}

```
:::


:::{.exercise #NigerianWater}
A Nigerian study of using solar energy to disinfect water (SODIS) modelled the number of days exposure needed to disinfect the water [@nwankwo2022solar].
The threshold for disinfection was a single day recording $4$\ kWh/m^$2$^ daily cumulative solar irradiance.
Define\ $p$ as the probability that a single day records more than this threshold.

1. Suppose $p = 0.5$.
   How many days exposure would be needed, on average, to achieve disinfection?
1. Suppose $p = 0.25$.
   How many days exposure would be needed, on average, to achieve disinfection?
1. If $p = 0.25$, what is the variance of the number of days needed to achieve disinfection?
1. If $p = 0.25$, what is the probability that disinfection will be achieved in three days or fewer?
:::



:::{.exercise}
A negative binomial distribution\index{Negative binomial distribution} was used to model the number of parasites on feral cats on on Kerguelen Island [@hwang2016estimating].
The model used is parameterised so that $\mu = 8.7$ and $k = 0.4$, where $\operatorname{var}[X] = \mu + \mu^2/k$.

1. Use the above information to determine the negative binomial parameters used for the parameterisation in Sect. \@ref(def:GeometricDistributionALT).
1. Determine the probability that a feral cat has more than\ $10$ parasites.
1. The cats with the largest\ $10$% of parasites have how many parasites?
:::

:::{.exercise}
A study investigating bacterial sample preparation procedures for single-cell studies  [@koyama2016bacterial] studied, among other bacteria, *E.\ coli\ 110*.
The number of bacteria in 2$\mu$L samples was modelled using a Poisson distribution\index{Poisson distribution} with $\lambda = 1.04$.

1. What is the probability that a sample has more than\ $4$ bacteria? 
1. What is the probability that a sample has more than\ $4$ bacteria, given it has bacteria? 
1. Would you expect the negative binomial distribution to fit the data better than the Poisson?
:::



:::{.exercise}
A study of accidents in coal mines [@sari2009stochastic] used a Poisson distribution\index{Poisson distribution} with $\lambda = 12.87$.

1. Plot the distribution.
1. Compute the probability of more than one accident per day in February.
:::


:::{.exercise}
In a study of heat spells [@furrer2010statistical] examined three cities.
In Paris, a 'hot' day was defined as a day with a maximum over\ $27$^$\circ$^C; in Phoenix, a 'hot' day was defined as a day with a maximum over $40.8$^$\circ$^C.

The length of a heat spell\ $X$ was modelled using a geometric distribution,\index{Geometric distribution} with $p = 0.40$ in Paris, and $\lambda = 0.24$ in Phoenix.

1. On the same graph, plot both probability functions.
1. For each city, what is the probability that a heat spell last longer than a week?
1. For each city, what is the probability that a heat spell last longer than a week, given it has lasted two days?
:::


:::{.exercise}
A study of crashes at intersections [@lord2005poisson] examined high risk intersections, and modelled the number of crashes with a Poisson distribution\index{Poisson distribution} using $\lambda = 11.5$ per day.

1. What proportion of such intersection would be expected to have zero crashes?
1. What proportion of such intersection would be expected to have more than five crashes?
:::




:::{.exercise #LayDate}
A *negative binomial* distribution\index{Negative binomial distribution!alternative parameterisation} was used to model the day on which eggs were layed by glaucous-winged gulls [@zador2006balancing].
The authors used a different parameterisation of the negative binomial distribution, with the two parameters\ $\mu$ (the mean) and\ $\phi$ (the overdispersion parameter).
(In\ **R**, these are called `mu` and `size` respectively when calling `dnbinom()` and friends.)
The *expected* 'lay date' (from Day\ $0$) was $23.0$ in 1999, and $19.5$ in 2000;
the overdispersion parameter was $\phi = 20.6$ in 1999 and $\phi = 8.9$ in 2000.

1. Using this information, plot the probability function for both years (on the same graph), and comment.
1. For both years, compute the probability that the lay date exceeds $30$, and comment.
1. For both years, find the lay date for the earliest $15$% of birds.
1. The data in Table \@ref(tab:ClutchSize) shows the clutch size (number of eggs) found in the birds' nest.
   Compute the mean and standard deviation of the number of eggs per clutch.
:::


```{r ClutchSize, echo=FALSE}
EggsTable <- array( dim = c(1, 3))

EggsTable[1, ] <- c(9, 29, 199)

rownames(EggsTable) <- c("Number of clutches")


if( knitr::is_latex_output() ) {
  knitr::kable(EggsTable,
               format = "latex",
               booktabs = TRUE,
               align = "r",
               longtable = FALSE,                
               escape = FALSE,
               col.names = c("1 egg",
                             "2 eggs",
                             "3 eggs"),
               caption = "The numbers of eggs per clutch.") %>%
    kable_styling(font_size = 10) %>%
    row_spec(0, bold = TRUE) 
}
if( knitr::is_html_output() ) {
  knitr::kable(EggsTable,
               escape = TRUE,
               format = "html",
               booktabs = TRUE,
               align = "r",
               longtable = FALSE,                
               col.names = c("1 egg",
                             "2 eggs",
                             "3 eggs"),
               caption = "The numbers of eggs per clutch.") %>%
    row_spec(1, bold = TRUE) 
}


```







::: {.exercise #NegativeBinomialALT}
The negative binomial distribution\index{Negative binomial distribution!alternative parameterisation} was defined in Sect \@ref(NegativeBinomialDistribution) for the random variable\ $X$, which represented the number of failures until the $r$th\ success.
An alternative parameterisation is to define the random variable\ $Y$ as the number of trials necessary to obtain\ $r$ successes.

1. Define the range space for\ $Y$; explain.
1. Deduce the probability function for\ $Y$.
1. Determine the mean, variance and MGF of\ $Y$, using the results already available for\ $X$.
:::



:::{.exercise #PoissonTypos}
Suppose typos made by a typist on a typing test form a *Poisson process* with the mean rate $2.5$\ typos per minute and that the test lasts five minutes.

1. Determine the probability that the typist makes exactly\ $10$ errors during the test.
1. Determine the probability that the typist makes exactly\ $6$ errors during the first 3 minutes, and exactly\ $4$ errors during the last $2$\ minutes of the test.
1. Determine the probability that the typist makes exactly\ $6$ errors during the first 3 minutes, and exactly\ $6$ during the last $3$\ minutes of the test.
   (Note: These times overlap.)
:::


:::{.exercise #RiverDepth}
The depth of a river varies from the 'normal' level, say\ $Y$ (in metres), a specific location with a PDF given by $f(y) = 1/4$ for $-2 \le y \le 2$.

1. Find the probability that\ $Y$ is greater than $1\ms$.
2. If readings on four different days are taken what is the probability that exactly two are greater than $1\ms$?
:::


:::{.exercise #QueuingPois}
Poisson distributions\index{Poisson distribution!queuing} are used in *queuing theory* to model the formation of queues.
Suppose that a certain queue is modelled with a Poisson distribution with a mean of $0.5$\ arrivals per minute.

1. Use **R** to simulate the arrivals from 8AM to 9AM (for one simulation, plot the queue length after each minute).
   Produce $100$\ simulations, and hence compute the mean and standard deviation of people in the queue at 9AM.
1. Suppose a server begins work at 8:30AM serving customers (e.g., removing them from the queue) at the rate of $0.7$\ per minute.
   Again, use **R** to simulate the arrivals for $60\mins$ (for one simulation, plot the queue length after each minute).
   Produce $100$\ simulations, and hence compute the mean and standard deviation of people in the queue at 9AM.
1. Suppose one server begins work at 8:30AM as above, and another servers begins at 8:45; together the two can serve customers (e.g., removing them from the queue) at the rate of $1.3$\ per minute.
   Again, use **R** to simulate the arrivals for 60 minutes (for one simulation, plot the queue length after each minute).
   Produce $100$\ simulations, and hence compute the mean and standard deviation of people in the queue at 9AM.
:::


:::{.exercise #PoissonEqualValues}
For the Poisson distribution in \@ref(eq:PoissonPMF), determine the values of $\lambda$ such that $\Pr(X) = \Pr(X + 1)$.
:::

:::{.exercise #HypergeometricFPC}
Consider the mean and variance for the hypergeometric distribution.\index{Hypergeometric distribution}
In this exercise, write $p = m/N$.

1. Show that the mean of the hypergeometric and binomial distributions are the same.
1. Show that the variance of the hypergeometric and binomial distributions are by connected by the *Finite Population Correction* factor (seen later in Def. \@ref(def:FPC)): $(N - k)/(N - 1)$.
:::


:::{.exercise #DiscreteUniformVar}
In this exercise, we find the variance of the discrete uniform distribution,\index{Discrete uniform distribution} using the definitions of\ $Y$ and\ $X$ in Sect.\ \@ref(DiscreteUniform).

1. First find $\operatorname{E}[Y^2]$ 
1. Then find $\operatorname{var}[X]$.
:::


:::{.exercise #GeometricPropertiesProof}
Prove the results in Theorem\ \@ref(thm:GeometricProperties), by first finding the MGF.
:::


:::{.exercise #PoissonZeroTruncated}
Prove these results from Sect.\ \@ref(PoissonExtensions): $\operatorname{E}[X] = \lambda + 1$ and $\operatorname{var}[X] = \lambda$ for the zero-truncated Poisson distribution.\index{Poisson distribution!zero truncated}
:::


:::{.exercise #GammaSpecificValues}
Prove this result from Theorem\ \@ref(thm:GammaFunctionProperties): $\Gamma(1) = \Gamma(2) = 1$.
:::

