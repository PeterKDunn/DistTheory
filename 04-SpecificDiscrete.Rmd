# Standard discrete distributions {#DiscreteDistributions}


::: {.objectivesBox .objectives data-latex="{iconmonstr-target-4-240.png}"}
Upon completion of this chapter, you should be able to:

* be familiar with the probability functions and underlying parameters of uniform, binomial, geometric, negative binomial, Poisson, and hypergeometric random variables.
* know the basic properties of the above discrete distributions.
* apply these discrete distributions as appropriate to problem solving.
:::


In this chapter, some popular discrete distributions are discussed.
Properties such as definitions and applications are considered.


## Discrete uniform distribution {#DiscreteUniform}

If a discrete random variable $Y$ can assume $k$ different and distinct values with *equal* probability, then $Y$ is said to have a *discrete uniform distribution*. 
This is one of the simplest discrete distributions.


:::{.definition #DiscreteUniform name="Discrete uniform distribution"}
If a random variable $X$ with range space $\{a, a + 1, a + 2, \dots, b\}$ where $a$ and $b$ ($a < b$) are integers has the pf
\[
   p_X(x) = \frac{1}{b - a + 1}\text{ for $x = a, a + 1, \dots, b$}
\]
then $X$ has a *discrete uniform distribution*.
We write $X\sim U(a, b)$ or $X\sim \text{Unif}(a, b)$.
:::

This distribution is also called the *rectangular distribution*.

A plot of the probability function for a discrete uniform distribution is shown in Fig. \@ref(fig:DiscreteUnform).




```{r DiscreteUnform, echo=FALSE, fig.align="center", fig.cap="The pf for the discrete uniform distribution $\\text{Unif}(a, b)$", fig.width=4, fig.height=3.5}
x <- c(1:4, 7)
y <- rep(1, length(x))

plot(x = x,
     y = y,
     ylim = c(0, 1.3),
     type = "h",
     axes = FALSE,
     lty = 3,
     col = "grey",
     main = "A discrete uniform distribution",
     xlab = expression(italic(x)),
     ylab = expression(
        paste(italic(f)[italic(X)](italic(x)))
     ),
     las = 1)
points(x = x,
       y = y,
       col = plotColour1,
       pch = 19)
       
# Ellipses
points( x = c(5.3, 5.5, 5.7),
        y = c(0.4, 0.4, 0.4),
        col = plotColour1,
        cex = 0.4,
        pch = 20) # Smaller dots than  pch = 19
points( x = c(5.3, 5.5, 5.7),
        y = c(0.8, 0.8, 0.8),
        col = plotColour1,
        cex = 0.4,
        pch = 20) # Smaller dots than  pch = 19



axis(side = 2,
     at = c(0, 1),
     labels = expression(0, 1/(italic(b) - italic(a) + 1) ) )
axis(side = 1,
     at = c(1, 2, 3, 4, 7),
     labels = expression(
                 italic(a),
                 italic(a) + 1,
                 italic(a) + 2,
                 italic(a) + 3,
                 italic(b) )
                 )
box()
```


:::{.example #DiscreteUniform name="Discrete uniform"}
Let $X$ be the number of spots showing after a single throw of a fair die.
Then $X \sim \text{Unif}(1, 6)$.
     
For an experiment involving selection of a single-digit number from a table of random digits, the number chosen, $X$, has probability distribution $\text{Unif}(0, 9)$.
:::

The following are the basic properties of the discrete uniform distribution.

:::{.theorem #DiscreteUniform name="Discrete uniform"}
If $X\sim \text{Unif}(a, b)$ then

1. $\displaystyle \text{E}(X) = \frac{a + b}{2}$.
2. $\displaystyle \text{var}(X) = \frac{(b - a)(b - a + 2)}{12}$.
:::

:::{.proof}
Working with  $Y = X - a$ rather than $X$ itself is easier.
Since $\text{E}(X) = \text{E}(Y) + a$ and $\text{var}(Y) = \text{var}(X)$, it is easier to find $\text{E}(Y)$ and $\text{var}(Y)$ directly than $\text{E}(X)$ and $\text{var}(X)$.

Since $Y\sim\text{Unif}(0, b - a)$, then using \@ref(eq:SumNaturalNumbers)
\begin{align*}
  \text{E}(Y)
  & =\sum_{i = 0}^{b - a} i\frac1{b - a + 1}\\
  & =\frac1{b - a + 1}(0 + 1 + 2 + \dots + (b - a))\\
  & =\frac{(b - a)(b - a + 1)}{2(b - a + 1)}\\
  & =\frac{b - a}{2}.
\end{align*}
Therefore,
\[
   \text{E}(X) = \text{E}(Y) + a = \frac{b - a}{2 + a} = \frac{a + b}{2}.
\]

Now $\text{var}(Y) = \text{E}(Y^2) - \text{E}(Y)^2$; so:
\begin{align*}
  \text{E}(Y^2)
  &= \sum_{i = 0}^{b - a} i^2\frac1{b - a + 1}\\
  &= \frac1{b - a + 1}(0^2 + 1^2 + 2^2 + \dots  +(b - a)^2)\\
  &= \frac1{b - a + 1}\frac{(b - a)(b - a + 1)(2(b - a) + 1)}{6}\\
  &= \frac{(b - a)(2(b - a) + 1)}{6}
\end{align*}
using \@ref(eq:SumSquaredNaturalNumbers). 
Therefore
\begin{align*}
   \text{var}(X)
   =
   \text{var}(Y)
   &= \frac{(b - a)(2(b - a) + 1)}{6} - \left(\frac{b - a}2\right)^2\\
   &= \frac{(b - a)(b - a + 2)}{12}.
\end{align*}
:::


:::{.example #LottoStats name="Lotto"}
Oz Lotto, like many lottery games, challenges players to match randomly chosen numbers.
Oz Lotto randomly picks numbers between 1 and 45 (inclusive).
Each number should have an equal chance of selection, so the discrete uniform distribution $U(1,45)$ is appropriate.

If the data follow a discrete uniform distribution exactly, the mean number drawn should be  $(1 + 45) / 2 = 23$, and the variance $(44 \times 46)/12 = 168.66$.

Using the sample data, the sample mean is computed to be ??? and the sample variance as ???; these are both very close to what one might expect for a $U(1, 45)$ random variable.
:::

FIG HERE. get the Oz Lotto data...



## Binomial distribution {#BinomialDistribution}

A binomial distribution is used in a situation where the same 'experiment' is repeated a number of times, and one of two outcomes is observed.
A simple example is tossing a coin ten times and observing if a head falls. 

The same experiment is repeated (tossing the coin), there are only two outcomes on each trial (a head or a tail), and the probability of a head remains constant on each trial.

Consider tossing a die five times and observing the number of times a `r knitr::include_graphics("Dice/die1.png", dpi=2000)` is rolled.
The probability of observing three `r knitr::include_graphics("Dice/die1.png", dpi=2000)` can be found as follows:
In the five tosses, a `r knitr::include_graphics("Dice/die1.png", dpi=2000)` must appear three times; there are ${5\choose 3}$ ways of allocating on which of the five rolls they will appear.
In the five rolls, `r knitr::include_graphics("Dice/die1.png", dpi=2000)` must appear three times with probability $1/6$; the other two rolls must produce another number with probability $5/6$.

So the probability will be
\[
   \Pr(\text{3 ones}) = {5\choose 3} (1/6)^3 (5/6)^2 = 0.032,
\]
assuming independence of the events.
Using this approach, the pmf for the binomial distribution can be developed.

Formally, situations giving rise to a binomial distribution are defined in terms of *Bernoulli trials*.

:::{.definition #BernoulliTrials name="Bernoulli trials"}
A *Bernoulli trial* is an experiment with only two possible outcomes, usually labelled 'success' $s$ and 'failure' $f$.
The sample space can be denoted by $S = \{ s, f\}$.
:::

::: {.importantBox .important data-latex="{iconmonstr-warning-8-240.png}"}
The terms 'success' and 'failure' are not literal.
\smallskip

'Success' simply means that the event is of interest.
If the event of interest is the number of damaging tornadoes, this is still called a 'success'.
:::


A binomial situation arises if a *sequence* of Bernoulli trials is observed, in each of which $\Pr(\{ s\} ) = p$ and $\Pr(\{ f\} ) = q = 1 - p$.
If $n$ such trials are conducted, consider the random variable $X$, where $X$ is the number of successes in $n$ trials. 
Now $X$ will have value set $R_X = \{ 0, 1, 2, \dots, n\}$.
$p$ must be constant from trial to trial, and the $n$ trials must be independent.

Consider the event $X = r$ (where $0\leq r\leq n$). 
This could correspond to the sample point
\[ 
   \underbrace{S \quad S \quad S \dots S \quad S \quad S \quad S}_r\quad
   \underbrace{F \quad F \dots F \quad F}_{n - r} 
\]
which is the intersection of $n$ independent events consisting of $r$ successes and $n - r$ failures, and hence the probability is $p^r q^{n - r}$.

Every other sample point in the event $X = r$ will appear as a rearrangement of the $S$'s and $F$'s in the sample point described above and will therefore have the same probability. 

Now the number of distinct arrangements of the $r\,S$'s and $(n - r)$ $F$'s is ${n\choose r}$, so
\begin{equation}
     \Pr(X = r) = {n\choose r} p^r q^{n - r},\quad r = 0,1,\dots,n.
        (\#eq:BinomialPMF)
\end{equation}

Note that the sum of the probabilities is 1 as the binomial expansion of $(p + q)^n$ \@ref(eq:BinomialSeries) is just
\begin{equation}
   \sum_{r = 0}^n {n\choose r} p^r q^{n - r} = (p + q)^n = 1\label{EQN:sumbin}
\end{equation}
since $p + q = 1$. 
So we have the following:

:::{.definition #BinomialDistribution name="Binomial distribution"}
Let $X$ be the number of successes in $n$ independent Bernoulli trials with $\Pr(\text{Success}) = p$ ($0\le p\le 1$) constant in each trial.
Then $X$ is said to have a *binomial probability distribution* with parameters $n$, $p$ and pf given by \@ref(eq:BinomialPMF).
We write $X\sim\text{Bin}(n, p)$.
:::

Fig. \@ref(fig:BinomialExamples) shows the pf for the binomial distribution for various parameter values.


```{r BinomialExamples, echo=FALSE, fig.align="center", fig.cap="The pf for the binomial distribution for $p=0.2$ and $p=0.7$; $n=10$ and $n=20$", fig.height = 7, fig.width=7, out.width='80%'}
par( mfrow = c(2, 2))

n <- c(8, 8, 
       15, 15)
prob <- c(0.2, 0.6, 
          0.2, 0.6)

for (i in (1:4)) {
  ni <- n[i]
  probi <- prob[i]
  x <-  0:ni
  y <- dbinom(x, 
              size = ni,
              prob = probi)
  
   plot( x = x,
         y = y,
         xlab = expression(italic(x)),
         ylab = "Probability function",
         main = bquote("Binomial: " ~ italic(n) == .(ni) ~ " and " ~ italic(p) == .(probi)),
         las = 1,
         lty = 3,
         col = "grey",
         type = "h")
  points(x = x,
         y = y,
         col = plotColour1,
         pch = 19)
  
}
```

:::{.example #ThrowDice name="Throwing dice"}
A die is thrown 4 times. 
What is the probability of exactly 2 sixes?

There are 4 Bernoulli trials with $p = 1/6$.
Let the random variable $X$ be the  number of 6's in 4 tosses. 
Then
\[ 
  \Pr(X = 2) = 
  {4\choose 2} \left(\frac16\right)^2\left(\frac56\right)^2 = 150/1296.
\]
:::

The following are the basic properties of the binomial distribution.

:::{.theorem #BinomialProperties name="Properties of the binomial distribution"}
If $X\sim\text{Bin}(n,p)$ then

1. $\text{E}(X) = np$.
2. $\text{var}(X) = np(1 - p) = npq$.
3. $M_X(t) = (pe^t + q)^n$.
:::

:::{.proof}
We make use of \@ref(eq:BinomialSeries) in the following:
\begin{align*}
     \text{E}(X) 
     & = \sum^n_{x = 0} x{n\choose x} p^xq^{n - x}\\
     & = \sum^n_{x = 1} x \frac nx {n - 1\choose x - 1} p^xq^{n - x}\\
     & = np\sum^n_{x = 1} {n-1\choose x-1} p^{x - 1}q^{n - x}\\
     & = np \sum^{n - 1}_{y = 0} {n - 1\choose y}p^yq^{n - 1 - y}\quad
\text{putting }y = x - 1.
\end{align*}
Now the sum is 1 since a term in the sum is the probability of $y$ successes in $(n - 1)$ Bernoulli trials and the sum is over all values in $R_X$.
Note also that in the second line the sum is over $x$ from $1$ to $n$ because for $x = 0$ we have a binomial coefficient of the form ${b\choose a}$ with $a < 0$, which is defined to be zero. 
Thus,
\[ 
   \text{E}(X) = np.
\]

To find the variance, we use the computational formula $\text{var}(X)=\text{E}(X^2)-\text{E}(X)^2$.
Firstly, to find $\text{E}(X^2)$, write $\text{E}(X^2)$ as $\text{E}[X(X-1)+X]$ then write it as $\text{E}[X(X-1)]+\text{E}(X)$.
\begin{align*}
     \text{E}(X^2) 
     &= \sum^n_{x = 0} x(x - 1)\Pr(X = x) + np\\
     &= \sum^n_{x = 2} x(x  -1)\frac{n(n - 1)}{x(x - 1)} {n - 2\choose x - 2} p^xq^{n - x} + np\\
     &= \sum^n_{x = 2} n(n - 1){n - 2\choose x - 2} p^x q^{n - x} + np\\
     &= n(n - 1)p^2 \sum^{n - 2}_{y = 0} {n - 2\choose y} p^yq^{n - 2 - y} + np,
\end{align*}
putting $y = x - 2$.
For the same reason as before, the sum is 1, so
\[ 
   \text{E}(X^2) = n^2 p^2 - np^2 + np 
\]
and hence
\[ 
   \text{var}(X) 
   = \text{E}(X^2) - [\text{E}(X)]^2 
   = n^2p^2 - np^2 + np -n^2 p^2
   = np (1- p). 
\]

The mgf of $X$ is given by
\begin{align*}
   M_X(t)
   &= \text{E}(\exp(tX))\\
   &= \sum^n_{x = 0} e^{tx}{n\choose x} p^x q^{n - x}\\
   &= \sum^n_{x = 0}{n\choose x} (pe^t)^x q^{n - x}\\
   &= (pe^t + q)^n.
\end{align*}
Proving the third result first, and then using it to prove the others, is easier (using of the methods in Sect. \@ref(MGFMoments)---try this as an exercise.)
:::


::: {.importantBox .important data-latex="{iconmonstr-warning-8-240.png}"}
A binomial situation requires the trials to be *independent*, and the probability of success $p$ to be *constant* throughout the trials.
\smallskip

For example, drawing cards from a pack without replacing them is **not** a binomial situation; after drawing one card, the probabilities will then change for the drawing of the next card.
\smallskip

In this case, the *hypergeometric distribution* could be used (Sect. \@ref(HypergeometricDistribution)).
:::


:::{.example name="Freezing lake"}
@BIB:Wilks:statistical (p. 68) notes that in the 200 years from 1796 to 1995, Cayuga Lake has frozen only in ten of those years.
Since the lake is deep, it will only freeze during exceptionally cold weather.
The probability the lake freezes during any one year can be estimated as $p = 10/200 = 0.05$.

Using this information, the number of times the lake will *not* freeze in ten randomly chosen years in given by the random variable $X$ where $X \sim \text{Bin}(10, 0.95)$.
The probability that the lake will not freeze in these ten years is $\Pr(X = 10) = {10\choose 10} 0.95^{10} 0.05^{0} \approx 0.599$, or about 60%.

Note we could define the random variable $Y$ as the number of times the lake *will* freeze in the ten randomly chosen years.
Then, $Y\sim\text{Bin}(10, 0.05)$ and we would compute $\Pr(Y = 0)$ and get the same answer.
:::

Tables of binomial probabilities are commonly available, but computers (e.g., using R) can also be used to generate the probabilities.
If the number of 'successes' has a binomial distribution, so does the number of 'failures'.
Specifically if $X\sim \text{Bin}(n,p)$, then $Y = (n - X) \sim \text{Bin}(n, 1 - p)$.

Binomial probabilities can sometimes be approximated using the normal distribution (Sect. \@ref(NormalApproxBinomial)) or the Poisson distribution (Sect. \@ref(PoissonBinomial)).



## Poisson distribution {#PoissonDistribution}

The Poisson distribution is commonly used to model the number of occurrences of an event which occurs randomly in time or space.

The Poisson distribution arises as a result of assumptions that are made about a random experiment, and roughly these are:

* Events that occur in one time-interval (or region) are independent of those occurring in any other non-overlapping time-interval (or region)
* For a small time-interval, the probability that an event occurs in it is proportional to the length of the interval
* The probability that 2 or more events occur in a very small time-interval is so small that it can be neglected.

Whenever these assumptions are valid, or approximately so, the Poisson distribution is appropriate, and quite a number of natural phenomena fall into this category.

:::{.definition #PoissonDistribution name="Poisson distribution"}
A random variable $X$ is said to have a *Poisson distribution* if its pmf is
\[
   p_X(x) = \frac{\exp(-\mu) \mu^x}{x!}\quad   \text{for $x=0$, $1$, $2$, $\dots$}
\]
where the parameter is $\mu > 0$.
We write $X\sim\text{Pois}(\mu)$.
:::

The pmf for a Poisson distribution for different values of $\mu$ is shown in Fig. \@ref(fig:PoissonExamples).


```{r PoissonExamples, echo=FALSE, fig.align="center", fig.cap="The pf for the Poisson distribution for various values of $\\mu$", fig.height = 7, fig.width=7, out.width='80%'}
par( mfrow = c(2, 2))

xmax <- 10
mu <- c(0.5, 1, 2, 5)

for (i in (1:4)) {
  muvalue <- mu[i]
  x <-  0:xmax
  y <- dpois(x, 
             lambda = muvalue )
  
   plot( x = x,
         y = y,
         xlim = c(0, xmax),
         xlab = expression(italic(x)),
         ylab = "Probability function",
         main = bquote("Poisson mean:" ~ mu == .(muvalue)),
         las = 1,
         lty = 3,
         col = "grey",
         type = "h")
  points(x = x,
         y = y,
         col = plotColour1,
         pch = 19)
  
}
```


The following are the basic properties of the Poisson distribution.

::: {.theorem}
If $X\sim\text{Pois}(\mu)$ then

1. $\text{E}(X) = \mu$.
2. $\text{var}(X) = \mu$.
3. $M_X(t) = \exp[ -\mu\{1 - \exp(t)\}]$.
:::

:::{.proof}
The third result is proven as follows:
\begin{align*}
   M_X(t) = \text{E}(e^{tX}) 
     &= \sum^\infty_{x = 0} e^{xt} e^{-\mu} \mu^x / x!\\
     &= e^{-\mu} \sum^\infty_{x = 0} \frac{(\mu\,e^t)^x}{x!} \\
     &= e^{-\mu} \left[ 1 + \mu\,e^t + \frac{(\mu\,e^t)^2}{2!} + \dots \right]\\
     &= e^{-\mu} e^{\mu\,e^t}\\
     &= e^{-\mu (1 - e^t)}.
\end{align*}
The first two results follow from differentiating the mgf.
:::

:::{.example #Queues name="Queuing"}
Customers enter a waiting line 'at random' at a rate of 4 per minute.
Assuming that the number entering the line in any given time interval has a Poisson distribution, determine the probability that at least one customer enters the line in a given $\frac 12$-minute interval, we have (since $\mu = 2$):
\[ 
   \Pr(X\geq 1) = 1 - \Pr(X = 0) = 1 - e^{-2} = 0.865.
\]
:::

:::{.example #Bombs name="Bomb hits"}
@clarke1946application (quoted in @data:hand:handbook, Dataset 289) gives the number of flying bomb hits on London during World War II in a 36 square kilometre area of South London.
The area was gridded into 0.25 km squares and the number of bombs falling in each grid was counted. 
If the hits are random, a Poisson distribution should fit the data.
The data are given in Table \@ref(tab:BombTable).

Using the proportions as estimates of the true probabilities, the sample mean can be computed using the definition of the mean.
\begin{align*}
   \text{E}(X)
   &\approx \sum x p_X(x) \\
   &= (0\times 0.3976)  + (1\times 0.2663) + \cdots + (7 \times 0.0017)\\
   &=  0.9323
\end{align*}
Using this value as an estimate of the Poisson mean, the pf for the Poisson distribution can be compared to the empirical probabilities computed above; see Table \@ref(tab:BombTable).
(For example, the probability of zero hits is 
\[
   \frac{\exp(-0.9323) (0.9323)^0}{0!} \approx 0.3936.
\]
The two probabilities are very close; the Poisson distributions fits the data very well.
:::


```{r BombTable, echo=FALSE}
BombTable <- array( dim = c(8, 4))
colnames(BombTable) <- c("Hits",
                         "Number bombs",
                         "Proportion bombs",
                         "Poisson proportion")
BombTable[, 1] <- 0:7
BombTable[, 2] <- c(229,
                    211,
                    93,
                    35,
                    7,
                    0,
                    0,
                    1)
BombTable[, 3] <- round(BombTable[, 2] / sum(BombTable[, 2]), 4)
lambda <- sum( BombTable[, 1] * BombTable[, 3])

BombTable[, 4] <- dpois(0:7,
                        lambda = lambda )
BombTable[, 4] <- round( BombTable[, 4], 4)
BombTable[ 8, 4] <- 1 - sum( BombTable[1:7, 4])

BombTable.caption <- "The number of flying bomb hits on London during World War II in a 36 square kilometre area of South London. The proportion of the 576 grid squares receiving 0, 1, ...hits was also computed. The observed and empirical probabilities are shown. The Poisson distribution fits the data  well."

if( knitr::is_latex_output() ) {
  knitr::kable( BombTable,
                format = "latex",
                booktabs = TRUE,
                longtable = FALSE,
                col.names = c("Hits",
                              "Number bombs", 
                              "Proportion bombs", 
                              "Poisson proportion"),
                caption = BombTable.caption) %>%
    kable_styling(font_size = 10) %>%
    row_spec(0, bold = TRUE)
}
if( knitr::is_html_output() ) {
  knitr::kable( BombTable,
                format = "html",
                booktabs = TRUE,
                longtable = FALSE,
                col.names = c("Hits",
                              "Number bombs", 
                              "Proportion bombs", 
                              "Poisson proportion"),
                caption = BombTable.caption) %>%
    kable_styling(font_size = 10) %>%
    row_spec(0, bold = TRUE)
}

```



### Relationship to the binomial distribution {#PoissonBinomial}

If the number of trials is very large and the probability of success in a single trial is very small, then the computation of binomial probability is not easy. 
For example, consider $X\sim \text{Bin}(n = 2000, p = 0.005)$. 
Then, the pf is
\[
   p_X(x) = {2000\choose x}(0.005)^x 0.995^{2000 - x}.
\]

Using the pmf, computing some probabilities, such as $\Pr(X > 101)$, is very tedious.
(Try computing ${2000\choose 102}$ on your calculator, for example.)
However, the Poisson distribution can be used to approximate this probability.

We could set the Poisson mean to equal the binomial mean (that is, $\mu = np$).
The same can be done for the variance, setting $\mu = np(1 - p)$.
Since $\text{E}(Y) = \text{var}(Y) = \mu$ for the Poisson distribution, this can only be (approximately) true here if $p$ is close to zero.

A general guideline is that the Poisson distribution can be used to approximate the binomial when $n$ is large, $p$ is small and $np$ is less than about 7.


```{r BinomialPoisson, echo=FALSE, fig.align="center", fig.cap="The Poisson distribution is an excellent approximation to the binomial distribution when $p$ is small and $n$ is large. The binomial pf is shown using empty circles; the Poisson pf using crosses.", fig.height = 7, fig.width=7, out.width='80%'}
par( mfrow = c(2, 2))

n <- c(10, 50, 
       100, 1000)
p <- c(0.2, 0.1, 
       0.010, 0.005)
xmax <- c(10, 20, 
          10, 15)

for (i in (1:4)) {
   pValue <- p[i]
   nValue <- n[i]
   
  x <-  0:n[i]
  y <- dbinom(x, 
              size = n[i],
              prob = p[i])
  
   plot( x = x,
         y = y,
         xlim = c(0, xmax[i]),
         xlab = expression(italic(x)),
         ylab = "Probability",
         main = bquote("Binomial: " ~ italic(n) == ~ .(nValue) ~ "and" ~ italic(p) == .(pValue) ),
         las = 1,
         lty = 3,
         col = "grey",
         type = "h")
  points(x = x,
         y = y,
         col = plotColour,
         pch = 19)

  muPoisson <- nValue * pValue
  xValues <- 0:xmax[i]
  points( x = xValues,
          y = dpois(xValues,
                    lambda = muPoisson),
          pch = 4)
  
  if (i == 4 ){
    legend("topright", 
           col = c("black",
                   plotColour),
           pch = c(1, 19),
           legend = c("binomial",
                      "Poisson approx."))
  }
  
}

```

### Extensions

Zero-inflated.

Truncated.


### Applications:

@brude1993models: Accidents involving pedestrians and cyclists 

@sari2009stochastic: Accidents per month in a coal mine

Rain events per month??



## Geometric distribution {#GeometricDistribution}

Consider now an experiment where independent Bernoulli trials are repeated until the *first* success occurs. 
What is the distribution of the number of trials required?

Let the random variable $Y$ be the number of trials necessary to obtain the first success.
Since the first success may occur on the first trial, or second trial or third trial, and so on, $Y$ is a random variable with range space $\{1, 2, 3, \dots\}$.

The distribution is easy to derive.
To observe the first success on the $y$th trial, there must be $y - 1$ failures followed by one success.
Since the probability of failure is $q$ and the probability of success is $p$, the probability of the first success on trial $y$ is
\begin{align*}
   \text{$(y - 1)$ failures}    &\times \text{first success} \\
   q^{y - 1}                    &\times  p.
\end{align*}
This derivation assumes the events are independent.


:::{.definition #GeometricDistribution name="Geometric distribution"}
A random variable $X$ has a *geometric distribution* if the pf of $X$ is given by
\[
   p_X(x) =  q^{x-1} p\quad\text{for $x = 1,2,\dots$}
\]
where $q = 1 - p$ and $0 < p < 1$ is the parameter of the distribution.
We write $X\sim\text{Geom}(p)$.
:::


The pf for a geometric distribution for various values of $p$ is shown in Fig. \@ref(fig:Geometric).

```{r Geometric, echo=FALSE, fig.align="center", fig.cap="The pf for the geometric distribution for $p = 0.2$, $0.3$, $0.5$ and $0.8$", fig.height = 7, fig.width=7, out.width='80%'}
par( mfrow = c(2, 2))

xmax <- c(10, 15, 15, 15)
p <- c(0.1, 0.3, 
       0.5, 0.8)

for (i in (1:4)) {
  pValue <- p[i]
  x <-  0:n[i]
  y <- dgeom(x,
             prob = pValue)
  
   plot( x = x,
         y = y,
         xlim = c(0, xmax[i]),
         xlab = expression(italic(x)),
         ylab = "Probability function",
         main = bquote("Geometric: " ~ italic(p) == .(pValue) ),
                  las = 1,
         col = "grey",
         lty = 3,
         type = "h")
  points(x = x,
         y = y,
         col = plotColour1,
         pch = 19)
  
}

```
The following are the basic properties of the geometric distribution.

:::{.theorem #GeometricProperties name="Properties of geometric distribution"}
If $X\sim\text{Geom}(p)$ then

1. $\text{E}(X) = 1/p$.
2. $\text{var}(X) = (1 - p)/p^2$.
3. $M_X(t) = pe^t/\{1 - (1 - p)e^t\}$.
:::

:::{.proof}
The first and last results can be proven directly, but easier is to prove the third results and then use the mgf to prove the first two.
This is left as an exercise.
:::


:::{.example #GeometricComputerFail name="Computer failure"}
A computer system fails at random with probability $0.02$ on any given day.


Let $Y$ be the day on which the system fails, so that $Y = \{1, 2,3, \dots\}$.
Then $Y \sim\text{Geom}(0.02)$.

Suppose we wish to find the probability that the system will first fail before day $20$.
We seek $\Pr(Y < 20) = \Pr(Y \le 19)$.
This can be done by hand, or using a computer.
In R, the command `dgeom()` gives the probability mass function for the geometric distribution, and `pgeom()` gives values from the distribution function of the geometric distribution:
```{r echo=TRUE}
sum( dgeom( x = 1:19, 
            prob = 0.02))
pgeom(q = 19, 
      prob = 0.02) # i.e., up to 18 failures at p=0.02
```
The probability is $0.32$ that the system will fail before day $20$.

The number of days the system can be expected to work until failure is $\text{E}(Y) = 1/p = 1/0.02 = 50$.
The system could be expected to last fifty days before the first failure.
:::




:::{.example #BabyBoom name="Baby Boom"}
An article appearing in *The Sunday Mail* on 21 December 1997 give the birth weight, gender, and time of birth of 44 babies born in the 24-hour period of 18 December 1997 at the Mater Mother's Hospital in Brisbane, Australia.

KEEP FOR EXERCISE OR ASSIGNMENT


The gender of the births is a random process, and so the number of births observed until a boy is born should have approximately a geometric distribution.
By counting the number of births until a boy is born (and restarting the count when a girl is born), Table ??? has been constructed.
The theoretical probabilities have been based on the probability of a boy being born as being $0.5$.
The fit is reasonable (but not brilliant) for the small sample size; see Table ???.
:::



### Application: Estimating population sizes

Geometric distributions are simple, but have applications in estimating population sizes.

Consider a population of animals of a certain species of unknown size $N$.
We then trap a certain number of animals in an area, tag them, and then release them.
At a later point (EQUALLY SPACED?), we capture (TEH SAME NUMBER OF) animals, and observe how many have been tagged before.
Define $n_x$ as the number of animals captured $x$ times, in a total of $s$ trapping events (where $s$ is 'large'),
and $p$ is the probabiiuty that ana animal is trapped zero times in the $s$ trappingf events.

Then,
\[
   P(X = x) = p (1 - p)^x.
\]
Then,
\[
   n_x = N p (1 - p)^x
\]
and, after taking logarithms,
\[
   \log n_x = \log Np + x\log(1 - p).
\]
Information available is $n_x$ for various values of $x$, so that the estimates of $Np$ and $\log(1 - p)$ are found from a regression of $\log n_x$ on $x$.
From these, estimates of $N$ and $p$ can be found.

See: [@romesburg1979fitting]


### Application Modelling hot and dry spells

[@furrer2010statistical]:
The annual frequency of hot spells was modeled by a
Poisson distribution, and their length by a geometric distribution. 

"Given its memoryless property, the geometric distri-
bution is the simplest plausible model for spell length.
Smith et al. (1997) used it to model the cluster length of
low minimum daily temperatures, although they found
some evidence that a distribution with a heavier tail
might be needed. "


## Negative binomial distribution {#NegativeBinomialDistribution}

### Standard parameterisation

Consider a random process where independent Bernoulli trials are repeated until the $r$th success occurs. 
Let random variable $Y$ be the number of trials necessary to obtain $r$ successes.
Clearly $Y$ will be at least $r$ and the range space for $Y$ is $\{ r, r + 1, r + 2,\dots \}$.

To observe the $r$th success in the $y$th trial, there must have been $y - r$ failures and $r - 1$ successes in the first $y - 1$ trials; the $r$th success then happens on the $y$th trial.
There are ${y - 1\choose r - 1}$ ways in which to allocate these successes to the first $y - 1$ trials.
Each of the $r - 1$ successes occur with probability $p$, and the $y - r$ failures with probability $1 - p$ (assuming events are independent).

Hence the probability of observing the $r$th success in trial $y$ is
\begin{align*}
   \text{No. ways}
   &\times \text{$(y - r)$ failures} \times \text{$r$ successes} \\
   {y - 1 \choose r - 1} 
   &\times (1 - p)^{y - r} \times p^r.
\end{align*}


:::{.definition #NegativeBinomialDistribution name="Negative binomial distribution"}
A random variable $X$ with pf
\begin{equation}
   p_X(x) = {x - 1\choose r - 1}(1 - p)^{x - r} p^r
   (\#eq:NegativeBinomialPMF)
\end{equation}
has a *negative binomial distribution* with parameters $r$ (an integer $\ge 1$) and $p$ ($0\le p\le1$).
We write $X\sim\text{NB}(r, p)$.
:::


The pf for the negative binomial distribution for various values of $p$ and $r$ is shown in Fig. \@ref(fig:NegativeBinomial).

When $r = 1$, the negative binomial distribution is the same as the geometric distribution: the geometric distribution is a special case of the negative binomial.



```{r NegativeBinomial, echo=FALSE, fig.align="center", fig.cap="The pf for the negative binomial distribution for $p = 0.2$ and $0.7$ and $r = 1$ and $r = 3$", fig.height = 7, fig.width=7, out.width='80%'}
par( mfrow = c(2, 2))
xmax <- 15

r <- c(1, 3, 
       1, 3)
p <- c(0.2, 0.2, 
       0.6, 0.6)

for (i in (1:4)) {
  rValue <- r[i]
  pValue <- p[i]
  
  x <-  0:n[i]
  y <- dnbinom(x, 
              size = r[i],
              prob = p[i])
  
   plot( x = x,
         y = y,
         xlim = c(0, xmax),
         xlab = expression(italic(x)),
         ylab = "Probability",
         main = bquote("Negative binomial: " ~ italic(r) == ~ .(rValue) ~ ";" ~ italic(p) == .(pValue) ),
         las = 1,
         lty = 3,
         col = "grey",
         type = "h")
  points(x = x,
         y = y,
         col = plotColour1,
         pch = 19)
  
}

```


The following are the basic properties of the negative binomial distribution.

:::{.theorem #NegBinomialProperties}
If $X\sim \text{NB}(r,p)$ with pf \@ref(eq:NegativeBinomialPMF) then

1. $\text{E}(X) = r/p$.
2. $\text{var}(X) = r(1 - p)/p^2$.
3. $M_X(t) = \left[ pe^t/\{1 - (1 - p)e^t\} \right]^r$
:::

:::{.proof}
Again, proving the third statement initially is sensible, which is left as an exercise.
:::


:::{.example #Demonstration name="Negative binomial"}
A telephone marketer invites customers, over the phone, to a product demonstration.
Ten people are needed for the demonstration. 
The probability that a randomly chosen person accepts the invitation is only $0.15$.

* How likely is it that the marketer will need to make more than $100$ calls to secure ten acceptances?

In this situation, a 'success' is an acceptance to attend the demonstration.
Let $Y$ be the number of calls necessary to secure ten acceptances.
Then $Y$ has a negative binomial distribution such that $Y\sim\text{NBin}(p = 0.15, r = 10)$.
The mean number of calls to be made will be $\text{E}(Y) = r/p = 10/0.15 \approx 66.7$.

To determine $\Pr(Y > 100) = 1 - \Pr(Y\le 100)$, using a computer is the easiest approach. 
In **R**, the command `dnbinom()` returns probabilities from the probability function of the negative binomial distribution, and `pnbinom()` returns the cumulative distribution probabilities:

```{r}
x.values <- seq(1, 100, 
                by = 1)
1 - sum(dnbinom( x.values, 
                 size = 10, 
                 prob = 0.15))
# Alternatively:
1 - pnbinom(100, 10, 0.15)

```

The probability is about $0.0244$.

* Suppose each call take an average of 5 minutes.
  How long is the marketer expected to be calling to find ten acceptances?


Let $T$ be the time to make the calls in minutes.
Then $T = 5Y$.
Hence, $\text{E}(T) = 5\text{E}(Y) = 5 \times 66.7 = 333.5$, or about 5.56 hours.


* Suppose each call costs $25$ cents.
  If the company pays the marketer \$30 per hour, what is the average cost of securing ten acceptances?

Let $C$ be the total cost in dollars.
The cost of employing the marketer is, on average, $30\times 5.56 = \$166.75$.
Then $C = 0.25 Y + 166.75$, so $\text{E}(C) = 0.25 \text{E}(Y) + 166.75 = C = 0.25 \times 66.7 + 166.75 = \$183.43$.
:::


### Alternative parameterisation

Often the negative binomial distribution is used in a different parameterisation. 
This form can be seen as the number of *failures* which occur in a sequence of independent trials until $r$ successes are reached. 
The pf is this reparameterised form in
\begin{equation}
   p_X(x) = {x + r - 1 \choose x} p^r (1 - p)^x
   (\#eq:NegativeBinomialPMFAlternative)
\end{equation}
where $X$ is the number of *failures* until the $r$th success is observed. 
Note that the variables $X$ and $Y$ are related by $Y = X + r$ for constant $r$. 
Hence, this pf is defined for $x = 0$, $1$, $2$, $\dots$.

$r$ can be any positive number, not just an integer, in either parameterisation.
Setting $r$ as non-integer, however, is most common with the alternative parameterisation in \@ref(eq:NegativeBinomialPMFAlternative).

When $r$ is non-integer, the above interpretations are lost, but the distribution is more flexible.
Relaxing the restriction on $r$ in the form of \@ref(eq:NegativeBinomialPMFAlternative) gives the pf as
\begin{equation}
   p_X(x) = \frac{\Gamma(x + r)}{\Gamma(r) x!} p^r (1 - p)^x,
   (\#eq:NegativeBinomialPMFAlternative2)
\end{equation}
for $x = 0$, $1$, $2$, $\dots$ and $r > 0$.
In this expression, $\Gamma(r)$ is the *gamma function* (see below), and is like a factorial;
indeed, $\Gamma(r) = (r - 1)!$ if $r$ is a positive integer.
See Sect. \@ref(GammaFunction).

This form of the negative binomial distribution is sometimes used in place of the Poisson distribution used to model count data (they are both defined on $\{0, 1, 2, \dots\}$).
Since the negative binomial distribution has two parameters and the Poison only one, the negative binomial distribution often produces a better fit.

For the reparameterised version of the negative binomial distribution, Theorem \@ref(thm:NegBinomialProperties) is restated.

:::{.theorem #NegBinomialProperties2}
If $X\sim \text{NB}(r, p)$ with pf (\@ref(eq:NegativeBinomialPMFAlternative)), then

1. $\text{E}(X) = r(1 - p)/p$
2. $\text{var}(X) = r(1 - p)/p^2$
3. $M_X(t) = \left[ p/\{1 - (1 - p)e^t\} \right]^r$
:::


### The gamma function {#GammaFunction}

:::{.definition #GammaFunction name="Gamma function"}
The function $\Gamma(\cdot)$ is called the *gamma function* and is defined as
\[
   \Gamma(r) = \int_0^\infty x^{r - 1}\exp(-x)\, dx
\]
for $r > 0$.
:::

The gamma function has the property that
\begin{equation}
   \Gamma(r) = (r - 1)!
   (\#eq:gammaprop)
\end{equation}
if $r$ is a positive integer (Fig. \@ref(fig:GammaFunction)).

Important properties of the gamma function are given below.


:::{.theorem #GammaFunctionProperties name="Gamma function properties"}
For the gamma function $\Gamma(\cdot)$,

1. $\Gamma(r) = (r - 1)\Gamma(r - 1)$ if $r > 0$.
2. $\Gamma(1/2) = \sqrt{\pi}$.
:::

:::{.proof}
Integration by parts gives
\begin{align*}
   \Gamma(r)
   &= \int_0^{\infty}-x^{r - 1} \, d(e^{-x})\\
   &= 0 + \ (r - 1) \int_0^{\infty}  e^{-x}  x^{r - 2} \, dx\\
   &= ( r - 1) \Gamma(r - 1).
\end{align*}
The second property follows directly from this result (Exercise!).

Putting $r = 1/2$ in the first part of this theorem gives
\begin{align*}
   \Gamma(1/2)
   &= \int_0^{\infty}e^{-x}  x^{-\frac{1}{2}}\,dx\\
   &= \sqrt{2}\int_0^{\infty}e^{-\frac{1}{2}z^2}\,dz, \text { putting $x = z^2/2\quad z \geq 0$}\\
   &= \sqrt{2}  \sqrt{2 \pi} \underbrace{\int_0^{\infty}\frac{1}{\sqrt{2 \pi}} e^{-\frac{1}{2}z^2}\,dz}_{1/2}.
\end{align*}
The integral is $\frac{1}{2}$, being half the area under a normal curve. 
So
\[
   \Gamma(1/2) = \sqrt{\pi}.
\]
:::


```{r GammaFunction, echo=FALSE, fig.cap="The gamma function is like the factorial function but has a continuous argument. The line corresponds to the gamma function $\\Gamma(z)$; the solid points correspond to the factorial $(z - 1)! = \\Gamma(z)$ for integer $z$", fig.align="center", fig.width=4, fig.height=3.75}

xPoints <- 1:5
xCont <- seq(0.1, 5, 
             length = 100)

plot( x = c(0, 5),
      y = c(0, 25),
      type = "n",
      las = 1,
      col = plotColour1,
      xlab = expression(italic(z)),
      ylab = bquote("Gamma function" ~ Gamma(z)),
      main = "The Gamma function")
lines( x = xCont,
       col = plotColour1,
       y = gamma(xCont),
       lwd = 2)
points(x = xPoints,
       y = gamma(xPoints),
       col = plotColour1,
       pch = 19)
legend("top",
       pch = c(NA, 19),
       lty = c(1, NA),
       col = plotColour1,
       lwd = c(2, NA),
       legend = c(bquote("Gamma function" ~ Gamma(z)),
                  expression(paste( group("(", list(z - 1),")"), "!" ) ) ) )
```

:::{.example #Computing name="Negative binomial"}
Consider the computer system in Example \@ref(exm:GeometricComputerFail). 
Suppose after five failures, the system is upgraded.

* What is the probability that an upgrade will happen within one year?

Let $D$ be the days on which the system fails for the fifth time.
Then $D\sim \text{NBin}(p = 0.02, r = 5)$, and $D = \{5, 6, 7, \dots\}$.

We seek $\Pr(D < 365)$ or, if using the alternate parameterisation where $X = D - r$, we seek $\Pr(X < 360)$.
Using **R**:

```{r echo=TRUE}
pnbinom(360, 
        size = 5, 
        prob = 0.02)
```
So the probability of upgrading within one year is about $85$%.

* What is the expected number of days between upgrades?

$\text{E}(D) = 5/0.02 = 250$. 
About $250$ days should elapse between upgrades, on average,
:::


:::{.example #NBMites name="Mites"}
@BIB:Bliss:negbin gives data from the counts of adult European red mites on leaves selected at random from six similar apple trees; see Table \@ref(tab:MitesTable).
From the data, the mean mites per leaf is
\[
   \text{E}(X) = (0.467 \times 0) + (0.253 \times 1) +\cdots \approx 1.14667.
\]
To compute the variance,
\[
   \text{E}(X^2) = (0.467 \times 0^2) + (0.253 \times 1^2) +\cdots \approx 3.5733,
\]
so that $\text{var}(X) = 3.573333 - (1.146667)^2 =  2.258489$.
The Poisson distribution has an equal mean and variance; the Poisson distribution may not model the data well.

For the second parameterisation of the negative binomial distribution, the expression for the mean and variance can be set to the
computed values above and solved for $p$ and $r$; then $p\approx 0.5077$ and $r\approx 1.1826$.
Using these values, the estimated probability function for both the Poisson and negative binomial distributions are given in Table \@ref(tabMitesTable); the negative binomial distribution fits better as expected.
:::


```{r MitesTable, echo=FALSE}
MitesTab <- array( dim = c(9, 5))

MitesTab[1, ] <- c(0 ,  70 ,  0.467 ,  0.318 ,  0.449)
MitesTab[2, ] <- c(1 ,  38 ,  0.253 ,  0.364 ,  0.261)
MitesTab[3, ] <- c(2 ,  17 ,  0.113 ,  0.209 ,  "0.140")
MitesTab[4, ] <- c(3 ,  10 ,  0.067 ,  "0.080" ,  0.073)
MitesTab[5, ] <- c(4 ,  9 ,  "0.060" ,  0.023 ,  0.038)
MitesTab[6, ] <- c(5 ,  3 ,  "0.020" ,  0.005 ,  0.019)
MitesTab[7, ] <- c(6 ,  2 ,  0.013 ,  0.001 ,  "0.010")
MitesTab[8, ] <- c(7 ,  1 ,  0.007 ,  "0.000" ,  0.005)
MitesTab[9, ] <- c("8+" ,  0 ,  "0.000" ,  "0.000" ,  "0.000")

MitesTab.caption <- "Counts of mites on leaves selected at random from six similar apple trees"

if( knitr::is_latex_output() ) {
   knitr::kable(MitesTab,
                format = "latex",
                booktabs = TRUE,
                longtable = FALSE,
                align = "r",
                col.names = c("No. leaves", 
                              "No. mites",
                              "Emp. prob",
                              "Poisson prob",
                              "Neg. bin. prob"),
                caption = MitesTab.caption) %>%
  kable_styling(font_size = 10) %>%
  row_spec(0, bold = TRUE) 
}
if( knitr::is_html_output() ) {
   knitr::kable(MitesTab,
                format = "html",
                booktabs = TRUE,
                longtable = FALSE,
                align = "r",
                col.names = c("No. leaves", 
                              "No. mites",
                              "Emp. prob",
                              "Poisson prob",
                              "Neg. bin. prob"),
                caption = MitesTab.caption) %>%
  kable_styling(font_size = 10) %>%
  row_spec(0, bold = TRUE) 
}
```



### Applications: Car crashes

@lord2005poisson discusses **binomial** and **Poisson approx** with some discusson of how they model the situations.
And prob also neg bin.

@white1996analysis: 
Counts of biologuical populations: "The Poisson dis-
tribution assumes spatial independence, whereas this
assumption is relaxed with the negative binomial dis-
tribution."

NEED to talk abiout that... How neg bin can be found by not assuking mu is constant.
And can we show it in TRANNSFORM chapter using MGFs later even?


Estimating population sizes again:
@boyce2001negative: 
Assuming that the number of sightings of an
individual comes from a Poisson distribution
allows sightings to occur at any time during data
collection and does not require a discrete number
of trapping occasions as in many mark-recapture
studies. In such studies, the number of sightings
per animal would follow a binomial distribution;
however, the Poisson distribution could be used as
an approximation if the number of trapping occa-
sions is large, with a small probability of a sighting
occurring within each trappin

Neg bin regression.



## Hypergeometric distribution {#HypergeometricDistribution}

When the selection of items a fixed number of times is done *with* replacement, the probability of an item being selected stays the same and the binomial distribution can be used.
However, when the selection of items is done *without* replacement, the trials are not independent, making the binomial model unsuitable.
In these situations, a hypergeometric distribution is appropriate.

Consider a simple example: A bag containing six red balls and four blue balls.
The variable of interest, say $X$, is the number of red balls drawn in three random selections from the bag, without replacing the balls.
Since the balls are not replaced, $\Pr(\text{draw a red ball})$ is not constant, and so the binomial distribution cannot be used.

The probabilities can be computed, however, using probabilistic ideas from Chap. \@ref(Probability).
There are a total of ${10\choose 3}$ ways of selecting a sample of size 3 from the bag.
Consider the case $X = 0$.
The number of ways of drawing no red balls is ${6\choose 0}$ and the number of ways of drawing the three blue balls in ${4\choose 3}$, so the probability is
\[
   \Pr(X = 0) = \frac{{6\choose 0}{4\choose 3}}{{10 \choose 3}} \approx 0.00833.
\]
Likewise, the number of ways to draw one red ball (and hence two blue balls) is ${6\choose 1}\times {4\choose 2}$, so
\[
   \Pr(X = 1) = \frac{ {6\choose 1}\times {4\choose 2} }{{10 \choose 3}}.
\]
Similarly,
\begin{align*}
   \Pr(X = 2) &=\frac{ {6\choose 2}\times {4\choose 1} }{{10 \choose 3}}\\
   \Pr(X = 3) &=\frac{ {6\choose 3}\times {4\choose 0} }{{10 \choose 3}}
\end{align*}

In general, if there are $N$ balls in total in the bag, and $r$ of them are red, and we select a sample of size $n$ from the bag *without replacement*, then the probability of finding $x$ red balls in the sample of size $n$ is
\[
   \Pr(X = x) = \frac{{r\choose x}{N - r\choose n - x}}{{N\choose n}}
\]
where $X$ is the number of red balls in sample of size $n$.
(In the example, $n = 3$, $r = 6$ and $N = 10$.)

In the formula, ${r\choose x}$ is the number of ways of selecting $x$ red balls from the $r$ red balls in the bag; ${N - r\choose n - x}$ is the number of ways of selecting all the remaining $n - x$ to be the other colour (and there are $N - r$ of those in the bag); and ${N \choose n}$ is the number of ways of selecting a sample of size $n$ if there are $N$ balls in the bag in total.


:::{.definition #HypergeometricDistribution name="Hypergeometric distribution"}
Consider a set of $N$ items of which $r$ are of one kind (call them 'successes') and other $N - r$ are of another kind (call them 'failures').
We are interested in the probability of $x$ successes in $n$ trials, when the selection (or drawing) is made *without* replacement. 
Then the random variable $X$ is said to have a *hypergeometric distribution* with pf
\begin{equation}
   p_X(x) = \frac{ {r\choose x}{N - r\choose n - x}}{{N\choose n}}
   (\#eq:HypergeometricPMF)
\end{equation}
where $x = 0$, $1$, $\dots$, $n$; $x \le r$, and $n - x \le N - r$.
:::

The following are the basic properties of the hypergeometric  distribution.

:::{.theorem}
If $X$ has a hypergeometric distribution with pf \@ref(eq:HypergeometricPMF), then

1. $\text{E}(X) = nr/N$
2. $\text{var}(X) = {n\left(\frac{r}{N}\right)\left(\frac{N-r}{N}\right)\left(\frac{N-n}{N-1}\right)}$
:::
The moment generating function is far too difficult and will not be considered here.

If the population is much larger than the sample size (that is, $N$ is much larger than $n$), then the probability of a success will be approximately constant.

Consider the example at the start of this section.
The probability of drawing a red ball initially is $6/10 = 0.6$, and the probability that the next ball is red is $5/9 = 0.556$.
But suppose there are 10000 balls in the bag, of which 6000 are red.
The probability of drawing a red ball initially is $6000 / 10000 = 0.6$, and the probability that the next ball is red becomes $5999/9999 = 0.59996$; the probability is almost the same.
In this case, we might consider using the binomial distribution with $p\approx 0.6$.

In general, if $N$ is much larger than $n$, the population proportion then will be approximately $p\approx r/N$, and so $1 - p \approx (N - r)/N$.
Using this information,
\[
   \text{E}(X) = n \times (r/N) \approx np
\]
and
\begin{align*}
   \text{var}(X)
   &= n\left(\frac{r}{N}\right)\left(\frac{N - r}{N}\right)\left(\frac{N - n}{N - 1}\right)\\
   &\approx n\left( p \right ) \left( 1 - p \right) \left(1 \right ) \\
   &= n p (1 - p),
\end{align*}
which correspond to the mean and variance of the binomial distribution.


:::{.example #Mice name="Mice"}
Twenty mice are available to be used in an experiment; seven of the mice are female and $13$ are male.
Five mice are required and will be sacrificed.
What is the probability that more than three of the mice are males?

Let $X$ be the number of male mice chosen in a sample of size $5$.
Then $X$ has  a hypergeometric distribution (since mice are chosen without replacement) where $N = 20$, $n = 5$, $r = 13$ and we seek
\begin{align*}
   \Pr(X > 3)
   &= \Pr(X = 4) + \Pr(X = 5) \\
   &= \frac{ {13\choose 4} {7\choose 1}}{ {20\choose 5} } +
       \frac{ {13\choose 5} {7\choose 0}}{ {20\choose 5} } \\
   & \approx  0.3228 + 0.0830 = 0.4058
\end{align*}
The probability is about 41%.
:::


