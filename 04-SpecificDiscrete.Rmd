# Standard discrete distributions {#DiscreteDistributions}


::: {.objectivesBox .objectives data-latex="{iconmonstr-target-4-240.png}"}
Upon completion of this chapter, you should be able to:

* recognise the probability functions and underlying parameters of uniform, Bernoulli, binomial, geometric, negative binomial, Poisson, and hypergeometric random variables.
* know the basic properties of the above discrete distributions.
* apply these discrete distributions as appropriate to problem solving.
:::


 


## Introduction

In this chapter, some popular discrete distributions are discussed.
Properties such as definitions and applications are considered.


## Discrete uniform distribution {#DiscreteUniform}

If a discrete random variable $Y$ can assume $k$ different and distinct values with *equal* probability, then $Y$ is said to have a *discrete uniform distribution*. 
This is one of the simplest discrete distributions.


:::{.definition #DiscreteUniform name="Discrete uniform distribution"}
If a random variable $X$ with range space $\{a, a + 1, a + 2, \dots, b\}$ where $a$ and $b$ ($a < b$) are integers has the pf

\[
   p_X(x) = \frac{1}{b - a + 1}\text{ for $x = a, a + 1, \dots, b$}
\]
then $X$ has a *discrete uniform distribution*.
We write $X\sim U(a, b)$ or $X\sim \text{Unif}(a, b)$.
:::

This distribution is also called the *rectangular distribution*.
A plot of the probability function for a discrete uniform distribution is shown in Fig. \@ref(fig:DiscreteUnform).




```{r DiscreteUnform, echo=FALSE, fig.align="center", fig.cap="The pf for the discrete uniform distribution $\\text{Unif}(a, b)$", fig.width=4, fig.height=3.5}
x <- c(1:4, 7)
y <- rep(1, length(x))

plot(x = x,
     y = y,
     ylim = c(0, 1.3),
     type = "h",
     axes = FALSE,
     lty = 3,
     col = "grey",
     main = "A discrete uniform distribution",
     xlab = expression(italic(x)),
     ylab = expression(
        paste(italic(f)[italic(X)](italic(x)))
     ),
     las = 1)
points(x = x,
       y = y,
       pch = 19)
       
# Ellipses
points( x = c(5.3, 5.5, 5.7),
        y = c(0.4, 0.4, 0.4),
        col = plotColour1,
        cex = 0.4,
        pch = 20) # Smaller dots than  pch = 19
points( x = c(5.3, 5.5, 5.7),
        y = c(0.8, 0.8, 0.8),
        col = plotColour1,
        cex = 0.4,
        pch = 20) # Smaller dots than  pch = 19



axis(side = 2,
     at = c(0, 1),
     labels = expression(0, 1/(italic(b) - italic(a) + 1) ) )
axis(side = 1,
     at = c(1, 2, 3, 4, 5.5, 7),
     labels = expression(
                 italic(a),
                 italic(a) + 1,
                 italic(a) + 2,
                 italic(a) + 3,
                 cdots,
                 italic(b) )
                 )
box()
```


:::{.example #DiscreteUniform name="Discrete uniform"}
Let $X$ be the number of spots showing after a single throw of a fair die.
Then $X \sim \text{Unif}(1, 6)$.
     
For an experiment involving selection of a single-digit number from a table of random digits, the number chosen, $X$, has probability distribution $\text{Unif}(0, 9)$.
:::

The following are the basic properties of the discrete uniform distribution.

:::{.theorem #DiscreteUniformProperties name="Discrete uniform properties"}
If $X\sim \text{Unif}(a, b)$ then

1. $\displaystyle \text{E}(X) = (a + b)/2$.
2. $\displaystyle \text{var}(X) = \frac{(b - a)(b - a + 2)}{12}$.
:::

:::{.proof}
Working with  $Y = X - a$ rather than $X$ itself is easier (as $Y$ is defined on $[0, b - a]$), using that $\text{E}(X) = \text{E}(Y) + a$ and $\text{var}(Y) = \text{var}(X)$.

Since $Y\sim\text{Unif}(0, b - a)$, then using \@ref(eq:SumNaturalNumbers)

\begin{align*}
  \text{E}(Y)
  & =\sum_{i = 0}^{b - a} i\frac1{b - a + 1}\\
  & =\frac1{b - a + 1}(0 + 1 + 2 + \dots + (b - a))\\
  & =\frac{(b - a)(b - a + 1)}{2(b - a + 1)} = (b - a)/2.
\end{align*}
Therefore,
\[
   \text{E}(X) = \text{E}(Y) + a = \frac{b - a}{2} + a = \frac{a + b}{2}.
\]

Since $\text{var}(Y) = \text{E}(Y^2) - \text{E}(Y)^2$, continue (using \@ref(eq:SumSquaredNaturalNumbers)):

\begin{align*}
  \text{E}(Y^2)
  &= \sum_{i = 0}^{b - a} i^2\frac1{b - a + 1}\\
  &= \frac1{b - a + 1}(0^2 + 1^2 + 2^2 + \dots  +(b - a)^2)\\
  &= \frac1{b - a + 1}\frac{(b - a)(b - a + 1)(2(b - a) + 1)}{6}\\
  &= \frac{(b - a)(2(b - a) + 1)}{6}.
\end{align*}
Therefore
\begin{align*}
   \text{var}(X)
   =
   \text{var}(Y)
   &= \frac{(b - a)(2(b - a) + 1)}{6} - \left(\frac{b - a}2\right)^2\\
   &= \frac{(b - a)(b - a + 2)}{12}.
\end{align*}
:::


:::{.example #LottoStats name="Lotto"}
Oz Lotto, like many lottery games, challenges players to match randomly chosen numbers.
Oz Lotto randomly picks numbers between 1 and 45 (inclusive).
Each number should have an equal chance of selection, so the discrete uniform distribution $U(1,45)$ is appropriate.\smallskip

If the data follow a discrete uniform distribution exactly, the mean number drawn should be  $(1 + 45) / 2 = 23$, and the variance $(44 \times 46)/12 = 168.66$.
:::


## Bernoulli distribution {#BernoulliDistribution}

A Bernoulli distribution is used in a situation where a single trial of a random process has two possible outcomes, where the probability of each outcome remains constant on each trial.
A simple example is tossing a coin and observing if a head falls. 


The probability function is simple:

\[
   p_X(x) = 
   \begin{cases}
      1 - p & \text{if $x = 0$};\\
      p     & \text{if $x = 1$},
    \end{cases}
\]
so that $p$ represents the probability of $x = 1$, called a 'success' (while $x = 0$ is called a 'failure').
More succinctly:

\begin{equation}
   p_X(x) = p^x (1 - p)^{1 - x} \quad\text{for $x = 0, 1$}.
   (\#eq:BermoulliPMF)
\end{equation}
We write $X\sim\text{Bern}(p)$.


:::{.definition #BrnoulliDistribution name="Bernoulli distribution"}
Let $X$ be the number of successes in one independent trials with $\Pr(\text{Success}) = p$ ($0\le p\le 1$), constant in each trial.
Then $X$ has a *Bernoulli probability distribution* with parameter $p$ and pf given by \@ref(eq:BermoulliPMF).
We write $X\sim\text{Bern}(p)$.
:::


::: {.importantBox .important data-latex="{iconmonstr-warning-8-240.png}"}
The terms 'success' and 'failure' are not literal.
\smallskip

'Success' simply means that the event is of interest.
If the event of interest is whether a cyclone causes damage, this is still called a 'success'.
:::


These ideas also introduces a common idea of a *Bernoulli trial*.


:::{.definition #BernoulliTrials name="Bernoulli trials"}
A *Bernoulli trial* is an experiment with only two possible outcomes, usually labelled 'success' $s$ and 'failure' $f$.
The sample space can be denoted by $S = \{ s, f\}$.
:::


The following are the basic properties of the Bernoulli distribution.

:::{.theorem #BernoulliProperties name="Bernoulli distribution properties"}
If $X\sim\text{Bern}(p)$ then

1. $\text{E}(X) = p$.
2. $\text{var}(X) = p(1 - p) = pq$ where $q = 1 - p$s.
3. $M_X(t) = pe^t + q$.
:::

:::{.proof}
We make use of \@ref(eq:BinomialSeries) in the following:

\[
     \text{E}(X) = \sum^1_{x = 0} x p_X(x) = 0\times (1 - p) + 1\times p = p.
\]
To find the variance, we use the computational formula $\text{var}(X) = \text{E}(X^2) - \text{E}(X)^2$.
Then,

\[
     \text{E}(X^2) = \sum^1_{x = 0} x^2 p_X(x) = 0^2\times (1 - p) + 1^2\times p = p,
\]
and so 

\[ 
   \text{var}(X) 
   = \text{E}(X^2) - [\text{E}(X)]^2 
   = p - p^2
   = p (1- p). 
\]

The mgf of $X$ is given by

\begin{align*}
   M_X(t)
   &= \text{E}\big(\exp(tX)\big)\\
   &= \sum^1_{x = 0} e^{tx} p^x q^{1 - x}\\
   &= \sum^1_{x = 0}(pe^t)^x q^{1 - x}\\
   &= pe^t + q.
\end{align*}
Proving the third result first, and then using it to prove the others, is easier (using of the methods in Sect. \@ref(MGFMoments)---try this as an exercise.)
:::




## Binomial distribution {#BinomialDistribution}

A binomial distribution is used in a situation where a Bernoulli trial is repeated many times, and one of two outcomes is observed on each trial.
A simple example is tossing a coin ten times and observing how often a head falls. 

The same experiment is repeated (tossing the coin), there are only two outcomes on each trial (a head or a tail), and the probability of a head remains constant on each trial.

Consider tossing a die five times and observing the number of times a `r knitr::include_graphics("Dice/die1.png", dpi=2000)` is rolled.
The probability of observing three `r knitr::include_graphics("Dice/die1.png", dpi=2000)` can be found as follows:
In the five tosses, a `r knitr::include_graphics("Dice/die1.png", dpi=2000)` must appear three times; there are $binom{5}{3}$ ways of allocating on which of the five rolls they will appear.
In the five rolls, `r knitr::include_graphics("Dice/die1.png", dpi=2000)` must appear three times with probability $1/6$; the other two rolls must produce another number with probability $5/6$.

So the probability will be

\[
   \Pr(\text{3 ones}) = \binom{5}{3} (1/6)^3 (5/6)^2 = 0.032,
\]
assuming independence of the events.
Using this approach, the pmf for the binomial distribution can be developed.


A binomial situation arises if a *sequence* of Bernoulli trials is observed, in each of which $\Pr(\{ s\} ) = p$ and $\Pr(\{ f\} ) = q = 1 - p$.
If $n$ such trials are conducted, consider the random variable $X$, where $X$ is the number of successes in $n$ trials. 
Now $X$ will have value set $R_X = \{ 0, 1, 2, \dots, n\}$.
$p$ must be constant from trial to trial, and the $n$ trials must be independent.

Consider the event $X = r$ (where $0\leq r\leq n$). 
This could correspond to the sample point

\[ 
   \underbrace{s \quad s \quad s \dots s \quad s \quad s \quad s}_r\quad
   \underbrace{f \quad f \dots f \quad f}_{n - r} 
\]
which is the intersection of $n$ independent events consisting of $r$ successes and $n - r$ failures, and hence the probability is $p^r q^{n - r}$.

Every other sample point in the event $X = r$ will appear as a rearrangement of the $S$'s and $F$'s in the sample point described above and will therefore have the same probability. 

Now the number of distinct arrangements of the $r$ successes $s$ and $(n - r)$ failures $f$ is $\binom{n}{r}$, so

\begin{equation}
     \Pr(X = r) = \binom{n}{r} p^r q^{n - r},\quad r = 0, 1, \dots, n.
        (\#eq:BinomialPMF)
\end{equation}

The sum of the probabilities is 1 as the binomial expansion of $(p + q)^n$ (using \@ref(eq:BinomialSeries)) is

\begin{equation}
   \sum_{r = 0}^n \binom{n}{r} p^r q^{n - r} = (p + q)^n = 1\label{EQN:sumbin}
\end{equation}
since $p + q = 1$. 
This leads to the *binomial distribution*.


:::{.definition #BinomialDistribution name="Binomial distribution"}
Let $X$ be the number of successes in $n$ independent Bernoulli trials with $\Pr(\text{Success}) = p$ ($0\le p\le 1$), constant for each trial.
Then $X$ has a *binomial probability distribution* with parameters $n$, $p$ and pf given by \@ref(eq:BinomialPMF).
We write $X\sim\text{Bin}(n, p)$.
:::

Fig. \@ref(fig:BinomialExamples) shows the pf for the binomial distribution for various parameter values.


```{r BinomialExamples, echo=FALSE, fig.align="center", fig.cap="The pf for the binomial distribution for various values of $p$ and $n$", fig.height = 7, fig.width=7, out.width='80%'}
par( mfrow = c(2, 2))

n <- c(8, 8, 
       15, 15)
prob <- c(0.2, 0.6, 
          0.2, 0.6)

for (i in (1:4)) {
  ni <- n[i]
  probi <- prob[i]
  x <-  0:ni
  y <- dbinom(x, 
              size = ni,
              prob = probi)
  
   plot( x = x,
         y = y,
         xlab = expression(italic(x)),
         ylab = "Probability function",
         main = bquote("Binomial: " ~ italic(n) == .(ni) ~ " and " ~ italic(p) == .(probi)),
         las = 1,
         lty = 3,
         col = "grey",
         type = "h")
  points(x = x,
         y = y,
         pch = 19)
  
}
```

:::{.example #ThrowDice name="Throwing dice"}
A die is thrown 4 times. 
What is the probability of exactly 2 sixes?\smallskip

There are 4 Bernoulli trials with $p = 1/6$.
Let the random variable $X$ be the  number of 6's in 4 tosses. 
Then

\[ 
  \Pr(X = 2) = 
  \binom{4}{2} \left(\frac{1}{6}\right)^2\left(\frac{5}{6}\right)^2 = 150/1296.
\]
:::

The following are the basic properties of the binomial distribution.

:::{.theorem #BinomialProperties name="Binomial distribution properties"}
If $X\sim\text{Bin}(n,p)$ then

1. $\text{E}(X) = np$.
2. $\text{var}(X) = np(1 - p) = npq$.
3. $M_X(t) = (pe^t + q)^n$.
:::

:::{.proof}
Using \@ref(eq:BinomialSeries):

\begin{align*}
     \text{E}(X) 
     & = \sum^n_{x = 0} x\binom{n}{x} p^x q^{n - x}\\
     & = \sum^n_{x = 1} x \frac{n}{x} \binom{n - 1}{x - 1} p^x q^{n - x}\\
     & = np\sum^n_{x = 1} \binom{n - 1}{x - 1} p^{x - 1} q^{n - x}\\
     & = np \sum^{n - 1}_{y = 0} \binom{n - 1}{y}p^y q^{n - 1 - y}\quad \text{putting $y = x - 1$}.
\end{align*}
The sum is 1 since a term in the sum is the probability of $y$ successes in $(n - 1)$ Bernoulli trials and the sum is over all values in $R_X$ (so it represents a probability function).
Note also that in the second line the sum is over $x$ from $1$ to $n$ because for $x = 0$ we have a binomial coefficient of the form $\binom{b}{a}$ with $a < 0$, which is defined to be zero. 
Thus,

\[ 
   \text{E}(X) = np.
\]

To find the variance, use the computational formula $\text{var}(X) = \text{E}(X^2) - \text{E}(X)^2$.
Firstly, to find $\text{E}(X^2)$, write $\text{E}(X^2)$ as $\text{E}[X(X - 1) + X]$ so that $\text{E}[X(X - 1)] + \text{E}(X)$.

\begin{align*}
     \text{E}(X^2) 
     &= \sum^n_{x = 0} x(x - 1)\Pr(X = x) + np\\
     &= \sum^n_{x = 2} x(x  -1)\frac{n(n - 1)}{x(x - 1)} \binom{n - 2}{x - 2} p^x q^{n - x} + np\\
     &= \sum^n_{x = 2} n(n - 1)\binom{n - 2}{x - 2} p^x q^{n - x} + np\\
     &= n(n - 1)p^2 \sum^{n - 2}_{y = 0} \binom{n - 2}{y} p^yq^{n - 2 - y} + np,
\end{align*}
putting $y = x - 2$.
For the same reason as before, the sum is 1, so

\[ 
   \text{E}(X^2) = n^2 p^2 - np^2 + np 
\]
and hence

\[ 
   \text{var}(X) 
   = \text{E}(X^2) - [\text{E}(X)]^2 
   = n^2p^2 - np^2 + np -n^2 p^2
   = np (1- p). 
\]

The mgf of $X$ is given by

\begin{align*}
   M_X(t)
   &= \text{E}(\exp(tX))\\
   &= \sum^n_{x = 0} e^{tx}\binom{n}{x} p^x q^{n - x}\\
   &= \sum^n_{x = 0}\binom{n}{x} (pe^t)^x q^{n - x}
    = (pe^t + q)^n.
\end{align*}
Proving the third result first, and then using it to prove the others, is easier (using of the methods in Sect. \@ref(MGFMoments)---try this as an exercise.)
:::


::: {.softwareBox .software data-latex="{iconmonstr-laptop-4-240.png}"}
In **R**, the binomial distribution functions have the form `[dpqr]binom(size, prob)`.
:::


::: {.importantBox .important data-latex="{iconmonstr-warning-8-240.png}"}
A binomial situation requires the trials to be *independent*, and the probability of success $p$ to be *constant* throughout the trials.
\smallskip

For example, drawing cards from a pack without replacing them is **not** a binomial situation; after drawing one card, the probabilities will then change for the drawing of the next card.
\smallskip

In this case, the *hypergeometric distribution* could be used (Sect. \@ref(HypergeometricDistribution)).
:::


:::{.example name="Freezing lake"}
@BIB:Wilks:statistical (p. 68) notes that in the 200 years from 1796 to 1995, Cayuga Lake has frozen only in ten of those years.
Since the lake is deep, it will only freeze during exceptionally cold weather.
The probability the lake freezes during any one year can be estimated as $p = 10/200 = 0.05$.\smallskip

Using this information, the number of times the lake will *not* freeze in ten randomly chosen years in given by the random variable $X$ where $X \sim \text{Bin}(10, 0.95)$.
The probability that the lake will not freeze in these ten years is $\Pr(X = 10) = \binom{10}{10} 0.95^{10} 0.05^{0} \approx 0.599$, or about 60%.\smallskip

Note we could define the random variable $Y$ as the number of times the lake *will* freeze in the ten randomly chosen years.
Then, $Y\sim\text{Bin}(10, 0.05)$ and we would compute $\Pr(Y = 0)$ and get the same answer.
:::

Tables of binomial probabilities are commonly available, but computers (e.g., using **R**) can also be used to generate the probabilities.
If the number of 'successes' has a binomial distribution, so does the number of 'failures'.
Specifically if $X\sim \text{Bin}(n,p)$, then $Y = (n - X) \sim \text{Bin}(n, 1 - p)$.

Binomial probabilities can sometimes be approximated using the normal distribution (Sect. \@ref(NormalApproxBinomial)) or the Poisson distribution (Sect. \@ref(PoissonBinomial)).


## Geometric distribution {#GeometricDistribution}

Consider now an experiment where independent Bernoulli trials are repeated until the *first* success occurs. 
What is the distribution of the number of trials required?

Let the random variable $Y$ be the number of trials necessary to obtain the first success.
Since the first success may occur on the first trial, or second trial or third trial, and so on, $Y$ is a random variable with range space $\{1, 2, 3, \dots\}$.

To observe the first success on the $y$th trial, $y - 1$ failures must be followed by one success.
Since the probability of failure is $q = 1 - p$ and the probability of success is $p$, the probability of the first success on trial $y$ is

\begin{align*}
   \Pr(\text{$(y - 1)$ failures})    &\times \Pr(\text{first success}) \\
   q^{y - 1}                         &\times  p.
\end{align*}
This derivation assumes the events are independent.


:::{.definition #GeometricDistribution name="Geometric distribution"}
A random variable $X$ has a *geometric distribution* if the pf of $X$ is given by

\begin{equation}
   p_X(x) =  q^{x - 1} p\quad\text{for $x = 1, 2, \dots$}
   (\#eq:GeometricPMF)
\end{equation}
where $q = 1 - p$ and $0 < p < 1$ is the parameter of the distribution.
We write $X\sim\text{Geom}(p)$.
:::


The pf for a geometric distribution for various values of $p$ is shown in Fig. \@ref(fig:Geometric).
The following are the basic properties of the geometric distribution.

:::{.theorem #GeometricProperties name="Geometric distribution properties"}
If $X\sim\text{Geom}(p)$ as defined in Eq. \@ref(eq:GeometricPMF), then

1. $\text{E}(X) = 1/p$.
2. $\text{var}(X) = (1 - p)/p^2$.
3. $M_X(t) = pe^t/\{1 - (1 - p)e^t\}$ for $t < -\log(1 - p)$.
:::

:::{.proof}
The first two result can be proven directly but proving the third result, and using the mgf to prove the first two, is easier.
This is left as an exercise.
:::


The parameterisation above is for the *number of trials* needed for the first success, so that $x = 1, 2, 3, \dots$.
An alternative parameterisation is for the *number of failures* before a success, when $x = 0, 1, 2, \dots$.
Then,
\[
   p_X(x) =  q^{x} p\quad\text{for $x = 0, 1, 2, \dots$}.
\]
This is the parametrisation used by **R**.


:::{.definition #GeometricDistributionALT name="Geometric distribution: Alternative parameterisation"}
A random variable $X$ has a *geometric distribution* if the pf of $X$ is given by
\begin{equation}
   p_X(x) =  q^{x - 1} p\quad\text{for $x = 1, 2, \dots$}
   (\#eq:GeometricPMFAlternative)
\end{equation}
where $q = 1 - p$ and $0 < p < 1$ is the parameter of the distribution.
We write $X\sim\text{Geom}(p)$.
:::


:::{.theorem #GeometricPropertiesALT name="Geometric distribution properties"}
If $X\sim\text{Geom}(p)$ as defined in Eq. \@ref(eq:GeometricPMFAlternative), then

1. $\text{E}(X) = (1 - p)/p$.
2. $\text{var}(X) = (1 - p)/p^2$.
3. $M_X(t) = p/\{ 1 - (1 - p)e^t \}$ for $t < -\log(1 - p)$.
:::

:::{.proof}
The first two result can be proven from Theorem \@ref(thm:GeometricProperties).
:::




```{r Geometric, echo=FALSE, fig.align="center", fig.cap="The pf for the geometric distribution for $p = 0.1$, $0.3$, $0.5$ and $0.8$", fig.height = 7, fig.width=7, out.width='80%'}
par( mfrow = c(2, 2))

xmax <- c(15, 15, 15, 15)
p <- c(0.1, 0.3, 
       0.5, 0.8)

for (i in (1:4)) {
  pValue <- p[i]
  x <-  0:xmax[i]
  y <- dgeom(x,
             prob = pValue)
  
   plot( x = x,
         y = y,
         xlim = c(0, xmax[i]),
         xlab = expression(italic(x)),
         ylab = "Probability function",
         main = bquote("Geometric: " ~ italic(p) == .(pValue) ),
         las = 1,
         ylim = c(0, max(y)),
         col = "grey",
         lty = 3,
         type = "h")
  points(x = x,
         y = y,
         pch = 19)
  
}

```

::: {.softwareBox .software data-latex="{iconmonstr-laptop-4-240.png}"}
In **R**, the geometric distribution functions have the form `[dpqr]geom(prob)`.
**R** used the parameterisation that models the number of *failures* before the first success, as in \@ref(eq:GeometricPMFAlternative).
:::



:::{.example #GeometricComputerFail name="Computer failure"}
A computer system fails at random with probability $0.02$ on any given day.
Let $Y$ be the day on which the system fails, so that $Y = \{1, 2, 3, \dots\}$ using \@ref(eq:GeometricPMF).
Then $Y \sim\text{Geom}(0.02)$.\smallskip

The probability that the system will first fail before day $20$ (that is, $\Pr(Y < 20) = \Pr(Y \le 19)$) can be found by hand, or using a computer.\smallskip

To use **R**, the alternative parameterisation \@ref(eq:GeometricPMFAlternative) is needed.
Let $X$ be the number of days before the first system failure, so that $X = \{0, 1, 2, 3, \dots\}$ using \@ref(eq:GeometricPMFAlternative).\smallskip

Then, the probability that the system will first fail before day $20$ means there are $19$ days with no failure (that is, $\Pr(X < 19) = \Pr(Y \le 18)$) can be found in **R**.
The command `dgeom()` gives the probability mass function for the geometric distribution, and `pgeom()` gives values from the distribution function of the geometric distribution:

```{r echo=TRUE}
sum( dgeom( x = 0:18, # For alt. parameterisation
            prob = 0.02))
pgeom(q = 18, 
      prob = 0.02) # i.e., up to 18 failures at p=0.02
```
The probability is $0.32$ that the system will fail before day $20$.\smallskip

The number of days the system can be expected to work until failure is $\text{E}(Y) = 1/p = 1/0.02 = 50$.
The system could be expected to last fifty days before the first failure.
:::




<!-- :::{.example #BabyBoom name="Baby Boom"} -->
<!-- An article appearing in *The Sunday Mail* on 21 December 1997 give the birth weight, gender, and time of birth of 44 babies born in the 24-hour period of 18 December 1997 at the Mater Mother's Hospital in Brisbane, Australia. -->

<!-- KEEP FOR EXERCISE OR ASSIGNMENT -->


<!-- The gender of the births is a random process, and so the number of births observed until a boy is born should have approximately a geometric distribution. -->
<!-- By counting the number of births until a boy is born (and restarting the count when a girl is born), Table ??? has been constructed. -->
<!-- The theoretical probabilities have been based on the probability of a boy being born as being $0.5$. -->
<!-- The fit is reasonable (but not brilliant) for the small sample size; see Table ???. -->
<!-- ::: -->



## Poisson distribution {#PoissonDistribution}

The Poisson distribution is commonly used to model the number of occurrences of an event which occurs randomly in time or space.
The Poisson distribution arises as a result of assumptions made about a random process:

* Events that occur in one time-interval (or region) are independent of those occurring in any other non-overlapping time-interval (or region).
* The probability that an event occurs in a small time-interval is proportional to the length of the interval.
* The probability that 2 or more events occur in a very small time-interval is so small that it can be neglected.

Whenever these assumptions are valid, or approximately so, the Poisson distribution is appropriate.
Many natural phenomena fall into this category.


:::{.definition #PoissonDistribution name="Poisson distribution"}
A random variable $X$ is said to have a *Poisson distribution* if its pmf is

\[
   p_X(x) = \frac{\exp(-\mu) \mu^x}{x!}\quad   \text{for $x = 0, 1, 2, \dots$}
\]
where the parameter is $\mu > 0$.
We write $X\sim\text{Pois}(\mu)$.
:::

The pmf for a Poisson distribution for different values of $\mu$ is shown in Fig. \@ref(fig:PoissonExamples).




```{r PoissonExamples, echo=FALSE, fig.align="center", fig.cap="The pf for the Poisson distribution for various values of $\\mu$", fig.height = 7, fig.width=7, out.width='80%'}
par( mfrow = c(2, 2))

xmax <- 10
mu <- c(0.5, 1, 2, 5)

for (i in (1:4)) {
  muvalue <- mu[i]
  x <-  0:xmax
  y <- dpois(x, 
             lambda = muvalue )
  
   plot( x = x,
         y = y,
         xlim = c(0, xmax),
         xlab = expression(italic(x)),
         ylab = "Probability function",
         main = bquote("Poisson mean:" ~ mu == .(muvalue)),
         las = 1,
         lty = 3,
         col = "grey",
         type = "h")
  points(x = x,
         y = y,
         pch = 19)
  
}
```


The following are the basic properties of the Poisson distribution.

::: {.theorem #PoissonProperties name="Poission distribution properties"}
If $X\sim\text{Pois}(\mu)$ then

1. $\text{E}(X) = \mu$.
2. $\text{var}(X) = \mu$.
3. $M_X(t) = \exp[ -\mu\{1 - \exp(t)\}]$.
:::

:::{.proof}
The third result is proven, then the other results follow:

\begin{align*}
   M_X(t) = \text{E}(e^{tX}) 
     &= \sum^\infty_{x = 0} e^{xt} e^{-\mu} \mu^x / x!\\
     &= e^{-\mu} \sum^\infty_{x = 0} \frac{(\mu\, e^t)^x}{x!} \\
     &= e^{-\mu} \left[ 1 + \mu\, e^t + \frac{(\mu\,e^t)^2}{2!} + \dots \right]\\
     &= e^{-\mu} e^{\mu\, e^t}\\
     &= e^{-\mu (1 - e^t)}
\end{align*}
using \@ref(eq:Exponential).
The first two results follow from differentiating the mgf.
:::


Notice that the Poisson distribution has the variance equal to the mean.


::: {.softwareBox .software data-latex="{iconmonstr-laptop-4-240.png}"}
In **R**, the Poisson distribution functions have the form `[dpqr]poiss(lambda)`.
:::


:::{.example #Queues name="Queuing"}
Customers enter a waiting line 'at random' at a rate of 4 per minute.
Assume that the number entering the line in any given time interval has a Poisson distribution.
To determine the probability that at least one customer enters the line in a given $\frac{1}{2}$-minute interval, use (since $\mu = 2$):

\[ 
   \Pr(X\geq 1) = 1 - \Pr(X = 0) = 1 - e^{-2} = 0.865.
\]
:::


:::{.example #Bombs name="Bomb hits"}
@clarke1946application (quoted in @data:hand:handbook, Dataset 289) gives the number of flying bomb hits on London during World War II in a 36 square kilometre area of South London.
The area was gridded into 0.25 km squares and the number of bombs falling in each grid was counted (Table \@ref(tab:BombTable)).
If the hits are random, a Poisson distribution should fit the data.\smallskip

Using the proportions as estimates of the true probabilities, the sample mean can be computed using the definition of the mean:

\begin{align*}
   \text{E}(X)
   &\approx \sum x p_X(x) \\
   &= (0\times 0.3976)  + (1\times 0.2663) + \cdots + (7 \times 0.0017)\\
   &=  0.9323.
\end{align*}
Using this value as an estimate of the Poisson mean, the pf for the Poisson distribution can be compared to the empirical probabilities computed above; see Table \@ref(tab:BombTable).
For example, the probability of zero hits is 

\[
   \frac{\exp(-0.9323) (0.9323)^0}{0!} \approx 0.3936.
\]
The two probabilities are very close; the Poisson distributions fits the data very well.
:::


```{r BombTable, echo=FALSE}
BombTable <- array( dim = c(8, 4))
colnames(BombTable) <- c("Hits",
                         "Number of bombs",
                         "Proportion of bombs",
                         "Poisson proportion")
BombTable[, 1] <- 0:7
BombTable[, 2] <- c(229,
                    211,
                    93,
                    35,
                    7,
                    0,
                    0,
                    1)
BombTable[, 3] <- round(BombTable[, 2] / sum(BombTable[, 2]), 4)
lambda <- sum( BombTable[, 1] * BombTable[, 3])

BombTable[, 4] <- dpois(0:7,
                        lambda = lambda )
BombTable[, 4] <- round( BombTable[, 4], 4)
BombTable[ 8, 4] <- 1 - sum( BombTable[1:7, 4])

BombTable.caption <- "The number of flying bomb hits on London during World War II in a 36 square kilometre area of South London. The proportion of the 576 grid squares receiving 0, 1, ...hits was also computed. The observed and empirical probabilities are shown. The Poisson distribution fits the data  well."

if( knitr::is_latex_output() ) {
  knitr::kable( BombTable,
                format = "latex",
                booktabs = TRUE,
                longtable = FALSE,
                linesep = c("", "", "\\addlinespace"),
                col.names = c("Hits",
                              "Number bombs", 
                              "Proportion bombs", 
                              "Poisson proportion"),
                caption = BombTable.caption) %>%
    kable_styling(font_size = 10) %>%
    row_spec(0, bold = TRUE)
}
if( knitr::is_html_output() ) {
  knitr::kable( BombTable,
                format = "html",
                booktabs = TRUE,
                longtable = FALSE,
                linesep = c("", "", "\\addlinespace"),
                col.names = c("Hits",
                              "Number bombs", 
                              "Proportion bombs", 
                              "Poisson proportion"),
                caption = BombTable.caption) %>%
    row_spec(1, bold = TRUE)
}

```



### Relationship to the binomial distribution {#PoissonBinomial}

If the number of trials is very large and the probability of success in a single trial is very small, then computing binomial probability is tedious. 
For example, consider $X\sim \text{Bin}(n = 2000, p = 0.005)$. 
Then, the pf is

\[
   p_X(x) = \binom{2000}{x}(0.005)^x 0.995^{2000 - x}.
\]
Using the pmf, computing some probabilities, such as $\Pr(X > 101)$, is very tedious.
(Try computing $\binom{2000}{102}$ on your calculator, for example.)
However, the Poisson distribution can be used to approximate this probability.

Set the Poisson mean to equal the binomial mean (that is, $\mu = np$).
Since $\text{E}(Y) = \text{var}(Y) = \mu$ for the Poisson distribution, this means the variance is also set to $\sigma^2 = np$.
Of course, the binomial mean is $np(1 - p)$, so this can only be (approximately) true if $p$ is close to zero.

A general guideline is that the Poisson distribution can be used to approximate the binomial when $n$ is large, $p$ is small and $np \le 7$.


::: {.linkBox .link data-latex="{iconmonstr-link-1-240.png}"}
If the random variable $X$ has the binomial distribution $X \sim \text{Bin}(n, p)$, the probability function can be approximated by the Poisson distribution $X \sim \text{Pois}(\lambda)$, where $\lambda = np$.
\smallskip

The approximation is good when $n$ is large, $p$ is small, and $np\le 7$.
:::


```{r BinomialPoisson, echo=FALSE, fig.align="center", fig.cap="The Poisson distribution is an excellent approximation to the binomial distribution when $p$ is small and $n$ is large. The binomial pf is shown using empty circles; the Poisson pf using crosses.", fig.height = 7, fig.width=7, out.width='80%'}
par( mfrow = c(2, 2))

n <- c(10, 50, 
       100, 1000)
p <- c(0.2, 0.1, 
       0.010, 0.005)
xmax <- c(10, 20, 
          10, 15)

for (i in (1:4)) {
   pValue <- p[i]
   nValue <- n[i]
   
  x <-  0:n[i]
  y <- dbinom(x, 
              size = n[i],
              prob = p[i])
  
   plot( x = x,
         y = y,
         xlim = c(0, xmax[i]),
         xlab = expression(italic(x)),
         ylab = "Probability",
         main = bquote("Binomial: " ~ italic(n) == ~ .(nValue) ~ "and" ~ italic(p) == .(pValue) ),
         las = 1,
         lty = 3,
         col = "grey",
         type = "h")
  points(x = x,
         y = y,
         pch = 19)

  muPoisson <- nValue * pValue
  xValues <- 0:xmax[i]
  points( x = xValues,
          y = dpois(xValues,
                    lambda = muPoisson),
          pch = 4)
  
  if (i == 4 ){
    legend("topright", 
           col = c("black",
                   plotColour),
           pch = c(1, 19),
           legend = c("Binomial",
                      "Poisson approx."))
  }
  
}

```

### Extensions

The Poisson distribution is commonly-used to model independent counts.
However, sometimes these counts explicitly *exclude* a count of zero.
Then, the probability functions can be expressed as

\[
   p_X(x) = \frac{\exp(-\mu) \mu^{x - 1}}{(x - 1)!}\quad   \text{for $x = 1, 2, 3, \dots$}
\]
where the parameter is $\mu > 0$.
In this case, $\text{E}(X) = \lambda + 1$ and $\text{var}(X) = \lambda$.

In some cases, the random variable is a count, but has an upper limit on the possible number of counts.
This is the *truncated Poisson distribution*.
If the counts have a maximum of $K$, the the probability function is

\[
   p_X(x) =
   \begin{cases}
      \frac{\exp(-\mu) \mu^x}{x!}                           &  \text{for $x = 0, 1, 2, \dots, K$};\\
      \displaystyle
      \sum_{x = K + 1}^\infty \frac{\exp(-\mu) \mu^x}{x!}   & \text{for $x = K + 1, K + 2, \infty$};
   \end{cases}
\]

Other situations exist where the the proportions of zeros exceed the proportions expected by the Poisson distribution, but the Poisson distributions seems to otherwise be suitable.
In these situations, the *zero-inflated Poisson distribution* may be suitable.



## Negative binomial distribution {#NegativeBinomialDistribution}

### Standard parameterisation

Consider a random process where independent Bernoulli trials are repeated until the $r$th success occurs. 
Let the random variable $Y$ be the number of *failures* before the $r$th success is observed, so that $Y = 0, 1, 2, \dots$.

To observe the $r$th success after $y$ failures, we need to observe $y$ failures and $r - 1$ successes first.
There are $\binom{y + r - 1}{r - 1}$ ways in which to allocate these successes to the first $y + r - 1$ trials.
Each of the $r - 1$ successes occur with probability $p$, and the $y$ failures with probability $1 - p$ (assuming events are independent).

Hence the probability of observing the $r$th success in trial $y$ is

\begin{align*}
   \text{No. ways}
   &\times \Pr\big(\text{$y$ failures}\big) \times \Pr(\text{$r$ successes}) \\
   \binom{y + r - 1}{r - 1} 
   &\times (1 - p)^{y} \times p^r.
\end{align*}


:::{.definition #NegativeBinomialDistribution name="Negative binomial distribution"}
A random variable $Y$ with pf

\begin{equation}
   p_Y(y) = \binom{y +r - 1}{r - 1}(1 - p)^{y} p^r
 \quad\text{for $y = 0, 1, 2, \dots$}
   (\#eq:NegativeBinomialPMF)
\end{equation}
has a *negative binomial distribution* with parameters $r$ (an integer $\ge 1$) and $p$ ($0\le p\le1$).
We write $Y\sim\text{NB}(r, p)$.
:::


The following are the basic properties of the negative binomial distribution.


:::{.theorem #NegBinomialProperties name="Negative binomial properties"}
If $X\sim \text{NB}(r,p)$ with pf \@ref(eq:NegativeBinomialPMF) then

1. $\text{E}(X) = \{r(1 - p)\}/p$.
2. $\text{var}(X) = r(1 - p)/p^2$.
3. $M_X(t) = \left[ p / \{1 - (1 - p)e^t\} \right]^r$.
:::

:::{.proof}
Proving the third statement is left as an exercise.
Then the first two are then derived from the mgf.
:::


:::{.example #Demonstration name="Negative binomial"}
 A telephone marketer invites customers, over the phone, to a product demonstration. 
Ten people are needed for the demonstration. 
The probability that a randomly-chosen person accepts the invitation is only 0.15.\smallskip

Consider finding the probability that the marketer will need to make more than 100 calls to secure ten acceptances. 
Here, a ‘success’ is an acceptance to attend the demonstration. 
Let $Y$ be the number of *failed* calls necessary to secure ten acceptances.
Then $Y$ has a negative binomial distribution with $p = 0.15$ and $r = 10$.
The mean number of failures made will is $\text{E}(Y) = \{r(1 - p)\}/p = \{10 \times (1 - 0.15)\}/0.15 \approx  56.66667$.\smallskip

Hence, including the ten calls that leads to a person accepting the invitation, the mean number of calls to be made will is $10 + \text{E}(Y) = 66.66\dots$.
:::


Often, a different parameterisation for the negative binomial distribution; see Exercise \@ref(exr:NegativeBinomialALT).
However, the parameterisation presented here is used by **R**.

Since the parameterisation in \@ref(eq:NegativeBinomialPMF) is defined over the non-negative integers, it is often used to model count data (as an alternative to the [Poisson distribution](PoissonDistribution)).
Since the negative binomial distribution has two parameters and the Poison only one, the negative binomial distribution often produces a better fit.


::: {.softwareBox .software data-latex="{iconmonstr-laptop-4-240.png}"}
In **R**, the negative binomial distribution functions are based on the paramaterisation in \@ref(eq:NegativeBinomialPMF), and have the form `[dpqr]nbinom(size, prob)`, where `prob` is $p$ and `size` is $r$.
:::



::: {.linkBox .link data-latex="{iconmonstr-link-1-240.png}"}
When $r = 1$, the negative binomial distribution is the same as the geometric distribution: the geometric distribution is a special case of the negative binomial.
:::


The pf for the negative binomial distribution for various values of $p$ and $r$ is shown in Fig. \@ref(fig:NegativeBinomial).


```{r NegativeBinomial, echo=FALSE, fig.align="center", fig.cap="The pf for the negative binomial distribution for $p = 0.2$ and $0.7$ and $r = 1$ and $r = 3$", fig.height = 7, fig.width=7, out.width='80%'}
par( mfrow = c(2, 2))
xmax <- 15

r <- c(1, 3, 
       1, 3)
p <- c(0.2, 0.2, 
       0.6, 0.6)

for (i in (1:4)) {
  rValue <- r[i]
  pValue <- p[i]
  
  x <-  0:xmax
  y <- dnbinom(x, 
              size = r[i],
              prob = p[i])
  
   plot( x = x,
         y = y,
         xlim = c(0, xmax),
         xlab = expression(italic(x)),
         ylab = "Probability",
         main = bquote("Negative binomial: " ~ italic(r) == ~ .(rValue) ~ ";" ~ italic(p) == .(pValue) ),
         las = 1,
         lty = 3,
         col = "grey",
         type = "h")
  points(x = x,
         y = y,
         pch = 19)
  
}

```



:::{.example #DemonstrationNB name="Negative binomial"}
Consider Example \@ref(exm:Demonstration), concerning a telephone marketer inviting customers, over the phone, to a product demonstration.
Ten people are needed for the demonstration. 
The probability that a randomly chosen person accepts the invitation is only $0.15$.\smallskip

Consider finding the probability that the marketer will need to make more than $100$ calls to secure ten acceptances.
In this situation, a 'success' is an acceptance to attend the demonstration.
Let $Y$ be the number of failed calls before securing ten acceptances.
Then $Y$ has a negative binomial distribution such that $Y\sim\text{NBin}(p = 0.15, r = 10)$.

To determine $\Pr(Y > 100) = 1 - \Pr(Y\le 100)$, using a computer is the easiest approach. 
In **R**, the command `dnbinom()` returns probabilities from the probability function of the negative binomial distribution, and `pnbinom()` returns the cumulative distribution probabilities:


```{r}
x.values <- seq(1, 100, 
                by = 1)
1 - sum(dnbinom( x.values, 
                 size = 10, 
                 prob = 0.15))
# Alternatively:
1 - pnbinom(100, 
            size = 10, 
            prob = 0.15)
```

The probability is about $0.0244$.\smallskip

Assuming each call take an average of 5 minutes, we can determine how long is the marketer expected to be calling to find ten acceptances.
Let $T$ be the time to make the calls in minutes.
Then $T = 5Y$.
Hence, $\text{E}(T) = 5\text{E}(Y) = 5 \times 66.7 = 333.5$, or about 5.56 hours.\smallskip

Assume that each call costs $25$ cents, and that the company pays the marketer \$30 per hour.
To determine the total cost, let $C$ be the total cost in dollars.
The cost of employing the marketer is, on average, $30\times 5.56 = \$166.75$.
Then $C = 0.25 Y + 166.75$, so $\text{E}(C) = 0.25 \text{E}(Y) + 166.75 = C = 0.25 \times 66.7 + 166.75 = \$183.43$.
:::



The negative binomial distribution can be extended so that $r$ can be any positive number, not just an integer.
When $r$ is non-integer, the above interpretations are lost, but the distribution is more flexible.
Relaxing the restriction on $r$ gives the pf as

\begin{equation}
   p_X(x) = \frac{\Gamma(x + r)}{\Gamma(r) x!} p^r (1 - p)^x,
   (\#eq:NegativeBinomialPMFAlternative2)
\end{equation}
for $x = 0$, $1$, $2$, $\dots$ and $r > 0$.
In this expression, $\Gamma(r)$ is the *gamma function* (Def. \@ref(def:GammaFunction)), and is like a factorial;
for instance, $\Gamma(r) = (r - 1)!$ if $r$ is a positive integer.



:::{.definition #GammaFunction name="Gamma function"}
The function $\Gamma(\cdot)$ is called the *gamma function* and is defined as

\[
   \Gamma(r) = \int_0^\infty x^{r - 1}\exp(-x)\, dx
\]
for $r > 0$.
:::

The gamma function has the property that

\begin{equation}
   \Gamma(r) = (r - 1)!
   (\#eq:gammaprop)
\end{equation}
if $r$ is a positive integer (Fig. \@ref(fig:GammaFunction)).
Important properties of the gamma function are given below.


:::{.theorem #GammaFunctionProperties name="Gamma function properties"}
For the gamma function $\Gamma(\cdot)$,

1. $\Gamma(r) = (r - 1)\Gamma(r - 1)$ if $r > 0$.
2. $\Gamma(1/2) = \sqrt{\pi}$.
3. $\lim_{r\to 0} \to \infty$.
:::

:::{.proof}
Integration by parts gives

\begin{align*}
   \Gamma(r)
   &= \int_0^{\infty}-x^{r - 1} \, d(e^{-x})\\
   &= 0 + \ (r - 1) \int_0^{\infty}  e^{-x}  x^{r - 2} \, dx\\
   &= ( r - 1) \Gamma(r - 1).
\end{align*}

For the second property, put $r = 1/2$ in the first part gives

\begin{align*}
   \Gamma(1/2)
   &= \int_0^{\infty}e^{-x}  x^{-\frac{1}{2}}\,dx\\
   &= \sqrt{2}\int_0^{\infty}e^{-\frac{1}{2}z^2}\,dz, \text { putting $x = z^2/2\quad z \geq 0$}\\
   &= \sqrt{2}  \sqrt{2 \pi} \underbrace{\int_0^{\infty}\frac{1}{\sqrt{2 \pi}} e^{-\frac{1}{2}z^2}\,dz}_{1/2}.
\end{align*}
The integral is $1/2$, being half the area under a normal curve (which we see later in Sect. \@ref(Normal)). 
So

\[
   \Gamma(1/2) = \sqrt{\pi}.
\]
:::


```{r GammaFunction, echo=FALSE, fig.cap="The gamma function is like the factorial function but has a continuous argument. The line corresponds to the gamma function $\\Gamma(z)$; the solid points correspond to the factorial $(z - 1)! = \\Gamma(z)$ for integer $z$.", fig.align="center", fig.width=5, fig.height=3.75, out.width='75%'}

xPoints <- 1:5
xCont <- seq(0.1, 5.1, 
             length = 100)

plot( x = c(0, 5.3),
      y = c(0, 25),
      type = "n",
      las = 1,
      ylim = c(0, 30),
      xlab = expression(italic(z)),
      ylab = bquote("Gamma function" ~ Gamma(z)),
      main = expression(paste("The Gamma function"~Gamma(italic(z)))))
lines( x = xCont,
       y = gamma(xCont),
       lwd = 2)
points(x = xPoints,
       y = gamma(xPoints),
       pch = 19)
legend("top",
       pch = c(NA, 19),
       lty = c(1, NA),
       bty = "n",
       lwd = c(2, NA),
       legend = c(bquote("Gamma function" ~ Gamma(italic(z))),
                  expression(paste( group("(", list(italic(z) - 1),")"), "!" ) ) ) )

text(x = xPoints,
     y = gamma(xPoints),
     pos = 3,
     labels = c("0!",
                "1!",
                "2!",
                "3!",
                "4!") )
```

:::{.example #Computing name="Negative binomial"}
Consider the computer system in Example \@ref(exm:GeometricComputerFail). 
Suppose after five failures, the system is upgraded.\smallskip

To find the probability that an upgrade will happen within one year, let $D$ be the number of days before the fifth failure.
So, we seek $\Pr(X < 360)$.
Using **R**:

```{r echo=TRUE}
pnbinom(360, 
        size = 5, 
        prob = 0.02)
```
So the probability of upgrading within one year is about $85$%.
:::


:::{.example #NBMites name="Mites"}
@BIB:Bliss:negbin gives data from the counts of adult European red mites on leaves selected at random from six similar apple trees (Table \@ref(tab:MitesTable)).
From the data, the mean mites per leaf is

\[
   \text{E}(X) = (0.467 \times 0) + (0.253 \times 1) +\cdots \approx 1.14667.
\]
To compute the variance,

\[
   \text{E}(X^2) = (0.467 \times 0^2) + (0.253 \times 1^2) +\cdots \approx 3.5733,
\]
so that $\text{var}(X) = 3.573333 - (1.146667)^2 =  2.258489$.
The Poisson distribution has an equal mean and variance; the Poisson distribution may not model these data well.\smallskip

The expression for the mean and variance of the negative binomial distribution can be set to the computed values above, then solved for $p$ and $r$; then $p\approx 0.5077$ and $r\approx 1.1826$.
Using these values, the estimated probability function for both the Poisson and negative binomial distributions are given in Table \@ref(tab:MitesTable); the negative binomial distribution fits better as expected.
:::


```{r MitesTable, echo=FALSE}
MitesTab <- array( dim = c(9, 5))

MitesTab[1, ] <- c(0 ,  70 ,  0.467 ,  0.318 ,  0.449)
MitesTab[2, ] <- c(1 ,  38 ,  0.253 ,  0.364 ,  0.261)
MitesTab[3, ] <- c(2 ,  17 ,  0.113 ,  0.209 ,  "0.140")
MitesTab[4, ] <- c(3 ,  10 ,  0.067 ,  "0.080" ,  0.073)
MitesTab[5, ] <- c(4 ,  9 ,  "0.060" ,  0.023 ,  0.038)
MitesTab[6, ] <- c(5 ,  3 ,  "0.020" ,  0.005 ,  0.019)
MitesTab[7, ] <- c(6 ,  2 ,  0.013 ,  0.001 ,  "0.010")
MitesTab[8, ] <- c(7 ,  1 ,  0.007 ,  "0.000" ,  0.005)
MitesTab[9, ] <- c("8+" ,  0 ,  "0.000" ,  "0.000" ,  "0.000")

MitesTab.caption <- "Counts of mites on leaves selected at random from six similar apple trees"

if( knitr::is_latex_output() ) {
   knitr::kable(MitesTab,
                format = "latex",
                booktabs = TRUE,
                longtable = FALSE,
                align = "r",
                linesep = c("", "", "\\addlinespace"),
                col.names = c("No. leaves", 
                              "No. mites",
                              "Empirical prob.",
                              "Poisson prob.",
                              "Neg. bin. prob."),
                caption = MitesTab.caption) %>%
  kable_styling(font_size = 10) %>%
  row_spec(0, bold = TRUE) 
}
if( knitr::is_html_output() ) {
   knitr::kable(MitesTab,
                format = "html",
                booktabs = TRUE,
                longtable = FALSE,
                align = "r",
                linesep = c("", "", "\\addlinespace"),
                col.names = c("No. leaves", 
                              "No. mites",
                              "Empirical prob.",
                              "Poisson prob.",
                              "Neg. bin. prob."),
                caption = MitesTab.caption) %>%
  row_spec(1, bold = TRUE) 
}
```



## Hypergeometric distribution {#HypergeometricDistribution}

When the selection of items a fixed number of times is done *with* replacement, the probability of an item being selected stays the same and the binomial distribution can be used.
However, when the selection of items is done *without* replacement, the trials are not independent, making the binomial model unsuitable.
In these situations, a hypergeometric distribution is appropriate.

Consider a simple example: A bag contains six red balls and four blue balls.
The variable of interest, say $X$, is the number of red balls drawn in three random selections from the bag, without replacing the balls.
Since the balls are not replaced, $\Pr(\text{draw a red ball})$ is not constant, and so the binomial distribution cannot be used.

The probabilities can be computed, however, using the counting ideas from Chap. \@ref(Probability).
There are a total of $\binom{10}{3}$ ways of selecting a sample of size 3 from the bag.
Consider the case $X = 0$.
The number of ways of drawing no red balls is $\binom{6}{0}$ and the number of ways of drawing the three blue balls in $\binom{4}{3}$, so the probability is

\[
   \Pr(X = 0) = \frac{\binom{6}{0}\binom{4}{3}}{\binom{10}{3}} \approx 0.00833.
\]
Likewise, the number of ways to draw one red ball (and hence two blue balls) is $\binom{6}{1}\times \binom{4}{2}$, so

\[
   \Pr(X = 1) = \frac{ \binom{6}{1}\times \binom{4}{2} }{\binom{10}{3}}.
\]
Similarly,

\[
   \Pr(X = 2) =\frac{ \binom{6}{2}\times \binom{4}{1} }{\binom{10}{3}}
   \quad\text{and}\quad
   \Pr(X = 3) =\frac{ \binom{6}{3}\times \binom{4}{0} }{\binom{10}{3}}.
\]

In general, if there are $N$ balls in total in the bag, and $r$ of them are red, and we select a sample of size $n$ from the bag *without replacement*, then the probability of finding $x$ red balls in the sample of size $n$ is

\[
   \Pr(X = x) = \frac{ \binom{r}{x}{ \binom{N - r}{n - x}}}{\binom{N}{n}}
\]
where $X$ is the number of red balls in sample of size $n$.
(In the example, $n = 3$, $r = 6$ and $N = 10$.)

In the formula, $\binom{r}{x}$ is the number of ways of selecting $x$ red balls from the $r$ red balls in the bag; $\binom{N - r}{n - x}$ is the number of ways of selecting all the remaining $n - x$ to be the other colour (and there are $N - r$ of those in the bag); and $\binom{N}{n}$ is the number of ways of selecting a sample of size $n$ if there are $N$ balls in the bag in total.


:::{.definition #HypergeometricDistribution name="Hypergeometric distribution"}
Consider a set of $N$ items of which $r$ are of one kind (call them 'successes') and other $N - r$ are of another kind (call them 'failures').
We are interested in the probability of $x$ successes in $n$ trials, when the selection (or drawing) is made *without* replacement. 
Then the random variable $X$ is said to have a *hypergeometric distribution* with pf

\begin{equation}
   p_X(x) = \frac{ \binom{r}{x}\binom{N - r}{n - x}}{\binom{N}{n}}
   (\#eq:HypergeometricPMF)
\end{equation}
where $x = 0$, $1$, $\dots$, $n$; $x \le r$, and $n - x \le N - r$.
:::

The following are the basic properties of the hypergeometric distribution.

:::{.theorem #HypergeomatricDistributionProperties name="Hypergeometric distribution properties"}
If $X$ has a hypergeometric distribution with pf \@ref(eq:HypergeometricPMF), then

1. $\text{E}(X) = nr/N$.
2. $\text{var}(X) = {n\left(\frac{r}{N}\right)\left(\frac{N-r}{N}\right)\left(\frac{N-n}{N-1}\right)}$.
:::
The moment generating function is difficult and will not be considered.


::: {.softwareBox .software data-latex="{iconmonstr-laptop-4-240.png}"}
In **R**, the hypergeometric distribution functions have the form `[dpqr]hyper(m, n, k)`.
:::


::: {.linkBox .link data-latex="{iconmonstr-link-1-240.png}"}
If the population is much larger than the sample size (that is, $N$ is much larger than $n$), then the probability of a success will be approximately constant, and the binomial distribution can be used to give approximate probabilities.
:::


Consider the example at the start of this section.
The probability of drawing a red ball initially is $6/10 = 0.6$, and the probability that the next ball is red is $5/9 = 0.556$.
But suppose there are 10000 balls in the bag, of which 6000 are red.
The probability of drawing a red ball initially is $6000 / 10000 = 0.6$, and the probability that the next ball is red becomes $5999/9999 = 0.59996$; the probability is almost the same.
In this case, we might consider using the binomial distribution with $p\approx 0.6$.

In general, if $N$ is much larger than $n$, the population proportion then will be approximately $p\approx r/N$, and so $1 - p \approx (N - r)/N$.
Using this information,

\[
   \text{E}(X) = n \times (r/N) \approx np
\]
and
\begin{align*}
   \text{var}(X)
   &= n\left(\frac{r}{N}\right)\left(\frac{N - r}{N}\right)\left(\frac{N - n}{N - 1}\right)\\
   &\approx n\left( p \right ) \left( 1 - p \right) \left(1 \right ) \\
   &= n p (1 - p),
\end{align*}
which correspond to the mean and variance of the binomial distribution.


:::{.example #Mice name="Mice"}
Twenty mice are available to be used in an experiment; seven of the mice are female and $13$ are male.
Five mice are required and will be sacrificed.
What is the probability that more than three of the mice are males?\smallskip

Let $X$ be the number of male mice chosen in a sample of size $5$.
Then $X$ has  a hypergeometric distribution (since mice are chosen without replacement) where $N = 20$, $n = 5$, $r = 13$ and we seek

\begin{align*}
   \Pr(X > 3)
   &= \Pr(X = 4) + \Pr(X = 5) \\
   &= \frac{ \binom{13}{4} \binom{7}{1}}{ \binom{20}{5} } +
       \frac{ \binom{13}{5} \binom{7}{0}}{ \binom{20}{5} } \\
   & \approx  0.3228 + 0.0830 = 0.4058.
\end{align*}
The probability is about 41%.
:::


## Simulation {#SimulationDiscrete}

Distributions can be used to *simulate* practical situations, using random numbers generated from the distributions.
In **R**, these function start with the letter `r`; for example, to generate random numbers from a Poisson distribution, use `rpois()`:

```{r echo=FALSE}
set.seed(107283)
```

```{r echo=TRUE}
rpois(3, # Generate three random numbers...
      lambda = 4) # ... with mean = 4
```

So, for example, if the Poisson distribution with mean $0.5$ was used to model the number of new cases of a disease in a given town each month, the number of cases in a *year* could be simulated in this way:

```{r echo=FALSE}
set.seed(1072908)
```

```{r echo=TRUE}
cases <- 0  # Initialise
for (m in (1:12)){   # For each month...
   cases <- cases + rpois(1, 0.5)  # Add new cases for each month
   cat("Month ", m, "; Number of cases so far: ", cases, "\n")
}
```

The code can be repeatedly run, as each run will produce different random numbers.
If we run the above code 1000 times, we can record how many cases are recorded at the end of each year (Fig. \@ref(fig:SimAPlot)):


```{r SimA, echo=TRUE, results='hide', fig.show="hide"}
set.seed(10729083)
numberCasesEndOfYear <- array( dim = 1000)  # Array to store cases

for (sim in (1:1000)) { # For every simulation...
   cases <- 0           # Initialise afresh for each sim.
   for (m in (1:12)){   # For each month
      cases <- cases + rpois(1, 0.5)   # Add cases
   }
   numberCasesEndOfYear[sim] <- cases  # Add to the array
}
hist( numberCasesEndOfYear,
      las = 1,
      breaks = seq(0, 20, by = 2),
      xlim = c(0, 20),
      xlab = "Number of cases in one year",
      main = "Histogram of number of annual cases\n over 1000 simulations")
```

```{r SimAPlot, echo=FALSE, fig.cap="The number of cases per year", fig.align="center", fig.width=4}
<<SimA>>
```

Then, empirical probabilities can be computed:

```{r echo=TRUE}
sum( numberCasesEndOfYear > 10) / 1000
```
The chance that more than ten cases are observed is about `r round(sum( numberCasesEndOfYear > 10) / 1000*100, 1)`%.

Of course, these probabilities can be computed from a knowledge of the Poisson distribution too, but simulation makes studying more complex examples easier.
Suppose, for instance, that the number of cases of the disease has a mean of $0.5$ for most of the year, but a mean of $1.2$ over winter.
Again, the number of new cases in a *year* could be simulated (Fig. \@ref(fig:SimB)):


```{r SimB, echo=TRUE, fig.cap="The number of cases per year", fig.align="center", fig.width=4}
numberCasesEndOfYear <- array( dim = 1000)

for (sim in (1:1000)) {
   cases <- 0
   for (m in (1:12)){
     mn <- 0.5  # The usual mean
     if (m %in% c(6, 7, 8) ) mn <- 1.2  # Alter mean for winter
    
     cases <- cases + rpois(1, mn)  
   }
   numberCasesEndOfYear[sim] <- cases
}
hist( numberCasesEndOfYear,
      las = 1,
      breaks = seq(0, 20, by = 2),
      xlim = c(0, 20),
      xlab = "Number of cases in one year",
      main = "Histogram of number of annual cases\n over 1000 simulations")

sum( numberCasesEndOfYear > 10) / 1000

```

The chance that more than ten cases are observed has increased to about `r round(sum( numberCasesEndOfYear > 10) / 1000*100, 1)`%.


## Exercises

:::{.exercise #BinChange}
If $X \sim \text{Bin}(n, 1 - p)$, show that $(n - X) \sim \text{Bin}(n, p)$.
:::


:::{.exercise #SaltIntake}
A study by @data:Sutherland:SaltIntake found that about 30% of Britons 'generally added salt' at the dinner table.

1. In a group of 25 Britons, what is the probability that at least 10 added salt?
1. In a group of 25 Britons, what is the probability that no more than 9 added salt?
1. In a group of 25 Britons, what is the probability that between 5 and 10 (inclusive) added salt?
1. What is the probability that 6 Britons would need to be selected to find one that adds salt?
1. What is the probability that at least 8 Britons would need to be selected to find the first that add salt?
1. What is the probability that at least 8 Britons would need to be selected to find three that add salt?
1. What assumptions are being made in the above calculations?
:::


:::{.exercise #Placebos}
A study by @loyeung2018experimental examined whether people could identify potential placebos.
The 81 subjects were each presented with five different supplements, and asked to select which *one* was the legitimate herbal supplement based on the taste (the rest were placebos).

1. What is the probability that more than 15 will correctly identify the legitimate supplement?
1. What is the probability that at least 12 correctly identify the legitimate supplement?
1. What is the probability that the first person to identify the legitimate supplement is the third person tested?
1. What is the probability that the fifth person to identify the legitimate supplement is the 10th person tested?
1. In the study, 50 people correctly selected the true herbal supplement.
   What does this suggest?
:::


:::{.exercise #NoisyMiners}
A study by @data:Maron:eucthreshold used statistical modelling to show that the mean number of noisy miners (a type of bird) in sites with about 15 eucalypts per two hectares was about 3.

1. In a two hectare site with 15 eucalypts, what is the probability of observing no noisy miners?
1. In a two hectare site with 15 eucalypts, what is the probability of observing more than 5 noisy miners?
1. In a four hectare site with 30 eucalypts, what is the probability of observing two noisy miners?
1. What is the mean of the number of noisy miners observed in a two hectare site with 15 eucalypts?
:::


:::{.exercise #DisaggregationPoisson}
In a study of rainfall disaggregation [@connolly1998daily] (extracting small-scale rainfall features from large-scale measurements), the number of non-overlapping rainfall events per day at Katherine was modelled using a Poisson distribution (for $x = 1, 2, 3, \dots$) with $\lambda = 2.5$ in summer and $\lambda = 1.9$ in winter.

Denote the number of rainfall events in summer as $S$, and in winter as $W$.

1. Plot the two distributions, and compare summer and winter.
1. What is the probability of more than 3 rainfall events per day in winter? 
1. What is the probability of more than 3 rainfall events per day in summer? 
1. Describe what is meant by the statement $\Pr(S > 3 \mid S > 1)$, and compute the probability.
1. Describe what is meant by the statement $\Pr(W > 2 \mid W > 1)$, and compute the probability.
::: 


:::{.exercise #UnknownSize}
Consider a population of animals of a certain species of unknown size $N$ [@romesburg1979fitting].
A certain number of animals in an area are trapped and tagged, then released.
At a later point, more animals are trapped, and the number previously tagged is noted.

Define $p$ as the probability that an animal is captured zero times during the study,  and $n_x$ as the number of animals captured $x$ times (for $x = 0, 1, 2, \dots$).
($n_0$ is unknown.)
The study consist of $s$ trapping events, so we have $n_1$, $n_2, \dots n_s$ where $s$ is sufficiently 'large' that the truncation is negligible.
Then,

\begin{equation}
   n_x = N p (1 - p)^x \quad \text{for $x = 0, 1, 2, \dots$}.
   (\#eq:GeomCapture)
\end{equation}
While $p$ and $N$ are unknown, the value of $N$ is sought.

1. Take logarithms of \@ref(eq:GeomCapture).
   Write this equation in the form of a linear regression equation $\hat{y} = \beta_0 + \beta_1 x$.
1. From the regression equation, identify how to estimate $p$ from the slope of the regression line, and then $N$ from the $y$-intercept.
1. Use the data in Table \@ref(tab:RabbitPop), from a study of rabbits in an area Michigan [@eberhardt1963problems], to estimate the rabbit population.


```{r RabbitPop, echo=FALSE}
Rabbits <- array( dim = c(2, 7) )

Rabbits[1, ] <- c("$x$", 1:6)
Rabbits[2, ] <- c("$n_x$", 247, 63, 20, 4, 2, 1)


if( knitr::is_latex_output() ) {
  knitr::kable(Rabbits, 
               format = "latex",
               booktabs = TRUE,
               align = "r",
               longtable = FALSE,                
               escape = FALSE,
               caption = "The number of rabbits $n_x$ at each trapping who had been previously trapped") %>%
    kable_styling(font_size = 10) %>%
    row_spec(0, bold = TRUE) 
}
if( knitr::is_html_output() ) {
  knitr::kable(Rabbits, 
               format = "html",
               booktabs = TRUE,
               align = "r",
               longtable = FALSE,                
               escape = FALSE,
               caption = "The number of rabbits $n_x$ at each trapping who had been previously trapped") %>%
    kable_styling(font_size = 10) 
}

```
:::


:::{.exercise}
A Nigerian study of using solar energy to disinfect water (SODIS) modelled the number of days exposure needed to disinfect the water [@nwankwo2022solar].
The threshold for disinfection was a day recording 4 kWh/m^2^ daily cumulative solar irradiance.

1. Suppose $p = 0.5$.
   How many days exposure would be needed, on average, to achieve disinfection?
1. Suppose $p = 0.25$.
   How many days exposure would be needed, on average, to achieve disinfection?
:::


:::{.exercise}
A negative binomial distribution was used to model the number of parasites on feral cats on on Kerguelen Island [@hwang2016estimating].
The model used is parameterised so that $\mu = 8.7$ and $k = 0.4$, where $\text{var}(X) = \mu + \mu^2/k$.

1. Use the above information to determine the negative binomial parameters used for the parameterisation in Sect. \@ref(def:GeometricDistributionALT).
1. Determine the probability that a feral cat has more than 10 parasites.
1. The cats with the largest 10% of parasites have how many parasites?
:::


:::{.exercise}
A study investigating bacterial sample preparation procedures for single-cell studies  [@koyama2016bacterial] studied, among other bacteria, *E. coli* 110.
The number of bacteria in 2$\mu$L samples was modelled using a Poisson distribution with $\lambda = 1.04$.

1. What is the probability that a sample has more than 4 bacteria? 
1. What is the probability that a sample has more than 4 bacteria, given it has bacteria? 1. Would you expect the negative binomial distribution to fit the data better than the Poisson?
:::



:::{.exercise}
A study of accidents in coal mines [@sari2009stochastic] used a Poisson model with $\lambda = 12.87$.

1. Plot the distribution.
1. Compute the probability of more than one accident per day in February.
:::


:::{.exercise}
In a study of heat spells [@furrer2010statistical] examined three cities.
In Paris, a 'hot' day was defined as a day with a maximum over 27^$\circ$^C; in Phoenix, a 'hot' day was defined as a day with a maximum over  40.8^$\circ$^C.

The length of a heat spell $X$ was modelled using a geometric distribution, with $p = 0.40$ in Paris, and $\lambda = 0.24$ in Phoenix.

1. On the same graph, plot both probability functions.
1. For each city, what is the probability that a hear spell last longer than a week?
1. For each city, what is the probability that a hear spell last longer than a week, given it has lasted two days?
:::


:::{.exercise}
A study of crashes at intersections [@lord2005poisson] examined high risk intersections, and modelled the number of crashes with a Poisson distribution using $\lambda = 11.5$.

1. What proportion of such intersection would be expected to have zero crashes?
1. What proportion of such intersection would be expected to have more than five crashes?
:::


:::{.exercise}
In a study of counting birds [@white1996analysis], the parametrisation of the negative binomial distribution used by the authors is

\begin{equation}
   p_X(x) 
   = \binom{k + x - 1}{k - 1} \left( \frac{m}{k} \right) 
   \left( 1 + \frac{m}{k} \right)
   (\#eq:NegativeBinomialPMFAlternative)
\end{equation}
for $x = 0, 1, 2, \dots$ and $k > 0$ and mean $m > 0$.

The fitted parameters are, for orange-crowned warblers:

* in ponderosa pine stand (riparian sites): $m = 0.11$ and $k = 0.275$; and
* n lodgepole pine stands (again, riparian sites): $m = 0.04$ and $k= 0.275$.

1. On the same graph, plot both distributions.
   Comment.
1. Compute the probability of observing fewer than 5 birds in each environment.
1. Compute the probability of observing fewer than 5 birds in each environment, given that the bird was seen.
:::





::: {.exercise #NegativeBinomialALT}
The negative binomial distribution was defined in Sect \@ref(NegativeBinomialDistribution) for the random variable $Y$, which represented the number of failures until the $r$th success.
An alternative parameterisation is to define the random variable $X$ as the number of trials necessary to obtain $r$ successes.

1. Define range space for $X$.
1. Deduce the pf for $X$.
1. Determine the mean, variance and mgf of $X$, using the results already available for $Y$.
:::



:::{.exercise}
Suppose typos made by a typist on a typing test form a Poisson process with the mean rate 2.5 typos per minute and that the test lasts five minutes.

1. Determine the probability that the typist makes exactly 10 errors during the test.
1. Determine the probability that the typist makes exactly 6 errors during the first 3 minutes, and exactly 4 errors during the last 2 minutes of the test.
1. Determine the probability that the typist makes exactly 6 errors during the first 3 minutes, and exactly 6 during the last 3 minutes of the test.  (Note: These times overlap.)
:::


:::{.exercise}
The depth of a river varies from the 'normal' level, say $Y$ (in metres), a specific location with a pdf given by $f(y) = 1/4$ for $-2 \le y \le 2$.

1. Find the probability that  $Y$ is greater than 1m.
2. If readings on four different days are taken what is the probability that exactly two are greater than 1m?
:::


:::{.exercise #QueuingPois}
Poisson distributions are used in *queuing theory* to model the formation of queues.
Suppose that a certain queue is modelled with a Poisson distribution with a mean of $0.5$ arrivals per minute.

1. Use **R** to simulate the arrivals from 8AM to 9AM (print the queue length after each minute).
   Produce 100 simulations, and hence compute the mean and standard deviation of people in the queue at 9AM.
1. Suppose a server begins work at 8:30AM serving customers (e.g., removing them from the queue) at the rate of $0.75$ per minute.
   Again, use **R** to simulate the arrivals for 60 minutes (print the queue length after each minute).
   Produce 100 simulations, and hence compute the mean and standard deviation of people in the queue at 9AM.
1. Suppose two servers begins work at 8:30AM who together serve customers (e.g., removing them from the queue) at the rate of $1.5$ per minute.
   Again, use **R** to simulate the arrivals for 60 minutes (print the queue length after each minute).
   Produce 100 simulations, and hence compute the mean and standard deviation of people in the queue at 9AM.
:::


:::{.exercise}
Suppose that a certain queue is modelled with a Poisson distribution with a mean of $0.5$ arrivals per minute.

1. Use **R** to simulate the arrivals from 8AM to 9AM (print the queue length after each minute).
   Produce 100 simulations, and hence compute the mean and standard deviation of people in the queue at 9AM.
1. Suppose a server begins work at 8:30AM serving customers (e.g., removing them from the queue) at the rate of $0.75$ per minute.
   Again, use **R** to simulate the arrivals for 60 minutes (print the queue length after each minute).
   Produce 100 simulations, and hence compute the mean and standard deviation of people in the queue at 9AM.
1. Suppose two servers begins work at 8:30AM who together serve customers (e.g., removing them from the queue) at the rate of $1.5$ per minute.
   Again, use **R** to simulate the arrivals for 60 minutes (print the queue length after each minute).
   Produce 100 simulations, and hence compute the mean and standard deviation of people in the queue at 9AM.
:::



