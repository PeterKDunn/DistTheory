\part{Appendices}

# (APPENDIX) Appendices {-}



# Some useful series {#UsefulSeries}

For convenience and reference we state some results here involving series that occur quite frequently in probability and statistical theory.


## Finite series

* Sum of natural numbers:
\begin{equation}
   1 + 2 + 3 + \ldots + n 
   = \frac{n(n + 1)}{2}.
   (\#eq:SumNaturalNumbers)
\end{equation}

* Sum of squares of natural numbers:
\begin{equation}
   1^2 + 2^2 + 3^2 + \ldots + n^2
   = \frac{n}{6}(n + 1)(2n + 1)
   (\#eq:SumSquaredNaturalNumbers)
 \end{equation}
 
* Geometric series
\begin{align}
   a + ar + ar^2 + \ldots + ar^{n - 1}
   &= \frac{a(1 - r^n)}{1 - r}
   (\#eq:SumGeometricFinite)\\
   &\rightarrow \frac{a}{1 - r} \text{ as $n  \rightarrow  \infty$}
  (\#eq:SumGeometricInfinite)
\end{align}
where the infinite series only converges if $|r| < 1$.

* Binomial expansion:
\begin{equation}
   (a + b)^n = b^n + {n \choose 1}ab^{n - 1} + \ldots + {n \choose r}a^r b^{n - r} + \ldots a^n.
   (\#eq:BinomialSeries)
\end{equation}



## Power series

A power series is a series of the form $\sum_{n = 0}^{\infty}a_n z^n$.

* Exponential function:
\begin{align}
   e^z
   &= 1 + z + \frac{z^2}{2!} + \frac{z^3}{3!} + \ldots
   (\#eq:Exponential)\\
   e^{-z}
   &= 1 - z + \frac{z^2}{2!} - \frac{z^3}{3!} + \ldots
   (\#eq:ExponentialNegative)
\end{align}

* Logarithmic series:
\begin{align}
   \log(1 + z)
   &= z - \frac{z^2}{2} + \frac{z^3}{3} - \ldots
   (\#eq:Logarithmic)\\
   \log(1 - z)
   &= -z - \frac{z^2}{2} - \frac{z^3}{3} - \ldots
   (\#eq:LogarithmicNegative)
\end{align}

* Others:
\begin{align}
   (1 - z)^{-1}
   &= 1 + z + z^2 + z^3 + \ldots
   (\#eq:Other1)\\
   (1 - z)^{-r}
   &= 1 + rz + \frac{r(r + 1)z^2}{2!} + \frac{r(r + 1)(r + 2)z^3}{3!} + \ldots
   (\#eq:Other2)
\end{align}
Note that the expressions on the RHS above do not converge to the LHS for all values of $z$.

DOES THIS COMMENT APPLY TO ALL????



# Using R with univariate distributions {#UseRDistributions}

In **R**, standard distributions have four associated function:

* Computing the [probability function](#ProbabilityFunction): All begin with `p` (such as `pnorm()` for the normal distribution).
* Computing the [distribution function](DistributionFunction): All begin with `d`(such as `dnorm()`).
* Compiting the quantile function (XREF): All begin with `q` (such as `qnorm()`).
* Generating random numbers (XREF): All begin with `r` (such as `rnorm()`).

ADD PARAMETERS TOO... if enough room.

In **R**, four function are available for most distributions

* Computing the [probability function](#ProbabilityFunction): Functions start with `d`
* Computing the [distribution function](#DistributionFunction): Functions start with `p`
* Computing the quantiles: Functions start with `q`
* Generatimg random numbers: Functions start with `r`.

For example, the fours function above are called, for the normal distribution (denoted `norm` in **R**):

* `dnorm()`
* `pnorm()`
* `qnorm()`
* `rnorm()`

ADD a chapter on how to generate random numbers and quantiles and so on.



## Discrete distributions

Distribution       | **R** function          | Common parameters
-------------------|-------------------------|-----------------------
Discrete uniform   | `sample()`              | ~                     
Binomial           | `dbinom(x, size, prob)` | `x`:  Values where to evaluate the probability function
.                  | `pbinom(q, size, prob)` | `q`: Quantiles
.                  | `qbinom(p, size, prob)` | `p`: Probabilities
.                  | `rbinom(n, size, prob)` | `n`: The number of random observations to generate
.                  |  .                      | `size`: The number of trials
.                  |  .                      | `prob`: The probability of 'success' in each trial
Poisson            | `dpois(x, lambda)`      | `x`: Values where to evaluate the probability function  
.                  | `ppois(q, lambda)`      | `q`: Quantiles
.                  | `qpois(p, lambda)`      | `p`: Probabilities
.                  | `rpois(n, lambda)`      | `n`: The number of random observations to generate
.                  |  .                      | `lambda`: The Poisson mean
Geometric          | `dgeom(x, prob)`        | `x`: Values where to evaluate the probability function  
.                  | `pgeom(q, prob)`        | `q`: Quantiles
.                  | `qgeom(p. prob)`        | `p`: Probabilities   
.                  | `rgeom(n, prob)`        | `n`: The number of random observations to generate
.                  |  .                      | `prob`: The probability of 'success' in each trial
Negative binomial  | `dnbinom(x, size, prob, mu)` | `x`: Values where to evaluate the probability function          
.                  | `pnbinom(q, size, prob, mu)` | `q`: Quantiles
.                  | `qnbinom(p, size, prob, mu)` | `p`: Probabilities   
.                  | `rnbinom(p, size, prob, mu)` | `n`: The number of random observations to generate
.                  |  .                           | `size`: The number of trials
.                  |  .                           | `prob`: The probability of 'success' in each trial
.                  |  .                           | `mu`: Useful for alternative parameterisation


## Continuous distributions


Distribution       | Probability function | Distribution function | Quantile function     | Random numbers
-------------------|----------------------|-----------------------|-----------------------|-----------------
Discrete uniform   | `sample()`           | ~                     |   ~                   | ~
Binomial           | `dunif()`            | `punif()`             | `qunif()`             | `runif()`
Poisson            | `dunif()`            | `punif()`             | `qunif()`             | `runif()`
Geometric          | `dgeom()`            | `pgeom()`             | `qunif()`             | `runif()`
Negative binomial  | `dunif()`            | `punif()`             | `qunif()`             | `runif()`
Continuous uniform | `dunif()`            | `punif()`             | `qunif()`             | `runif()`
Normal|            | `dunif()`            | `punif()`             | `qunif()`             | `runif()`
Gamma|             | `dunif()`            | `punif()`             | `qunif()`             | `runif()`
$t$                | `dunif()`            | `punif()`             | `qunif()`             | `runif()`
$\chi$-squared     | `dunif()`            | `punif()`             | `qunif()`             | `runif()`
$F$                | `dunif()`            | `punif()`             | `qunif()`             | `runif()`



## Multivariate distributions 

ADD??




Distribution       | Probability function | Distribution function | Quantile function     | Random numbers
-------------------|----------------------|-----------------------|-----------------------|-----------------
Discrete uniform   | `sample()`           | ~                     |   ~                   | ~
Binomial           | `dunif()`            | `punif()`             | `qunif()`             | `runif()`
Poisson            | `dunif()`            | `punif()`             | `qunif()`             | `runif()`
Geometric          | `dgeom()`            | `pgeom()`             | `qunif()`             | `runif()`
Negative binomial  | `dunif()`            | `punif()`             | `qunif()`             | `runif()`
Continuous uniform | `dunif()`            | `punif()`             | `qunif()`             | `runif()`
Normal|            | `dunif()`            | `punif()`             | `qunif()`             | `runif()`
Gamma|             | `dunif()`            | `punif()`             | `qunif()`             | `runif()`
$t$                | `dunif()`            | `punif()`             | `qunif()`             | `runif()`
$\chi$-squared     | `dunif()`            | `punif()`             | `qunif()`             | `runif()`
$F$                | `dunif()`            | `punif()`             | `qunif()`             | `runif()`



Distribution       | **R**  functions (with common parameter)
-------------------|-----------------------------------------------------------------------------
Discrete uniform   | Random sample: `sample()`  
Binomial           | `dbinom(x, size, prob)`   
.                   | `pbinom(q, size, prob)`  
.                   | `qbinom(p, size, prob)`  
.                   | `rbinom(n, size, prob)`
Poisson            | `dpois(x, lambda)`   
.                   | `ppois(q, lambda)`   
.                   | `qpois(p, lambda)`   
.                   | `rpois(n, lambda)`
Geometric          | `dgeom(x, prob)`   
.                   | `pgeom(q, prob)`   
.                   | `qgeom(p, prob)`   
.                   | `rgeom(n, prob)`
Negative binomial  | `dnbinom(x, size, prob, mu)` 
.                   | `pnbinom(q, size, prob, mu)` 
.                   | `qdbinom(p, size, prob, mu)` 
.                   | `rnbinom(n, size, prob, mu)`
                   

## Standard univariate continuous distributions
                 
                   
Distribution       | **R**  functions (with common parameter)
-------------------|-----------------------------------------------------------------------------
Continuous uniform | `dunif(x, min = 0, max = 1)`   
.                   | `punif(q, min = 0, max = 1)`   
.                   | `qunif(p, min = 0, max = 1)`   
.                   | `runif(r, min = 0, max = 1)`
Normal             | `dnorm(x, mean = 0, sd = 1)`   
.                   | `pnorm(q, mean = 0, sd = 1)`   
.                   | `qnorm(p, mean = 0, sd = 1)`   
.                   | `rnorm(n, mean = 0, sd = 1)`
Gamma              | `dgamma(x, shape, rate = 1, scale = 1 / rate)`  
.                   | `pgamma(q, shape, rate = 1, scale = 1 / rate)`  
.                   | `qgamma(p, shape, rate = 1, scale = 1 / rate)`  
.                   | `rgamma(n, shape, rate = 1, scale = 1 / rate)`
$t$                | `dt(x, df)`      
.                   | `pt(q, df)`      
.                   | `qt(p, df)`      
.                   | `rt(n, df)`
$\chi$-squared     | `dchisq(x, df)`  
.                   | `pchisq(q, df)`  
.                   | `qchisq(p, df)`  
.                   | `rchisq(n, df)`
$F$                | `df(x, df1, df2)`      
.                   | `pf(q, df1, df2)`      
.                   | `qf(p, df1, df2)`      
.                   | `rf(n, df1, df2)`



## Standard bivariate distributions

Add `mvtnorm::dmvnor()`???



# Solutions

## Answers for Chap. 1

:::{.answer}
Answer to Exercise \@ref(exr:BasicProbs)

1. $0.11$
2. $0.24$
3. $0.32$
4. $0.11/0.35$
5. $\Pr(A) \times \Pr(B) = 0.44\times 0.35 = 0.154 \ne \Pr(A\cap B) = 0.11$; not independent.
:::

:::{.answer}
Answer to Exercise \@ref(exr:Quadratic).

* $S = \{ (a,b,c) \mid a\in(-\infty, 0)\cap (0, \infty), -\infty < b < \infty, -\infty < c < \infty\}$.
* The solutions to a quadratic equation are given by
\[
   x = \frac{-b \pm \sqrt{b^2 - 4ac} }{2a}.
\]
For two equal roots, $b^2 - 4ac = 0$, so event $A$ is defined as $A = \{ (a,b,c) \mid b^2 - 4ac = 0\}$.
:::

:::{.answer}
Answer to Exercise \@ref(exr:HatData).

Tree diagram: Fig, \@ref(fig:HatTree); table: Table \@ref(tab:HatTable); Venn diagram: Fig. \@ref(fig:HatVenn).
:::


```{r HatTree, echo=FALSE, fig.align="center", fig.cap="Tree diagram for the hat-wearing example"}

plot( x = c(0, 1),
      y = c(0, 1),
      type = "n",
      main = "Tree diagram for the hat data",
      xlab = "",
      ylab = "",
      axes = FALSE)
#STEP 1
lines( x = c(0.2, 0.40),
       y = c(0.5, 0.75),
       lwd = 2)
lines( x = c(0.2, 0.40),
       y = c(0.5, 0.25),
       lwd = 2)
# STEP 2
lines( x = c(0.60, 0.80),
       y = c(0.75, 0.90),
       lwd = 2)
lines( x = c(0.60, 0.80),
       y = c(0.75, 0.60),
       lwd = 2)

lines( x = c(0.60, 0.80),
       y = c(0.25, 0.40),
       lwd = 2)
lines( x = c(0.60, 0.80),
       y = c(0.25, 0.10),
       lwd = 2)

# TEXT
text(x = 0.50,
     y = 0.75,
     labels = "Males")
text(x = 0.50,
     y = 0.25,
     labels = "Females")

text(x = 0.90,
     y = 0.90,
     labels = "Hat")
text(x = 0.90,
     y = 0.60,
     labels = "No hat")

text(x = 0.90,
     y = 0.40,
     labels = "Hat")
text(x = 0.90,
     y = 0.10,
     labels = "No hat")

# ADD PROBS
text(x = 0.38,
     y = 0.63,
     cex = 0.9,
     labels = "0.513")
text(x = 0.38,
     y = 0.37,
     cex = 0.9,
     labels = "0.487")

text(x = 0.65,
     y = 0.84,
     cex = 0.9,
     labels = "0.205")
text(x = 0.65,
     y = 0.66,
     cex = 0.9,
     labels = "0.795")

text(x = 0.65,
     y = 0.34,
     cex = 0.9,
     labels = "0.060")
text(x = 0.65,
     y = 0.16,
     cex = 0.9,
     labels = "0.940")

## STEPS
text(x = 0.50,
     y = 0.01,
     label = expression(bold("Step 1"))
     )
text(x = 0.90,
     y = 0.01,
     label = expression(bold("Step 2"))
     )
```


```{r HatTable, echo=FALSE}
BRIS <- read.csv("Data/sunglasses.csv")
BRIS$Gender <- factor(BRIS$Gender,
                      levels = c(1, 2),
                      labels = c("Males", 
                                 "Females"))
BRIS$Hat <- factor(BRIS$Hat,
                   levels = c(0, 1),
                   labels = c("Not Hat",
                              "Hat"))

HatTable <- xtabs( Count ~ Gender + Hat,
                   data = BRIS)
HatTable <- cbind( HatTable,
                   "Total" = rowSums(HatTable) )
knitr::kable(HatTable,
             caption = "The numbers of males and females wearing a hat in the middle of the day in Brisbane")
```



```{r HatVenn, echo=FALSE, fig.height=7, out.width = '95%', fig.align="center", fig.cap="The hat-wearing data as a Venn diagram"}
par( mfrow = c(2, 2))
colourMale <- rgb(0, 0, 255, 
                  max = 255, 
                  alpha = 125)
colourHat  <- rgb(0, 255, 0, 
                  max = 255, 
                  alpha = 125)

plot( x = c(0, 1),
      y = c(0, 1),
      type = "n",
      ylab = "",
      xlab = "",
      main = expression(paste("Events ", italic(M), " and ", bar(italic(M)))),
      axes = FALSE)

polygon( x = c(0, 0, 1, 1),
         y = c(0, 1, 1, 0))
plotrix::draw.circle(x = 0.4, 
                     y = 0.5,
                     radius = 0.2,
                     col = colourMale)
text(x = 0.4,
     y = 0.5,
     labels = expression(paste(italic(M), ": 307")))
text(x = 0.8,
     y = 0.5,
     labels = expression(paste(bar(italic(M)), ": 366")))
mtext(text = expression(italic(S)),
      side = 1,
      adj = 0.25)


###############


plot( x = c(0, 1),
      y = c(0, 1),
      type = "n",
      ylab = "",
      xlab = "",
      main = expression(paste("Events ", italic(H), " and ", bar(italic(H)))) ,
      axes = FALSE)


polygon( x = c(0, 0, 1, 1),
         y = c(0, 1, 1, 0))

plotrix::draw.circle(x = 0.6,
                     y = 0.5,
                     radius = 0.2,
                     col = colourHat)

text(x = 0.6,
     y = 0.5,
     labels = expression(paste(italic(H), ": 101")))
text(x = 0.2,
     y = 0.5,
     labels = expression(paste( bar(italic(H)), ": 651")))

mtext(text = expression(italic(S)),
      side = 1,
      adj = 0.25)


####################


plot( x = c(0, 1),
      y = c(0, 1),
      type = "n",
      ylab = "",
      xlab = "",
      main = expression(paste("Events ", italic(M), " and ", italic(H))) ,
      axes = FALSE)


polygon( x = c(0, 0, 1, 1),
         y = c(0, 1, 1, 0))
plotrix::draw.circle(x = 0.4, 
                     y = 0.5,
                     radius = 0.2,
                     col = colourMale)
plotrix::draw.circle(x = 0.6,
                     y = 0.5,
                     radius = 0.2,
                     col = colourHat)
text(x = 0.3,
     y = 0.5,
     labels = expression(italic(M)))
text(x = 0.7,
     y = 0.5,
     labels = expression(italic(H)))
mtext(text = expression(italic(S)),
      side = 1,
      adj = 0.25)


#################



plot( x = c(0, 1),
      y = c(0, 1),
      type = "n",
      ylab = "",
      xlab = "",
      main = "The numbers in each section",
      axes = FALSE)


polygon( x = c(0, 0, 1, 1),
         y = c(0, 1, 1, 0))
plotrix::draw.circle(x = 0.4, 
                     y = 0.5,
                     radius = 0.2,
                     col = colourMale)
plotrix::draw.circle(x = 0.6,
                     y = 0.5,
                     radius = 0.2,
                     col = colourHat)
text(x = 0.3,
     y = 0.5,
     labels = "307")
text(x = 0.7,
     y = 0.5,
     labels = "22")
text(x = 0.7,
     y = 0.85,
     labels = expression(bar(italic(M)) ~ intersect() ~ bar(italic(H)) ~ ": 344") )
mtext(text = expression(italic(S)),
      side = 1,
      adj = 0.25)

# Intersection
text(x = 0.5, 
     y = 0.5, 
     label = "79")

```



:::{.answer}
**Answer to Exercise \@ref(exr:CompareMeansStress).**

```{r}
PreHours <- c(10.0,6.5,8.0,12.0,5.0,11.5,5.0,3.5,7.5,5.8,
             4.7,8.0,7.0,17.0,8.8,17.0,15.0,4.4,2.0)
             
PreMins <- c(6.5,14.0,13.5,18.0,14.5,9.0,18.0,42.0,7.5,6.0,
             25.0,12.0,52.0,20.0,16.0,15.0,11.5,2.5,2.0)

MeanAll <- mean( c(PreHours, PreMins))
MeanHours <- mean( PreHours )
MeanMins <- mean( PreMins )

# The difference in the sample:
MeanMins - MeanHours

```

The data show that 12 patients had beta-endorphin concentrations higher *minutes* before surgery rather than *hours* before surgery.

Is this evidence that the beta-endorphine concentrations are higher in the minutes before surgery rather than the hours before surgery?
No, it is not.
Even if there was *no difference* between the beta-endorphin concentrations between the two times, random variation means that some patients would have the higher reading in the *minutes* before, and some would have the higher reading in the *hours* before.

We can use computer simulation to study the situation.
Assume there is no difference between the beta-endorphin concentrations between the two time points.
Using **R**, we can randomly allocate each of the 19 people in the study to have higher concentrations in one of the two time points: 


```{r echo=FALSE}
set.seed(869812)
```


```{r}
HigherMins <- array(dim = 19)  # Store info 


#For each patient:
for (i in 1:19){ 
  # Select 1 option from either "Hours" or "Mins to be higher, 
  # with 0.5 prob for each option:
  WhichHigher <- sample(x = c("Hours", "Mins"), 
                        size = 1, 
                        prob = c(0.5, 0.5) )
  # Record keeping: 1 means higher for "Mins"
  HigherMins[i] <- WhichHigher == "Mins"
}
```

So for this simulated outcome, we find:
```{r}
# So for this simulation:
HigherMins
```
That is,
```{r}
# For this simulation, how many higher for "Mins"?
sum(HigherMins)
```
... of the 19 patients had a beta-endorphin concentration *minutes* before surgery.

We can repeat this process many times, say 1000 times:

```{r}
numSims <- 1000   # 1000 simulations
# Record the outcome for each simulation:
NumHigherMins <- array( dim = numSims)

for (n in 1:numSims){
  #For each patient:
  for (i in 1:19){ 
    # Select 1 option from either "Hours" or "Mins to be higher, 
    # with 0.5 prob for each option:
    WhichHigher <- sample(x = c("Hours", "Mins"), 
                          size = 1, 
                          prob = c(0.5, 0.5) )
    # Record keeping: 1 means higher for "Mins"
    HigherMins[i] <- WhichHigher == "Mins"
  }
  # So for this simulation:
  HigherMins
  # For this simulation, how many higher for "Mins"?
  NumHigherMins[n] <- sum(HigherMins)
}
```

```{r}
# Percentage of times when this many have Mins as the higher:
SummaryTable <- table(NumHigherMins) / numSims * 100
SummaryTable
```

This table shows us how often we find the given number of patients having a beta-endorphin concentration higher in the *minutes* before surgery compared to *hours* before surgery, just by chance, if there really was no difference.

What we found in the data was that this occurred 12 times for 19 patients.
The simulation shows that this is not at all unusual, even when there is no difference between the two time points.
In fact, 12 people---or even more than 12 people---having a higher concentration minutes before is 

```{r}
nums <- as.character(12:19) # 12 to 19, as characters
SummaryTable[nums]
sum(SummaryTable[nums], na.rm = TRUE)
```

```{ echo=FALSE}
PC.hi <- sum(SummaryTable[nums], na.rm=TRUE)

nums.lo <- as.character(1:7) # 1:7, as characters
PC.lo <- sum(SummaryTable[nums.lo], na.rm = TRUE)
```


That is, the chance of seeing 12 patients or more with a higher reading *minutes* before is about `r PC.lo`%. 
In addition, the chance of seeing 12 patients or more with a lower reading *minutes* before is about `r PC.hi`%.
So the chance that we would see something as extreme as we did---or even more extreme---is about `r PC.hi + PC.lo`%, just by chance alone.

This is hardly convincing evidence of a difference in beta-endorphin concentrations in the minutes and the hours before surgery.


:::


## Answers for Chap. 2



## Answers for Chap. 3



:::{.answer}
Answer to Exercise \@ref(exr:C3DiscreteA).

1. $\alpha = 1/2$.
1. $\text{E}(M) = 13/6$, $\text{var}(N) \approx 0.8056$.
1. $M_M(t) = \exp(t)/3 + \exp(2t)/6 + \exp(3t)/2$.
:::


:::{.answer}
Answer to Exercise \@ref(exr:C3DiscreteB).

1. $M_Z'(t) = 0.6\exp(t)[0.3\exp(t) + 0.7]$ so $\text{E}(Z) = 0.6$.
   Also, $M''_Z(t) = 0.18\exp(2t) + 0.6\exp(t)[0.3\exp(t) + 0.7]$ so $\text{E}(Z^2) = 0.78$, hence so $\text{var}(Z) = 0.42$ (be careful with the derivatives here!); expand the quadratic and find $\Pr(Z = 0) = 0.49$, $\Pr(Z = 1) = 0.42$,
$\Pr(Z = 2) = 0.09$.
:::


:::{.answer}
Answer to Exercise \@ref(exr:C3MGFA).

1. $M'_G(t) = \alpha\beta(1 - \beta t)^{-\alpha - 1}$ so $\text{E}(G) = \alpha\beta$.
   $M''_G(t) = \alpha\beta^2(\alpha + 1)(1 - \beta t)^{-\alpha - 2}$ so $\text{E}(G^2) = \alpha\beta^2(\alpha + 1)$ and $\text{var}(G) = \alpha\beta^2$.
:::


:::{.answer}
Answer to Exercise \@ref(exr:C3Moments).

1. $\mu'_r = \text{E}(X^r) = \int_{x = 0}^1 x^r 2(1 - x)\, dx = 2\left[ \left(\frac{x^{r + 1}}{r + 1} - \frac{x^{r + 2}}{r + 2}\right)\Big|_{0}^{1}\right] = 2\left[ \frac{1}{r + 1} - \frac{1}{r + 2}\right]$.
1. Expanding, $\text{E}((X + 3)^2) = \text{E}(X^2) + 6\text{E}(X) + 9$.
   Now, $\text{E}(X) = \mu'_1 = 1/3$ from above, and $\text{E}(X^2) = \mu'_2 = 1/6$ from above.
   Hence $\text{E}((X + 3)^2) = 67/6$.
1. $\text{var}(X) = \text{E}(X^2) - \text{E}(X)^2 = 1/18$.
:::






:::{.answer}
Answer to Exercise \@ref(exr:C3Cauchy).

1. See Fig. \@ref(fig:Cauchy) (left panel).
1. Evaluate $F_X(t) = \displaystyle \int_{-\infty}^t \frac{1}{\pi(1 + x^2)}\,dx = \frac{1}{\pi}\arctan(x) + \frac{1}{2}$.
   See Fig. \@ref(fig:Cauchy) (right panel).
1. $\displaystyle \text{E}(X) =  \int_{-\infty}^t \frac{x}{\pi(1 + x^2)}\,dx = ...$, which is not defined.
:::

```{r Cauchy, fig.align="center", fig.cap="The Cauchy distribution"}
par( mfrow = c(1, 2))
x <- seq(-5, 5,
         length = 100)
y <- 1 / (pi * (1 + x^2))
plot(y ~ x,
     lwd = 2,
     las = 1,
     type = "l",
     main = "Density function for\nthe Cauchy distribution",
     xlab = "X",
     ylab = "Density function")

Y <- atan(x)/pi + 0.5
plot(Y ~ x,
     lwd = 2,
     las = 1,
     ylim = c(0, 1),
     type = "l",
     main = "Distribution function for\nthe Cauchy distribution",
     xlab = "X",
     ylab = "Distribution function")
```



:::{.answer}
Answer to Exercise \@ref(exr:C3MGFSymmetric).

Begin with Definition \@ref(def:MGF) for $M_X(t)$ and use fact that if a distribution is symmetric about 0 then $f_X(x) = f_X(-x)$ using symmetry. 
Transform the resulting integral.
:::






## Answers for Chap. 4



## Answers for Chap. 5



## Answers for Chap. 6



## Answers for Chap. 7



## Answers for Chap. 8

