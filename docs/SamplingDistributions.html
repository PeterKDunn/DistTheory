<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>12 Describing samples | The Theory of Statistical Distributions</title>
<meta name="author" content="Peter K. Dunn">
<meta name="description" content="use appropriate techniques to determine the sampling distributions of \(t\), \(F\), and \(\chi^2\) distributions. explain how the above distributions are related to the normal distribution. apply...">
<meta name="generator" content="bookdown 0.45 with bs4_book()">
<meta property="og:title" content="12 Describing samples | The Theory of Statistical Distributions">
<meta property="og:type" content="book">
<meta property="og:description" content="use appropriate techniques to determine the sampling distributions of \(t\), \(F\), and \(\chi^2\) distributions. explain how the above distributions are related to the normal distribution. apply...">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="12 Describing samples | The Theory of Statistical Distributions">
<meta name="twitter:description" content="use appropriate techniques to determine the sampling distributions of \(t\), \(F\), and \(\chi^2\) distributions. explain how the above distributions are related to the normal distribution. apply...">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.6.0/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.6.0/bootstrap.bundle.min.js"></script><script src="libs/bs3compat-0.9.0/transition.js"></script><script src="libs/bs3compat-0.9.0/tabs.js"></script><script src="libs/bs3compat-0.9.0/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><script src="libs/kePrint-0.0.1/kePrint.js"></script><link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet">
<script src="libs/htmlwidgets-1.6.4/htmlwidgets.js"></script><script src="libs/rglWebGL-binding-1.3.31/rglWebGL.js"></script><link href="libs/rglwidgetClass-1.3.31/rgl.css" rel="stylesheet">
<script src="libs/rglwidgetClass-1.3.31/rglClass.min.js"></script><script src="libs/CanvasMatrix4-1.3.31/CanvasMatrix.min.js"></script><link rel="shortcut icon" href="icons/iconmonstr-chart-1-240.png">
<script>
    document.addEventListener('DOMContentLoaded', function() {
      // Find all R code blocks that should be toggleable.
      // Our Lua filter adds the 'r-code-box' class to the code block.
      var codeBlocks = document.querySelectorAll('.r-code-box');

      codeBlocks.forEach(function(codeBlock) {
        // Create the button element
        var button = document.createElement('button');
        button.textContent = 'Show R Code'; // Initial text for the button
        button.className = 'code-toggle-button'; // Assign CSS class

        // Insert the button directly before the code block.
        // The codeBlock's parentNode is the div.figure-with-code container.
        // We insert the button as a sibling of the codeBlock within that container.
        codeBlock.parentNode.insertBefore(button, codeBlock);

        // Hide the code block initially by default.
        codeBlock.style.display = 'none';

        // Add a click event listener to the button
        button.addEventListener('click', function() {
          if (codeBlock.style.display === 'none') {
            codeBlock.style.display = 'block'; // Show the code block
            button.textContent = 'Hide R Code'; // Change button text
          } else {
            codeBlock.style.display = 'none'; // Hide the code block
            button.textContent = 'Show R Code'; // Change button text back
          }
        });
      });
    });
    </script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- CSS --><style type="text/css">
    
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  </style>
<style type="text/css">
    /* Used with Pandoc 2.11+ new --citeproc when CSL is used */
    div.csl-bib-body { }
    div.csl-entry {
      clear: both;
          margin-bottom: 0em;
        }
    .hanging div.csl-entry {
      margin-left:2em;
      text-indent:-2em;
    }
    div.csl-left-margin {
      min-width:2em;
      float:left;
    }
    div.csl-right-inline {
      margin-left:2em;
      padding-left:1em;
    }
    div.csl-indent {
      margin-left: 2em;
    }
  </style>
<link rel="stylesheet" href="bs4_style.css">
<link rel="stylesheet" href="css/style.css">
<link rel="stylesheet" href="css/columns.css">
<link rel="stylesheet" href="html/largerDie.css">
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="">The Theory of Statistical Distributions</a>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
</form>

      <nav aria-label="Table of contents"><h2>Table of contents</h2>
        <ul class="book-toc list-unstyled">
<li><a class="" href="index.html">Preface</a></li>
<li class="book-part">Theoretical foundations</li>
<li><a class="" href="ChapterSetTheory.html"><span class="header-section-number">1</span> Essentials of set theory</a></li>
<li><a class="" href="ChapterProbability.html"><span class="header-section-number">2</span> Probability</a></li>
<li><a class="" href="DistributionRandomVariables.html"><span class="header-section-number">3</span> Random variables and their distributions</a></li>
<li><a class="" href="ChapBivariate.html"><span class="header-section-number">4</span> Bivariate distributions</a></li>
<li><a class="" href="ChapExpectation.html"><span class="header-section-number">5</span> Mathematical expectation</a></li>
<li><a class="" href="ChapterTransformations.html"><span class="header-section-number">6</span> Transformations of random variables</a></li>
<li class="book-part">Standard univariate probability distributions</li>
<li><a class="" href="DiscreteDistributions.html"><span class="header-section-number">7</span> Standard discrete distributions</a></li>
<li><a class="" href="ContinuousDistributions.html"><span class="header-section-number">8</span> Standard continuous distributions</a></li>
<li><a class="" href="ChapterMixedDistributions.html"><span class="header-section-number">9</span> Mixed distributions</a></li>
<li class="book-part">Multivariate random variables and distributions*</li>
<li><a class="" href="ChapMultivariate.html"><span class="header-section-number">10</span> Multivariate distributions*</a></li>
<li><a class="" href="MultivariateExtensions.html"><span class="header-section-number">11</span> Expectations for multivariate distributions*</a></li>
<li class="book-part">Sampling distributions</li>
<li><a class="active" href="SamplingDistributions.html"><span class="header-section-number">12</span> Describing samples</a></li>
<li><a class="" href="OrderStatisticsChapter.html"><span class="header-section-number">13</span> Order statistcs</a></li>
<li><a class="" href="BayesianIntro.html"><span class="header-section-number">14</span> Introduction to Bayesian statistics</a></li>
<li class="book-part">Appendices</li>
<li><a class="" href="SymbolsUsed.html"><span class="header-section-number">A</span> Symbols used</a></li>
<li><a class="" href="UsefulSeries.html"><span class="header-section-number">B</span> Some useful series</a></li>
<li><a class="" href="ShortRIntro.html"><span class="header-section-number">C</span> Short R introduction</a></li>
<li><a class="" href="UseRDistributions.html"><span class="header-section-number">D</span> Using R with distributions</a></li>
<li><a class="" href="selected-solutions.html"><span class="header-section-number">E</span> Selected solutions</a></li>
<li><a class="" href="references.html"><span class="header-section-number">F</span> References</a></li>
</ul>

        <div class="book-extra">
          <p><a id="book-repo" href="https://github.com/PeterKDunn/DistTheory">View book source <i class="fab fa-github"></i></a></p>
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="SamplingDistributions" class="section level1" number="12">
<h1>
<span class="header-section-number">12</span> Describing samples<a class="anchor" aria-label="anchor" href="#SamplingDistributions"><i class="fas fa-link"></i></a>
</h1>
<div class="objectivesBox objectives">
<ul>
<li>use appropriate techniques to determine the sampling distributions of <span class="math inline">\(t\)</span>, <span class="math inline">\(F\)</span>, and <span class="math inline">\(\chi^2\)</span> distributions.</li>
<li>explain how the above distributions are related to the normal distribution.</li>
<li>apply the concepts of the Central Limit Theorem in appropriate circumstances.</li>
<li>use the Central Limit Theorem to approximate binomial probabilities by normal probabilities in appropriate circumstances.</li>
</ul>
</div>
<div id="SamplingIntro" class="section level2" number="12.1">
<h2>
<span class="header-section-number">12.1</span> Introduction<a class="anchor" aria-label="anchor" href="#SamplingIntro"><i class="fas fa-link"></i></a>
</h2>
<p>This chapter introduces the application of distribution theory to the discipline of statistics.
Recall (Sect. <a href="ChapterSetTheory.html#Introduction">1.1</a>): distribution theory is about describing random variables using probability, and statistics is about data collection and extracting information from data.</p>
</div>
<div id="from-theoretical-distributions-to-practical-observations" class="section level2" number="12.2">
<h2>
<span class="header-section-number">12.2</span> From theoretical distributions to practical observations<a class="anchor" aria-label="anchor" href="#from-theoretical-distributions-to-practical-observations"><i class="fas fa-link"></i></a>
</h2>
<p>Until now, our results have concerned theoretical probability distributions (i.e., distribution <em>theory</em>) that describe ideal distributions of infinite populations.
In practice, of course, we do not have infinite populations, and we observe finite (usually relatively small) samples from unknown distributions.</p>
<div class="example">
<p><span id="exm:Heights" class="example"><strong>Example 12.1  </strong></span>Suppose our <em>population</em> of interest is only female residents currently living in a local nursing home.
(Usually, populations are much broader than this.)
While the heights <span class="math inline">\(X\)</span> of such residents may be described theoretically as, for example, <span class="math inline">\(X\sim N(170, 3^2)\)</span>, the actual heights of each of the <span class="math inline">\(N = 86\)</span> female residents is one value from this theoretical distribution, and then rounded (for convenience) to the nearest centimetre.</p>
<p>For example, we may find the heights are: <span class="math inline">\(160\,\text{cm}\)</span>; <span class="math inline">\(173\,\text{cm}\)</span>; <span class="math inline">\(169\,\text{cm}\)</span>; <span class="math inline">\(\ldots\)</span>; <span class="math inline">\(171\,\text{cm}\)</span>.</p>
</div>
<p>So while heights are (in theory) continuous, we <em>observe</em> rounded measurements, which must be discrete values.
Using these values then, the <em>expected value</em> (or population mean) height can be found as
<span class="math display">\[
  \operatorname{E}[X] = \mu = \sum_{i = 1}^{86} p_X(x_i) \times x_i
\]</span>
using the formula for a discrete random variable (using Def. <a href="ChapExpectation.html#def:Expectation">5.1</a>).
Since the population contains <span class="math inline">\(N = 86\)</span> equally-important observations, the expected value is
<span class="math display">\[
  \operatorname{E}[X] = \mu = \sum_{i = 1}^{86} \frac{1}{86} \times x_i.
\]</span>
Using this same idea more generally, the <em>population mean</em> can be computed as
<span class="math display">\[
  \mu = \frac{1}{N} \sum_{i = 1}^N x_i.
\]</span>
Similarly, the variance of the population can be found as (Def. <a href="ChapExpectation.html#def:Variance">5.3</a>):
<span class="math display">\[
  \sigma^2 = \frac{1}{N} \sum_{i = 1}^N (x_i - \mu)^2.
\]</span></p>
<div id="estimating-the-population-mean" class="section level3" number="12.2.1">
<h3>
<span class="header-section-number">12.2.1</span> Estimating the population mean<a class="anchor" aria-label="anchor" href="#estimating-the-population-mean"><i class="fas fa-link"></i></a>
</h3>
<p>
Having all values of a population available is almost never possible, and we instead study a sample of size <span class="math inline">\(n\)</span> from some probably-unknown distribution: <span class="math inline">\(x_1, x_2, \ldots, x_n\)</span>.
Given the formula for computing value of the <em>population</em> mean, a sensible approach for computing the value of the <em>sample</em> would seem to be using
<span class="math display">\[
  \bar{X} = \frac{1}{n}(X_1 + X_2 + \cdots X_n) = \frac{1}{n}\sum_{i=1}^n X_i.
\]</span>
In turns out that this is a ‘good’ estimate of <span class="math inline">\(\mu\)</span>, in the sense that is an <em>unbiased</em> estimate of <span class="math inline">\(\mu\)</span>.</p>
<div class="definition">
<p><span id="def:Unbiased" class="definition"><strong>Definition 12.1  (Unbiased estimator) </strong></span>
For some parameter <span class="math inline">\(\theta\)</span>, the estimate <span class="math inline">\(\hat{\theta}\)</span> is called an <em>unbiased</em> estimator of <span class="math inline">\(\theta\)</span> if
<span class="math display">\[
  \operatorname{E}[\hat{\theta}] = \theta.
\]</span></p>
</div>
<p>This means that an unbiased estimator produces estimates that, <em>on average</em>, have the correct (population) value that they are estimating.
This seems a sensible property for a ‘good’ estimator.
(There are other properties of ‘good’ estimators too, we but do not explore these here.)
When we say that <span class="math inline">\(\bar{X}\)</span> is an unbiased estimator of <span class="math inline">\(\mu\)</span> then, we mean that <span class="math inline">\(\operatorname{E}[\bar{X}] = \mu\)</span>,</p>
<div class="theorem">
<p><span id="thm:SampleMeanUnbiased" class="theorem"><strong>Theorem 12.1  (An unbiased estimator for $\mu$) </strong></span>The sample mean
<span class="math display">\[
  \bar{X} = \frac{1}{n} \sum_{i = 1}^n X_i
\]</span>
is an unbiased estimator of the population mean <span class="math inline">\(\mu\)</span>.</p>
</div>
<div class="proof">
<p><span id="unlabeled-div-70" class="proof"><em>Proof</em>. </span>Using the definition of an unbiased estimator:
<span class="math display">\[\begin{align*}
  \operatorname{E}[\bar{X}]
  &amp;= \operatorname{E}\left[ \frac{1}{n}(X_1 + X_2 + \cdots X_n) \right]\\
  &amp;= \frac{1}{n} \operatorname{E}[ X_1 + X_2 + \cdots X_n ]\\
  &amp;= \frac{1}{n} \left( \operatorname{E}[ X_1 ] + \operatorname{E}[X_2] + \cdots \operatorname{E}[X_n] \right) \\
  &amp;= \frac{1}{n} (n\times \mu) = \mu.
\end{align*}\]</span>
That is, the expected value of <span class="math inline">\(\bar{X}\)</span> is equal to <span class="math inline">\(\mu\)</span>.
The estimator <span class="math inline">\(\bar{X}\)</span> is an <em>unbiased</em> of <span class="math inline">\(\mu\)</span>.</p>
</div>
</div>
<div id="EstimatePopVariance" class="section level3" number="12.2.2">
<h3>
<span class="header-section-number">12.2.2</span> Estimating the population variance<a class="anchor" aria-label="anchor" href="#EstimatePopVariance"><i class="fas fa-link"></i></a>
</h3>
<p>
Similarly, given the formula for the population variance, we might suggest the following estimate of the population variance.</p>
<div class="theorem">
<p><span id="thm:SampleVarianceUnbiased" class="theorem"><strong>Theorem 12.2  (An unbiased estimator for $\sigma^2$) </strong></span>The sample variance
<span class="math display" id="eq:S2WithPopMean">\[\begin{equation}
  S^2_n = \frac{1}{n} \sum_{i=1}^n (X_i - \mu)^2.
   \tag{12.1}
\end{equation}\]</span>
is an unbiased estimator of the population variance <span class="math inline">\(\sigma^2\)</span>.</p>
</div>
<div class="proof">
<p><span id="unlabeled-div-71" class="proof"><em>Proof</em>. </span>Again starting with the definition of an unbiased estimator:
<span class="math display">\[\begin{align*}
    \operatorname{E}[S^2_n]
    &amp;= \operatorname{E}\left[ \frac{1}{n} \sum_{i=1}^n (X_i - \mu)^2\right]\\
    &amp;= \frac{1}{n} \operatorname{E}\left[  (X_1 - \mu)^2 + (X_2 - \mu)^2 + \cdots + (X_n - \mu)^2\right]\\
    &amp;= \frac{1}{n} \left( \operatorname{E}[  (X_1 - \mu)^2] + \operatorname{E}[(X_2 - \mu)^2] + \cdots + \operatorname{E}[X_n - \mu]^2\right)\\
    &amp;= \frac{1}{n} \left( \sigma^2 + \sigma^2 + \cdots + \sigma^2 \right)\\
    &amp;= \frac{1}{n} \times n\sigma^2 = \sigma^2.
\end{align*}\]</span>
Thus, <span class="math inline">\(S^2_n\)</span> is an unbiased estimator of <span class="math inline">\(\sigma^2\)</span>.</p>
</div>
<p>Equation <a href="SamplingDistributions.html#eq:S2WithPopMean">(12.1)</a> has a practical difficulty however; this equation is used to estimate the <em>unknown</em> value of the population variance, but relies on knowing the value of the population mean <span class="math inline">\(\mu\)</span>.
Knowing the value of <span class="math inline">\(\mu\)</span> but not the value of <span class="math inline">\(\sigma^2\)</span> seems unlikely.
Almost always, we would only have an estimate of the population mean <span class="math inline">\(\bar{X}\)</span>.</p>
<p>This may suggest using
<span class="math display">\[
  \frac{1}{n} \sum_{i=1}^n (X_i - \bar{X})^2.
\]</span>
to estimate the population variance.
However, <em>this estimate is not unbiased</em> (i.e., it is biased).
Since <span class="math inline">\(\bar{X}\)</span> estimates <span class="math inline">\(\mu\)</span> imprecisely (from just one of the many possible samples), this introduces an extra source of variation into the estimation of <span class="math inline">\(\sigma^2\)</span>.</p>
<p>To see this, start again with the definition of an unbiased estimator:
<span class="math display">\[\begin{align*}
  \operatorname{E}\left[ \frac{1}{n} \sum_{i=1}^n (X_i - \bar{X})^2 \right]
  &amp;= \frac{1}{n}\operatorname{E}\left[ \sum_{i=1}^n (X_i - \bar{X})^2\right]\\
  &amp;= \frac{1}{n}\operatorname{E}\left[ \sum_{i=1}^n \left(X^2_i - 2X_i\bar{X} + \bar{X}^2\right)\right]\\
  &amp;= \frac{1}{n}\operatorname{E}\left[ \sum_{i=1}^n X^2_i - \bar{X}\sum_{i=1}^n 2X_i + \sum_{i=1}^n \bar{X}^2\right].
\end{align*}\]</span>
Now observe that since <span class="math inline">\(\bar{X} = \sum_i X_i/n\)</span>, then <span class="math inline">\(\sum_i X_i = n \bar{X}\)</span>; hence
<span class="math display">\[\begin{align*}
  \operatorname{E}\left[ \frac{1}{n} \sum_{i=1}^n (X_i - \bar{X})^2 \right]
  &amp;= \frac{1}{n}\operatorname{E}\left[ \sum_{i=1}^n X^2_i - \bar{X}^2 + n \bar{X}^2\right]\\
  &amp;= \frac{1}{n}\operatorname{E}\left[ \sum_{i=1}^n X^2_i - n \bar{X}^2 \right]\\
  &amp;= \frac{1}{n}\left(\operatorname{E}\left[ \sum_{i=1}^n X^2_i\right] - n\operatorname{E}\left[ \bar{X}^2 \right]\right).
\end{align*}\]</span>
This expression can be simplified by noting that
<span class="math display">\[
  \operatorname{var}[\bar{X}]
  = \operatorname{E}[\bar{X}^2] - \operatorname{E}[\bar{X}]^2
  = \operatorname{E}[\bar{X}^2] - \mu^2,
\]</span>
and so <span class="math inline">\(\operatorname{E}[\bar{X}^2] = \operatorname{var}{\bar{X}} + \mu^2 = \sigma^2/n + \mu^2\)</span>.
Also,
<span class="math display">\[
  \operatorname{var}[X]
  = \operatorname{E}[X^2] - \operatorname{E}[X]^2
  = \operatorname{E}[X^2] - \mu^2,
\]</span>
so that <span class="math inline">\(\operatorname{E}[X^2] = \sigma + \mu^2\)</span>.
Hence,
<span class="math display">\[\begin{align*}
  \operatorname{E}\left[ \frac{1}{n} \sum_{i=1}^n (X_i - \bar{X})^2 \right]
  &amp;= \frac{1}{n}\left(\operatorname{E}\left[ \sum_{i=1}^n X^2_i\right] - n\operatorname{E}\left[\sum_{i=1}^n \bar{X}^2 \right]\right)\\
  &amp;= \frac{1}{n}\left\{ \sum(\sigma^2 + \mu^2) - n\left(\frac{\sigma^2}{n} + \mu^2\right)\right\}\\
  &amp;= \frac{1}{n}\left\{ n\sigma^2 + n\mu^2 - \sigma^2 - n\mu^2\right\}\\
  &amp;= \frac{1}{n}\left\{ n\sigma^2  - \sigma^2\right\}\\
  &amp;= \frac{n - 1}{n} \sigma^2.
\end{align*}\]</span>
Thus the estimator is a <em>biased estimator</em> of <span class="math inline">\(\sigma^2\)</span>.
However, multiplying the estimator by <span class="math inline">\((n - 1)/n\)</span> <em>would</em> produce an unbiased estimator.</p>
<div class="theorem">
<p><span id="thm:SampleVarianceUnbiasedXbar" class="theorem"><strong>Theorem 12.3  (An unbiased estimator for $\sigma^2$ ($\mu$ unknown)) </strong></span>When the value of <span class="math inline">\(\mu\)</span> is unknown, an <em>unbiased</em> estimator of <span class="math inline">\(\sigma^2\)</span> is
<span class="math display" id="eq:S2WithoutPopMean">\[\begin{equation}
  S^2_{n - 1} = \frac{1}{n - 1} \sum_{i=1}^n (X_i - \bar{X})^2.
   \tag{12.2}
\end{equation}\]</span>
Notice that the divisor in front of the summation is now <span class="math inline">\(n - 1\)</span> rather than <span class="math inline">\(n\)</span>.</p>
</div>
<div class="proof">
<p><span id="unlabeled-div-72" class="proof"><em>Proof</em>. </span>Easily shown using the result above.</p>
</div>
</div>
</div>
<div id="a-example-using-a-small-population" class="section level2" number="12.3">
<h2>
<span class="header-section-number">12.3</span> A example using a small population<a class="anchor" aria-label="anchor" href="#a-example-using-a-small-population"><i class="fas fa-link"></i></a>
</h2>
<div id="some-results-for-samples" class="section level3" number="12.3.1">
<h3>
<span class="header-section-number">12.3.1</span> Some results for samples<a class="anchor" aria-label="anchor" href="#some-results-for-samples"><i class="fas fa-link"></i></a>
</h3>
<p>
Suppose a <em>simple random sample</em> of size <span class="math inline">\(n\)</span> is taken from a population.
The main interest is in the <em>mean</em> of the sample to estimate the unknown mean of the population <span class="math inline">\(\mu\)</span>.
Typically, a population will be <em>very</em> large, and often it will not even be possible to list all elements of the population.
However, for demonstration purposes, suppose the population contains only <span class="math inline">\(N = 5\)</span> elements:
<span class="math display">\[
   2\qquad 3\qquad 3\qquad 4\qquad 8.
\]</span>
From above, the mean of the population is
<span class="math display">\[
  \mu = \frac{2 + 3 + 3 + 4 + 8}{5} = 4.
\]</span>
Using <strong>R</strong>:</p>
<div class="sourceCode" id="cb87"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">Population</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">2</span>, <span class="fl">3</span>, <span class="fl">3</span>, <span class="fl">4</span>, <span class="fl">8</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">Population</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] 4</span></span></code></pre></div>
<p>Similarly the variance of the population is
<span class="math display">\[
  \sigma^2 = \frac{1}{N} \sum_{i = 1}^n  (X_i - \mu)^2 = 4.4:
\]</span></p>
<div class="sourceCode" id="cb88"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">N</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/length.html">length</a></span><span class="op">(</span><span class="va">Population</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span> <span class="op">(</span><span class="va">Population</span> <span class="op">-</span> <span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">Population</span><span class="op">)</span><span class="op">)</span><span class="op">^</span><span class="fl">2</span> <span class="op">)</span> <span class="op">/</span> <span class="va">N</span></span>
<span><span class="co">#&gt; [1] 4.4</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/stats/cor.html">var</a></span><span class="op">(</span><span class="va">Population</span><span class="op">)</span> <span class="op">*</span> <span class="op">(</span><span class="va">N</span> <span class="op">-</span> <span class="fl">1</span><span class="op">)</span> <span class="op">/</span> <span class="va">N</span></span>
<span><span class="co">#&gt; [1] 4.4</span></span></code></pre></div>
<p>The cumbersome second formula in <strong>R</strong> for the <em>population</em> variance is necessary because the <strong>R</strong> function <code><a href="https://rdrr.io/r/stats/cor.html">var()</a></code> computes the variance of a <em>sample</em>, rather than the variance of a <em>population</em> as we have here.
The factor <span class="math inline">\((n - 1)/n\)</span> comes from the difference between the formulas (Sect. <a href="SamplingDistributions.html#EstimatePopVariance">12.2.2</a>).</p>
</div>
<div id="distribution-of-the-sample-mean-n-2-without-replacement" class="section level3" number="12.3.2">
<h3>
<span class="header-section-number">12.3.2</span> Distribution of the sample mean: <span class="math inline">\(n = 2\)</span> (<em>without</em> replacement)<a class="anchor" aria-label="anchor" href="#distribution-of-the-sample-mean-n-2-without-replacement"><i class="fas fa-link"></i></a>
</h3>
<p>
Assume we do not know the values of <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma^2\)</span>, which is usually the case.
Then, to estimate the value of <span class="math inline">\(\mu\)</span>, we take a sample of size <span class="math inline">\(n = 2\)</span> from this population, <em>without</em> replacement.
Because samples are taken at random, we do not know which elements will be in the sample, so we do not know what the value of <span class="math inline">\(\bar{X}\)</span> will be.</p>
<p><em>Simple random sampling</em> refers to <em>all</em> possible samples of size <span class="math inline">\(n\)</span> being equally likely.
Since there are <span class="math inline">\(\binom{N}{n}\)</span> samples of size <span class="math inline">\(n\)</span> when sampling without replacement from <span class="math inline">\(N\)</span> objects, <span class="math inline">\(\binom{5}{2} = 10\)</span> equally-likely possible samples of size <span class="math inline">\(n = 2\)</span> are possible in this example.
For instance, the sample <span class="math inline">\(\{2, 3\}\)</span> has a probability of <span class="math inline">\(1/10\)</span> of being selected, and the mean of this sample is <span class="math inline">\(2.5\)</span>.</p>
<p>We can use <strong>R</strong> to list <em>all</em> possible samples of size <span class="math inline">\(n = 2\)</span> from the population:</p>
<div class="sourceCode" id="cb89"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co">### All possible combinations of 2 elements:</span></span>
<span><span class="va">AllSamples</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/utils/combn.html">combn</a></span><span class="op">(</span><span class="va">Population</span>,</span>
<span>                    m <span class="op">=</span> <span class="fl">2</span><span class="op">)</span></span>
<span><span class="va">AllSamples</span></span>
<span><span class="co">#&gt;      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9]</span></span>
<span><span class="co">#&gt; [1,]    2    2    2    2    3    3    3    3    3</span></span>
<span><span class="co">#&gt; [2,]    3    3    4    8    3    4    8    4    8</span></span>
<span><span class="co">#&gt;      [,10]</span></span>
<span><span class="co">#&gt; [1,]     4</span></span>
<span><span class="co">#&gt; [2,]     8</span></span>
<span></span>
<span><span class="co">### Mean of each possible sample:</span></span>
<span><span class="va">MeanAllSamples</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/colSums.html">colMeans</a></span><span class="op">(</span><span class="va">AllSamples</span><span class="op">)</span></span>
<span><span class="va">MeanAllSamples</span></span>
<span><span class="co">#&gt;  [1] 2.5 2.5 3.0 5.0 3.0 3.5 5.5 3.5 5.5 6.0</span></span></code></pre></div>
<p>All <span class="math inline">\(10\)</span> possible samples of size <span class="math inline">\(n = 2\)</span> can be listed, with the mean of each sample:</p>
<div class="sourceCode" id="cb90"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/cbind.html">cbind</a></span><span class="op">(</span>                 <span class="fu"><a href="https://rdrr.io/r/base/t.html">t</a></span><span class="op">(</span><span class="va">AllSamples</span><span class="op">)</span>, </span>
<span>       <span class="st">"Sample mean"</span> <span class="op">=</span> <span class="va">MeanAllSamples</span>,</span>
<span>       <span class="st">"Probability"</span> <span class="op">=</span> <span class="fl">1</span><span class="op">/</span><span class="fl">10</span><span class="op">)</span></span>
<span><span class="co">#&gt;           Sample mean Probability</span></span>
<span><span class="co">#&gt;  [1,] 2 3         2.5         0.1</span></span>
<span><span class="co">#&gt;  [2,] 2 3         2.5         0.1</span></span>
<span><span class="co">#&gt;  [3,] 2 4         3.0         0.1</span></span>
<span><span class="co">#&gt;  [4,] 2 8         5.0         0.1</span></span>
<span><span class="co">#&gt;  [5,] 3 3         3.0         0.1</span></span>
<span><span class="co">#&gt;  [6,] 3 4         3.5         0.1</span></span>
<span><span class="co">#&gt;  [7,] 3 8         5.5         0.1</span></span>
<span><span class="co">#&gt;  [8,] 3 4         3.5         0.1</span></span>
<span><span class="co">#&gt;  [9,] 3 8         5.5         0.1</span></span>
<span><span class="co">#&gt; [10,] 4 8         6.0         0.1</span></span></code></pre></div>
<p>The <em>sample</em> mean varies from sample to sample.
The mean of a sample depends on <em>which</em> sample is selected at random.
From this information, the <em>distribution</em> of the sample mean itself can be determined (Fig. <a href="SamplingDistributions.html#fig:SampleMeanDistn">12.1</a>):</p>
<div class="sourceCode" id="cb91"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/proportions.html">prop.table</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/table.html">table</a></span><span class="op">(</span><span class="va">MeanAllSamples</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt; MeanAllSamples</span></span>
<span><span class="co">#&gt; 2.5   3 3.5   5 5.5   6 </span></span>
<span><span class="co">#&gt; 0.2 0.2 0.2 0.1 0.2 0.1</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span> <span class="fu"><a href="https://rdrr.io/r/base/proportions.html">prop.table</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/table.html">table</a></span><span class="op">(</span><span class="va">MeanAllSamples</span><span class="op">)</span><span class="op">)</span>,</span>
<span>      las <span class="op">=</span> <span class="fl">1</span>,</span>
<span>      ylim <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0</span>, <span class="fl">0.22</span><span class="op">)</span>,</span>
<span>      xlab <span class="op">=</span> <span class="st">"Sample mean"</span>,</span>
<span>      ylab <span class="op">=</span> <span class="st">"Probability"</span>,</span>
<span>      main <span class="op">=</span> <span class="st">"The distribution of sample means"</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/points.html">points</a></span><span class="op">(</span> x <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/names.html">names</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/proportions.html">prop.table</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/table.html">table</a></span><span class="op">(</span><span class="va">MeanAllSamples</span><span class="op">)</span><span class="op">)</span><span class="op">)</span>,</span>
<span>        y <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/proportions.html">prop.table</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/table.html">table</a></span><span class="op">(</span><span class="va">MeanAllSamples</span><span class="op">)</span><span class="op">)</span>,</span>
<span>        pch <span class="op">=</span> <span class="fl">19</span><span class="op">)</span></span></code></pre></div>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:SampleMeanDistn"></span>
<img src="12-Sampling_files/figure-html/SampleMeanDistn-1.png" alt="The distribution of the sample means." width="672"><p class="caption">
FIGURE 12.1: The distribution of the sample means.
</p>
</div>
<p>This distribution is known as the <em>sampling distribution of the sample mean</em>, recognising that the distribution is that of the sample mean.</p>
<div class="importantBox important">
<p>The sample mean has a distribution: the value of the sample mean varies, depending on which sample is obtained.</p>
</div>
<p>The mean <em>of the sampling distribution</em> in Fig.  <a href="SamplingDistributions.html#fig:SampleMeanDistn">12.1</a> is <span class="math inline">\(\mu_{\bar{X}} = 4\)</span>:</p>
<div class="sourceCode" id="cb92"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">MeanAllSamples</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] 4</span></span></code></pre></div>
<p>The variance <em>of the sampling distribution</em> in Fig.  <a href="SamplingDistributions.html#fig:SampleMeanDistn">12.1</a> is <span class="math inline">\(\sigma^2_{\bar{X}} = 1.65\)</span>:</p>
<div class="sourceCode" id="cb93"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">n</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/length.html">length</a></span><span class="op">(</span><span class="va">MeanAllSamples</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/stats/cor.html">var</a></span><span class="op">(</span><span class="va">MeanAllSamples</span><span class="op">)</span> <span class="op">*</span> <span class="op">(</span><span class="va">n</span> <span class="op">-</span> <span class="fl">1</span><span class="op">)</span> <span class="op">/</span> <span class="va">n</span></span>
<span><span class="co">#&gt; [1] 1.65</span></span></code></pre></div>
<p>Note that since we have the mean of <em>all</em> possible samples, we have the ‘population’ of sample means.</p>
</div>
<div id="distribution-of-the-sample-mean-larger-samples-without-replacement" class="section level3" number="12.3.3">
<h3>
<span class="header-section-number">12.3.3</span> Distribution of the sample mean: larger samples (<em>without</em> replacement)<a class="anchor" aria-label="anchor" href="#distribution-of-the-sample-mean-larger-samples-without-replacement"><i class="fas fa-link"></i></a>
</h3>
<p>
We can increase the sample size to <span class="math inline">\(n = 3\)</span>, and repeat:</p>
<div class="sourceCode" id="cb94"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co">### All possible combinations of 3 elements:</span></span>
<span><span class="va">AllSamples</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/utils/combn.html">combn</a></span><span class="op">(</span><span class="va">Population</span>,</span>
<span>                    m <span class="op">=</span> <span class="fl">3</span><span class="op">)</span></span>
<span></span>
<span><span class="co">### Mean of each possible sample:</span></span>
<span><span class="va">MeanAllSamples</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/colSums.html">colMeans</a></span><span class="op">(</span><span class="va">AllSamples</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">MeanAllSamples</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] 4</span></span>
<span><span class="va">n</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/length.html">length</a></span><span class="op">(</span><span class="va">MeanAllSamples</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/stats/cor.html">var</a></span><span class="op">(</span><span class="va">MeanAllSamples</span><span class="op">)</span> <span class="op">*</span> <span class="op">(</span><span class="va">n</span> <span class="op">-</span> <span class="fl">1</span><span class="op">)</span> <span class="op">/</span> <span class="va">n</span></span>
<span><span class="co">#&gt; [1] 0.7333333</span></span></code></pre></div>
<p>The mean of all possible samples is the same (<span class="math inline">\(\mu_{\bar{X}} = 4\)</span>), but the variance is smaller (<span class="math inline">\(\sigma^2_{\bar{X}} = 0.7333\)</span> rather than <span class="math inline">\(1.65\)</span>).
For <span class="math inline">\(n = 4\)</span>, the mean of all possible samples remains the same, but the variance is smaller again:</p>
<div class="sourceCode" id="cb95"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co">### All possible combinations of 3 elements:</span></span>
<span><span class="va">AllSamples</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/utils/combn.html">combn</a></span><span class="op">(</span><span class="va">Population</span>,</span>
<span>                    m <span class="op">=</span> <span class="fl">4</span><span class="op">)</span></span>
<span></span>
<span><span class="co">### Mean of each possible sample:</span></span>
<span><span class="va">MeanAllSamples</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/colSums.html">colMeans</a></span><span class="op">(</span><span class="va">AllSamples</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">MeanAllSamples</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] 4</span></span>
<span><span class="va">n</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/length.html">length</a></span><span class="op">(</span><span class="va">MeanAllSamples</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/stats/cor.html">var</a></span><span class="op">(</span><span class="va">MeanAllSamples</span><span class="op">)</span> <span class="op">*</span> <span class="op">(</span><span class="va">n</span> <span class="op">-</span> <span class="fl">1</span><span class="op">)</span> <span class="op">/</span> <span class="va">n</span></span>
<span><span class="co">#&gt; [1] 0.275</span></span></code></pre></div>
</div>
<div id="SamplingWithReplacement" class="section level3" number="12.3.4">
<h3>
<span class="header-section-number">12.3.4</span> Distribution of the sample mean: <span class="math inline">\(n = 2\)</span> (<em>with</em> replacement)<a class="anchor" aria-label="anchor" href="#SamplingWithReplacement"><i class="fas fa-link"></i></a>
</h3>
<p>
All the above examples use sampling <em>without replacement</em>.
However, <em>sampling randomly with replacement</em> could be used too.
Then, there are <span class="math inline">\(5\times 5 = 25\)</span> equally-likely samples possible:</p>
<div class="sourceCode" id="cb96"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">AllSamplesWithReplacement</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/expand.grid.html">expand.grid</a></span><span class="op">(</span><span class="va">Population</span>, <span class="va">Population</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/names.html">names</a></span><span class="op">(</span><span class="va">AllSamplesWithReplacement</span><span class="op">)</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="cn">NA</span>, </span>
<span>                                      <span class="cn">NA</span><span class="op">)</span></span>
<span><span class="va">MeanAllSamplesWithReplacement</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/colSums.html">rowMeans</a></span><span class="op">(</span><span class="va">AllSamplesWithReplacement</span><span class="op">)</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/cbind.html">cbind</a></span><span class="op">(</span> <span class="va">AllSamplesWithReplacement</span><span class="op">[</span>, <span class="fl">1</span><span class="op">]</span>,</span>
<span>       <span class="va">AllSamplesWithReplacement</span><span class="op">[</span>, <span class="fl">2</span><span class="op">]</span>,</span>
<span>       <span class="st">"Sample mean"</span> <span class="op">=</span> <span class="va">MeanAllSamplesWithReplacement</span>, </span>
<span>       <span class="st">"Probability"</span> <span class="op">=</span> <span class="fl">1</span><span class="op">/</span><span class="fl">25</span><span class="op">)</span></span>
<span><span class="co">#&gt;           Sample mean Probability</span></span>
<span><span class="co">#&gt;  [1,] 2 2         2.0        0.04</span></span>
<span><span class="co">#&gt;  [2,] 3 2         2.5        0.04</span></span>
<span><span class="co">#&gt;  [3,] 3 2         2.5        0.04</span></span>
<span><span class="co">#&gt;  [4,] 4 2         3.0        0.04</span></span>
<span><span class="co">#&gt;  [5,] 8 2         5.0        0.04</span></span>
<span><span class="co">#&gt;  [6,] 2 3         2.5        0.04</span></span>
<span><span class="co">#&gt;  [7,] 3 3         3.0        0.04</span></span>
<span><span class="co">#&gt;  [8,] 3 3         3.0        0.04</span></span>
<span><span class="co">#&gt;  [9,] 4 3         3.5        0.04</span></span>
<span><span class="co">#&gt; [10,] 8 3         5.5        0.04</span></span>
<span><span class="co">#&gt; [11,] 2 3         2.5        0.04</span></span>
<span><span class="co">#&gt; [12,] 3 3         3.0        0.04</span></span>
<span><span class="co">#&gt; [13,] 3 3         3.0        0.04</span></span>
<span><span class="co">#&gt; [14,] 4 3         3.5        0.04</span></span>
<span><span class="co">#&gt; [15,] 8 3         5.5        0.04</span></span>
<span><span class="co">#&gt; [16,] 2 4         3.0        0.04</span></span>
<span><span class="co">#&gt; [17,] 3 4         3.5        0.04</span></span>
<span><span class="co">#&gt; [18,] 3 4         3.5        0.04</span></span>
<span><span class="co">#&gt; [19,] 4 4         4.0        0.04</span></span>
<span><span class="co">#&gt; [20,] 8 4         6.0        0.04</span></span>
<span><span class="co">#&gt; [21,] 2 8         5.0        0.04</span></span>
<span><span class="co">#&gt; [22,] 3 8         5.5        0.04</span></span>
<span><span class="co">#&gt; [23,] 3 8         5.5        0.04</span></span>
<span><span class="co">#&gt; [24,] 4 8         6.0        0.04</span></span>
<span><span class="co">#&gt; [25,] 8 8         8.0        0.04</span></span></code></pre></div>
<p>From this table, the <em>distribution</em> of the sample mean itself can again be determined (Fig. <a href="SamplingDistributions.html#fig:SampleMeanDistnWithReplacement">12.2</a>):</p>
<div class="sourceCode" id="cb97"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/proportions.html">prop.table</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/table.html">table</a></span><span class="op">(</span><span class="va">MeanAllSamplesWithReplacement</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt; MeanAllSamplesWithReplacement</span></span>
<span><span class="co">#&gt;    2  2.5    3  3.5    4    5  5.5    6    8 </span></span>
<span><span class="co">#&gt; 0.04 0.16 0.24 0.16 0.04 0.08 0.16 0.08 0.04</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span> <span class="fu"><a href="https://rdrr.io/r/base/proportions.html">prop.table</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/table.html">table</a></span><span class="op">(</span><span class="va">MeanAllSamplesWithReplacement</span><span class="op">)</span><span class="op">)</span>,</span>
<span>      las <span class="op">=</span> <span class="fl">1</span>,</span>
<span>      ylim <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0</span>, <span class="fl">0.25</span><span class="op">)</span>,</span>
<span>      xlab <span class="op">=</span> <span class="st">"Sample mean"</span>,</span>
<span>      ylab <span class="op">=</span> <span class="st">"Probability"</span>,</span>
<span>      main <span class="op">=</span> <span class="st">"The distribution of sample means\n(sampling with replacement)"</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/points.html">points</a></span><span class="op">(</span> x <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/names.html">names</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/proportions.html">prop.table</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/table.html">table</a></span><span class="op">(</span><span class="va">MeanAllSamplesWithReplacement</span><span class="op">)</span><span class="op">)</span><span class="op">)</span>,</span>
<span>        y <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/proportions.html">prop.table</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/table.html">table</a></span><span class="op">(</span><span class="va">MeanAllSamplesWithReplacement</span><span class="op">)</span><span class="op">)</span>,</span>
<span>        pch <span class="op">=</span> <span class="fl">19</span><span class="op">)</span></span></code></pre></div>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:SampleMeanDistnWithReplacement"></span>
<img src="12-Sampling_files/figure-html/SampleMeanDistnWithReplacement-1.png" alt="The distribution of the sample means." width="60%"><p class="caption">
FIGURE 12.2: The distribution of the sample means.
</p>
</div>
<p>The mean and variance of the sampling distribution in this case are <span class="math inline">\(\mu_{\bar{X}} = 4\)</span> and <span class="math inline">\(\sigma^2_{\bar{X}} = 2.2\)</span> respectively:</p>
<div class="sourceCode" id="cb98"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">MeanAllSamplesWithReplacement</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] 4</span></span>
<span><span class="va">n</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/length.html">length</a></span><span class="op">(</span><span class="va">MeanAllSamplesWithReplacement</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/stats/cor.html">var</a></span><span class="op">(</span><span class="va">MeanAllSamplesWithReplacement</span><span class="op">)</span> <span class="op">*</span> <span class="op">(</span><span class="va">n</span> <span class="op">-</span> <span class="fl">1</span><span class="op">)</span> <span class="op">/</span> <span class="va">n</span></span>
<span><span class="co">#&gt; [1] 2.2</span></span></code></pre></div>
</div>
<div id="the-sampling-distribution-of-other-statistics" class="section level3" number="12.3.5">
<h3>
<span class="header-section-number">12.3.5</span> The sampling distribution of other statistics<a class="anchor" aria-label="anchor" href="#the-sampling-distribution-of-other-statistics"><i class="fas fa-link"></i></a>
</h3>
<p>In principle, the distribution of <em>any</em> statistic could be found using the approach in the example above; that is, the median, variance, range, etc. all have a sampling distribution.
For example, we can display the distribution of the sample <em>range</em>:</p>
<div class="sourceCode" id="cb99"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co">## Define a function to compute the range</span></span>
<span><span class="va">myRange</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">x</span><span class="op">)</span><span class="op">{</span> <span class="fu"><a href="https://rdrr.io/r/base/Extremes.html">max</a></span><span class="op">(</span><span class="va">x</span><span class="op">)</span> <span class="op">-</span> <span class="fu"><a href="https://rdrr.io/r/base/Extremes.html">min</a></span><span class="op">(</span><span class="va">x</span><span class="op">)</span> <span class="op">}</span></span>
<span></span>
<span><span class="co">## The range of the population:</span></span>
<span><span class="fu">myRange</span><span class="op">(</span><span class="va">Population</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] 6</span></span>
<span></span>
<span><span class="co">## Sampling:</span></span>
<span><span class="va">AllSamplesWithReplacement</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/expand.grid.html">expand.grid</a></span><span class="op">(</span><span class="va">Population</span>, <span class="va">Population</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/names.html">names</a></span><span class="op">(</span><span class="va">AllSamplesWithReplacement</span><span class="op">)</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="cn">NA</span>, </span>
<span>                                      <span class="cn">NA</span><span class="op">)</span></span>
<span><span class="va">RangeAllSamplesWithReplacement</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/apply.html">apply</a></span><span class="op">(</span><span class="va">AllSamplesWithReplacement</span>,</span>
<span>                                        <span class="fl">1</span>,</span>
<span>                                       <span class="st">"myRange"</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/proportions.html">prop.table</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/table.html">table</a></span><span class="op">(</span><span class="va">RangeAllSamplesWithReplacement</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt; RangeAllSamplesWithReplacement</span></span>
<span><span class="co">#&gt;    0    1    2    4    5    6 </span></span>
<span><span class="co">#&gt; 0.28 0.32 0.08 0.08 0.16 0.08</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span> <span class="fu"><a href="https://rdrr.io/r/base/proportions.html">prop.table</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/table.html">table</a></span><span class="op">(</span><span class="va">RangeAllSamplesWithReplacement</span><span class="op">)</span><span class="op">)</span>,</span>
<span>      las <span class="op">=</span> <span class="fl">1</span>,</span>
<span>      ylim <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0</span>, <span class="fl">0.35</span><span class="op">)</span>,</span>
<span>      xlab <span class="op">=</span> <span class="st">"Sample range"</span>,</span>
<span>      ylab <span class="op">=</span> <span class="st">"Probability"</span>,</span>
<span>      main <span class="op">=</span> <span class="st">"The distribution of sample range\n(sampling with replacement)"</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/points.html">points</a></span><span class="op">(</span> x <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/names.html">names</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/proportions.html">prop.table</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/table.html">table</a></span><span class="op">(</span><span class="va">RangeAllSamplesWithReplacement</span><span class="op">)</span><span class="op">)</span><span class="op">)</span>,</span>
<span>        y <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/proportions.html">prop.table</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/table.html">table</a></span><span class="op">(</span><span class="va">RangeAllSamplesWithReplacement</span><span class="op">)</span><span class="op">)</span>,</span>
<span>        pch <span class="op">=</span> <span class="fl">19</span><span class="op">)</span></span></code></pre></div>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:SampleRangeDistnWithReplacement"></span>
<img src="12-Sampling_files/figure-html/SampleRangeDistnWithReplacement-1.png" alt="The distribution of the sample range." width="60%"><p class="caption">
FIGURE 12.3: The distribution of the sample range.
</p>
</div>
</div>
</div>
<div id="SamplingDistributionsIntro" class="section level2" number="12.4">
<h2>
<span class="header-section-number">12.4</span> Sampling distributions<a class="anchor" aria-label="anchor" href="#SamplingDistributionsIntro"><i class="fas fa-link"></i></a>
</h2>
<p>The previous section displays numerous <em>sampling distributions</em>: the distributions of sample statistics.
In practice, populations are almost always very large and the number of possible samples huge, infinite, or the exact elements of the population difficult or impossible to list.
In these cases, listing all possible samples and computing the exact sampling distributions is impossible.</p>
<p>In terms of distribution theory, the probability distributions in Figs. <a href="SamplingDistributions.html#fig:SampleMeanDistn">12.1</a> and <a href="SamplingDistributions.html#fig:SampleMeanDistnWithReplacement">12.2</a> are just the distribution of a random variable, since <em>the sample mean is a random variable</em>.
Statistically, these distribution describe what to expect for the sample means when we randomly sample from a population.</p>
<div class="importantBox important">
<p>Any quantity computed from a sample is a random variable, since its value is unknown beforehand.
The observed value of the quantity depends on <em>which</em> sample is selected (at random).</p>
<p>Consequently, sample means, sample standard deviations, sample medians etc. all are random variables.</p>
</div>
<p>This knowledge is essential for extracting information from samples.
Observe that the mean of the sampling distribution in the example (when taken <em>without replacement</em>) is the same as the mean of the population itself: <span class="math inline">\(\mu = (2 + 3 + 3 + 4 + 8 )/5 = 4\)</span>.
Also, the variance of the sampling distribution is <em>smaller</em> than the variance of the population (<span class="math inline">\(\sigma^2 = 4.4\)</span>).
The exact relationship between <span class="math inline">\(\sigma^2_{\bar{X}}\)</span> and <span class="math inline">\(\sigma^2\)</span> is expanded on in Theorem <a href="SamplingDistributions.html#thm:SamplingDistMean">12.4</a>.</p>
<div class="definition">
<p><span id="def:Statistic" class="definition"><strong>Definition 12.2  (Statistic) </strong></span>A <em>statistic</em> is a real-valued function of the observations in a <em>sample</em>.</p>
</div>
<p>If <span class="math inline">\(X_1, X_2, \dots, X_n\)</span> represents a numerical sample of size <span class="math inline">\(n\)</span> from some population, then <span class="math inline">\(T = g(X_1, X_2, \dots, X_n)\)</span> represents a <em>statistic</em>.
Examples include the sample mean, sample standard deviation, sample variance and sample proportion.
The individual observations in a sample are also statistics.
Hence, for example, the minimum value in a sample (usually denoted <span class="math inline">\(\min\)</span> or <span class="math inline">\(X_{[1]}\)</span>), the maximum value, and the range <span class="math inline">\(\max - \min\)</span> are statistics.
Furthermore, even the <em>first</em> observation in a sample <span class="math inline">\(X_1\)</span> is a statistic.</p>
<p>A random variable is a numeric variable which is fully determined by the values of all, some or even no members of the sample.</p>
<div class="example">
<p><span id="exm:SamplingDistributionStatistic" class="example"><strong>Example 12.2  (Sampling distribution of a statistic) </strong></span>The <em>sampling distribution of a statistic</em> is the theoretical probability distribution of the statistic associated with <em>all</em> possible samples of a particular size drawn from a particular population.</p>
</div>
<p>Notice that this definition is not confined to <em>random</em> samples.
However, in practically all applications the sample involved is assumed to be random in some sense.
Random sampling imposes a probability structure on the possible values of the statistic, allowing the sampling distribution to be defined.</p>
<p>We use <span class="math inline">\(\bar{X}\)</span> to represent the mean of a sample whose value is as yet unknown (and depends on <em>which</em> sample is selected).
The symbol <span class="math inline">\(\overline{x}\)</span> is used to denoted the value of the <em>sample</em> mean, computed from the data found in a particular sample.</p>
<div id="RandomSamples" class="section level3" number="12.4.1">
<h3>
<span class="header-section-number">12.4.1</span> Random sampling<a class="anchor" aria-label="anchor" href="#RandomSamples"><i class="fas fa-link"></i></a>
</h3>
<p>For our purposes, a random sample is defined as follows:</p>
<div class="definition">
<p><span id="def:RandomSample" class="definition"><strong>Definition 12.3  (Random sample) </strong></span>The set of random variables <span class="math inline">\(X_1, X_2,\dots,X_n\)</span> is said to be a <em>random sample</em> of size <span class="math inline">\(n\)</span> from a population with df <span class="math inline">\(F_X(x)\)</span> if the <span class="math inline">\(X_i\)</span> are identically and independently distributed (iid) with df <span class="math inline">\(F_X(x)\)</span>.</p>
</div>
<p>The definition uses the distribution function to accommodate both discrete and continuous random variables.</p>
<p>This definition of a random sample is standard in theoretical statistics.
In statistical practice, however, many sampling designs are called “random”, such as simple random sampling, stratified sampling, and systematic sampling.
These sampling methods produce ‘random samples’, even though the samples produced don’t necessarily satisfy Def. <a href="SamplingDistributions.html#def:RandomSample">12.3</a>.
Usually the context makes it clear whether the strict or loose meaning of ‘random sample’ is intended.</p>
<p>When a sample of size <span class="math inline">\(n\)</span> is assumed to be chosen ‘at random’ <em>without replacement</em> from a population of size <span class="math inline">\(N\)</span>, this sample does <em>not</em> constitute a random sample in the sense of Def. <a href="SamplingDistributions.html#def:RandomSample">12.3</a>.
Even though each member of the sample has the same distribution as the population, the members of the sample are not independent.
For example, consider the second element <span class="math inline">\(X_2\)</span> of the samples in Sect. <a href="SamplingDistributions.html#SamplingIntro">12.1</a>.
Here, <span class="math inline">\(\Pr(X_2 = 2) = 0.2\)</span> but that <span class="math inline">\(\Pr(X_2 = 2 \mid X_1 = 2) = 0\)</span>.
Therefore <span class="math inline">\(\Pr(X_2 = 2 \mid X_1 = 2) \ne\Pr(X_2 = 2)\)</span> and so <span class="math inline">\(X_2\)</span> and <span class="math inline">\(X_1\)</span> are dependent.</p>
<p>However, sampling at random without replacement <em>approximates</em> a random sample <em>if the sample size <span class="math inline">\(n\)</span> is small compared to the population size <span class="math inline">\(N\)</span></em>.
In this case, the impact of removing some members of the population has minimal impact on the distribution of the population remaining, and the observations will be approximately independent.
Populations sizes are commonly <em>much</em> larger than sample sizes (which we write as <span class="math inline">\(N&gt;\!&gt;n\)</span>).</p>
<p>In Sect. <a href="SamplingDistributions.html#SamplingIntro">12.1</a>, if the sampling is performed <em>with replacement</em> then <span class="math inline">\(\Pr(X_2 = 2 \mid X_1 = 2) = \Pr(X_2 = 2) = \Pr(X_1 = 2)\)</span>.
Similar statements can be made for all sample members and all values of the population.
In this case, the sample is a random sample according to Def. <a href="SamplingDistributions.html#def:RandomSample">12.3</a>.</p>
<p>Sampling at random <em>with</em> replacement gives rise to a random sample, and sampling at random <em>without</em> replacement approximates a random sample provided <span class="math inline">\(N &gt;\!&gt; n\)</span>.
Sampling without replacement when the sample size is not much smaller than the population size does not give rise to a random sample.
This situation is often referred to as <em>sampling from a finite population</em>.</p>
<p>Sampling at random <em>with</em> replacement is also known as <em>repeated sampling</em>.
A typical example is a sequence of independent trials (Bernoulli or otherwise) such as occur when a coin or die is tossed repeatedly.
Each trial involves random sampling from a population, which in the case of a coin comprises H and T with equal probability.</p>
<p>Sampling from a continuous distribution is really a theoretical concept in which the population is assumed uncountably infinite.
Consequently, sampling at random in this situation is assumed to produce a random sample.</p>
</div>
<div id="SamplingDistributionMean" class="section level3" number="12.4.2">
<h3>
<span class="header-section-number">12.4.2</span> The sampling distribution of the mean<a class="anchor" aria-label="anchor" href="#SamplingDistributionMean"><i class="fas fa-link"></i></a>
</h3>
<p>Some powerful results exist to describe the sampling distribution of the mean of a random sample.</p>
<div class="theorem">
<p><span id="thm:SamplingDistMean" class="theorem"><strong>Theorem 12.4  (Sampling distribution of the mean) </strong></span>If <span class="math inline">\(X_1, X_2, \dots, X_n\)</span> is a <em>random sample</em> of size <span class="math inline">\(n\)</span> from a population with mean <span class="math inline">\(\mu\)</span> and variance <span class="math inline">\(\sigma^2\)</span>, then the sample mean <span class="math inline">\(\bar{X}\)</span> has a sampling distribution with mean <span class="math inline">\(\mu\)</span> and variance <span class="math inline">\(\sigma^2 / n\)</span>.</p>
</div>
<div class="proof">
<p><span id="unlabeled-div-73" class="proof"><em>Proof</em>. </span>This is a application of Corollary <a href="MultivariateExtensions.html#cor:IID">11.1</a> with <span class="math inline">\(a_i = 1/n\)</span>.</p>
</div>
<p>Theorem <a href="SamplingDistributions.html#thm:SamplingDistMean">12.4</a> applies to a random sample as defined in Sect. <a href="SamplingDistributions.html#RandomSamples">12.4.1</a>.
The remarkable feature of Theorem <a href="SamplingDistributions.html#thm:SamplingDistMean">12.4</a> is that the distribution of <span class="math inline">\(X\)</span> is not important.</p>
<div class="importantBox important">
<p>In Theorem <a href="SamplingDistributions.html#thm:SamplingDistMean">12.4</a>, the distribution of the population from which the sample is drawn is irrelevant.</p>
</div>
<p>Theorem <a href="SamplingDistributions.html#thm:SamplingDistMean">12.4</a> applies to samples randomly selected <em>with replacement</em> from a finite population as described in Sect. <a href="SamplingDistributions.html#SamplingWithReplacement">12.3.4</a>.
In the case where sample are taken <em>without replacement</em>, <span class="math inline">\(\mu_{\bar{X}}\)</span> still equals <span class="math inline">\(\mu\)</span> (as this result does not depend on the observations being independent), but
<span class="math display">\[\begin{equation}
   \operatorname{var}[\bar{X}]
   = \frac{\sigma^2}{n} \times \frac{(N - n)}{(N - 1)}
\end{equation}\]</span>
which is <em>smaller</em> than <span class="math inline">\(\sigma^2/n\)</span> by the factor <span class="math inline">\((N - n)/(N - 1)\)</span>.
This means that the <em>standard deviation</em> of <span class="math inline">\(\bar{X}\)</span> is <em>smaller</em> than <span class="math inline">\(\sigma/\sqrt{n}\)</span> by a factor of <span class="math inline">\(\sqrt{(N - n)/(N - 1)}\)</span>.
(This factor was also seen in Exercise <a href="DiscreteDistributions.html#exr:HypergeometricFPC">7.19</a>.)</p>
<div class="definition">
<p><span id="def:FPC" class="definition"><strong>Definition 12.4  (Finite Population Correction (FPC)) </strong></span>The finite population correction (FPC) factor is
<span class="math display">\[
   \sqrt{ \frac{N - n}{N - 1} }.
\]</span></p>
</div>
<div class="example">
<p><span id="exm:CLTwithoutReplacement" class="example"><strong>Example 12.3  (Sampling without replacement) </strong></span>In Sect. <a href="SamplingDistributions.html#SamplingIntro">12.1</a>, <span class="math inline">\(\mu = 4.4\)</span> and <span class="math inline">\(\sigma^2 = 5.04\)</span> for the <em>population</em>.
The FPC is
<span class="math display">\[
   \text{FPC} = \sqrt{ \frac{5 - 2}{5 - 1} } = 0.8660\dots.
\]</span>
If sampling is done <em>with</em> replacement, then <span class="math inline">\(\mu_{\bar{X}} = \mu = 4.4\)</span> and <span class="math inline">\(\operatorname{var}[\bar{X}] = 2.52 = 5.04/2\)</span>, agreement with the theorem.</p>
<p>If sampling is done <em>without</em> replacement, the sampling distribution of the mean has mean <span class="math inline">\(\mu_{\bar{X}} = \mu = 4.4\)</span> and variance <span class="math inline">\(\operatorname{var}[\bar{X}] = 1.89 = \frac{5.04}{2} \times \text{FPC}^2\)</span>, in agreement with the note following Theorem <a href="SamplingDistributions.html#thm:CLT">12.13</a>.</p>
</div>
<p>Theorem <a href="SamplingDistributions.html#thm:CLT">12.13</a> describes the location and spread of the sampling distribution of the mean, but not the shape of the sampling distribution.
Remarkably, the <em>shape</em> of the sampling distribution only depends on the shape of the population distribution <em>when the sample is small</em> (Sect. <a href="SamplingDistributions.html#CentralLimitTheorem">12.6</a>).</p>
</div>
</div>
<div id="SamplingDistributionsRelatedToNormal" class="section level2" number="12.5">
<h2>
<span class="header-section-number">12.5</span> Sampling distributions related to the normal distribution<a class="anchor" aria-label="anchor" href="#SamplingDistributionsRelatedToNormal"><i class="fas fa-link"></i></a>
</h2>
<div id="NormalGeneralResults" class="section level3" number="12.5.1">
<h3>
<span class="header-section-number">12.5.1</span> General results<a class="anchor" aria-label="anchor" href="#NormalGeneralResults"><i class="fas fa-link"></i></a>
</h3>
<p>The normal distribution is used to model many naturally occurring phenomena (Sect. <a href="ContinuousDistributions.html#Normal">8.3</a>).
Consequently, we begin by studying the sampling distributions of statistics of random samples that come from normal populations.</p>
<p>First, we present a useful result.</p>
<div class="theorem">
<p><span id="thm:LinearComb" class="theorem"><strong>Theorem 12.5  (Linear combinations) </strong></span>Let <span class="math inline">\(X_1, X_2, \dots, X_n\)</span> be a set of independent random variables where <span class="math inline">\(X_i\sim N(\mu_i,\sigma^2_i)\)</span>.
Define the linear combination <span class="math inline">\(Y\)</span> as
<span class="math display">\[
   Y = a_1 X_1 + a_2 X_2 + \dots + a_nX_n.
\]</span>
Then <span class="math inline">\(Y \sim N(\Sigma a_i\mu_i, \Sigma a^2_i\sigma^2_i)\)</span>.</p>
</div>
<div class="proof">
<p><span id="unlabeled-div-74" class="proof"><em>Proof</em>. </span>In Theorem <a href="ContinuousDistributions.html#thm:NormalProperties">8.2</a>, the MGF of the random variable <span class="math inline">\(X_i\)</span> was shown to be
<span class="math display">\[
   M_{X_i} (t)
   = \exp\left\{\mu_i t + \frac{1}{2} \sigma^2_i t^2\right\}\quad\text{for $i = 1, 2, \dots, n$}.
\]</span>
So, for a constant <span class="math inline">\(a_i\)</span> (using Theorem <a href="ChapExpectation.html#thm:ExpectationLinear">5.1</a> with <span class="math inline">\(\beta = 0\)</span>, <span class="math inline">\(\alpha = a_i\)</span>):
<span class="math display">\[\begin{align*}
   M_{a_i X_i}(t)
   &amp;= M_{X_i}(a_it)\\
   &amp;= \exp\left\{\mu_i a_i t + \frac{1}{2} \sigma^2_i a^2_i t^2\right\}.
\end{align*}\]</span>
Since the MGF of a sum of independent random variables is equal to the product of their MGFs, then
<span class="math display">\[\begin{align*}
     M_Y(t)   
     &amp;= \prod^n_{i = 1} \exp\left( \mu_i a_i t + \frac{1}{2} \sigma^2_i a^2_i t^2 \right)\\
     &amp;= \exp\left( t\Sigma a_i\mu_i +\frac{1}{2} t^2 \Sigma a^2_i \sigma^2_i \right).
\end{align*}\]</span>
This is the MGF of a normal random variable with mean <span class="math inline">\(\operatorname{E}[Y] = \Sigma a_i\mu_i\)</span> and variance <span class="math inline">\(\operatorname{var}[Y] = \Sigma a^2_i\sigma^2_i\)</span>.</p>
</div>
<div class="example">
<p><span id="exm:unlabeled-div-75" class="example"><strong>Example 12.4  </strong></span>Theorem <a href="SamplingDistributions.html#thm:LinearComb">12.5</a> can be demonstrated in <strong>R</strong>; see below.</p>
</div>
<div class="sourceCode" id="cb100"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">8979704</span><span class="op">)</span> <span class="co"># For reproducibility</span></span>
<span><span class="va">numberVars</span> <span class="op">&lt;-</span> <span class="fl">1000</span> <span class="co"># That is: n</span></span>
<span></span>
<span><span class="co"># The values of a_i, the coefficients in the linear combination</span></span>
<span><span class="va">ai</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/sample.html">sample</a></span><span class="op">(</span>x <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/seq.html">seq</a></span><span class="op">(</span><span class="op">-</span><span class="fl">2</span>, <span class="fl">3</span><span class="op">)</span>,    <span class="co"># We select values of a_i between -2 and 3</span></span>
<span>             size <span class="op">=</span> <span class="va">numberVars</span>, <span class="co"># Need one for each X value</span></span>
<span>             replace <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span>    <span class="co"># Select with replacement</span></span>
<span></span>
<span><span class="co"># The means of the individual distributions</span></span>
<span><span class="va">mui</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Uniform.html">runif</a></span><span class="op">(</span><span class="va">numberVars</span>, <span class="co"># We take means from a uniform dist; one for each X</span></span>
<span>             min <span class="op">=</span> <span class="op">-</span><span class="fl">5</span>,   <span class="co"># Min value is -5</span></span>
<span>             max <span class="op">=</span> <span class="fl">5</span><span class="op">)</span>    <span class="co"># Max value is 5</span></span>
<span><span class="co"># The variances of the distributions</span></span>
<span><span class="va">sigma2i</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Uniform.html">runif</a></span><span class="op">(</span><span class="va">numberVars</span>,  <span class="co"># Variance from a uniform distn also</span></span>
<span>                min <span class="op">=</span> <span class="fl">1</span>,    <span class="co"># Minimum values of 1</span></span>
<span>                max <span class="op">=</span> <span class="fl">3</span><span class="op">)</span>    <span class="co"># Maximum value of 3</span></span>
<span></span>
<span><span class="co"># Compute the values directly:</span></span>
<span><span class="va">Xi</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="va">numberVars</span>,      <span class="co"># X_i with </span></span>
<span>            mean <span class="op">=</span> <span class="va">mui</span>,   <span class="co"># means </span></span>
<span>            sd <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">sqrt</a></span><span class="op">(</span><span class="va">sigma2i</span><span class="op">)</span> <span class="op">)</span></span>
<span></span>
<span><span class="co"># What we expect for Y = a_1 X_2 + a_2 X_2 + ...</span></span>
<span><span class="co"># according to the Theorem:</span></span>
<span><span class="va">Y</span> <span class="op">&lt;-</span> <span class="va">ai</span> <span class="op">*</span> <span class="va">Xi</span>  <span class="co"># The linear combination Y</span></span>
<span></span>
<span><span class="co"># Show some:</span></span>
<span><span class="va">someInfo</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/cbind.html">cbind</a></span><span class="op">(</span><span class="va">ai</span>, <span class="va">mui</span>, <span class="va">sigma2i</span>, <span class="va">Xi</span>, <span class="va">Y</span><span class="op">)</span></span>
<span><span class="va">someInfo</span><span class="op">[</span><span class="fl">1</span><span class="op">:</span><span class="fl">5</span>, <span class="op">]</span></span>
<span><span class="co">#&gt;      ai        mui  sigma2i        Xi         Y</span></span>
<span><span class="co">#&gt; [1,] -1  0.1690571 2.685062  1.993591 -1.993591</span></span>
<span><span class="co">#&gt; [2,]  1 -4.8907282 1.712808 -5.100795 -5.100795</span></span>
<span><span class="co">#&gt; [3,] -1 -0.4511475 1.562472 -1.184639  1.184639</span></span>
<span><span class="co">#&gt; [4,] -1  0.6878348 2.445416  4.870628 -4.870628</span></span>
<span><span class="co">#&gt; [5,] -2 -1.9851899 1.032315 -2.497384  4.994768</span></span>
<span></span>
<span></span>
<span><span class="co"># Compare:</span></span>
<span><span class="va">compareTable</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/array.html">array</a></span><span class="op">(</span> dim <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">2</span>, <span class="fl">2</span><span class="op">)</span> <span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/colnames.html">colnames</a></span><span class="op">(</span><span class="va">compareTable</span><span class="op">)</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"Means:"</span>,</span>
<span>                            <span class="st">"Variances:"</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/colnames.html">rownames</a></span><span class="op">(</span><span class="va">compareTable</span><span class="op">)</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"Using theorem"</span>,</span>
<span>                            <span class="st">"Computing directly"</span><span class="op">)</span></span>
<span></span>
<span><span class="va">compareTable</span><span class="op">[</span>, <span class="fl">1</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span> <span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">ai</span> <span class="op">*</span> <span class="va">Xi</span><span class="op">)</span>, </span>
<span>                        <span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">Y</span><span class="op">)</span> <span class="op">)</span></span>
<span><span class="va">compareTable</span><span class="op">[</span>, <span class="fl">2</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span> <span class="fu"><a href="https://rdrr.io/r/stats/cor.html">var</a></span><span class="op">(</span><span class="va">ai</span> <span class="op">*</span> <span class="va">Xi</span><span class="op">)</span>, </span>
<span>                        <span class="fu"><a href="https://rdrr.io/r/stats/cor.html">var</a></span><span class="op">(</span><span class="va">Y</span><span class="op">)</span> <span class="op">)</span></span>
<span><span class="va">compareTable</span></span>
<span><span class="co">#&gt;                        Means: Variances:</span></span>
<span><span class="co">#&gt; Using theorem      -0.2656963    32.6838</span></span>
<span><span class="co">#&gt; Computing directly -0.2656963    32.6838</span></span></code></pre></div>
<p>The sampling distribution of the sum and mean of a random sample from a normal population follows directly from Theorem <a href="SamplingDistributions.html#thm:LinearComb">12.5</a>.</p>
<div class="corollary">
<p><span id="cor:SumMean" class="corollary"><strong>Corollary 12.1  (Sum and mean of a random sample) </strong></span>Let <span class="math inline">\(X_1, X_2, \dots, X_n\)</span> be a random sample of size <span class="math inline">\(n\)</span> from <span class="math inline">\(N(\mu,\sigma^2)\)</span>.
Define the sum <span class="math inline">\(S\)</span> and mean <span class="math inline">\(\bar{X}\)</span> respectively as
<span class="math display">\[\begin{align*}
   S
   &amp;= X_1 + X_2 + \dots + X_n\\
   \bar{X}
   &amp;= (X_1+X_2 + \dots + X_n)/n
\end{align*}\]</span>
Then <span class="math inline">\(S\sim N(n\mu, n\sigma^2)\)</span> and <span class="math inline">\(\bar{X}\sim N(\mu, \sigma^2/n)\)</span>.</p>
</div>
<div class="proof">
<p><span id="unlabeled-div-76" class="proof"><em>Proof</em>. </span>Exercise.</p>
</div>
<p>Corollary <a href="SamplingDistributions.html#cor:SumMean">12.1</a> relies only on the population from which the sample is drawn having a normal distribution, and on the properties of expectation; it does not depend on the sample size <span class="math inline">\(n\)</span>.
It is the basis for inference about the population mean of a normal distribution with known variance.</p>
<div class="example">
<p><span id="exm:CLTEnvelopes" class="example"><strong>Example 12.5  (Sums of rvs) </strong></span>Suppose envelopes are collected into packs of <span class="math inline">\(25\)</span> by weighing them.
The weight of an envelope is distributed normally with mean <span class="math inline">\(3\,\text{g}\)</span> and standard deviation <span class="math inline">\(0.6\,\text{g}\)</span>.
Any weighed pack of envelopes is declared as containing <span class="math inline">\(25\)</span> if it weighs between <span class="math inline">\(70\,\text{g}\)</span> and <span class="math inline">\(80\,\text{g}\)</span>.
What is the probability that a pile of <span class="math inline">\(25\)</span> will <em>not</em> be counted as such?</p>
<p>Let random variable <span class="math inline">\(X_i\)</span> be the weight of the <span class="math inline">\(i\)</span>th envelope; then <span class="math inline">\(X_i \sim N(3, 0.36)\)</span>.
Let <span class="math inline">\(Y = X_1 + X_2 + \dots + X_{25}\)</span>; then <span class="math inline">\(Y \sim N(75, 9)\)</span>.
We require <span class="math inline">\(1 - \Pr(70 &lt; Y &lt; 80)\)</span>; see Fig. <a href="SamplingDistributions.html#fig:EnvelopeDist">12.4</a>.
Since the normal distribution is symmetric, proceed:
<span class="math display">\[\begin{align*}
   1 - \Pr(70 &lt; Y &lt; 80)
   &amp;= 2\times \Pr\left( Z &lt; \frac{70 - 75}{\sqrt{9}} \right)\\
   &amp;= 2\times \Phi(-1.667) \\
   &amp;= 0.10.
\end{align*}\]</span>
In <strong>R</strong>:</p>
<div class="sourceCode" id="cb101"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fl">2</span> <span class="op">*</span> <span class="op">(</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">pnorm</a></span><span class="op">(</span> <span class="op">-</span><span class="fl">1.667</span> <span class="op">)</span> <span class="op">)</span></span>
<span><span class="co">#&gt; [1] 0.0955144</span></span></code></pre></div>
</div>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:EnvelopeDist"></span>
<img src="12-Sampling_files/figure-html/EnvelopeDist-1.png" alt="The envelope question. Left panel: The distribution of the weight of individual envelopes; right panel: The distribution of the weight of packs of $25$\ envelopes." width="90%"><p class="caption">
FIGURE 12.4: The envelope question. Left panel: The distribution of the weight of individual envelopes; right panel: The distribution of the weight of packs of <span class="math inline">\(25\)</span> envelopes.
</p>
</div>
<div class="example">
<p><span id="exm:CLTweights" class="example"><strong>Example 12.6  (CLT) </strong></span>The IQs for a large population of <span class="math inline">\(10\)</span> year-old boys (assumed normally distributed) were determined and found to have a mean of <span class="math inline">\(110\)</span> and a variance of <span class="math inline">\(144\)</span>.
How large a sample would have to be taken in order to have a probability of <span class="math inline">\(0.9\)</span> that the <em>mean</em> IQ of the sample would not differ from the expected value <span class="math inline">\(110\)</span> by more than <span class="math inline">\(5\)</span>?</p>
<p>Let <span class="math inline">\(X_i\)</span> be the IQ of the <span class="math inline">\(i\)</span>th boy; then <span class="math inline">\(X_i \sim N(110, 144)\)</span>.
Consider a sample of size <span class="math inline">\(n\)</span> and let <span class="math inline">\(\bar{X} = \sum^n_{i = 1}X_i/n\)</span>; then <span class="math inline">\(\bar{X}\sim N(110, 144/n)\)</span>.
Then
<span class="math display">\[
   \Pr(|\bar{X} - 110|\leq 5) = 0.90.
\]</span>
That is
<span class="math display">\[
   \Pr\left(\frac{|\bar{X} - 110|\sqrt{n}}{12} \leq \frac{5\sqrt{n}}{12}\right) = 0.90
\]</span>
hence
<span class="math display">\[
   \Pr(Z \leq 5\sqrt{n} /12) = 0.90.
\]</span>
Using <strong>R</strong>:</p>
<div class="sourceCode" id="cb102"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">z</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">qnorm</a></span><span class="op">(</span><span class="fl">0.95</span><span class="op">)</span></span>
<span><span class="va">z</span></span>
<span><span class="co">#&gt; [1] 1.644854</span></span>
<span></span>
<span><span class="va">n</span> <span class="op">&lt;-</span> <span class="op">(</span><span class="va">z</span> <span class="op">*</span> <span class="fl">12</span> <span class="op">/</span> <span class="fl">5</span><span class="op">)</span> <span class="op">^</span> <span class="fl">2</span></span>
<span><span class="va">n</span></span>
<span><span class="co">#&gt; [1] 15.58393</span></span></code></pre></div>
<p>The <em>smallest</em> size sample, then, would be a sample of <span class="math inline">\(n = 16\)</span>.</p>
</div>
<div class="example">
<p><span id="exm:CLTPlunger" class="example"><strong>Example 12.7  (Two random variable) </strong></span>A certain product involves a plunger fitting into a cylindrical tube.
The diameter of the plunger can be considered a normal random variable with mean <span class="math inline">\(2.1\,\text{cm}\)</span> and standard deviation <span class="math inline">\(0.1\,\text{cm}\)</span>.
The inside diameter of the cylindrical tube is a normal random variable with mean <span class="math inline">\(2.3\,\text{cm}\)</span> and standard deviation <span class="math inline">\(0.05\,\text{cm}\)</span>.
For a plunger and tube chosen randomly from a day’s production run, find the probability that the plunger will not fit into the cylinder.</p>
<p>Let <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> be the diameter of the plunger and cylinder respectively.
Then <span class="math inline">\(X\sim N\big(2.1, (0.1)^2\big)\)</span> and <span class="math inline">\(Y\sim N\big( 2.3, (0.05)^2\big)\)</span>, and we seek <span class="math inline">\(\Pr(Y &lt; X)\)</span>.
The distribution of <span class="math inline">\(Y - X\)</span> is <span class="math inline">\(N(2.3 - 2.1, 0.0025 + 0.01)\)</span> so that
<span class="math display">\[\begin{align*}
   \Pr(Y - X &lt; 0)
   &amp;= \Pr\left(Z &lt;\frac{0 - 0.2}{\sqrt{0.0125}}\right) \quad\text{where $Z\sim N(0,1)$}\\
   &amp;= \Pr(Z &lt; -1.78)\\
          &amp;= 0.0375.
\end{align*}\]</span>
In <strong>R</strong>:</p>
<div class="sourceCode" id="cb103"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">pnorm</a></span><span class="op">(</span> <span class="op">-</span><span class="fl">1.78</span> <span class="op">)</span></span>
<span><span class="co">#&gt; [1] 0.03753798</span></span></code></pre></div>
</div>
</div>
<div id="ChiSquareDistribution" class="section level3" number="12.5.2">
<h3>
<span class="header-section-number">12.5.2</span> The <span class="math inline">\(\chi^2\)</span> distribution<a class="anchor" aria-label="anchor" href="#ChiSquareDistribution"><i class="fas fa-link"></i></a>
</h3>
<p>
Having studied the sampling distribution of the <em>mean</em>, we now study the sampling distribution of the <em>variance</em> in a normal population.
First, we present a useful result which is a variation of Theorem <a href="ContinuousDistributions.html#thm:SumSquaredNormals">8.12</a>.</p>
<div class="theorem">
<p><span id="thm:ChiSquare" class="theorem"><strong>Theorem 12.6  (Chi-square distribution) </strong></span>Let <span class="math inline">\(X_1, X_2, \dots, X_n\)</span> be a random sample of size <span class="math inline">\(n\)</span> from <span class="math inline">\(N(\mu, \sigma^2)\)</span>.
Then <span class="math inline">\(\sum_{i = 1}^n \left(\displaystyle{\frac{X_i - \mu}{\sigma}}\right)^2\)</span> has a chi-square distribution with <span class="math inline">\(n\)</span> degrees of freedom (df), written as <span class="math inline">\(\chi^2_n\)</span>.</p>
</div>
<div class="proof">
<p><span id="unlabeled-div-77" class="proof"><em>Proof</em>. </span>If <span class="math inline">\(Z_i = \displaystyle{\frac{X_i - \mu}{\sigma}}\)</span>, then
<span class="math display">\[
   \sum_{i = 1}^n \left(\frac{X_i - \mu}{\sigma}\right)^2 = \sum_{i = 1}^n Z_i^2.
\]</span>
Here, <span class="math inline">\(Z_i\)</span> is the standardised version of <span class="math inline">\(X_i\)</span> and has a standard normal <span class="math inline">\(N(0, 1)\)</span> distribution.
Also, the random variables <span class="math inline">\(Z_i\)</span> are independent because the <span class="math inline">\(X_i\)</span> are independent (<span class="math inline">\(i = 1, 2, \dots, n\)</span>).
The required result follows directly from Theorem <a href="ContinuousDistributions.html#thm:SumSquaredNormals">8.12</a>.</p>
</div>
<div class="softwareBox software">
<p>The four <strong>R</strong> functions for working with the <span class="math inline">\(\chi^2\)</span> distribution have the form <code>[dpqr]chisq(df)</code>, where <code>df</code> is the degrees of freedom (see Appendix  <a href="UseRDistributions.html#UseRDistributions">D</a>).</p>
</div>
<p>The sample variance is given by
<span class="math display" id="eq:SampleVarDist">\[\begin{equation}
   S^2
   = \frac{1}{n - 1}\sum_{i = 1}^n (X_i - \bar{X})^2.
   \tag{12.3}
\end{equation}\]</span>
All sample estimates (‘statistics’) vary from sample to sample, so the <em>sample</em> variance has a distribution.
The sampling distribution of the sample variance follows.</p>
<div class="theorem">
<p><span id="thm:SampleVarDist" class="theorem"><strong>Theorem 12.7  (Distribution of the sample variance) </strong></span>Let <span class="math inline">\(X_1, X_2, \dots, X_n\)</span> be a random sample of size <span class="math inline">\(n\)</span> from <span class="math inline">\(N(\mu, \sigma^2)\)</span>.
Then
<span class="math display">\[
   \frac{(n - 1)S^2}{\sigma^2} \sim \chi^2_{n - 1}.
\]</span></p>
</div>
<div class="proof">
<p><span id="unlabeled-div-78" class="proof"><em>Proof</em>. </span>Only a <em>partial proof</em> is provided.
Using <a href="SamplingDistributions.html#eq:SampleVarDist">(12.3)</a>,
<span class="math display">\[
   \frac{(n - 1)S^2}{\sigma^2}
   = \sum_{i = 1}^n \left(\frac{X_i - \bar{X}}{\sigma}\right)^2,
\]</span>
which looks much like the conditions for Theorem <a href="SamplingDistributions.html#thm:ChiSquare">12.6</a>.
The difference is that <span class="math inline">\(\bar{X}\)</span> replaces <span class="math inline">\(\mu\)</span>.
<em>This difference is important:</em>
<span class="math inline">\(\mu\)</span> is a fixed value that does not vary, whereas <span class="math inline">\(\bar{X}\)</span> is a random variable that has a distribution.</p>
<p>Writing
<span class="math display">\[\begin{align*}
  \sum_{i = 1}^n \left( \frac{X_i - \bar{X}}{\sigma} \right)^2
    &amp;= \sum_{i = 1}^n \left( \frac{X_i - \bar{X} + \mu - \mu}{\sigma} \right)^2\\
    &amp;= \sum_{i = 1}^n \left( \frac{X_i - \mu}{\sigma} \right)^2 -
       n\frac{(\bar{X} - \mu)^2}{\sigma^2}.
\end{align*}\]</span>
(Check!)
Rewrite as <span class="math inline">\(V = W - U\)</span> where
<span class="math display">\[
   V
   = \sum_{i = 1}^n \left(\frac{X_i - \bar{X}}{\sigma}\right)^2;\qquad
   W
   = \sum_{i = 1}^n \left( \frac{X_i - \mu}{\sigma} \right)^2;\quad\text{and}\qquad
   U
   = n\frac{(\bar{X} - \mu)^2}{\sigma^2}.\qquad
\]</span>
So we can write <span class="math inline">\(W = U + V\)</span>.
Then observe:</p>
<ul>
<li>
<span class="math inline">\(W \sim \chi^2_n\)</span> by Theorem <a href="SamplingDistributions.html#thm:ChiSquare">12.6</a>, so that the MGF of <span class="math inline">\(W\)</span> is <span class="math inline">\(M_W(t) = (1 - 2t)^{-n}\)</span>.</li>
<li>
<span class="math inline">\(V \sim \chi^2_1\)</span> because <span class="math inline">\(V\)</span> is the square of a standard normal random variable, since <span class="math inline">\(\bar{X}\sim N(\mu, \sigma^2/n)\)</span>.
Therefore, the MGF of <span class="math inline">\(V\)</span> is <span class="math inline">\(M_V(t) = (1 - 2t)^{-1}\)</span>.</li>
</ul>
<p>Provided <span class="math inline">\(W\)</span> and <span class="math inline">\(V\)</span> are independent, the MGF of <span class="math inline">\(W\)</span> is the product of the MGFs of <span class="math inline">\(U\)</span> and <span class="math inline">\(V\)</span>.
This means that the MGF of <span class="math inline">\(U\)</span> is
<span class="math display">\[
   M_U(t)
   = \frac{M_W(t)}{M_V(t)}
   = (1 - 2t)^{n - 1}
\]</span>
which is the MGF of the chi-square distribution with <span class="math inline">\(n - 1\)</span> df.</p>
<p>This is only a <em>partial proof</em>, because the independence of <span class="math inline">\(U\)</span> and <span class="math inline">\(V\)</span> has not been shown.</p>
</div>
<div class="example">
<p><span id="exm:SampleVar" class="example"><strong>Example 12.8  (Distribution of sample variance) </strong></span>A random sample of size <span class="math inline">\(n = 10\)</span> is selected from the <span class="math inline">\(N(20, 5)\)</span> distribution.
What is the probability that the variance of this sample exceeds <span class="math inline">\(10\)</span>?</p>
<p>By Theorem <a href="SamplingDistributions.html#thm:SampleVarDist">12.7</a>,
<span class="math display">\[
   \frac{(n - 1)S^2}{\sigma^2}
   = \frac{9 S^2}{5} \sim \chi^2_9.
\]</span>
Therefore
<span class="math display">\[
   \Pr(S^2 &gt; 10)
   = \Pr\left( \frac{9S^2}{5} &gt; \frac{9\times10}{5} \right)
   = \Pr(X^2 &gt; 18),
\]</span>
where <span class="math inline">\(X^2 \sim \chi^2(9)\)</span>.
Using <strong>R</strong>, <span class="math inline">\(\Pr(S^2 &gt; 10) = 0.03517\)</span>:</p>
<div class="sourceCode" id="cb104"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fl">1</span> <span class="op">-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Chisquare.html">pchisq</a></span><span class="op">(</span><span class="fl">18</span>, df <span class="op">=</span> <span class="fl">9</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] 0.03517354</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/stats/Chisquare.html">pchisq</a></span><span class="op">(</span><span class="fl">18</span>, df <span class="op">=</span> <span class="fl">9</span>, lower.tail <span class="op">=</span> <span class="cn">FALSE</span><span class="op">)</span>   <span class="co"># Alternatively</span></span>
<span><span class="co">#&gt; [1] 0.03517354</span></span></code></pre></div>
</div>
</div>
<div id="TDistribution" class="section level3" number="12.5.3">
<h3>
<span class="header-section-number">12.5.3</span> The <span class="math inline">\(t\)</span> distribution<a class="anchor" aria-label="anchor" href="#TDistribution"><i class="fas fa-link"></i></a>
</h3>
<p>
The basis of the independence assumed in the proof of Theorem <a href="SamplingDistributions.html#thm:SampleVarDist">12.7</a> is the (perhaps surprising) result that the mean and variance of a random sample from a normal population are independent random variables.</p>
<div class="theorem">
<p><span id="thm:MeanVarInd" class="theorem"><strong>Theorem 12.8  (Sample mean and variance: Independent) </strong></span>Let <span class="math inline">\(X_1, X_2,\dots, X_n\)</span> be a random sample of size <span class="math inline">\(n\)</span> from <span class="math inline">\(N(\mu, \sigma^2)\)</span>.
Then the sample mean
<span class="math display">\[
   \bar{X}
   =
   \frac1n\sum_{i=1}^n X_i
\]</span>
and sample variance
<span class="math display">\[
   S^2
   = \frac{1}{n - 1}\sum_{i=1}^n (X_i - \bar{X})^2
\]</span>
are independent.</p>
</div>
<div class="proof">
<p><span id="unlabeled-div-79" class="proof"><em>Proof</em>. </span>This proof is not given.</p>
</div>
<p>Another distribution that derives from sampling a normal population is the <span class="math inline">\(t\)</span> distribution.</p>
<div class="definition">
<p><span id="def:TDistribution" class="definition"><strong>Definition 12.5  (T distribution) </strong></span>Suppose <span class="math inline">\(Z \sim N(0, 1)\)</span> (i.e., a standard normal distribution) and <span class="math inline">\(V \sim \chi^2_n\)</span> (i.e., a chi-squared distribution with <span class="math inline">\(n\)</span> degrees of freedom).
Then
<span class="math display" id="eq:Tdist">\[\begin{equation}
  T = \frac{Z}{\sqrt{V/n}}
  \tag{12.4}
\end{equation}\]</span>
is defined as a <span class="math inline">\(T\)</span> distribution with <span class="math inline">\(n\)</span> degrees of freedom.</p>
</div>
<div class="definition">
<p><span id="def:TDistributionPDF" class="definition"><strong>Definition 12.6  (T distribution) </strong></span>A continuous random variable <span class="math inline">\(X\)</span> with probability density function
<span class="math display">\[\begin{equation}
   f_X(x)
   = \frac{\Gamma\left((\nu + 1)/2\right)}{\sqrt{\pi \nu}\,\Gamma(\nu/2)}
     \left(1 + \frac{x^2}{\nu}\right)^{-(\nu + 1)/2}
     \quad\text{for all $x$}
\end{equation}\]</span>
is said to have a <em><span class="math inline">\(t\)</span> distribution</em> with parameter <span class="math inline">\(\nu &gt; 0\)</span>.
The parameter <span class="math inline">\(\nu\)</span> is called the <em>degrees of freedom</em>.
We write <span class="math inline">\(X \sim t_\nu\)</span>.</p>
</div>
<div class="proof">
<p><span id="unlabeled-div-80" class="proof"><em>Proof</em>. </span>This proof is not given.</p>
</div>
<p>The PDF of the <span class="math inline">\(T\)</span>-distribution is very similar to the standard normal distribution (Fig. <a href="SamplingDistributions.html#fig:TPlots">12.5</a>): bell-shaped and symmetric about zero.
KURTOSIS?? SKEWNESS??</p>
<p>However, the variance is greater than one (Theorem <a href="SamplingDistributions.html#thm:TDistProperties">12.9</a>) is dependent on <span class="math inline">\(\nu\)</span>.</p>
<div class="softwareBox software">
<p>The four <strong>R</strong> functions for working with the <span class="math inline">\(t\)</span> distribution have the form <code>[dpqr]t(df)</code>, where <code>df</code> is the degrees of freedom (see Appendix <a href="UseRDistributions.html#UseRDistributions">D</a>).</p>
</div>
<div class="figure">
<span style="display:block;" id="fig:TPlots"></span>
<img src="12-Sampling_files/figure-html/TPlots-1.png" alt="Some $t$ distributions (with normal distributions in dashed lines), with mean\ $0$ and variance\ $1$." width="100%"><p class="caption">
FIGURE 12.5: Some <span class="math inline">\(t\)</span> distributions (with normal distributions in dashed lines), with mean <span class="math inline">\(0\)</span> and variance <span class="math inline">\(1\)</span>.
</p>
</div>
<div class="theorem">
<p><span id="thm:TDistProperties" class="theorem"><strong>Theorem 12.9  (Properties of the $T$ distribution) </strong></span>If <span class="math inline">\(X\sim t_\nu\)</span> then</p>
<ul>
<li>For <span class="math inline">\(\nu &gt; 1\)</span>, <span class="math inline">\(\operatorname{E}[X] = 0\)</span>.</li>
<li>For <span class="math inline">\(\nu &gt; 2\)</span>, <span class="math inline">\(\operatorname{var}[X] = \displaystyle{\frac{\nu}{\nu - 2}}\)</span>.</li>
<li>The MGF does not exist because only the first <span class="math inline">\(\nu - 1\)</span> moments exist.</li>
</ul>
</div>
<div class="proof">
<p><span id="unlabeled-div-81" class="proof"><em>Proof</em>. </span>This proof is not given.</p>
</div>
<p>Although we won’t prove it, as <span class="math inline">\(\nu\to\infty\)</span> the <span class="math inline">\(t\)</span> distribution converges to the standard normal (which can be seen in Fig. <a href="SamplingDistributions.html#fig:TPlots">12.5</a>).
In addition, from Theorem <a href="SamplingDistributions.html#thm:TDistProperties">12.9</a>, we see that as <span class="math inline">\(\nu \to \infty\)</span>, <span class="math inline">\(\operatorname{var}[X] \to 1\)</span> as for the normal distribution.</p>
<p>The usefulness of the <span class="math inline">\(t\)</span> distribution derives from the following important result.</p>
<div class="theorem">
<p><span id="thm:Tscores" class="theorem"><strong>Theorem 12.10  ($T$-scores) </strong></span>Let <span class="math inline">\(X_1, X_2, \dots, X_n\)</span> be a random sample of size <span class="math inline">\(n\)</span> from <span class="math inline">\(N(\mu, \sigma^2)\)</span>.
Then the random variable
<span class="math display">\[
   T = \frac{\bar{X} - \mu}{S/\sqrt{n}}
\]</span>
follows a <span class="math inline">\(t_{n - 1}\)</span> distribution where <span class="math inline">\(\bar{X}\)</span> is the sample variance and <span class="math inline">\(S^2\)</span> is the sample variance as defined in q.<a href="SamplingDistributions.html#eq:SampleVarDist">(12.3)</a>.</p>
</div>
<div class="proof">
<p><span id="unlabeled-div-82" class="proof"><em>Proof</em>. </span><strong>We give a partial proof only.</strong>
The statistic <span class="math inline">\(T\)</span> can be written as
<span class="math display">\[\begin{align*}
  T
    &amp;= \frac{\bar{X} - \mu}{S/\sqrt{n}}\\
    &amp;= \left(\frac{\bar{X} - \mu}{\sigma/\sqrt{n}}\right)   \frac{1}{\sqrt{\frac{(n - 1)S^2}{\sigma^2}/(n - 1)}}\\
    &amp;= \frac{Z}{Y/(n - 1)},
\end{align*}\]</span>
where <span class="math inline">\(Z\)</span> is a <span class="math inline">\(N(0, 1)\)</span> variable and <span class="math inline">\(Y\)</span> is a chi-square variable with <span class="math inline">\((n - 1)\)</span> df.
Using Def. <a href="SamplingDistributions.html#def:TDistribution">12.5</a> then, <span class="math inline">\(T\)</span> has a <span class="math inline">\(T\)</span> distribution.</p>
</div>
<p>Notice that <span class="math inline">\(T\)</span> represents a <em>standardised</em> version of the sample mean.
Because of this, the <span class="math inline">\(T\)</span> statistic is important in statistical inference.
You probably have used it is hypothesis testing, for example.</p>
<div class="example">
<p><span id="exm:CalcaltingT" class="example"><strong>Example 12.9  (Calculating $T$) </strong></span>A random sample <span class="math inline">\(21, 18, 16, 24, 16\)</span> is drawn from a normal population with mean of <span class="math inline">\(20\)</span>.</p>
<ol style="list-style-type: decimal">
<li>What is the value of <span class="math inline">\(T\)</span> for this sample?</li>
<li>In random samples from this population, what is the probability that <span class="math inline">\(T\)</span> is less than the value found above?</li>
</ol>
<p>From the sample: <span class="math inline">\(\overline{x} = 19.0\)</span> and <span class="math inline">\(s^2 = 12.0\)</span>.
Therefore <span class="math inline">\(t = \frac{19.0 - 20}{\sqrt{12.0/5}} = -0.645\)</span>.
(Notice lower-case symbols are used for specific values of statistics, and upper-case symbols for the random variables.)</p>
<p>Interest here is in <span class="math inline">\(\Pr(T &lt; -0.645)\)</span> where <span class="math inline">\(T\sim t_4\)</span>.
From <strong>R</strong>, the answer is <span class="math inline">\(0.277\)</span>:</p>
<div class="sourceCode" id="cb105"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/stats/TDist.html">pt</a></span><span class="op">(</span><span class="op">-</span><span class="fl">0.645</span>, df <span class="op">=</span> <span class="fl">4</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] 0.2770289</span></span></code></pre></div>
</div>
</div>
<div id="FDistribution" class="section level3" number="12.5.4">
<h3>
<span class="header-section-number">12.5.4</span> The <span class="math inline">\(F\)</span> distribution<a class="anchor" aria-label="anchor" href="#FDistribution"><i class="fas fa-link"></i></a>
</h3>
<p>
Definition <a href="SamplingDistributions.html#def:FDistribution">12.7</a> introduces the <span class="math inline">\(F\)</span> distribution, which describes the ratio of two sample variances from normal populations.
The <span class="math inline">\(F\)</span> distribution is used in inferences comparing two sample variances.
This distribution is also used in <em>analysis of variance</em>, a common technique used to test the equality of several means.</p>
<div class="definition">
<p><span id="def:FDistribution" class="definition"><strong>Definition 12.7  ($F$ distribution) </strong></span>A continuous random variable <span class="math inline">\(X\)</span> with probability density function
<span class="math display">\[\begin{equation}
  f_X(x)
  = \frac{\Gamma\big( (\nu_1 + \nu_2)/2 \big)
    \nu_1^{\nu_1/2} \nu_2^{\nu_2/2} x^{(\nu_1/2) - 1}}
   {\Gamma(\nu_1/2)\Gamma(\nu_2/2)(\nu_1 x + \nu_2)^{(\nu_1 + \nu_2)/2}}\quad\text{for $x &gt; 0$}
\end{equation}\]</span>
is said to have an <em><span class="math inline">\(F\)</span> distribution</em> with parameters <span class="math inline">\(\nu_1 &gt; 0\)</span> and <span class="math inline">\(\nu_2 &gt; 0\)</span>.
The parameters are known respectively as the numerator and denominator degrees of freedom.
We write <span class="math inline">\(X \sim F_{\nu_1, \nu_2}\)</span>.</p>
</div>
<p>Some <span class="math inline">\(F\)</span> distributions are shown in Fig. <a href="SamplingDistributions.html#fig:FPlots">12.6</a>.
The basic properties of the <span class="math inline">\(F\)</span> distribution are as follows.</p>
<div class="theorem">
<p><span id="thm:FProperties" class="theorem"><strong>Theorem 12.11  (Properties of $F$ distribution) </strong></span>If <span class="math inline">\(X\sim F(\nu_1, \nu_2)\)</span> then</p>
<ul>
<li>For <span class="math inline">\(\nu_2 &gt; 2\)</span>, <span class="math inline">\(\operatorname{E}[F] = \displaystyle{\frac{\nu_2}{\nu_2-2}}\)</span>.</li>
<li>For <span class="math inline">\(\nu_2 &gt; 4\)</span>, <span class="math inline">\(\operatorname{var}[F] = \displaystyle{\frac{2\nu_2^2(\nu_1 + \nu_2-2)} {\nu_1(\nu_2 - 2)^2(\nu_2 - 4)}}\)</span>.</li>
<li>The MGF does not exist.</li>
</ul>
</div>
<div class="proof">
<p><span id="unlabeled-div-83" class="proof"><em>Proof</em>. </span>Not covered.</p>
</div>
<div class="figure">
<span style="display:block;" id="fig:FPlots"></span>
<img src="12-Sampling_files/figure-html/FPlots-1.png" alt="Some $F$ distributions." width="100%"><p class="caption">
FIGURE 12.6: Some <span class="math inline">\(F\)</span> distributions.
</p>
</div>
<p>The relationship between the <span class="math inline">\(T\)</span> and <span class="math inline">\(F\)</span> distribution can be established.
Using Eq. <a href="SamplingDistributions.html#eq:Tdist">(12.4)</a>,
<span class="math display">\[
  T^2
  = \frac{Z^2}{V/n}
  = \frac{Z^2/1}{V/n}
\]</span>
and the numerator has a <span class="math inline">\(\chi^2_1\)</span> distribution; hence, <span class="math inline">\(T^2\)</span> has an <span class="math inline">\(F_{1, n}\)</span> distribution.</p>
<div class="theorem">
<p><span id="thm:FGenerate" class="theorem"><strong>Theorem 12.12  ($F$ distribution) </strong></span>Let <span class="math inline">\(X_1, X_2, \dots, X_{n_1}\)</span> be a random sample of size <span class="math inline">\(n_1\)</span> from <span class="math inline">\(N(\mu_1, \sigma_1^2)\)</span> and <span class="math inline">\(Y_1, Y_2,\dots, Y_{n_2}\)</span> be an independent random sample of size <span class="math inline">\(n_2\)</span> from <span class="math inline">\(N(\mu_2, \sigma_2^2)\)</span>.
Then the random variable
<span class="math display">\[
   F
   = \frac{S_1^2/\sigma_1^2}{S_2^2/\sigma_2^2}
\]</span>
follows a <span class="math inline">\(F_{n_1 - 1, n_2 - 1}\)</span> distribution.</p>
</div>
<div class="proof">
<p><span id="unlabeled-div-84" class="proof"><em>Proof</em>. </span><strong>We give a partial proof only.</strong>
The <span class="math inline">\(F\)</span> statistic can be rewritten as
<span class="math display">\[
   F
   = \frac{S_1^2/\sigma_1^2}{S_2^2/\sigma_2^2}
   = \frac{U_1/\nu_1}{U_2/\nu_2}
\]</span>
where
<span class="math display">\[\begin{align*}
   U_1
     &amp;= \displaystyle{\frac{(n_1 - 1)S_1^2}{\sigma_1^2}}
   \nu_1
     = n_1 - 1, \\
   U_2
     &amp;= \displaystyle{\frac{(n_2 - 1)S_2^2}{\sigma_2^2}},
   \nu_2
     = n_2 - 1.
\end{align*}\]</span>
<span class="math inline">\(U_1\)</span> and <span class="math inline">\(U_2\)</span> have chi-square distributions.
The PDF of <span class="math inline">\(\frac{U_1/\nu_1}{U_2/\nu_2}\)</span> and the rest of the proof is not given.</p>
</div>
<div class="softwareBox software">
<p>The four <strong>R</strong> functions for working with the <span class="math inline">\(F\)</span> distribution have the form <code>[dpqr]f(df1, df2)</code>, where <code>df1</code><span class="math inline">\({} = \nu_1\)</span> and <code>df2</code><span class="math inline">\({} = \nu2\)</span> (see Appendix <a href="UseRDistributions.html#UseRDistributions">D</a>).</p>
</div>
<div class="example">
<p><span id="exm:FProbs" class="example"><strong>Example 12.10  ($F$ distribution probabilities) </strong></span>Suppose <span class="math inline">\(X\sim F_{2, 10}\)</span>.
Find:</p>
<ul>
<li><span class="math inline">\(\Pr(X &lt; 1)\)</span></li>
<li>
<span class="math inline">\(x\)</span> such that <span class="math inline">\(\Pr(X &gt; x) = 0.01\)</span>
</li>
</ul>
<p>Using <strong>R</strong>:</p>
<div class="sourceCode" id="cb106"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/stats/Fdist.html">pf</a></span><span class="op">(</span><span class="fl">1</span>, </span>
<span>   df1 <span class="op">=</span> <span class="fl">2</span>, </span>
<span>   df2 <span class="op">=</span> <span class="fl">10</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] 0.5981224</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/stats/Fdist.html">qf</a></span><span class="op">(</span><span class="fl">0.99</span>, </span>
<span>   df1 <span class="op">=</span> <span class="fl">2</span>, </span>
<span>   df2 <span class="op">=</span> <span class="fl">10</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] 7.559432</span></span></code></pre></div>
</div>
</div>
</div>
<div id="CentralLimitTheorem" class="section level2" number="12.6">
<h2>
<span class="header-section-number">12.6</span> The Central Limit Theorem<a class="anchor" aria-label="anchor" href="#CentralLimitTheorem"><i class="fas fa-link"></i></a>
</h2>
<!-- SHOW, USING **R**, THAT MATLAB-LIKE THING: A distribution of X, sample... and x-bar is normal. -->
<p>Sampling distributions for various statistics of interest are well-developed when sampling is from a <em>normal</em> distribution (Sect. <a href="SamplingDistributions.html#SamplingDistributionsRelatedToNormal">12.5</a>).
Although these results are important, what if we do not know the distribution of the population from which the random sample is drawn (as is usually the case)?</p>
<p>In Sect. <a href="SamplingDistributions.html#SamplingDistributionMean">12.4.2</a> general results are given describing the <em>mean</em> and <em>variance</em> of the sample mean which hold for any population distribution, but do not say anything about the distribution.</p>
<p>The main result for this section is the following theorem, called <em>The Central Limit Theorem</em> (or <em>CLT</em>).</p>
<div class="importantBox important">
<p>The <em>Central Limit Theorem</em> is one of the most important theorems in statistics.</p>
</div>
<div class="theorem">
<p><span id="thm:CLT" class="theorem"><strong>Theorem 12.13  (Central Limit Theorem (CLT)) </strong></span>Let <span class="math inline">\(X_1, X_2, \dots, X_n\)</span> be a random sample from a distribution with mean <span class="math inline">\(\mu\)</span> and variance <span class="math inline">\(\sigma^2\)</span>.
Then the random variable
<span class="math display">\[
   Z_n
   = \frac{\bar{X} - \mu}{\sigma / \sqrt{n}}
\]</span>
converges in distribution to a standard normal variable as <span class="math inline">\(n\to\infty\)</span>.</p>
</div>
<div class="proof">
<p><span id="unlabeled-div-85" class="proof"><em>Proof</em>. </span>Let the random variable <span class="math inline">\(X_i\)</span> <span class="math inline">\((i = 1, \dots, n)\)</span> have MGF <span class="math inline">\(M_{X_i}(t)\)</span>.
Then
<span class="math display">\[
   M_{X_i}(t)
   = 1 + \mu'_1t + \mu'_2\frac{t^2}{2!} + \dots .
\]</span>
But <span class="math inline">\(\bar{X} = X_1/n + \dots + X_n/n\)</span>, so
<span class="math display">\[
   M_{\bar{X}}(t)
   = \prod^n_{i = 1} M_{X_i/n}(t)
   = \left[ M_{X_i} (t/n)\right]^n.
\]</span>
Now <span class="math inline">\(\displaystyle{Z_n = \frac{\sqrt{n}}{\sigma}\bar{X} - \frac{\sqrt{n}\mu}{\sigma}}\)</span>, which is of the form <span class="math inline">\(Y = aX + b\)</span> so
<span class="math display">\[\begin{align*}
  M_{Z_n}(t)
    &amp;= e^{-\sqrt{n}\mu  t/\sigma}M_{\bar{X}} (\sqrt{n}t/\sigma )\\
    &amp;= e^{-\sqrt{n}\mu t / \sigma} \left[ M_{X_i}\left(\frac{\sqrt{n}t}{\sigma n} \right)\right]^n\\
    &amp;  = e^{-\sqrt{n}\mu t /\sigma}\left[ M_{X_i}(t / \sigma \sqrt{n})\right]^n
\end{align*}\]</span>
Then
<span class="math display">\[\begin{align*}
     \log M_{Z_n}(t)
     &amp;= \frac{-\sqrt{n}\mu t}{\sigma} +
        n\log \left[ 1 + \frac{\mu'_1 t}{\sigma\sqrt{n}} + \frac{\mu'_2 t^2}{2! \, n\sigma^2}
        + \frac{\mu'_3 t^3}{3! \, n\sqrt{n}\sigma^3}  + \dots \right] \\
     &amp;= \frac{-\sqrt{n}\mu t}{\sigma} + n\left[ \mu'_1 \frac{t}{\sigma
\sqrt{n}} + \frac{\mu'_2 t^2}{2n\sigma^2} + \frac{\mu'_3t^3}{6n\sqrt{n}\sigma^3} + \dots
\right]\\
       &amp; \quad \text{} - \frac{n}{2} \left[ \frac{\mu'_1t}{\sigma\sqrt{n}} +
         \dots \right]^2 + \frac{n}{3} \left[\phantom{\frac{n}{3}}\dots\phantom{\frac{n}{3}}\right]^3 - \dots \\
     &amp;=  \frac{\mu'_2 t^2}{2\sigma^2} - \frac{(\mu'_1)^2 t^2}{2\sigma^2} +
\text{ terms in $1 / \sqrt{n}$, etc}.
\end{align*}\]</span>
Now,
<span class="math display">\[
   \lim_{n\to \infty} \log M_{Z_n}(t)
   =
   \frac{(\mu'_2- (\mu'_1)^2)}{\sigma^2}\frac{t^2}{2} = \frac{t^2}{2}
\]</span>
because the terms in <span class="math inline">\((1/\sqrt{n}\,)\)</span>, etc. tend to zero as <span class="math inline">\(n\to\infty\)</span>, and <span class="math inline">\(\mu'_2 - (\mu'_1)^2 = \sigma^2\)</span>.
Thus <span class="math inline">\(\displaystyle{ \lim_{n\to\infty} M_{Z_n}(t) = e^{\frac{1}{2} t^2}}\)</span>, which is the MGF of a <span class="math inline">\(N(0, 1)\)</span> random variable.
So <span class="math inline">\(Z_n\)</span> converges in distribution to a standard normal.</p>
</div>
<p>This is a partial proof since the concept of convergence in distribution has not been defined, and MGF of <span class="math inline">\(X_i\)</span> is assumed to exists, which does not need to be the case.
However, the arguments used in the proof are powerful and worth understanding.</p>
<p>‘Convergence in distribution’ means that <span class="math inline">\(Z_n\)</span> approaches normality as <span class="math inline">\(n\to \infty\)</span>.
So when <span class="math inline">\(n\)</span> is ‘large’, <span class="math inline">\(Z_n\)</span> is expected to approximate a standard normal distribution.
Transforming <span class="math inline">\(Z_n\)</span> back to the sample mean, <span class="math inline">\(\bar{X}\)</span> can be expected to approximate a <span class="math inline">\(N(\mu, \sigma^2/n)\)</span> distribution.</p>
<p>In practice, the distribution of <span class="math inline">\(\bar{X}\)</span> is sufficiently close to that of a normal distribution when <span class="math inline">\(n\)</span> is larger than about <span class="math inline">\(25\)</span> in most situations.
However, if the population distribution is severely skewed, larger samples sizes may be necessary for the approximate to be adequate in practice.</p>
<div class="importantBox important">
<p>The <em>Central Limit Theorem</em> states that, for <em>any</em> random variable <span class="math inline">\(X\)</span>, the distribution of the <em>sample means</em> will have an:</p>
<ul>
<li>approximate normal distribution,</li>
<li>with mean <span class="math inline">\(\mu\)</span> (the mean of <span class="math inline">\(X\)</span>), and</li>
<li>with a variance of <span class="math inline">\(\sigma^2/n\)</span> (where <span class="math inline">\(\sigma^2\)</span> is the variance of <span class="math inline">\(X\)</span>).</li>
</ul>
<p>The approximation improves as the sample size <span class="math inline">\(n\)</span> gets larger.</p>
<p>This applies for <em>any</em> random variable <span class="math inline">\(X\)</span>, <em>whatever its distribution</em>.
For many distributions that are not highly skewed, the approximation is reasonable for sample sizes larger than about <span class="math inline">\(20\)</span> to <span class="math inline">\(30\)</span>.
If the data comes from a normal distribution, the distribution of the sample mean <em>always</em> has a normal distribution for any <span class="math inline">\(n\)</span>.</p>
</div>
<div class="example">
<p><span id="exm:unlabeled-div-86" class="example"><strong>Example 12.11  </strong></span>In the <strong>R</strong> code below, data come from a uniform distribution (Fig. <a href="SamplingDistributions.html#fig:UniformCLT">12.7</a>, left panel).
However, the mean of samples of size <span class="math inline">\(n = 10\)</span> (centre panel) and <span class="math inline">\(n = 20\)</span> (right panel) are approximately distributed as a normal distribution.
The variance of the sample means for the larger sample size is smaller than that for <span class="math inline">\(n = 10\)</span>, and the distribution looks more like a normal distribution for the larger sample size.</p>
</div>
<div class="sourceCode" id="cb107"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># The distribution of individuals has a uniform distribution:</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">4516</span><span class="op">)</span> <span class="co"># For reproducibility</span></span>
<span><span class="va">sampleSize1</span> <span class="op">&lt;-</span> <span class="fl">10</span></span>
<span><span class="va">sampleSize2</span> <span class="op">&lt;-</span> <span class="fl">20</span></span>
<span><span class="va">sampleMeans1</span> <span class="op">&lt;-</span> <span class="cn">NULL</span></span>
<span><span class="va">sampleMeans2</span> <span class="op">&lt;-</span> <span class="cn">NULL</span></span>
<span></span>
<span><span class="va">numberOfSimulations</span> <span class="op">&lt;-</span> <span class="fl">1000</span></span>
<span><span class="kw">for</span> <span class="op">(</span><span class="va">i</span> <span class="kw">in</span> <span class="fl">1</span><span class="op">:</span><span class="va">numberOfSimulations</span><span class="op">)</span><span class="op">{</span></span>
<span>  <span class="co"># Take a sample of size 10</span></span>
<span>  <span class="va">Xs1</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Uniform.html">runif</a></span><span class="op">(</span><span class="va">sampleSize1</span>, </span>
<span>               min <span class="op">=</span> <span class="fl">0</span>,</span>
<span>               max <span class="op">=</span> <span class="fl">5</span><span class="op">)</span></span>
<span>  <span class="co"># Take a sample of size 10</span></span>
<span>  <span class="va">Xs2</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Uniform.html">runif</a></span><span class="op">(</span><span class="va">sampleSize2</span>, </span>
<span>               min <span class="op">=</span> <span class="fl">0</span>,</span>
<span>               max <span class="op">=</span> <span class="fl">5</span><span class="op">)</span></span>
<span></span>
<span>  <span class="co"># Find the sample mean</span></span>
<span>  <span class="va">sampleMeans1</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="va">sampleMeans1</span>, <span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">Xs1</span><span class="op">)</span> <span class="op">)</span></span>
<span>  <span class="va">sampleMeans2</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="va">sampleMeans2</span>, <span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">Xs2</span><span class="op">)</span> <span class="op">)</span></span>
<span><span class="op">}</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/par.html">par</a></span><span class="op">(</span>mfrow <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1</span>, <span class="fl">3</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span> x <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="op">-</span><span class="fl">1</span>, <span class="fl">0</span>, <span class="fl">0</span>, <span class="fl">5</span>, <span class="fl">5</span>, <span class="fl">6</span><span class="op">)</span>, </span>
<span>      y <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0</span>, <span class="fl">0</span>, <span class="fl">0.2</span>, <span class="fl">0.2</span>, <span class="fl">0</span>, <span class="fl">0</span><span class="op">)</span>,</span>
<span>      lwd <span class="op">=</span> <span class="fl">2</span>,</span>
<span>      las <span class="op">=</span> <span class="fl">1</span>,</span>
<span>      type <span class="op">=</span> <span class="st">"l"</span>,</span>
<span>      xlab <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/expression.html">expression</a></span><span class="op">(</span><span class="fu">italic</span><span class="op">(</span><span class="va">X</span><span class="op">)</span><span class="op">)</span>,</span>
<span>      ylab <span class="op">=</span> <span class="st">"Prob. fn"</span>,</span>
<span>      main <span class="op">=</span> <span class="st">"Distribution of the\nindividuals observations"</span><span class="op">)</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/hist.html">hist</a></span><span class="op">(</span> <span class="va">sampleMeans1</span>,</span>
<span>      las <span class="op">=</span> <span class="fl">1</span>,</span>
<span>      xlab <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/expression.html">expression</a></span><span class="op">(</span><span class="va">Sample</span><span class="op">~</span><span class="va">means</span><span class="op">~</span><span class="fu">bar</span><span class="op">(</span><span class="fu">italic</span><span class="op">(</span><span class="va">X</span><span class="op">)</span><span class="op">)</span> <span class="op">)</span>,</span>
<span>      main <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/expression.html">expression</a></span><span class="op">(</span> <span class="fu">atop</span><span class="op">(</span><span class="va">Distribution</span><span class="op">~</span><span class="va">of</span><span class="op">~</span><span class="va">sample</span><span class="op">~</span><span class="va">means</span>,</span>
<span>                              <span class="st">"for"</span><span class="op">~</span><span class="fu">italic</span><span class="op">(</span><span class="va">n</span><span class="op">)</span><span class="op">==</span><span class="fl">10</span><span class="op">)</span> <span class="op">)</span> <span class="op">)</span> </span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/hist.html">hist</a></span><span class="op">(</span> <span class="va">sampleMeans2</span>,</span>
<span>      las <span class="op">=</span> <span class="fl">1</span>,</span>
<span>      xlab <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/expression.html">expression</a></span><span class="op">(</span><span class="va">Sample</span><span class="op">~</span><span class="va">means</span><span class="op">~</span><span class="fu">bar</span><span class="op">(</span><span class="fu">italic</span><span class="op">(</span><span class="va">X</span><span class="op">)</span><span class="op">)</span> <span class="op">)</span>,</span>
<span>      main <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/expression.html">expression</a></span><span class="op">(</span> <span class="fu">atop</span><span class="op">(</span><span class="va">Distribution</span><span class="op">~</span><span class="va">of</span><span class="op">~</span><span class="va">sample</span><span class="op">~</span><span class="va">means</span>,</span>
<span>                              <span class="st">"for"</span><span class="op">~</span><span class="fu">italic</span><span class="op">(</span><span class="va">n</span><span class="op">)</span><span class="op">==</span><span class="fl">20</span><span class="op">)</span> <span class="op">)</span> <span class="op">)</span></span></code></pre></div>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:UniformCLT"></span>
<img src="12-Sampling_files/figure-html/UniformCLT-1.png" alt="The Central Limit Theorem. Left: the distribution of the individual observations. Centre: the sampling distribution of the sample mean for samples of size $n = 10$. Right: the sampling distribution of the sample mean for samples of size $n = 20$." width="100%"><p class="caption">
FIGURE 12.7: The Central Limit Theorem. Left: the distribution of the individual observations. Centre: the sampling distribution of the sample mean for samples of size <span class="math inline">\(n = 10\)</span>. Right: the sampling distribution of the sample mean for samples of size <span class="math inline">\(n = 20\)</span>.
</p>
</div>
<div class="example">
<p><span id="exm:SoftDrinkVending" class="example"><strong>Example 12.12  (CLT) </strong></span>A soft-drink vending machine is set so that the amount of drink dispensed is a random variable with a mean of <span class="math inline">\(200\,\text{mL}\)</span> and a standard deviation of <span class="math inline">\(15\,\text{mL}\)</span>.
What is the probability that the <em>average</em> amount dispensed in a random sample of size <span class="math inline">\(36\)</span> is at least <span class="math inline">\(204\,\text{mL}\)</span>?</p>
<p>If <span class="math inline">\(X\)</span> is the amount of drink dispensed in mL, then the distribution of <span class="math inline">\(X\)</span> is not given.
However, the mean and standard deviation of <span class="math inline">\(X\)</span> are given.
The distribution of the sample mean (average), <span class="math inline">\(\bar{X}\)</span>, can be approximated by the normal distribution with mean of <span class="math inline">\(\mu = 200\)</span> and standard error of <span class="math inline">\(\sigma_{\bar{X}} = \sigma/\sqrt{n} = 15/\sqrt{36} = 15/6\)</span>, according to the CLT.
That is, <span class="math inline">\(\bar{X} \sim N(200, (15/6)^2)\)</span>.
Now
<span class="math display">\[
   \Pr(\bar{X} \ge 204) \approx P_N\left(Z \ge \frac{204 - 200}{15/6}\right)
   = \Pr(Z \ge 1.6).
\]</span>
In <strong>R</strong>:</p>
<div class="sourceCode" id="cb108"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fl">1</span> <span class="op">-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">pnorm</a></span><span class="op">(</span><span class="fl">1.6</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] 0.05479929</span></span></code></pre></div>
<p>Hence, <span class="math inline">\(\Pr(\bar{X}\ge 204) \approx 0.0548\)</span>, or about <span class="math inline">\(5.5\)</span>%.
(Here, <span class="math inline">\(P_N(A)\)</span> is used to denote the probability of event <span class="math inline">\(A\)</span> involving a random variable assumed to be normal in distribution.)</p>
</div>
<div class="example">
<p><span id="exm:FairDiceSum" class="example"><strong>Example 12.13  (Throwing dice) </strong></span>Consider the experiment of throwing a fair die <span class="math inline">\(n\)</span> times where we observe the sum of the faces showing.
For <span class="math inline">\(n = 12\)</span>, find the probability that the sum of the faces is at least <span class="math inline">\(52\)</span>.</p>
<p>Let the random variable <span class="math inline">\(X_i\)</span> be the number showing on the <span class="math inline">\(i\)</span>th throw.
Then, define <span class="math inline">\(Y = X_1 + \dots + X_{12}\)</span>.
We seek <span class="math inline">\(\Pr(Y\geq 52)\)</span>.</p>
<p>In order to use Theorem <a href="SamplingDistributions.html#thm:CLT">12.13</a>, note that the event ‘<span class="math inline">\(Y\geq 52\)</span>’ is equivalent to ‘<span class="math inline">\(\bar{X}\geq 52/12\)</span>’, where <span class="math inline">\(\bar{X} = Y/12\)</span> is the mean number showing from the <span class="math inline">\(12\)</span> tosses.</p>
<p>Since the distribution of each <span class="math inline">\(X_i\)</span> is rectangular with <span class="math inline">\(\Pr(X_i = x) = 1/6\)</span> (for <span class="math inline">\(x = 1, 2, \dots, 6\)</span>), then <span class="math inline">\(\operatorname{E}[X_i] = 7/2\)</span> and <span class="math inline">\(\operatorname{var}[X_i] = 35/12\)</span>.</p>
<p>It follows that <span class="math inline">\(\operatorname{E}[\bar{X}] = 7/2\)</span> and <span class="math inline">\(\operatorname{var}[\bar{X}] = 35/(12^2)\)</span>.
Then from Theorem <a href="SamplingDistributions.html#thm:CLT">12.13</a>,
<span class="math display">\[\begin{align*}
  \Pr(Y\geq 52)
    &amp; \simeq \Pr(\bar{X}\geq 52/12)\\
    &amp;= P_N \left(Z\geq \frac{52/12 - 7/2}{\sqrt{35/144}}\right)\\
    &amp;= 1 - \Phi(1.690)\\
    &amp;= 0.0455
\end{align*}\]</span>
The probability is <em>approximately</em> <span class="math inline">\(4.5\)</span>%.</p>
</div>
<p>Example <a href="SamplingDistributions.html#exm:FairDiceSum">12.13</a> can also be solved using a generalisation of the Central Limit Theorem.
This generalisation indicates why the normal distribution plays such an important role in statistical theory.
It states that a large class of random variables converge in distribution to the standardized normal.</p>
<div class="theorem">
<p><span id="thm:ConvergeToN" class="theorem"><strong>Theorem 12.14  (Convergence to standard normal distribution) </strong></span>Let <span class="math inline">\(X_1, X_2, \dots, X_n\)</span> be a sequence of independent random variables with <span class="math inline">\(\operatorname{E}[X_i] = \mu_i\)</span>, <span class="math inline">\(\operatorname{var}[X_i] = \sigma^2_i\)</span>.
Define <span class="math inline">\(Y = a_1X_1 + a_2X_2 + \dots + a_nX_n\)</span>.</p>
<p>Then under certain general conditions, including <span class="math inline">\(n\)</span> large, <span class="math inline">\(Y\)</span> is distributed approximately <span class="math inline">\(N(\sum_i a_i\mu_i, \sum_i a^2_i\sigma^2_i)\)</span>.</p>
</div>
<div class="proof">
<p><span id="unlabeled-div-87" class="proof"><em>Proof</em>. </span>Not given.</p>
</div>
<p>Theorem <a href="SamplingDistributions.html#thm:ConvergeToN">12.14</a> makes no assumptions about the distribution of <span class="math inline">\(X_i\)</span>.
If the <span class="math inline">\(X_i\)</span> are <em>normally</em> distributed, then <span class="math inline">\(Y = \sum_{i = 1}^n a_i X_i\)</span> will have a normal distribution for any <span class="math inline">\(n\)</span>, large or small.</p>
<div class="example">
<p><span id="exm:NoiseVoltages" class="example"><strong>Example 12.14  (CLT (Voltages)) </strong></span>Suppose we have a number of independent noise voltages <span class="math inline">\(V_i\)</span> (for <span class="math inline">\(i = 1, 2, \dots, n\)</span>).
Let <span class="math inline">\(V\)</span> be the sum of the voltages, and suppose each <span class="math inline">\(V_i\)</span> is distributed <span class="math inline">\(U(0, 10)\)</span>.
For <span class="math inline">\(n = 20\)</span>, find <span class="math inline">\(\Pr(V &gt; 105)\)</span>.</p>
<p>This is an example of Theorem <a href="SamplingDistributions.html#thm:ConvergeToN">12.14</a> with each <span class="math inline">\(a_i = 1\)</span>.
To find <span class="math inline">\(\Pr(V &gt; 105)\)</span>, the distribution of <span class="math inline">\(V\)</span> must be known.</p>
<p>Since <span class="math inline">\(\operatorname{E}[V_i] = 5\)</span> and <span class="math inline">\(\operatorname{var}[V_i] = 100/12\)</span>, from Theorem <a href="SamplingDistributions.html#thm:ConvergeToN">12.14</a> <span class="math inline">\(V\)</span> has an approximate normal distribution with mean <span class="math inline">\(20\times 5 = 100\)</span> and variance <span class="math inline">\(20\times 100/12\)</span>.
That is, <span class="math inline">\(\displaystyle{\frac{V - 100}{10\sqrt{5/3}}}\)</span> is distributed <span class="math inline">\(N(0, 1)\)</span> approximately.
So
<span class="math display">\[
   \Pr(V &gt; 105)
   \approx P_N\left (Z &gt; \frac{105 - 100}{12.91}\right )
   = 1 - \Phi(0.387)
   = 0.352.
\]</span>
The probability is approximately <span class="math inline">\(35\)</span>%.</p>
</div>
</div>
<div id="NormalApproxCLT" class="section level2" number="12.7">
<h2>
<span class="header-section-number">12.7</span> The normal approximation to the binomial<a class="anchor" aria-label="anchor" href="#NormalApproxCLT"><i class="fas fa-link"></i></a>
</h2>
<p>
The normal approximation to the binomial distribution (Sect. <a href="ContinuousDistributions.html#NormalApproxBinomial">8.3.4</a> can be seen as an application of the <a href="SamplingDistributions.html#thm:CLT">Central Limit Theorem</a>.
The essential observation is that a sample proportion is a sample mean.
Consider a sequence of independent <a href="DiscreteDistributions.html#def:BernoulliTrials">Bernoulli trials</a> resulting in the random sample <span class="math inline">\(X_1, X_2, \dots, X_n\)</span> where
<span class="math display">\[
  X_i
  =
  \begin{cases}
     0  &amp;  \text{if failure}\\
     1  &amp;  \text{if success}
  \end{cases}
\]</span>
denotes whether or not the <span class="math inline">\(i\)</span>th trial is a success.
Then the sum
<span class="math display">\[
   Y = \sum_{i = 1}^n X_i
\]</span>
represents the number of successes in the <span class="math inline">\(n\)</span> trials and
<span class="math display">\[
   \bar{X}
   =
   \frac{1}{n}\sum_{i = 1}^n X_i
   =
   \frac{Y}{n}
\]</span>
is a sample mean representing the proportion or fraction of trials which are successful.
In this context, <span class="math inline">\(\bar{X}\)</span> is usually denoted by the sample proportion <span class="math inline">\(\widehat{p}\)</span>.</p>
<p>Note that <span class="math inline">\(\operatorname{E}[X_i] = p\)</span> and <span class="math inline">\(\operatorname{var}[X_i] = p(1 - p)\)</span>.
Therefore
<span class="math display">\[
   \operatorname{E}[Y] = np
   \quad\text{and}\quad
   \operatorname{var}[Y] = np(1 - p)
\]</span>
and
<span class="math display">\[
   \operatorname{E}[\bar{X}] = p
   \quad\text{and}\quad
   \operatorname{var}[\bar{X}] = \frac{p(1 - p)}{n}.
\]</span>
Theorems <a href="SamplingDistributions.html#thm:CLT">12.13</a> and <a href="SamplingDistributions.html#thm:ConvergeToN">12.14</a> are applicable to <span class="math inline">\(\bar{X}\)</span> and <span class="math inline">\(Y\)</span> respectively.
Hence
<span class="math display">\[
   \bar{X} = \widehat{p}
   \sim N\left(p, \frac{p(1 - p)}{n}\right)\text{ approximately}
\]</span>
and
<span class="math display">\[
   Y = n\widehat{p}
   \sim N(np, np(1 - p))\text{ approximately}.
\]</span></p>
</div>
<div id="exercises-1" class="section level2" number="12.8">
<h2>
<span class="header-section-number">12.8</span> Exercises<a class="anchor" aria-label="anchor" href="#exercises-1"><i class="fas fa-link"></i></a>
</h2>
<p>Selected answers appear in Sect. <a href="selected-solutions.html#AnswersChapSampling">E.12</a>.</p>
<div class="exercise">
<p><span id="exr:FPCF" class="exercise"><strong>Exercise 12.1  </strong></span>Consider the FPCF (Def. <a href="SamplingDistributions.html#def:FPC">12.4</a>).</p>
<ol style="list-style-type: decimal">
<li>What happens if the sample size is the same as the population size?
Explain why this is a sensible result.</li>
<li>Show that, for large <span class="math inline">\(n\)</span>, the FPCF is approximately <span class="math inline">\(\sqrt{ 1 - n/N}\)</span>.</li>
</ol>
</div>
<div class="exercise">
<p><span id="exr:unlabeled-div-88" class="exercise"><strong>Exercise 12.2  </strong></span>A random sample of size <span class="math inline">\(81\)</span> is taken from a population with mean <span class="math inline">\(128\)</span> and standard deviation <span class="math inline">\(6.3\)</span>.</p>
<ol style="list-style-type: decimal">
<li>What is the probability that an individual observation will fall between <span class="math inline">\(126.6\)</span> and <span class="math inline">\(129.4\)</span>?</li>
<li>What is the probability that the <strong>sample mean</strong> will fall between <span class="math inline">\(126.6\)</span> and <span class="math inline">\(129.4\)</span>?</li>
<li>What is the probability that the <strong>sample mean</strong> will <em>not</em> fall between <span class="math inline">\(126.6\)</span> and <span class="math inline">\(129.4\)</span>?</li>
</ol>
</div>
<div class="exercise">
<p><span id="exr:CLTProb" class="exercise"><strong>Exercise 12.3  </strong></span>Let <span class="math inline">\(Y_1\)</span>, <span class="math inline">\(Y_2\)</span>, <span class="math inline">\(\dots\)</span>, <span class="math inline">\(Y_n\)</span> be <span class="math inline">\(n\)</span> independent random variables, each with PDF
<span class="math display">\[
   f_Y(y) = \begin{cases}
               3y^2 &amp; \text{for $0\le y \le 1$};\\
               0 &amp; \text{otherwise}.
            \end{cases}
\]</span></p>
<ol style="list-style-type: decimal">
<li>Determine the probability that a single observation will be within one standard deviation of the population mean.</li>
<li>Determine the probability that the sample mean will be within one standard deviation of the population mean, using the <a href="SamplingDistributions.html#CentralLimitTheorem"><em>Central Limit Theorem</em></a>.</li>
<li>Determine the probability that the sample mean will be within one standard deviation of the population mean, using the <a href="SamplingDistributions.html#CentralLimitTheorem"><em>Central Limit Theorem</em></a>.</li>
</ol>
</div>
<div class="exercise">
<p><span id="exr:EggsVariance" class="exercise"><strong>Exercise 12.4  </strong></span>Suppose the weights of eggs in a dozen carton have a weight that is normally distributed with mean <span class="math inline">\(59\,\text{g}\)</span> and variance <span class="math inline">\(0.7\,\text{g}\)</span>.</p>
<ol style="list-style-type: decimal">
<li>Find the probability that, in a sample of <span class="math inline">\(20\)</span> cartons, the sample mean weight will exceed <span class="math inline">\(59.5\,\text{g}\)</span>.</li>
<li>Find the probability that a sample of twelve eggs will produce a sample variance of greater than <span class="math inline">\(1\)</span>.</li>
</ol>
</div>
<div class="exercise">
<p><span id="exr:EggsBroken" class="exercise"><strong>Exercise 12.5  </strong></span>In a carton of a dozen eggs, the number broken has a Poisson distribution with mean <span class="math inline">\(0.2\)</span>.</p>
<ol style="list-style-type: decimal">
<li>Find the probability that, in a sample of <span class="math inline">\(20\)</span> cartons, the sample mean of the number of broken eggs per carton is more than one.
(Use the <em>Central Limit Theorem</em>.)</li>
<li>Find the probability that, in any <em>single</em> carton, the probability that more than one egg is broken.</li>
</ol>
</div>
<div class="exercise">
<p><span id="exr:ContRVM" class="exercise"><strong>Exercise 12.6  </strong></span>The random variable <span class="math inline">\(M\)</span> has the following probability density function
<span class="math display">\[
    f_M(m) =
    \begin{cases}
       3m^2 &amp; \text{for $0 &lt; m &lt; 1$};\\
       0    &amp; \text{otherwise}.
    \end{cases}
\]</span>
A random sample of size <span class="math inline">\(9\)</span> is taken from the distribution, and the sample mean <span class="math inline">\(\overline{M}\)</span> is computed.</p>
<ol style="list-style-type: decimal">
<li>Compute the mean of <span class="math inline">\(M\)</span>.</li>
<li>Compute the variance of <span class="math inline">\(M\)</span>.</li>
<li>State the approximate distribution of <span class="math inline">\(\overline{M}\)</span> including the parameters.</li>
<li>Compute the probability that the sample mean will be within <span class="math inline">\(0.1\)</span> of the true mean.</li>
</ol>
</div>
<div class="exercise">
<p><span id="exr:RandomNormalSize9" class="exercise"><strong>Exercise 12.7  </strong></span>Generate a random sample of size <span class="math inline">\(n = 9\)</span> from a <span class="math inline">\(N(10, 36)\)</span> distribution hundreds of times.
Obtain the mean and variance of <span class="math inline">\(\sum X_i\)</span> and <span class="math inline">\(\overline{X}\)</span> for each sample.</p>
<ol style="list-style-type: decimal">
<li>Verify that the distribution of the sample means is approximately normally distributed as expected.
1, Explain <em>why</em> this is expected.</li>
</ol>
</div>
<div class="exercise">
<p><span id="exr:ABW" class="exercise"><strong>Exercise 12.8  </strong></span>Consider the random variable <span class="math inline">\(A\)</span>, defined as
<span class="math display">\[
    A = \frac{Z}{\sqrt{W/\nu}},
\]</span>
where <span class="math inline">\(Z\)</span> has a standard normal distribution independent of <span class="math inline">\(W\)</span>, which has a <span class="math inline">\(\chi^2\)</span> distribution with <span class="math inline">\(\nu\)</span> degrees of freedom.
Also, consider the random variable <span class="math inline">\(B\)</span>, defined as
<span class="math display">\[
    B = \frac{W_1/\nu_1}{W_2/\nu_2},
\]</span>
where <span class="math inline">\(W_1\)</span> and <span class="math inline">\(W_2\)</span> are independent <span class="math inline">\(\chi^2\)</span> variables with <span class="math inline">\(\nu_1\)</span> and <span class="math inline">\(\nu_2\)</span> degrees of freedom respectively.</p>
<ol style="list-style-type: decimal">
<li>Write down (do not derive) the distribution of <span class="math inline">\(A\)</span>, including the parameters of the distribution.</li>
<li>Write down (do not derive) the distribution of <span class="math inline">\(B\)</span>, including the parameters of the distribution.</li>
<li>Deduce that the distribution of <span class="math inline">\(A^2\)</span> is a special case of the distribution of <span class="math inline">\(B\)</span>, and state the values of the parameters for which this is true.</li>
</ol>
</div>
<div class="exercise">
<p><span id="exr:Pollution" class="exercise"><strong>Exercise 12.9  </strong></span>A manufacturing plant produces <span class="math inline">\(2\)</span> tonnes of waste product on a given day, with a standard deviation of <span class="math inline">\(0.2\)</span> tonnes per day.
Find the probability that, over a <span class="math inline">\(20\)</span> day period, the plant produces less than <span class="math inline">\(25\)</span> tonnes of waste if daily productions can be assumed independent.</p>
</div>
<div class="exercise">
<p><span id="exr:unlabeled-div-89" class="exercise"><strong>Exercise 12.10  </strong></span>Let <span class="math inline">\(Y\)</span> be the <em>change</em> in depth of a river from one day to the next measured (in cms) at a specific location.
Assume <span class="math inline">\(Y\)</span> is uniformly distributed for <span class="math inline">\(y \in [-70, 70]\)</span>.</p>
<ol style="list-style-type: decimal">
<li>Find the probability that the mean change in depth for a period of <span class="math inline">\(30\)</span> days will be greater than <span class="math inline">\(10\,\text{cm}\)</span>.</li>
<li>Use simulation to estimate the probability above.</li>
</ol>
</div>
<div class="exercise">
<p><span id="exr:Typhoid" class="exercise"><strong>Exercise 12.11  </strong></span>The number of deaths per year due to typhoid fever is assumed to have a Poisson distribution with rate <span class="math inline">\(\lambda = 4.1\)</span> per year.</p>
<ol style="list-style-type: decimal">
<li>If deaths from year to year can be assumed to be independent, what is the distribution over a <span class="math inline">\(20\)</span> year period?</li>
<li>Find the probability that there will be more than <span class="math inline">\(100\)</span> deaths due to typhoid fever in period of <span class="math inline">\(20\)</span> years.</li>
</ol>
</div>
<div class="exercise">
<p><span id="exr:Lymphocyte" class="exercise"><strong>Exercise 12.12  </strong></span>The probability that a cell is a lymphocyte is <span class="math inline">\(0.2\)</span>.</p>
<ol style="list-style-type: decimal">
<li>Write down an exact expression for the probability that in a sample containing <span class="math inline">\(150\)</span> cells that at least <span class="math inline">\(40\)</span> are lymphocytes.
Evaluate this expression using <strong>R</strong>.</li>
<li>Write down an approximate expression for this probability, and evaluate it.</li>
</ol>
</div>
<div class="exercise">
<p><span id="exr:unlabeled-div-90" class="exercise"><strong>Exercise 12.13  </strong></span>Suppose the probability of a person aged <span class="math inline">\(80+\)</span> years dying after receiving influenza vaccine is <span class="math inline">\(0.006\)</span>.
In a sample of <span class="math inline">\(200\)</span> persons aged <span class="math inline">\(80+\)</span> years:</p>
<ol style="list-style-type: decimal">
<li>Write down an exact expression for the probability that more than <span class="math inline">\(5\)</span> will die after vaccination for influenza.
Evaluate this expression using <strong>R</strong>.</li>
<li>Write down an approximate expression for this probability and evaluate it.</li>
<li>If <span class="math inline">\(4\)</span> persons died in a sample of <span class="math inline">\(200\)</span>, what conclusion would you make about the probability of dying after vaccination?
Justify your answer.</li>
</ol>
</div>
<div class="exercise">
<p><span id="exr:unlabeled-div-91" class="exercise"><strong>Exercise 12.14  </strong></span>Illustrate the Central Limit Theorem for the uniform distribution on <span class="math inline">\([-1, 1]\)</span> by simulation and repeated sampling, for various sample sizes.</p>
</div>
<div class="exercise">
<p><span id="exr:unlabeled-div-92" class="exercise"><strong>Exercise 12.15  </strong></span>Illustrate the Central Limit Theorem for the exponential distribution with mean <span class="math inline">\(1\)</span>, by simulation and repeated sampling, for various sample sizes.</p>
</div>
<div class="exercise">
<p><span id="exr:unlabeled-div-93" class="exercise"><strong>Exercise 12.16  </strong></span>Demonstrate the Central Limit Theorem using <strong>R</strong>:</p>
<ol style="list-style-type: decimal">
<li>For the uniform distribution, where <span class="math inline">\(-1 &lt; x &lt; 1\)</span> (a symmetric distribution).</li>
<li>For the exponential distribution with parameter <span class="math inline">\(1\)</span> (a skewed distribution).</li>
</ol>
</div>

</div>
</div>
<hr>
<div class="footer"><span style="color: gray; font-size:0.7em">Peter K. Dunn, 2024: <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a></span></div>
  <div class="chapter-nav">
<div class="prev"><a href="MultivariateExtensions.html"><span class="header-section-number">11</span> Expectations for multivariate distributions*</a></div>
<div class="next"><a href="OrderStatisticsChapter.html"><span class="header-section-number">13</span> Order statistcs</a></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page"><h2>On this page</h2>
      <ul class="nav navbar-nav">
<li><a class="nav-link" href="#SamplingDistributions"><span class="header-section-number">12</span> Describing samples</a></li>
<li><a class="nav-link" href="#SamplingIntro"><span class="header-section-number">12.1</span> Introduction</a></li>
<li>
<a class="nav-link" href="#from-theoretical-distributions-to-practical-observations"><span class="header-section-number">12.2</span> From theoretical distributions to practical observations</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#estimating-the-population-mean"><span class="header-section-number">12.2.1</span> Estimating the population mean</a></li>
<li><a class="nav-link" href="#EstimatePopVariance"><span class="header-section-number">12.2.2</span> Estimating the population variance</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#a-example-using-a-small-population"><span class="header-section-number">12.3</span> A example using a small population</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#some-results-for-samples"><span class="header-section-number">12.3.1</span> Some results for samples</a></li>
<li><a class="nav-link" href="#distribution-of-the-sample-mean-n-2-without-replacement"><span class="header-section-number">12.3.2</span> Distribution of the sample mean: \(n = 2\) (without replacement)</a></li>
<li><a class="nav-link" href="#distribution-of-the-sample-mean-larger-samples-without-replacement"><span class="header-section-number">12.3.3</span> Distribution of the sample mean: larger samples (without replacement)</a></li>
<li><a class="nav-link" href="#SamplingWithReplacement"><span class="header-section-number">12.3.4</span> Distribution of the sample mean: \(n = 2\) (with replacement)</a></li>
<li><a class="nav-link" href="#the-sampling-distribution-of-other-statistics"><span class="header-section-number">12.3.5</span> The sampling distribution of other statistics</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#SamplingDistributionsIntro"><span class="header-section-number">12.4</span> Sampling distributions</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#RandomSamples"><span class="header-section-number">12.4.1</span> Random sampling</a></li>
<li><a class="nav-link" href="#SamplingDistributionMean"><span class="header-section-number">12.4.2</span> The sampling distribution of the mean</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#SamplingDistributionsRelatedToNormal"><span class="header-section-number">12.5</span> Sampling distributions related to the normal distribution</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#NormalGeneralResults"><span class="header-section-number">12.5.1</span> General results</a></li>
<li><a class="nav-link" href="#ChiSquareDistribution"><span class="header-section-number">12.5.2</span> The \(\chi^2\) distribution</a></li>
<li><a class="nav-link" href="#TDistribution"><span class="header-section-number">12.5.3</span> The \(t\) distribution</a></li>
<li><a class="nav-link" href="#FDistribution"><span class="header-section-number">12.5.4</span> The \(F\) distribution</a></li>
</ul>
</li>
<li><a class="nav-link" href="#CentralLimitTheorem"><span class="header-section-number">12.6</span> The Central Limit Theorem</a></li>
<li><a class="nav-link" href="#NormalApproxCLT"><span class="header-section-number">12.7</span> The normal approximation to the binomial</a></li>
<li><a class="nav-link" href="#exercises-1"><span class="header-section-number">12.8</span> Exercises</a></li>
</ul>

      <div class="book-extra">
        <ul class="list-unstyled">
<li><a id="book-source" href="https://github.com/PeterKDunn/DistTheory/blob/main/12-Sampling.Rmd">View source <i class="fab fa-github"></i></a></li>
          <li><a id="book-edit" href="https://github.com/PeterKDunn/DistTheory/edit/main/12-Sampling.Rmd">Edit this page <i class="fab fa-github"></i></a></li>
        </ul>
</div>
    </nav>
</div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>The Theory of Statistical Distributions</strong>" was written by Peter K. Dunn. It was last built on Last updated: 2025-12-12.</p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script><script type="text/x-mathjax-config">const popovers = document.querySelectorAll('a.footnote-ref[data-toggle="popover"]');
for (let popover of popovers) {
  const div = document.createElement('div');
  div.setAttribute('style', 'position: absolute; top: 0, left:0; width:0, height:0, overflow: hidden; visibility: hidden;');
  div.innerHTML = popover.getAttribute('data-content');

  var has_math = div.querySelector("span.math");
  if (has_math) {
    document.body.appendChild(div);
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, div]);
    MathJax.Hub.Queue(function() {
      popover.setAttribute('data-content', div.innerHTML);
      document.body.removeChild(div);
    })
  }
}
</script>
</body>
</html>
