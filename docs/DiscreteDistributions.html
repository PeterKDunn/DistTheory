<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>7 Standard discrete distributions | The Theory of Statistical Distributions</title>
<meta name="author" content="Peter K. Dunn">
<meta name="description" content="Upon completion of this chapter, you should be able to: recognise the probability functions and underlying parameters of uniform, Bernoulli, binomial, geometric, negative binomial, Poisson, and...">
<meta name="generator" content="bookdown 0.45 with bs4_book()">
<meta property="og:title" content="7 Standard discrete distributions | The Theory of Statistical Distributions">
<meta property="og:type" content="book">
<meta property="og:description" content="Upon completion of this chapter, you should be able to: recognise the probability functions and underlying parameters of uniform, Bernoulli, binomial, geometric, negative binomial, Poisson, and...">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="7 Standard discrete distributions | The Theory of Statistical Distributions">
<meta name="twitter:description" content="Upon completion of this chapter, you should be able to: recognise the probability functions and underlying parameters of uniform, Bernoulli, binomial, geometric, negative binomial, Poisson, and...">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.6.0/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.6.0/bootstrap.bundle.min.js"></script><script src="libs/bs3compat-0.9.0/transition.js"></script><script src="libs/bs3compat-0.9.0/tabs.js"></script><script src="libs/bs3compat-0.9.0/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><script src="libs/kePrint-0.0.1/kePrint.js"></script><link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet">
<script src="libs/htmlwidgets-1.6.4/htmlwidgets.js"></script><script src="libs/rglWebGL-binding-1.3.31/rglWebGL.js"></script><link href="libs/rglwidgetClass-1.3.31/rgl.css" rel="stylesheet">
<script src="libs/rglwidgetClass-1.3.31/rglClass.min.js"></script><script src="libs/CanvasMatrix4-1.3.31/CanvasMatrix.min.js"></script><link rel="shortcut icon" href="icons/iconmonstr-chart-1-240.png">
<script>
    document.addEventListener('DOMContentLoaded', function() {
      // Find all R code blocks that should be toggleable.
      // Our Lua filter adds the 'r-code-box' class to the code block.
      var codeBlocks = document.querySelectorAll('.r-code-box');

      codeBlocks.forEach(function(codeBlock) {
        // Create the button element
        var button = document.createElement('button');
        button.textContent = 'Show R Code'; // Initial text for the button
        button.className = 'code-toggle-button'; // Assign CSS class

        // Insert the button directly before the code block.
        // The codeBlock's parentNode is the div.figure-with-code container.
        // We insert the button as a sibling of the codeBlock within that container.
        codeBlock.parentNode.insertBefore(button, codeBlock);

        // Hide the code block initially by default.
        codeBlock.style.display = 'none';

        // Add a click event listener to the button
        button.addEventListener('click', function() {
          if (codeBlock.style.display === 'none') {
            codeBlock.style.display = 'block'; // Show the code block
            button.textContent = 'Hide R Code'; // Change button text
          } else {
            codeBlock.style.display = 'none'; // Hide the code block
            button.textContent = 'Show R Code'; // Change button text back
          }
        });
      });
    });
    </script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- CSS --><style type="text/css">
    
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  </style>
<style type="text/css">
    /* Used with Pandoc 2.11+ new --citeproc when CSL is used */
    div.csl-bib-body { }
    div.csl-entry {
      clear: both;
          margin-bottom: 0em;
        }
    .hanging div.csl-entry {
      margin-left:2em;
      text-indent:-2em;
    }
    div.csl-left-margin {
      min-width:2em;
      float:left;
    }
    div.csl-right-inline {
      margin-left:2em;
      padding-left:1em;
    }
    div.csl-indent {
      margin-left: 2em;
    }
  </style>
<link rel="stylesheet" href="bs4_style.css">
<link rel="stylesheet" href="css/style.css">
<link rel="stylesheet" href="css/columns.css">
<link rel="stylesheet" href="html/largerDie.css">
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="">The Theory of Statistical Distributions</a>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
</form>

      <nav aria-label="Table of contents"><h2>Table of contents</h2>
        <ul class="book-toc list-unstyled">
<li><a class="" href="index.html">Preface</a></li>
<li class="book-part">Theoretical foundations</li>
<li><a class="" href="ChapterSetTheory.html"><span class="header-section-number">1</span> Essentials of set theory</a></li>
<li><a class="" href="ChapterProbability.html"><span class="header-section-number">2</span> Probability</a></li>
<li><a class="" href="DistributionRandomVariables.html"><span class="header-section-number">3</span> Random variables and their distributions</a></li>
<li><a class="" href="ChapBivariate.html"><span class="header-section-number">4</span> Bivariate distributions</a></li>
<li><a class="" href="ChapExpectation.html"><span class="header-section-number">5</span> Mathematical expectation</a></li>
<li><a class="" href="ChapterTransformations.html"><span class="header-section-number">6</span> Transformations of random variables</a></li>
<li class="book-part">Standard univariate probability distributions</li>
<li><a class="active" href="DiscreteDistributions.html"><span class="header-section-number">7</span> Standard discrete distributions</a></li>
<li><a class="" href="ContinuousDistributions.html"><span class="header-section-number">8</span> Standard continuous distributions</a></li>
<li><a class="" href="ChapterMixedDistributions.html"><span class="header-section-number">9</span> Mixed distributions</a></li>
<li class="book-part">Multivariate random variables and distributions*</li>
<li><a class="" href="ChapMultivariate.html"><span class="header-section-number">10</span> Multivariate distributions*</a></li>
<li><a class="" href="MultivariateExtensions.html"><span class="header-section-number">11</span> Expectations for multivariate distributions*</a></li>
<li class="book-part">Sampling distributions</li>
<li><a class="" href="SamplingDistributions.html"><span class="header-section-number">12</span> Describing samples</a></li>
<li><a class="" href="OrderStatisticsChapter.html"><span class="header-section-number">13</span> Order statistcs</a></li>
<li><a class="" href="BayesianIntro.html"><span class="header-section-number">14</span> Introduction to Bayesian statistics</a></li>
<li class="book-part">Appendices</li>
<li><a class="" href="SymbolsUsed.html"><span class="header-section-number">A</span> Symbols used</a></li>
<li><a class="" href="UsefulSeries.html"><span class="header-section-number">B</span> Some useful series</a></li>
<li><a class="" href="ShortRIntro.html"><span class="header-section-number">C</span> Short R introduction</a></li>
<li><a class="" href="UseRDistributions.html"><span class="header-section-number">D</span> Using R with distributions</a></li>
<li><a class="" href="selected-solutions.html"><span class="header-section-number">E</span> Selected solutions</a></li>
<li><a class="" href="references.html"><span class="header-section-number">F</span> References</a></li>
</ul>

        <div class="book-extra">
          <p><a id="book-repo" href="https://github.com/PeterKDunn/DistTheory">View book source <i class="fab fa-github"></i></a></p>
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="DiscreteDistributions" class="section level1" number="7">
<h1>
<span class="header-section-number">7</span> Standard discrete distributions<a class="anchor" aria-label="anchor" href="#DiscreteDistributions"><i class="fas fa-link"></i></a>
</h1>
<div class="objectivesBox objectives">
<p>Upon completion of this chapter, you should be able to:</p>
<ul>
<li>recognise the probability functions and underlying parameters of uniform, Bernoulli, binomial, geometric, negative binomial, Poisson, and hypergeometric random variables.</li>
<li>know the basic properties of the above discrete distributions.</li>
<li>apply these discrete distributions as appropriate to problem solving.</li>
</ul>
</div>
<div id="introduction-1" class="section level2" number="7.1">
<h2>
<span class="header-section-number">7.1</span> Introduction<a class="anchor" aria-label="anchor" href="#introduction-1"><i class="fas fa-link"></i></a>
</h2>
<p>In this chapter, some popular discrete distributions are discussed.
Properties such as definitions and applications are considered.</p>
</div>
<div id="DiscreteUniform" class="section level2" number="7.2">
<h2>
<span class="header-section-number">7.2</span> Discrete uniform distribution<a class="anchor" aria-label="anchor" href="#DiscreteUniform"><i class="fas fa-link"></i></a>
</h2>
<p>If a discrete random variable <span class="math inline">\(X\)</span> can assume <span class="math inline">\(k\)</span> different and distinct values with <em>equal</em> probability, then <span class="math inline">\(X\)</span> is said to have a <em>discrete uniform distribution</em>.
This is one of the simplest discrete distributions.</p>
<div class="definition">
<p><span id="def:DiscreteUniform" class="definition"><strong>Definition 7.1  (Discrete uniform distribution) </strong></span>If a random variable <span class="math inline">\(X\)</span> with range space <span class="math inline">\(\{a, a + 1, a + 2, \dots, b\}\)</span>, where <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> (<span class="math inline">\(a &lt; b\)</span>) are integers, has the probability function
<span class="math display" id="eq:DiscreteUniform">\[\begin{equation}
   p_X(x; a, b) = \frac{1}{b - a + 1}\text{ for $x = a, a + 1, \dots, b$}
   \tag{7.1}
\end{equation}\]</span>
then <span class="math inline">\(X\)</span> has a <em>discrete uniform distribution</em>.
We write <span class="math inline">\(X\sim U(a, b)\)</span> or <span class="math inline">\(X\sim \text{Unif}(a, b)\)</span>.</p>
</div>
<div class="importantBox important">
<p>The notation <span class="math inline">\(p_X(x; a, b)\)</span> means that the probability function for <span class="math inline">\(X\)</span> depends on the parameters <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span>.</p>
<p>The symbol <span class="math inline">\(\sim\)</span> means ‘is distributed with’; hence, ‘<span class="math inline">\(X\sim U(a, b)\)</span>’ means ‘<span class="math inline">\(X\)</span> is distributed with a discrete uniform distribution with parameters <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span>’.</p>
</div>
<p>A plot of the probability function for a discrete uniform distribution is shown in Fig. <a href="DiscreteDistributions.html#fig:DiscreteUnform">7.1</a>.</p>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:DiscreteUnform"></span>
<img src="07-SpecificDiscrete_files/figure-html/DiscreteUnform-1.png" alt="The probability function for the discrete uniform distribution $\text{Unif}(a, b)$." width="50%"><p class="caption">
FIGURE 7.1: The probability function for the discrete uniform distribution <span class="math inline">\(\text{Unif}(a, b)\)</span>.
</p>
</div>
<div class="definition">
<p><span id="def:DiscreteUniformDF" class="definition"><strong>Definition 7.2  (Discrete uniform distribution: distribution function) </strong></span>For a random variable <span class="math inline">\(X\)</span> with the uniform distribution given by the probability function in <a href="DiscreteDistributions.html#eq:DiscreteUniform">(7.1)</a>, the <em>distribution function</em> is
<span class="math display">\[
   F_X(x; a, b) =
   \begin{cases}
      0                                                        &amp; \text{for $x &lt; a$}\\
      \displaystyle \frac{\lfloor x\rfloor - a + 1}{b - a + 1} &amp; \text{for $x = a, a + 1, \dots, b$}\\
      1                                                        &amp; \text{for $x &gt; b$}
   \end{cases}
\]</span>
where <span class="math inline">\(\lfloor z \rfloor\)</span> is the <em>floor function</em> (i.e., round <span class="math inline">\(z\)</span> to the nearest integer in the direction of <span class="math inline">\(-\infty\)</span>).</p>
</div>
<div class="example">
<p><span id="exm:DiscreteUniform" class="example"><strong>Example 7.1  (Discrete uniform) </strong></span>Let <span class="math inline">\(X\)</span> be the number showing after a single throw of a fair die.
Then <span class="math inline">\(X \sim \text{Unif}(1, 6)\)</span>.</p>
<p>To select a single-digit number from a table of random digits, the number chosen, <span class="math inline">\(X\)</span>, has probability distribution <span class="math inline">\(\text{Unif}(0, 9)\)</span>.</p>
</div>
<p>The following are the basic properties of the discrete uniform distribution.</p>
<div class="theorem">
<p><span id="thm:DiscreteUniformProperties" class="theorem"><strong>Theorem 7.1  (Discrete uniform properties) </strong></span>If <span class="math inline">\(X\sim \text{Unif}(a, b)\)</span> then</p>
<ol style="list-style-type: decimal">
<li>
<span class="math inline">\(\displaystyle \operatorname{E}[X] = (a + b)/2\)</span>.</li>
<li>
<span class="math inline">\(\displaystyle \operatorname{var}[X] = \frac{(b - a)(b - a + 2)}{12}\)</span>.</li>
<li>
<span class="math inline">\(\displaystyle M_X(t) = \frac {e^{at} - e^{(b + 1)t}}{(b - a + 1)(1 - e^{t})}\)</span>.</li>
</ol>
</div>
<div class="proof">
<p><span id="unlabeled-div-26" class="proof"><em>Proof</em>. </span>The mean and variance are easier to find by working with <span class="math inline">\(Y = X - a\)</span> rather than <span class="math inline">\(X\)</span> itself (as <span class="math inline">\(Y\)</span> is defined on <span class="math inline">\(0 &lt; y &lt; (b - a)\)</span>), using that <span class="math inline">\(\operatorname{E}[X] = \operatorname{E}[Y] + a\)</span> and <span class="math inline">\(\operatorname{var}[Y] = \operatorname{var}[X]\)</span>.
Since <span class="math inline">\(Y\sim\text{Unif}(0, b - a)\)</span>:
<span class="math display">\[\begin{align*}
  \operatorname{E}[Y]
  &amp;= \sum_{y = 0}^{b - a} i\frac{1}{b - a + 1}\\
  &amp;= \frac{1}{b - a + 1}\big(0 + 1 + 2 + \dots + (b - a)\big)\\
  &amp;= \frac{(b - a)(b - a + 1)}{2(b - a + 1)} = (b - a)/2
\end{align*}\]</span>
using <a href="UsefulSeries.html#eq:SumNaturalNumbers">(B.1)</a>.
Therefore,
<span class="math display">\[
   \operatorname{E}[X]= \operatorname{E}[Y] + a = \frac{b - a}{2} + a = \frac{a + b}{2}.
\]</span>
The variance of <span class="math inline">\(Y\)</span> can be found similarly (Exercise <a href="DiscreteDistributions.html#exr:DiscreteUniformVar">7.20</a>).</p>
<p>To find the MGF:
<span class="math display">\[\begin{align*}
  M_X(t)
  &amp;= \sum_{x = a}^{b} \exp(xt) \frac{1}{b - a + 1}\\
  &amp;= \frac{1}{b - a + 1} \sum_{x = a}^{b} \exp(xt)\\
  &amp;= \frac{1}{b - a + 1} \left( \exp\{at\} + \exp\{(a + 1)t\} + \exp\{(a + 2)t\} + \dots + \exp\{bt\} \right) \\
  &amp;= \frac{\exp(at)}{b - a + 1} \left( 1 + \exp\{t\} + \exp\{2t\} + \dots + \exp\{(b - 1)t\} \right) \\
  &amp;= \frac{\exp(at)}{b - a + 1} \left( \frac{1 - \exp\{(b - a + 1)t\}}{1 - \exp(t)} \right)
\end{align*}\]</span>
using <a href="UsefulSeries.html#eq:SumGeometricFinite">(B.3)</a>.</p>
</div>
</div>
<div id="BernoulliDistribution" class="section level2" number="7.3">
<h2>
<span class="header-section-number">7.3</span> Bernoulli distribution<a class="anchor" aria-label="anchor" href="#BernoulliDistribution"><i class="fas fa-link"></i></a>
</h2>
<p>
A Bernoulli distribution is used in a situation where a single trial of a random process has two possible outcomes.
A simple example is tossing a coin and observing if a head falls.</p>
<p>The probability function is simple:
<span class="math display">\[
   p_X(x) =
   \begin{cases}
      1 - p &amp; \text{if $x = 0$};\\
      p     &amp; \text{if $x = 1$},
    \end{cases}
\]</span>
so that <span class="math inline">\(p\)</span> represents the probability of <span class="math inline">\(x = 1\)</span>, called a ‘success’ (while <span class="math inline">\(x = 0\)</span> is called a ‘failure’).
More succinctly:
<span class="math display" id="eq:BernoulliPMF">\[\begin{equation}
   p_X(x; p) = p^x (1 - p)^{1 - x} \quad\text{for $x = 0, 1$}.
   \tag{7.2}
\end{equation}\]</span></p>
<div class="definition">
<p><span id="def:BrnoulliDistribution" class="definition"><strong>Definition 7.3  (Bernoulli distribution) </strong></span>Let <span class="math inline">\(X\)</span> be the number of successes in a single trial with <span class="math inline">\(\Pr(\text{Success}) = p\)</span> (<span class="math inline">\(0\le p\le 1\)</span>).
Then <span class="math inline">\(X\)</span> has a <em>Bernoulli probability distribution</em> with parameter <span class="math inline">\(p\)</span> and probability function given by <a href="DiscreteDistributions.html#eq:BernoulliPMF">(7.2)</a>.
We write <span class="math inline">\(X\sim\text{Bern}(p)\)</span>.</p>
</div>
<div class="definition">
<p><span id="def:BrnoulliDistributionCDF" class="definition"><strong>Definition 7.4  (Bernoulli distribution: distribution function) </strong></span>For a random variable <span class="math inline">\(X\)</span> with the Bernoulli distribution given in <a href="DiscreteDistributions.html#eq:BernoulliPMF">(7.2)</a>, the <em>distribution function</em> is
<span class="math display">\[
  F_X(x; p) =
  \begin{cases}
    0     &amp; \text{if $x &lt; 0$}\\
    1 - p &amp; \text{if $0\leq x &lt; 1$}\\
    1     &amp; \text{if $x\geq 1$}.
  \end{cases}
\]</span></p>
</div>
<div class="importantBox important">
<p>The terms ‘success’ and ‘failure’ are not literal.
‘Success’ simply refers to the event of interest.
If the event of interest is whether a cyclone causes damage, this is still called a ‘success’.</p>
</div>
<p>These ideas also introduces a common idea of a <em>Bernoulli trial</em>.</p>
<div class="definition">
<p><span id="def:BernoulliTrials" class="definition"><strong>Definition 7.5  (Bernoulli trials) </strong></span>A <em>Bernoulli trial</em> is a random process with only two possible outcomes, usually labelled ‘success’ <span class="math inline">\(s\)</span> and ‘failure’ <span class="math inline">\(f\)</span>.
The sample space can be denoted by <span class="math inline">\(S = \{ s, f\}\)</span>.</p>
</div>
<p>The following are the basic properties of the Bernoulli distribution.</p>
<div class="theorem">
<p><span id="thm:BernoulliProperties" class="theorem"><strong>Theorem 7.2  (Bernoulli distribution properties) </strong></span>If <span class="math inline">\(X\sim\text{Bern}(p)\)</span> then</p>
<ol style="list-style-type: decimal">
<li>
<span class="math inline">\(\operatorname{E}[X] = p\)</span>.</li>
<li>
<span class="math inline">\(\operatorname{var}[X] = p(1 - p) = pq\)</span> where <span class="math inline">\(q = 1 - p\)</span>.</li>
<li>
<span class="math inline">\(M_X(t) = pe^t + q\)</span>.</li>
</ol>
</div>
<div class="proof">
<p><span id="unlabeled-div-27" class="proof"><em>Proof</em>. </span>By definition:
<span class="math display">\[
     \operatorname{E}[X] = \sum^1_{x = 0} x\, p_X(x) = 0\times (1 - p) + 1\times p = p.
\]</span>
To find the variance, use the computational formula <span class="math inline">\(\operatorname{var}[X] = \operatorname{E}[X^2] - \operatorname{E}[X]^2\)</span>.
Then,
<span class="math display">\[
     \operatorname{E}[X^2] = \sum^1_{x = 0} x^2\, p_X(x) = 0^2\times (1 - p) + 1^2\times p = p,
\]</span>
and so
<span class="math display">\[
   \operatorname{var}[X]
   = \operatorname{E}[X^2] - \operatorname{E}[X]^2
   = p - p^2
   = p (1- p).
\]</span></p>
<p>The MGF of <span class="math inline">\(X\)</span> is
<span class="math display">\[\begin{align*}
   M_X(t)
   &amp;= \operatorname{E}\big[\exp(tX)\big]\\
   &amp;= \sum^1_{x = 0} e^{tx} p^x q^{1 - x}\\
   &amp;= \sum^1_{x = 0}(pe^t)^x q^{1 - x}
    = pe^t + q.
\end{align*}\]</span>
Proving the third result first, and then using it to prove the others, is easier (using of the methods in Sect. <a href="ChapExpectation.html#MGFMoments">5.5.3</a>—try this as an exercise.)</p>
</div>
<p></p>
</div>
<div id="BinomialDistribution" class="section level2" number="7.4">
<h2>
<span class="header-section-number">7.4</span> Binomial distribution<a class="anchor" aria-label="anchor" href="#BinomialDistribution"><i class="fas fa-link"></i></a>
</h2>
<p>
A binomial distribution is used to model the number of successes in <span class="math inline">\(n\)</span> independent Bernoulli trials (Def. <a href="DiscreteDistributions.html#def:BernoulliTrials">7.5</a>).
A simple example is tossing a coin ten times and observing how often a head falls.
The same random process is repeated (tossing the coin), only two outcomes are possible on each trial (a head or a tail), and the probability of a head remains constant on each trial (i.e., the tosses are independent).</p>
<div id="BinomialDerivation" class="section level3" number="7.4.1">
<h3>
<span class="header-section-number">7.4.1</span> Derivation of a binomial distribution<a class="anchor" aria-label="anchor" href="#BinomialDerivation"><i class="fas fa-link"></i></a>
</h3>
<p>Consider tossing a die five times and observing the number of times a <img src="Dice/die2.png" width="11"> is rolled.
The probability of observing a <img src="Dice/die2.png" width="11"> three times can be found as follows:
In the five tosses, a <img src="Dice/die2.png" width="11"> must appear three times; there are <span class="math inline">\(\binom{5}{3}\)</span> ways of allocating on which of the five rolls they will appear.
In the five rolls, <img src="Dice/die2.png" width="11"> must appear three times with probability <span class="math inline">\(1/6\)</span>; the other two rolls must produce another number, with probability <span class="math inline">\(5/6\)</span>.
So the probability is
<span class="math display">\[
   \Pr(\text{3 ones}) = \binom{5}{3} (1/6)^3 (5/6)^2 = 0.032,
\]</span>
assuming independence of the events.
Using this approach, the PMF for the binomial distribution can be developed.</p>
<p>A binomial situation arises if a <em>sequence</em> of Bernoulli trials is observed, in each of which <span class="math inline">\(\Pr(\{ s\} ) = p\)</span> and <span class="math inline">\(\Pr(\{ f\} ) = q = 1 - p\)</span>.
For <span class="math inline">\(n\)</span> such trials, consider the random variable <span class="math inline">\(X\)</span>, where <span class="math inline">\(X\)</span> is the number of successes in <span class="math inline">\(n\)</span> trials.
Now <span class="math inline">\(X\)</span> will have value set <span class="math inline">\(\mathcal{R}_X = \{ 0, 1, 2, \dots, n\}\)</span>.
<span class="math inline">\(p\)</span> must be constant from trial to trial, and the <span class="math inline">\(n\)</span> trials must be independent.</p>
<p>Consider the event <span class="math inline">\(X = r\)</span> (where <span class="math inline">\(0\leq r\leq n\)</span>).
This could correspond to the sample point
<span class="math display">\[
   \underbrace{s \quad s \quad s \dots s \quad s \quad s \quad s}_r\quad
   \underbrace{f \quad f \dots f \quad f}_{n - r}
\]</span>
which is the intersection of <span class="math inline">\(n\)</span> independent events comprising <span class="math inline">\(r\)</span> successes and <span class="math inline">\(n - r\)</span> failures, and hence the probability is <span class="math inline">\(p^r q^{n - r}\)</span>.</p>
<p>Every other sample point in the event <span class="math inline">\(X = r\)</span> will appear as a rearrangement of these <span class="math inline">\(s\)</span>’s and <span class="math inline">\(f\)</span>’s in the sample point described above and will therefore have the same probability.</p>
<p>Now the number of distinct arrangements of the <span class="math inline">\(r\)</span> successes <span class="math inline">\(s\)</span> and <span class="math inline">\((n - r)\)</span> failures <span class="math inline">\(f\)</span> is <span class="math inline">\(\binom{n}{r}\)</span>, so
<span class="math display">\[
     \Pr(X = r) = \binom{n}{r} p^r q^{n - r}
\]</span>
for <span class="math inline">\(r = 0, 1, \dots, n\)</span>.
This is the <em>binomial distribution</em>.</p>
<p>Note that the sum of the probabilities is <span class="math inline">\(1\)</span>, as the binomial expansion of <span class="math inline">\((p + q)^n\)</span> (using <a href="UsefulSeries.html#eq:BinomialSeries">(B.4)</a>) is
<span class="math display">\[\begin{equation}
   \sum_{r = 0}^n \binom{n}{r} p^r q^{n - r} = (p + q)^n = 1\label{EQN:sumbin}
\end{equation}\]</span>
since <span class="math inline">\(p + q = 1\)</span>.</p>
</div>
<div id="BinomialDefinition" class="section level3" number="7.4.2">
<h3>
<span class="header-section-number">7.4.2</span> Definition and properties<a class="anchor" aria-label="anchor" href="#BinomialDefinition"><i class="fas fa-link"></i></a>
</h3>
<p>The definition of the binomial distribution can now be given.</p>
<div class="definition">
<p><span id="def:BinomialDistribution" class="definition"><strong>Definition 7.6  (Binomial distribution) </strong></span>Let <span class="math inline">\(X\)</span> be the number of successes in <span class="math inline">\(n\)</span> independent Bernoulli trials with <span class="math inline">\(\Pr(\text{Success}) = p\)</span> (for <span class="math inline">\(0\le p\le 1\)</span>) constant for each trial.
Then <span class="math inline">\(X\)</span> has a <em>binomial probability distribution</em> with parameters <span class="math inline">\(n\)</span>, <span class="math inline">\(p\)</span> and probability function given by
<span class="math display" id="eq:BinomialPMF">\[\begin{equation}
   p_X(x; n, p) = \binom{n}{x} p^x q^{n - x} \quad\text{for $x = 0, 1, \dots, n$}.
   \tag{7.3}
\end{equation}\]</span>
We write <span class="math inline">\(X\sim\text{Bin}(n, p)\)</span>.</p>
</div>
<p>The distribution function is complicated and is not given.
Fig. <a href="DiscreteDistributions.html#fig:BinomialExamples">7.2</a> shows the probability function for the binomial distribution for various parameter values.</p>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:BinomialExamples"></span>
<img src="07-SpecificDiscrete_files/figure-html/BinomialExamples-5.png" alt="The probability function for the binomial distribution for various values of\ $p$ and\ $n$." width="80%"><p class="caption">
FIGURE 7.2: The probability function for the binomial distribution for various values of <span class="math inline">\(p\)</span> and <span class="math inline">\(n\)</span>.
</p>
</div>
<p>The following are the basic properties of the binomial distribution.</p>
<div class="theorem">
<p><span id="thm:BinomialProperties" class="theorem"><strong>Theorem 7.3  (Binomial distribution properties) </strong></span>If <span class="math inline">\(X\sim\text{Bin}(n,p)\)</span> then</p>
<ol style="list-style-type: decimal">
<li>
<span class="math inline">\(\operatorname{E}[X] = np\)</span>.</li>
<li>
<span class="math inline">\(\operatorname{var}[X] = np(1 - p) = npq\)</span>.</li>
<li>
<span class="math inline">\(M_X(t) = (pe^t + q)^n\)</span>.</li>
</ol>
</div>
<div class="proof">
<p><span id="unlabeled-div-28" class="proof"><em>Proof</em>. </span>Using <a href="UsefulSeries.html#eq:BinomialSeries">(B.4)</a>:
<span class="math display">\[\begin{align*}
     \operatorname{E}[X]
     &amp; = \sum^n_{x = 0} x\binom{n}{x} p^x q^{n - x}\\
     &amp; = \sum^n_{x = 1} x \frac{n}{x} \binom{n - 1}{x - 1} p^x q^{n - x}
          \quad\text{(note the lower summation-index change)}\\
     &amp; = np\sum^n_{x = 1} \binom{n - 1}{x - 1} p^{x - 1} q^{n - x}\\
     &amp; = np \sum^{n - 1}_{y = 0} \binom{n - 1}{y}p^y q^{n - 1 - y}\quad \text{putting $y = x - 1$}.
\end{align*}\]</span>
The summation is <span class="math inline">\(1\)</span>, since it is equivalent to summing over all values of <span class="math inline">\(y\)</span> for the binomial probability function with <span class="math inline">\(y\)</span> successes in <span class="math inline">\((n - 1)\)</span> Bernoulli trials, and hence represents a probability function with a sum of one.
In the second line, the sum is over <span class="math inline">\(x\)</span> from <span class="math inline">\(1\)</span> to <span class="math inline">\(n\)</span> because, for <span class="math inline">\(x = 0\)</span>, the probability is multiplied by <span class="math inline">\(x = 0\)</span> and makes no contribution to the summation.
Thus,
<span class="math display">\[
   \operatorname{E}[X] = np.
\]</span></p>
<p>To find the variance, use the computational formula <span class="math inline">\(\operatorname{var}[X] = \operatorname{E}[X^2] - \operatorname{E}[X]^2\)</span>.
Firstly, to find <span class="math inline">\(\operatorname{E}[X^2]\)</span>, write <span class="math inline">\(\operatorname{E}[X^2]\)</span> as <span class="math inline">\(\operatorname{E}[X(X - 1) + X]\)</span> so that <span class="math inline">\(\operatorname{E}[X(X - 1)] + \operatorname{E}[X]\)</span>; then:
<span class="math display">\[\begin{align*}
     \operatorname{E}[X^2]
     &amp;= \sum^n_{x = 0} x(x - 1)\Pr(X = x) + np\\
     &amp;= np + \sum^n_{x = 2} x(x  -1)\frac{n(n - 1)}{x(x - 1)} \binom{n - 2}{x - 2} p^x q^{n - x}\\
     &amp;= np + \sum^n_{x = 2} n(n - 1)\binom{n - 2}{x - 2} p^x q^{n - x}\\
     &amp;= np + n(n - 1)p^2 \sum^{n - 2}_{y = 0} \binom{n - 2}{y} p^yq^{n - 2 - y},
\end{align*}\]</span>
putting <span class="math inline">\(y = x - 2\)</span>.
For the same reason as before, the summation is <span class="math inline">\(1\)</span>, so
<span class="math display">\[
   \operatorname{E}[X^2] = np + n^2 p^2 - np^2
\]</span>
and hence
<span class="math display">\[
   \operatorname{var}[X]
   = \operatorname{E}[X^2] - ([\operatorname{E}[X])^2
   = np + n^2p^2 - np^2 - n^2 p^2
   = np (1 - p).
\]</span></p>
<p>The MGF of <span class="math inline">\(X\)</span> is
<span class="math display">\[\begin{align*}
   M_X(t)
   &amp;= \operatorname{E}\big(\exp(tX)\big)\\
   &amp;= \sum^n_{x = 0} e^{tx}\binom{n}{x} p^x q^{n - x}\\
   &amp;= \sum^n_{x = 0}\binom{n}{x} (pe^t)^x q^{n - x}
    = (pe^t + q)^n.
\end{align*}\]</span>
Proving the third result first, and then using it to prove the others, is easier (using of the methods in Sect. <a href="ChapExpectation.html#MGFMoments">5.5.3</a>—try this as an exercise.)</p>
</div>
<p>Tables of binomial probabilities are commonly available, but computers (e.g., using <strong>R</strong>) can also be used to generate the probabilities.
If the number of ‘successes’ has a binomial distribution, so does the number of ‘failures’.
Specifically if <span class="math inline">\(X\sim \text{Bin}(n,p)\)</span>, then <span class="math inline">\(Y = (n - X) \sim \text{Bin}(n, 1 - p)\)</span>.</p>
<div class="softwareBox software">
<p>In <strong>R</strong>, many functions are built-in for computing the probability function, distribution function and other quantities for common distributions (see Appendix <a href="UseRDistributions.html#UseRDistributions">D</a>).</p>
<p>The four <strong>R</strong> functions for working with the binomial distribution have the form <code>[dpqr]binom(..., size, prob)</code>, where <code>prob</code><span class="math inline">\({} = p\)</span> and <code>size</code><span class="math inline">\({} = n\)</span>.
For example:</p>
<ul>
<li>The function <code>dbinom(x, size, prob)</code> computes the probability function for the binomial distribution at <span class="math inline">\(X = {}\)</span><code>x</code>;</li>
<li>The function <code>pbinom(q, size, prob)</code> computes the distribution function for the binomial distribution at <span class="math inline">\(X = {}\)</span><code>q</code>;</li>
<li>The function <code>qbinom(p, size, prob)</code> computes the quantiles of the binomial distribution function for cumulative probability <code>p</code>; and</li>
<li>The function <code>rbinom(n, size, prob)</code> generates <code>n</code> random numbers from the given binomial distribution.</li>
</ul>
</div>
<div class="example">
<p><span id="exm:ThrowDice" class="example"><strong>Example 7.2  (Throwing dice) </strong></span>A die is thrown <span class="math inline">\(4\)</span> times.
To find the probability of rolling exactly <span class="math inline">\(2\)</span> sixes, see that there are <span class="math inline">\(n = 4\)</span> Bernoulli trials with <span class="math inline">\(p = 1/6\)</span>.
Let the random variable <span class="math inline">\(X\)</span> be the number of 6’s in <span class="math inline">\(4\)</span> tosses.
Then
<span class="math display">\[
  \Pr(X = 2) =
  \binom{4}{2} \left(\frac{1}{6}\right)^2\left(\frac{5}{6}\right)^2 = 150/1296 \approx 0.1157.
\]</span>
In <strong>R</strong>:</p>
<div class="sourceCode" id="cb47"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/stats/Binomial.html">dbinom</a></span><span class="op">(</span><span class="fl">2</span>,</span>
<span>       prob <span class="op">=</span> <span class="fl">1</span><span class="op">/</span><span class="fl">6</span>,</span>
<span>       size <span class="op">=</span> <span class="fl">4</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] 0.1157407</span></span></code></pre></div>
</div>
<div class="importantBox important">
<p>A binomial situation requires the trials to be <em>independent</em>, and the probability of success <span class="math inline">\(p\)</span> to be <em>constant</em> throughout the trials.</p>
<p>For example, drawing cards from a pack <em>without</em> replacing them is <strong>not</strong> a binomial situation; after drawing one card, the probabilities will then change for the drawing of the next card.
In this case, the <em>hypergeometric distribution</em> should be used (Sect. <a href="DiscreteDistributions.html#HypergeometricDistribution">7.8</a>).</p>
</div>
<div class="example">
<p><span id="exm:unlabeled-div-29" class="example"><strong>Example 7.3  (Freezing lake) </strong></span>Based on <span class="citation">Daniel S. Wilks (<a href="references.html#ref-BIB:Wilks:statistical">1995a</a>)</span> (p. 68), the probability that Cayuga Lake freezes can be modelled by a binomial distribution with <span class="math inline">\(p = 0.05\)</span>.</p>
<p>Using this information, the number of times the lake will <em>not</em> freeze in <span class="math inline">\(n = 10\)</span> randomly chosen years in given by the random variable <span class="math inline">\(X\)</span>, where <span class="math inline">\(X \sim \text{Bin}(10, 0.95)\)</span>.
The probability that the lake will not freeze in any of these ten years is <span class="math inline">\(\Pr(X = 10) = \binom{10}{10} 0.95^{10} 0.05^{0} \approx 0.599\)</span>, or about 60%.</p>
<p>In <strong>R</strong>:</p>
<div class="sourceCode" id="cb48"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/stats/Binomial.html">dbinom</a></span><span class="op">(</span><span class="fl">0</span>,</span>
<span>       size <span class="op">=</span> <span class="fl">10</span>,</span>
<span>       prob <span class="op">=</span> <span class="fl">0.05</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] 0.5987369</span></span></code></pre></div>
<p>Note we could define the random variable <span class="math inline">\(Y\)</span> as the number of times the lake <em>will</em> freeze in the ten randomly chosen years.
Then, <span class="math inline">\(Y\sim\text{Bin}(10, 0.05)\)</span> and we would compute <span class="math inline">\(\Pr(Y = 0)\)</span> and get the same answer.</p>
<div class="sourceCode" id="cb49"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/stats/Binomial.html">dbinom</a></span><span class="op">(</span><span class="fl">10</span>,</span>
<span>       size <span class="op">=</span> <span class="fl">10</span>,</span>
<span>       prob <span class="op">=</span> <span class="fl">0.95</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] 0.5987369</span></span></code></pre></div>
</div>
<p>Binomial probabilities can sometimes be approximated using the normal distribution (Sect. <a href="ContinuousDistributions.html#NormalApproxBinomial">8.3.4</a>) or the Poisson distribution (Sect. <a href="DiscreteDistributions.html#PoissonBinomial">7.7.3</a>).
</p>
</div>
</div>
<div id="GeometricDistribution" class="section level2" number="7.5">
<h2>
<span class="header-section-number">7.5</span> Geometric distribution<a class="anchor" aria-label="anchor" href="#GeometricDistribution"><i class="fas fa-link"></i></a>
</h2>
<div id="GeometricDerivation" class="section level3" number="7.5.1">
<h3>
<span class="header-section-number">7.5.1</span> Derivation of a geometric distribution<a class="anchor" aria-label="anchor" href="#GeometricDerivation"><i class="fas fa-link"></i></a>
</h3>
<p>
Consider now a random process where independent Bernoulli trials (Def. <a href="DiscreteDistributions.html#def:BernoulliTrials">7.5</a>) are repeated until the <em>first</em> success occurs.
What is the distribution of the number of <em>failures</em> until the first success is observed?</p>
<p>Let the random variable <span class="math inline">\(X\)</span> be the number of failures before the first success is observed.
Since the first success may occur on the first trial, or second trial or third trial, and so on, <span class="math inline">\(X\)</span> is a random variable with range space <span class="math inline">\(\{0, 1, 2, 3, \dots\}\)</span> with no (theoretical) upper limit.</p>
<p>Since the probability of failure is <span class="math inline">\(q = 1 - p\)</span> and the probability of success is <span class="math inline">\(p\)</span>, the probability of <span class="math inline">\(x\)</span> failures before the first success is
<span class="math display">\[\begin{align*}
   \Pr(\text{$x$ failures})          &amp;\times \Pr(\text{first success}) \\
   q^x                               &amp;\times  p.
\end{align*}\]</span>
This derivation assumes the events are independent.</p>
</div>
<div id="GeometricDefinition" class="section level3" number="7.5.2">
<h3>
<span class="header-section-number">7.5.2</span> Definition and properties<a class="anchor" aria-label="anchor" href="#GeometricDefinition"><i class="fas fa-link"></i></a>
</h3>
<p>The definition of the geometric distribution can now be given.</p>
<div class="definition">
<p><span id="def:GeometricDistribution" class="definition"><strong>Definition 7.7  (Geometric distribution) </strong></span>A random variable <span class="math inline">\(X\)</span> has a <em>geometric distribution</em> if the probability function of <span class="math inline">\(X\)</span> is
<span class="math display" id="eq:GeometricPMF">\[\begin{equation}
   p_X(x; p) =  (1 - p)^x p = q^x p\quad\text{for $x = 0, 1, 2, \dots$}
   \tag{7.4}
\end{equation}\]</span>
where <span class="math inline">\(q = 1 - p\)</span> and <span class="math inline">\(0 &lt; p &lt; 1\)</span> is the parameter of the distribution.
We write <span class="math inline">\(X\sim\text{Geom}(p)\)</span>.</p>
</div>
<div class="definition">
<p><span id="def:GeometricDistributionDF" class="definition"><strong>Definition 7.8  (Geometric distribution: distribution function) </strong></span>For a random variable <span class="math inline">\(X\)</span> with the geometric distribution given in <a href="DiscreteDistributions.html#eq:GeometricPMF">(7.4)</a>, the distribution function is
<span class="math display">\[
  F_X(x; p) =
  \begin{cases}
    0                                  &amp; \text{for $x &lt; 0$}\\
    1 - (1 - p)^{\lfloor x\rfloor + 1} &amp; \text{for $x\ge 0$},
  \end{cases}
\]</span>
where <span class="math inline">\(\lfloor z \rfloor\)</span> is the <em>floor function</em>.</p>
</div>
<p>This is the parameterisation used by <strong>R</strong>.
The probability function for a geometric distribution for various values of <span class="math inline">\(p\)</span> is shown in Fig. <a href="DiscreteDistributions.html#fig:Geometric">7.3</a>.
The following are the basic properties of the geometric distribution.</p>
<div class="theorem">
<p><span id="thm:GeometricProperties" class="theorem"><strong>Theorem 7.4  (Geometric distribution properties) </strong></span>If <span class="math inline">\(X\sim\text{Geom}(p)\)</span> as defined in Eq. <a href="DiscreteDistributions.html#eq:GeometricPMF">(7.4)</a>, then</p>
<ol style="list-style-type: decimal">
<li>
<span class="math inline">\(\operatorname{E}[X] = (1 - p)/p\)</span>.</li>
<li>
<span class="math inline">\(\operatorname{var}[X] = (1 - p)/p^2\)</span>.</li>
<li>
<span class="math inline">\(M_X(t) = p/\{1 - (1 - p)e^t\}\)</span> for <span class="math inline">\(t &lt; -\log(1 - p)\)</span>.</li>
</ol>
</div>
<div class="proof">
<p><span id="unlabeled-div-30" class="proof"><em>Proof</em>. </span>The first two result can be proven directly but proving the third result, and using the MGF to prove the first two, is easier.
This is left as an exercise (Ex. <a href="DiscreteDistributions.html#exr:GeometricPropertiesProof">7.21</a>).</p>
</div>
<div class="softwareBox software">
<p>The four <strong>R</strong> functions for working with the geometric distribution have the form <code>[dpqr]geom(prob)</code>, where <code>prob</code><span class="math inline">\({} = p\)</span> (see Appendix <a href="UseRDistributions.html#UseRDistributions">D</a>).</p>
</div>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:Geometric"></span>
<img src="07-SpecificDiscrete_files/figure-html/Geometric-1.png" alt="The probability function for the geometric distribution for $p = 0.1$, $0.3$, $0.5$ and $0.8$." width="80%"><p class="caption">
FIGURE 7.3: The probability function for the geometric distribution for <span class="math inline">\(p = 0.1\)</span>, <span class="math inline">\(0.3\)</span>, <span class="math inline">\(0.5\)</span> and <span class="math inline">\(0.8\)</span>.
</p>
</div>
<div class="example">
<p><span id="exm:GeometricNetball" class="example"><strong>Example 7.4  (Netball shooting) </strong></span>Suppose a netball goal shooter has a probability of <span class="math inline">\(p = 0.2\)</span> of <em>missing</em> a goal.
In this case a ‘success’ refers to <em>missing</em> the shot with <span class="math inline">\(p = 0.2\)</span></p>
<p>Let <span class="math inline">\(X\)</span> be the number of shots made till the first miss.
Then, <span class="math inline">\(X = \{1, 2, 3, \dots \}\)</span>, since the first miss may occur on the first shot, or any subsequent shot.
<em>This is <strong>not</strong> the parameterisation used by <strong>R</strong></em>.</p>
<p>Instead, let <span class="math inline">\(Y\)</span> be the number of goals <em>scored</em> until the shooter first <em>misses</em>.
Notice that <span class="math inline">\(Y = \{0, 1, 2, \dots \}\)</span> so this parameterisation <em>does</em> correspond to that used by <strong>R</strong>, and <span class="math inline">\(Y\sim \text{Geom}(0.2)\)</span>.</p>
<p>The probability that the shooter makes <span class="math inline">\(4\)</span> goals before the first miss is:</p>
<div class="sourceCode" id="cb50"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/stats/Geometric.html">dgeom</a></span><span class="op">(</span><span class="fl">4</span>, prob <span class="op">=</span> <span class="fl">0.2</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] 0.08192</span></span></code></pre></div>
<p>The probability that the shooter makes <span class="math inline">\(4\)</span> or fewer goals before the first miss is <span class="math inline">\(\Pr(Y = 0, 1, 2, 3, \text{or } 4)\)</span>:</p>
<div class="sourceCode" id="cb51"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/stats/Geometric.html">pgeom</a></span><span class="op">(</span><span class="fl">4</span>, prob <span class="op">=</span> <span class="fl">0.2</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] 0.67232</span></span></code></pre></div>
<p>The expected number of shots until the first miss is <span class="math inline">\(\operatorname{E}[Y] = (1 - p)/p  = 4\)</span>.</p>
</div>
<p>The parameterisation above is for the <em>number of failures</em> needed for the first success, so that <span class="math inline">\(x = 0, 1, 2, 3, \dots\)</span>.
An alternative parameterisation is for the <em>number of trials</em> <span class="math inline">\(X\)</span> before observing a success, when <span class="math inline">\(x = 1, 2, \dots\)</span>.
The only way to distinguish which parameterisation is being used is to check the range space or the probability function.</p>
<div class="definition">
<p><span id="def:GeometricDistributionALT" class="definition"><strong>Definition 7.9  (Geometric distribution: Alternative parameterisation) </strong></span>A random variable <span class="math inline">\(X\)</span> has a <em>geometric distribution</em> if the probability function of <span class="math inline">\(X\)</span> is
<span class="math display" id="eq:GeometricPMFAlternative">\[\begin{equation}
   p_X(x) =  (1 - p)^{x - 1}p = q^{x - 1} p\quad\text{for $x = 1, 2, \dots$}
   \tag{7.5}
\end{equation}\]</span>
where <span class="math inline">\(q = 1 - p\)</span> and <span class="math inline">\(0 &lt; p &lt; 1\)</span> is the parameter of the distribution.
We write <span class="math inline">\(X\sim\text{Geom}(p)\)</span>.</p>
</div>
<div class="theorem">
<p><span id="thm:GeometricPropertiesALT" class="theorem"><strong>Theorem 7.5  (Geometric distribution properties) </strong></span>If <span class="math inline">\(X\sim\text{Geom}(p)\)</span> as defined in Eq. <a href="DiscreteDistributions.html#eq:GeometricPMFAlternative">(7.5)</a>, then</p>
<ol style="list-style-type: decimal">
<li>
<span class="math inline">\(\operatorname{E}[X] = 1/p\)</span>.</li>
<li>
<span class="math inline">\(\operatorname{var}[X] = (1 - p)/p^2\)</span>.</li>
<li>
<span class="math inline">\(M_X(t) = p\exp(t) /\{ 1 - (1 - p)e^t \}\)</span> for <span class="math inline">\(t &lt; -\log(1 - p)\)</span>.</li>
</ol>
</div>
<div class="proof">
<p><span id="unlabeled-div-31" class="proof"><em>Proof</em>. </span>The first two result can be proven from Theorem <a href="DiscreteDistributions.html#thm:GeometricProperties">7.4</a>.</p>
</div>
<p></p>
</div>
</div>
<div id="NegativeBinomialDistribution" class="section level2" number="7.6">
<h2>
<span class="header-section-number">7.6</span> Negative binomial distribution<a class="anchor" aria-label="anchor" href="#NegativeBinomialDistribution"><i class="fas fa-link"></i></a>
</h2>
<div id="NegBinStandard" class="section level3" number="7.6.1">
<h3>
<span class="header-section-number">7.6.1</span> Derivation: standard parameterisation<a class="anchor" aria-label="anchor" href="#NegBinStandard"><i class="fas fa-link"></i></a>
</h3>
<p>
Consider a random process where independent Bernoulli trials are repeated until the <span class="math inline">\(r\)</span>th success occurs.
Let the random variable <span class="math inline">\(X\)</span> be the number of <em>failures</em> before the <span class="math inline">\(r\)</span>th success is observed, so that <span class="math inline">\(X = 0, 1, 2, \dots\)</span>.</p>
<p>To observe the <span class="math inline">\(r\)</span>th success after <span class="math inline">\(x\)</span> failures, <span class="math inline">\(x\)</span> failures and <span class="math inline">\(r - 1\)</span> successes are observed first in these <span class="math inline">\(x + r - 1\)</span> trials.
There are <span class="math inline">\(\binom{x + r - 1}{r - 1}\)</span> ways in which to allocate these successes to the first <span class="math inline">\(x + r - 1\)</span> trials.
Each of the <span class="math inline">\(r - 1\)</span> successes occur with probability <span class="math inline">\(p\)</span>, and the <span class="math inline">\(x\)</span> failures with probability <span class="math inline">\(1 - p\)</span> (assuming events are independent).
Hence the probability of observing the <span class="math inline">\(r\)</span>th success in trial <span class="math inline">\(x\)</span> is
<span class="math display">\[\begin{align*}
   \text{No. ways}
   &amp;\times \Pr\big(\text{$x$ failures}\big) \times \Pr(\text{$r - 1$ successes}) \\
   \binom{x + r - 1}{r - 1}
   &amp;\times (1 - p)^{x} \times p^{r - 1}.
\end{align*}\]</span></p>
</div>
<div id="NegBinAlternative" class="section level3" number="7.6.2">
<h3>
<span class="header-section-number">7.6.2</span> Definition and properties: standard parameterisation<a class="anchor" aria-label="anchor" href="#NegBinAlternative"><i class="fas fa-link"></i></a>
</h3>
<p>The standard definition of the negative binomial distribution can now be given.</p>
<div class="definition">
<p><span id="def:NegativeBinomialDistribution" class="definition"><strong>Definition 7.10  (Negative binomial distribution) </strong></span>A random variable <span class="math inline">\(X\)</span> with probability function
<span class="math display" id="eq:NegativeBinomialPMF">\[\begin{equation}
   p_X(x; p, r) = \binom{x + r - 1}{r - 1}(1 - p)^{x} p^{r - 1}
\quad\text{for $x = 0, 1, 2, \dots$}
   \tag{7.6}
\end{equation}\]</span>
has a <em>negative binomial distribution</em> with parameters <span class="math inline">\(r\)</span> (an positive integer) and <span class="math inline">\(p\)</span> (<span class="math inline">\(0\le p\le 1\)</span>).
We write <span class="math inline">\(X\sim\text{NB}(r, p)\)</span>.</p>
</div>
<p>The distribution function is complicated and is not given.
The following are the basic properties of the negative binomial distribution.</p>
<div class="theorem">
<p><span id="thm:NegBinomialProperties" class="theorem"><strong>Theorem 7.6  (Negative binomial properties) </strong></span>If <span class="math inline">\(X\sim \text{NB}(r, p)\)</span> with the probability function in Eq. <a href="DiscreteDistributions.html#eq:NegativeBinomialPMF">(7.6)</a> then</p>
<ol style="list-style-type: decimal">
<li>
<span class="math inline">\(\operatorname{E}[X] = \{r(1 - p)\}/p\)</span>.</li>
<li>
<span class="math inline">\(\operatorname{var}[X] = r(1 - p)/p^2\)</span>.</li>
<li>
<span class="math inline">\(M_X(t) = \left[ p / \{1 - (1 - p)\exp(t)\} \right]^r\)</span> for <span class="math inline">\(t &lt; -\log p\)</span>.</li>
</ol>
</div>
<div class="proof">
<p><span id="unlabeled-div-32" class="proof"><em>Proof</em>. </span>Proving the third statement is left as an exercise.
Then the first two are then derived from the MGF.</p>
</div>
<div class="example">
<p><span id="exm:Demonstration" class="example"><strong>Example 7.5  (Negative binomial) </strong></span>A telephone marketer invites customers, over the phone, to a product demonstration.
Ten people are needed for the demonstration.
The probability that a randomly-chosen person accepts the invitation is only <span class="math inline">\(0.15\)</span>.</p>
<p>Here, a ‘success’ is an acceptance to attend the demonstration.
Let <span class="math inline">\(Y\)</span> be the number of <em>failed</em> calls necessary to secure ten acceptances.
Then <span class="math inline">\(Y\)</span> has a negative binomial distribution with <span class="math inline">\(p = 0.15\)</span> and <span class="math inline">\(r = 10\)</span>.
The mean number of failures made will is <span class="math inline">\(\operatorname{E}[Y] = \{r(1 - p)\}/p = \{10 \times (1 - 0.15)\}/0.15 \approx  56.66667\)</span>.</p>
<p>Hence, including the ten calls that leads to a person accepting the invitation, the mean number of calls to be made will is <span class="math inline">\(10 + \operatorname{E}[Y] = 66.66\dots\)</span>.</p>
</div>
<p>Since the parameterisation in <a href="DiscreteDistributions.html#eq:NegativeBinomialPMF">(7.6)</a> is defined over the non-negative integers, it is often used to model count data.</p>
<div class="softwareBox software">
<p>The four <strong>R</strong> functions for working with the negative binomial distribution are based on the paramaterisation in Eq. <a href="DiscreteDistributions.html#eq:NegativeBinomialPMF">(7.6)</a>, and have the form <code>[dpqr]nbinom(size, prob)</code>, where <code>prob</code> is <span class="math inline">\(p\)</span> and <code>size</code> is <span class="math inline">\(r\)</span> (see Appendix <a href="UseRDistributions.html#UseRDistributions">D</a>).</p>
</div>
<div class="linkBox link">
<p>When <span class="math inline">\(r = 1\)</span>, the negative binomial distribution is the same as the geometric distribution: the geometric distribution is a special case of the negative binomial.</p>
</div>
<p>The probability function for the negative binomial distribution for various values of <span class="math inline">\(p\)</span> and <span class="math inline">\(r\)</span> is shown in Fig. <a href="DiscreteDistributions.html#fig:NegativeBinomial">7.4</a>.</p>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:NegativeBinomial"></span>
<img src="07-SpecificDiscrete_files/figure-html/NegativeBinomial-1.png" alt="The probability function for the negative binomial distribution for $p = 0.2$ and $0.7$ and $r = 1$ and $r = 3$." width="80%"><p class="caption">
FIGURE 7.4: The probability function for the negative binomial distribution for <span class="math inline">\(p = 0.2\)</span> and <span class="math inline">\(0.7\)</span> and <span class="math inline">\(r = 1\)</span> and <span class="math inline">\(r = 3\)</span>.
</p>
</div>
<div class="example">
<p><span id="exm:DemonstrationNB" class="example"><strong>Example 7.6  (Negative binomial) </strong></span>Consider Example <a href="DiscreteDistributions.html#exm:Demonstration">7.5</a>, concerning a telephone marketer inviting customers, over the phone, to a product demonstration.
Ten people are needed for the demonstration.
The probability that a randomly chosen person accepts the invitation is only <span class="math inline">\(0.15\)</span>.</p>
<p>Consider finding the probability that the marketer will need to make more than <span class="math inline">\(100\)</span> calls to secure ten acceptances.
In this situation, a ‘success’ is an acceptance to attend the demonstration.
Let <span class="math inline">\(Y\)</span> be the number of failed calls before securing ten acceptances.
Then <span class="math inline">\(Y\)</span> has a negative binomial distribution such that <span class="math inline">\(Y\sim\text{NBin}(p = 0.15, r = 10)\)</span>.</p>
<p>To determine <span class="math inline">\(\Pr(Y &gt; 100) = 1 - \Pr(Y\le 100)\)</span>, using a computer is the easiest approach.
In <strong>R</strong>, the command <code><a href="https://rdrr.io/r/stats/NegBinomial.html">dnbinom()</a></code> returns probabilities from the probability function of the negative binomial distribution, and <code><a href="https://rdrr.io/r/stats/NegBinomial.html">pnbinom()</a></code> returns the cumulative distribution probabilities:</p>
<div class="sourceCode" id="cb52"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">x.values</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/seq.html">seq</a></span><span class="op">(</span><span class="fl">1</span>, <span class="fl">100</span>, </span>
<span>                by <span class="op">=</span> <span class="fl">1</span><span class="op">)</span></span>
<span><span class="fl">1</span> <span class="op">-</span> <span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/NegBinomial.html">dnbinom</a></span><span class="op">(</span> <span class="va">x.values</span>, </span>
<span>                 size <span class="op">=</span> <span class="fl">10</span>, </span>
<span>                 prob <span class="op">=</span> <span class="fl">0.15</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] 0.02442528</span></span>
<span><span class="co"># Alternatively:</span></span>
<span><span class="fl">1</span> <span class="op">-</span> <span class="fu"><a href="https://rdrr.io/r/stats/NegBinomial.html">pnbinom</a></span><span class="op">(</span><span class="fl">100</span>, </span>
<span>            size <span class="op">=</span> <span class="fl">10</span>, </span>
<span>            prob <span class="op">=</span> <span class="fl">0.15</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] 0.02442528</span></span></code></pre></div>
<p>The probability is about <span class="math inline">\(0.0244\)</span>.</p>
<p>Assuming each call take an average of <span class="math inline">\(5\)</span> minutes, we can determine how long is the marketer expected to be calling to find ten acceptances.
Let <span class="math inline">\(T\)</span> be the time to make the calls in minutes.
Then <span class="math inline">\(T = 5Y\)</span>.
Hence, <span class="math inline">\(\operatorname{E}[T] = 5\operatorname{E}[Y] = 5 \times 66.7 = 333.5\)</span>, or about <span class="math inline">\(5.56\,\text{h}\)</span>.</p>
<p>Assume that each call costs <span class="math inline">\(25\)</span> cents, and that the company pays the marketer $<span class="math inline">\(30\)</span> per hour.
To determine the total cost, let <span class="math inline">\(C\)</span> be the total cost in dollars.
The cost of employing the marketer is, on average, <span class="math inline">\(30\times 5.56 = \$166.75\)</span>.
Then <span class="math inline">\(C = 0.25 Y + 166.75\)</span>, so <span class="math inline">\(\operatorname{E}[C] = 0.25 \operatorname{E}[Y] + 166.75 = C = 0.25 \times 66.7 + 166.75 = \$183.43\)</span>.</p>
</div>
</div>
<div id="alternative-parameterisations" class="section level3" number="7.6.3">
<h3>
<span class="header-section-number">7.6.3</span> Alternative parameterisations<a class="anchor" aria-label="anchor" href="#alternative-parameterisations"><i class="fas fa-link"></i></a>
</h3>
<p>
Often, a different parameterisation for the negative binomial distribution; see Exercise <a href="DiscreteDistributions.html#exr:NegativeBinomialALT">7.14</a>.
However, the parameterisation presented in Sect. <a href="DiscreteDistributions.html#NegBinStandard">7.6.1</a> is used by <strong>R</strong>.</p>
<p>The negative binomial distribution can be extended so that <span class="math inline">\(r\)</span> can be <em>any</em> positive number, not just an integer.
When <span class="math inline">\(r\)</span> is non-integer, the above interpretations are lost, but the distribution is more flexible.
Relaxing the restriction on <span class="math inline">\(r\)</span> gives the probability function as
<span class="math display" id="eq:NegativeBinomialPMFAlternative2">\[\begin{equation}
   p_X(x; p, r) = \frac{\Gamma(x + r)}{\Gamma(r)\, x!} p^r (1 - p)^x,
   \tag{7.7}
\end{equation}\]</span>
for <span class="math inline">\(x = 0, 1, 2 \dots\)</span> and <span class="math inline">\(r &gt; 0\)</span>.
In this expression, <span class="math inline">\(\Gamma(r)\)</span> is the <em>gamma function</em> (Def. <a href="DiscreteDistributions.html#def:GammaFunction">7.11</a>), and is related to factorials when <span class="math inline">\(r\)</span> is integer: <span class="math inline">\(\Gamma(r) = (r - 1)!\)</span> if <span class="math inline">\(r\)</span> is a positive integer.</p>
<div class="definition">
<p><span id="def:GammaFunction" class="definition"><strong>Definition 7.11  (Gamma function) </strong></span>The function <span class="math inline">\(\Gamma(\cdot)\)</span> is called the <em>gamma function</em> and is defined as
<span class="math display">\[
   \Gamma(r) = \int_0^\infty x^{r - 1}\exp(-x)\, dx
\]</span>
for <span class="math inline">\(r &gt; 0\)</span>.</p>
</div>
<p>The gamma function has the property that
<span class="math display" id="eq:gammaprop">\[\begin{equation}
   \Gamma(r) = (r - 1)!
   \tag{7.8}
\end{equation}\]</span>
if <span class="math inline">\(r\)</span> is a positive integer (Fig. <a href="DiscreteDistributions.html#fig:GammaFunction">7.5</a>).
Important properties of the gamma function are given below.</p>
<div class="theorem">
<p><span id="thm:GammaFunctionProperties" class="theorem"><strong>Theorem 7.7  (Gamma function properties) </strong></span>For the gamma function <span class="math inline">\(\Gamma(\cdot)\)</span>,</p>
<ol style="list-style-type: decimal">
<li>
<span class="math inline">\(\Gamma(r) = (r - 1)\Gamma(r - 1)\)</span> if <span class="math inline">\(r &gt; 0\)</span>.</li>
<li>
<span class="math inline">\(\Gamma(1/2) = \sqrt{\pi}\)</span>.</li>
<li>
<span class="math inline">\(\lim_{r\to 0} \to \infty\)</span>.</li>
<li>
<span class="math inline">\(\Gamma(1) = \Gamma(2) = 1\)</span>.</li>
<li>
<span class="math inline">\(\Gamma(n) = (n - 1)\)</span> for all positive integers <span class="math inline">\(n\)</span>.</li>
</ol>
</div>
<div class="proof">
<p><span id="unlabeled-div-33" class="proof"><em>Proof</em>. </span>For the first property, integration by parts gives
<span class="math display">\[\begin{align*}
   \Gamma(r)
   &amp;= \left. -\exp(-x) \frac{x^2}{r}\right|_0^\infty + \int_0^\infty \exp(-x) (r - 1) x^{r - 2}\,dx\\
   &amp;= 0 + \ (r - 1) \int_0^{\infty}  e^{-x}  x^{r - 2} \, dx\\
   &amp;= (r - 1) \Gamma(r - 1).
\end{align*}\]</span></p>
</div>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:GammaFunction"></span>
<img src="07-SpecificDiscrete_files/figure-html/GammaFunction-1.png" alt="The gamma function is like the factorial function but has a continuous argument. The line corresponds to the gamma function $\Gamma(z)$; the solid points correspond to the factorial $(z - 1)! = \Gamma(z)$ for integer $z$." width="75%"><p class="caption">
FIGURE 7.5: The gamma function is like the factorial function but has a continuous argument. The line corresponds to the gamma function <span class="math inline">\(\Gamma(z)\)</span>; the solid points correspond to the factorial <span class="math inline">\((z - 1)! = \Gamma(z)\)</span> for integer <span class="math inline">\(z\)</span>.
</p>
</div>
<div class="example">
<p><span id="exm:Computing" class="example"><strong>Example 7.7  (Negative binomial) </strong></span>Consider a computer system that fails with probability <span class="math inline">\(p = 0.02\)</span> on any given day (so that the system failure is a ‘success’).
Suppose after five failures, the system is upgraded.</p>
<p>To find the probability that an upgrade will happen within one year, let <span class="math inline">\(D\)</span> be the number of days before the fifth failure.
So, we seek <span class="math inline">\(\Pr(X &lt; 360)\)</span>.
Using <strong>R</strong>:</p>
<div class="sourceCode" id="cb53"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/stats/NegBinomial.html">pnbinom</a></span><span class="op">(</span><span class="fl">360</span>, </span>
<span>        size <span class="op">=</span> <span class="fl">5</span>, </span>
<span>        prob <span class="op">=</span> <span class="fl">0.02</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] 0.8553145</span></span></code></pre></div>
<p>So the probability of upgrading within one year is about <span class="math inline">\(85\)</span>%.</p>
</div>
<div class="example">
<p><span id="exm:NBMites" class="example"><strong>Example 7.8  (Mites) </strong></span><span class="citation">Bliss (<a href="references.html#ref-BIB:Bliss:negbin">1953</a>)</span> gives data from the counts of adult European red mites on leaves selected at random from six similar apple trees (Table <a href="DiscreteDistributions.html#tab:MitesTable">7.1</a>).
The mean number of mites per leaf is <span class="math inline">\(1.14\)</span> with a variance of <span class="math inline">\(3.57\)</span>; since the Poisson distribution has an equal mean and variance, the Poisson distribution may not model these count data well.
However, the mean number of mites per leaf can be modelled using a negative binomial distribution with <span class="math inline">\(r = 1.18\)</span> and <span class="math inline">\(p = 0.5\)</span>.</p>
<p>The estimated probability function for both the Poisson and negative binomial distributions are given in Table <a href="DiscreteDistributions.html#tab:MitesTable">7.1</a>; the negative binomial distribution fits better as expected.</p>
</div>
<div class="inline-table"><table class="table table-sm">
<caption>
<span id="tab:MitesTable">TABLE 7.1: </span>Counts of mites on leaves selected at random from six similar apple trees.
</caption>
<thead>
<tr>
<th style="border-bottom:hidden;padding-bottom:0; padding-left:3px;padding-right:3px;text-align: center; font-weight: bold; " colspan="1">
<div style="">
Number of
</div>
</th>
<th style="border-bottom:hidden;padding-bottom:0; padding-left:3px;padding-right:3px;text-align: center; font-weight: bold; " colspan="1">
<div style="">
Number of leaves
</div>
</th>
<th style="border-bottom:hidden;padding-bottom:0; padding-left:3px;padding-right:3px;text-align: center; font-weight: bold; " colspan="1">
<div style="">
Empirical
</div>
</th>
<th style="border-bottom:hidden;padding-bottom:0; padding-left:3px;padding-right:3px;text-align: center; font-weight: bold; " colspan="1">
<div style="">
Poisson
</div>
</th>
<th style="border-bottom:hidden;padding-bottom:0; padding-left:3px;padding-right:3px;text-align: center; font-weight: bold; " colspan="1">
<div style="">
Negative binomial
</div>
</th>
</tr>
<tr>
<th style="text-align:center;">
mites per leaf
</th>
<th style="text-align:center;">
with that many mites
</th>
<th style="text-align:center;">
probability
</th>
<th style="text-align:center;">
probability
</th>
<th style="text-align:center;">
probability
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center;font-weight: bold;">
0
</td>
<td style="text-align:center;font-weight: bold;">
70
</td>
<td style="text-align:center;font-weight: bold;">
0.467
</td>
<td style="text-align:center;font-weight: bold;">
0.318
</td>
<td style="text-align:center;font-weight: bold;">
0.449
</td>
</tr>
<tr>
<td style="text-align:center;">
1
</td>
<td style="text-align:center;">
38
</td>
<td style="text-align:center;">
0.253
</td>
<td style="text-align:center;">
0.364
</td>
<td style="text-align:center;">
0.261
</td>
</tr>
<tr>
<td style="text-align:center;">
2
</td>
<td style="text-align:center;">
17
</td>
<td style="text-align:center;">
0.113
</td>
<td style="text-align:center;">
0.209
</td>
<td style="text-align:center;">
0.140
</td>
</tr>
<tr>
<td style="text-align:center;">
3
</td>
<td style="text-align:center;">
10
</td>
<td style="text-align:center;">
0.067
</td>
<td style="text-align:center;">
0.080
</td>
<td style="text-align:center;">
0.073
</td>
</tr>
<tr>
<td style="text-align:center;">
4
</td>
<td style="text-align:center;">
9
</td>
<td style="text-align:center;">
0.060
</td>
<td style="text-align:center;">
0.023
</td>
<td style="text-align:center;">
0.038
</td>
</tr>
<tr>
<td style="text-align:center;">
5
</td>
<td style="text-align:center;">
3
</td>
<td style="text-align:center;">
0.020
</td>
<td style="text-align:center;">
0.005
</td>
<td style="text-align:center;">
0.019
</td>
</tr>
<tr>
<td style="text-align:center;">
6
</td>
<td style="text-align:center;">
2
</td>
<td style="text-align:center;">
0.013
</td>
<td style="text-align:center;">
0.001
</td>
<td style="text-align:center;">
0.010
</td>
</tr>
<tr>
<td style="text-align:center;">
7
</td>
<td style="text-align:center;">
1
</td>
<td style="text-align:center;">
0.007
</td>
<td style="text-align:center;">
0.000
</td>
<td style="text-align:center;">
0.005
</td>
</tr>
<tr>
<td style="text-align:center;">
8+
</td>
<td style="text-align:center;">
0
</td>
<td style="text-align:center;">
0.000
</td>
<td style="text-align:center;">
0.000
</td>
<td style="text-align:center;">
0.000
</td>
</tr>
</tbody>
</table></div>
<p></p>
</div>
</div>
<div id="PoissonDistribution" class="section level2" number="7.7">
<h2>
<span class="header-section-number">7.7</span> Poisson distribution<a class="anchor" aria-label="anchor" href="#PoissonDistribution"><i class="fas fa-link"></i></a>
</h2>
<p>
The Poisson distribution is very commonly used to model the number of occurrences of an event which occurs randomly in time or space.
The Poisson distribution arises as a result of assumptions made about a random process:</p>
<ul>
<li>Events that occur in one time-interval (or region) are independent of those occurring in any other non-overlapping time-interval (or region).</li>
<li>The probability that an event occurs in a small time-interval is proportional to the length of the interval.</li>
<li>The probability that <span class="math inline">\(2\)</span> or more events occur in a very small time-interval is so small that it is negligible.</li>
</ul>
<p>Whenever these assumptions are valid, or approximately so, the Poisson distribution is appropriate.
Many natural phenomena fall into this category.</p>
<div id="PoissonDerivation" class="section level3" number="7.7.1">
<h3>
<span class="header-section-number">7.7.1</span> Derivation of Poisson distribution<a class="anchor" aria-label="anchor" href="#PoissonDerivation"><i class="fas fa-link"></i></a>
</h3>
<p>The binomial distribution (Sect. <a href="DiscreteDistributions.html#BinomialDistribution">7.4</a>) applies in situations where an event may occur a certain number of times in a fixed number of trials, say <span class="math inline">\(n\)</span>.
What if <span class="math inline">\(n\)</span> gets increasingly large though, so effectively there is no upper limit?
Mathematically, we might say: What happens if <span class="math inline">\(n\to\infty\)</span>?</p>
<p>Let’s find out.
Begin with the binomial probability function
<span class="math display" id="eq:BinomialPMF2">\[\begin{equation}
   p_X(x; n, p) = \binom{n}{x} p^x (1 - p)^{n - x} \quad\text{for $x = 0, 1, \dots, n$}
   \tag{7.9}
\end{equation}\]</span>
which has a mean of <span class="math inline">\(\lambda = np\)</span>.
First, re-writing <a href="DiscreteDistributions.html#eq:BinomialPMF2">(7.9)</a> in terms of the mean <span class="math inline">\(\lambda\)</span>:
<span class="math display" id="eq:BinomialPMF2Mean">\[\begin{equation}
   p_X(x; n, \lambda)
   =
   \binom{n}{x} \left(\frac{\lambda}{n}\right)^x \left(1 - \frac{\lambda}{n}\right)^{n - x} \quad\text{for $x = 0, 1, \dots, n$}.
   \tag{7.10}
\end{equation}\]</span>
Now consider the case <span class="math inline">\(n \to \infty\)</span> (for <span class="math inline">\(x = 0, 1, \dots\)</span>):
<span class="math display">\[\begin{align}
   \lim_{n\to\infty}
   p_X(x; n, \lambda)
   &amp;=
   \lim_{n\to\infty} \binom{n}{x}
   \left(\frac{\lambda}{n}\right)^x \left(1 - \frac{\lambda}{n}\right)^{n - x}\\
   &amp;= \frac{\lambda^x}{x!}
   \lim_{n\to\infty} \frac{n!}{(n - x)!} \frac{1}{n^x} \left(1 - \frac{\lambda}{n}\right)^{n}  \left(1 - \frac{\lambda}{n}\right)^{-x}.
\end{align}\]</span>
Now, since <em>the limit of product of functions is equal to product of their limits</em>, each component can be considered in turn.</p>
<p>Looking at the first component, see that
<span class="math display">\[\begin{align}
  \lim_{n\to\infty} \frac{n!}{(n - x)!} \times \frac{1}{n^x}
  &amp;= \lim_{n\to\infty} \frac{n \times (n - 1) \times (n - 2) \times\cdots\times 2 \times 1}
                            {(n - x)\times (n - x - 1) \times \cdots \times 2 \times 1} \times \frac{1}{n^x}\\
  &amp;= \lim_{n\to\infty}\frac{ \overbrace{n \times (n - 1) \times \cdots\times (n - x + 1)}^{\text{$x$ terms}}}
                           { \underbrace{n \times n \times\cdots \times n\times n}_{\text{$x$ terms}}}\\
  &amp;= \lim_{n\to\infty} \frac{n}{n} \times \frac{n - 1}{n} \times \frac{n - 2}{n} \times\cdots\times \frac{n - x + 1}{n}\\
  &amp;= 1 \times 1\times\cdots\times 1 = 1.
\end{align}\]</span>
For the next component, see that (using <a href="UsefulSeries.html#eq:ExponentialLimit">(B.10)</a>):
<span class="math display">\[
  \lim_{n\to\infty} \left(1 - \frac{\lambda}{n}\right)^{n} = \exp(-\lambda).
\]</span>
For the last term:
<span class="math display">\[
  \lim_{n\to\infty} \left(1 - \frac{\lambda}{n}\right)^{-x} = 1^{-x} = 1.
\]</span>
Putting the pieces together:
<span class="math display">\[\begin{align}
   \lim_{n\to\infty}
   p_X(x; n, \lambda)
   &amp;=
   \frac{\lambda^x}{x!}
   \lim_{n\to\infty} \frac{n!}{(n - x)!} \frac{1}{n^x}
   \left(1 - \frac{\lambda}{n}\right)^{n}  \left(1 - \frac{\lambda}{n}\right)^{-x}\\
   p_X(x; \lambda)
   &amp;= \frac{\lambda^k}{x!}
   1 \times \exp(-\lambda) \times 1\\
   &amp;= \frac{\exp(-\lambda) \lambda^x}{x!},
\end{align}\]</span>
for <span class="math inline">\(x = 0, 1, 2, \dots\)</span>.
This is the probability function for the <em>Poisson distribution</em>.</p>
<p>A <em>Poisson process</em> refers to events that occur at a <em>rate</em> <span class="math inline">\(\lambda\)</span>, but occur at random.</p>
</div>
<div id="PoissonDefinition" class="section level3" number="7.7.2">
<h3>
<span class="header-section-number">7.7.2</span> Definition and properties<a class="anchor" aria-label="anchor" href="#PoissonDefinition"><i class="fas fa-link"></i></a>
</h3>
<p>The definition of the Poisson distribution can now be given.</p>
<div class="definition">
<p><span id="def:PoissonDistribution" class="definition"><strong>Definition 7.12  (Poisson distribution) </strong></span>A random variable <span class="math inline">\(X\)</span> is said to have a <em>Poisson distribution</em> if its probability function is
<span class="math display" id="eq:PoissonPMF">\[\begin{equation}
   p_X(x; \lambda) = \frac{\exp(-\lambda) \lambda^x}{x!}\quad \text{for $x = 0, 1, 2, \dots$}
   \tag{7.11}
\end{equation}\]</span>
where the parameter is <span class="math inline">\(\lambda &gt; 0\)</span>.
We write <span class="math inline">\(X\sim\text{Pois}(\lambda)\)</span>.</p>
</div>
<div class="definition">
<p><span id="def:PoissonDistributionDF" class="definition"><strong>Definition 7.13  (Poisson distribution: distribution function) </strong></span>For a random variable <span class="math inline">\(X\)</span> with the Poisson distribution given in <a href="DiscreteDistributions.html#eq:PoissonPMF">(7.11)</a>, the distribution function is
<span class="math display">\[
  F_X(x; \lambda) =
  \begin{cases}
    0                                  &amp; \text{for $x &lt; 0$}\\
    \displaystyle \exp(-\lambda) \sum _{i = 0}^{\lfloor x \rfloor }{\frac {\lambda ^{i}}{i!}} &amp; \text{for $x\ge 0$};\\
  \end{cases}
\]</span>
where <span class="math inline">\(\lfloor z \rfloor\)</span> is the <em>floor function</em>.</p>
</div>
<p>The PMF for a Poisson distribution for different values of <span class="math inline">\(\lambda\)</span> is shown in Fig. <a href="DiscreteDistributions.html#fig:PoissonExamples">7.6</a>.</p>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:PoissonExamples"></span>
<img src="07-SpecificDiscrete_files/figure-html/PoissonExamples-1.png" alt="The probability function for the Poisson distribution for various values of $\lambda$." width="80%"><p class="caption">
FIGURE 7.6: The probability function for the Poisson distribution for various values of <span class="math inline">\(\lambda\)</span>.
</p>
</div>
<p>The following are the basic properties of the Poisson distribution.</p>
<div class="theorem">
<p><span id="thm:PoissonProperties" class="theorem"><strong>Theorem 7.8  (Poisson distribution properties) </strong></span>If <span class="math inline">\(X\sim\text{Pois}(\lambda)\)</span> then</p>
<ol style="list-style-type: decimal">
<li>
<span class="math inline">\(\operatorname{E}[X] = \lambda\)</span>.</li>
<li>
<span class="math inline">\(\operatorname{var}[X] = \lambda\)</span>.</li>
<li>
<span class="math inline">\(M_X(t) = \exp[ \lambda\{\exp(t) - 1\}]\)</span>.</li>
</ol>
</div>
<div class="proof">
<p><span id="unlabeled-div-34" class="proof"><em>Proof</em>. </span>The third result is proven, then the other results follow:
<span class="math display">\[\begin{align*}
   M_X(t) = \operatorname{E}[e^{tX}]
     &amp;= \sum^\infty_{x = 0} e^{xt} e^{-\lambda} \lambda^x / x!\\
     &amp;= e^{-\lambda} \sum^\infty_{x = 0} \frac{(\lambda\, e^t)^x}{x!} \\
     &amp;= e^{-\lambda} \left[ 1 + \lambda\, e^t + \frac{(\lambda\,e^t)^2}{2!} + \dots \right]\\
     &amp;= e^{-\lambda} e^{\lambda\, e^t}\\
     &amp;= e^{-\lambda (1 - e^t)},
\end{align*}\]</span>
using <a href="UsefulSeries.html#eq:Exponential">(B.8)</a>.
The first two results follow from differentiating the MGF.</p>
</div>
<p>Notice that the Poisson distribution has the variance equal to the mean.
The negative binomial distribution has two parameters and the Poison only one, so often the negative binomial distribution produces a better fit to data.</p>
<div class="softwareBox software">
<p>The four <strong>R</strong> functions for working with the Poisson distribution have the form <code>[dpqr]poiss(lambda)</code> (see Appendix <a href="UseRDistributions.html#UseRDistributions">D</a>).</p>
</div>
<div class="example">
<p><span id="exm:Queues" class="example"><strong>Example 7.9  (Queuing) </strong></span>Customers enter a service line ‘at random’ at a rate of 4 per minute.
Assume that the number entering the line in any given time interval has a Poisson distribution.
To determine the probability that at least one customer enters the line in a given <span class="math inline">\(\frac{1}{2}\)</span>-minute interval, use (since <span class="math inline">\(\lambda = 2\)</span> over half-hour):
<span class="math display">\[
   \Pr(X\geq 1) = 1 - \Pr(X = 0) = 1 - e^{-2} = 0.865.
\]</span>
In <strong>R</strong>:</p>
<div class="sourceCode" id="cb54"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fl">1</span> <span class="op">-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Poisson.html">dpois</a></span><span class="op">(</span><span class="fl">0</span>,</span>
<span>          lambda <span class="op">=</span> <span class="fl">2</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] 0.8646647</span></span></code></pre></div>
</div>
<div class="example">
<p><span id="exm:Bombs" class="example"><strong>Example 7.10  (Bomb hits) </strong></span><span class="citation">Clarke (<a href="references.html#ref-clarke1946application">1946</a>)</span> (quoted in <span class="citation">Hand et al. (<a href="references.html#ref-data:hand:handbook">1996</a>)</span>, Dataset 289) discusses the number of flying bomb hits on London during World War II in a <span class="math inline">\(36\)</span> square kilometre area of South London.
The area was gridded into <span class="math inline">\(0.25\)</span> km squares and the number of bombs falling in each grid was counted (Table <a href="DiscreteDistributions.html#tab:BombTable">7.2</a>).
Assuming random hits, the Poisson distribution can be used to model the data using <span class="math inline">\(\lambda = 0.93\)</span>.
The probabilityfunction for the Poisson distribution can be compared to the empirical probabilities computed above; see Table <a href="DiscreteDistributions.html#tab:BombTable">7.2</a>.
For example, the probability of zero hits is
<span class="math display">\[
   \frac{\exp(-0.93) (0.93)^0}{0!} \approx 0.39.
\]</span>
The two probabilities are very close; the Poisson distribution fits the data very well.</p>
</div>
<div class="inline-table"><table class="table table-sm">
<caption>
<span id="tab:BombTable">TABLE 7.2: </span>The number of flying bomb hits on London during World War II in a <span class="math inline">\(36\)</span> square kilometre area of South London. The proportion of the <span class="math inline">\(576\)</span> grid squares receiving <span class="math inline">\(0, 1, ...\)</span> hits was also computed. The observed and empirical probabilities are shown. The Poisson distribution fits the data well.
</caption>
<thead><tr>
<th style="text-align:right;">
Hits
</th>
<th style="text-align:right;">
Number bombs
</th>
<th style="text-align:right;">
Proportion bombs
</th>
<th style="text-align:right;">
Poisson proportion
</th>
</tr></thead>
<tbody>
<tr>
<td style="text-align:right;font-weight: bold;">
0
</td>
<td style="text-align:right;font-weight: bold;">
229
</td>
<td style="text-align:right;font-weight: bold;">
0.3976
</td>
<td style="text-align:right;font-weight: bold;">
0.3946
</td>
</tr>
<tr>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
211
</td>
<td style="text-align:right;">
0.3663
</td>
<td style="text-align:right;">
0.3669
</td>
</tr>
<tr>
<td style="text-align:right;">
2
</td>
<td style="text-align:right;">
93
</td>
<td style="text-align:right;">
0.1615
</td>
<td style="text-align:right;">
0.1706
</td>
</tr>
<tr>
<td style="text-align:right;">
3
</td>
<td style="text-align:right;">
35
</td>
<td style="text-align:right;">
0.0608
</td>
<td style="text-align:right;">
0.0529
</td>
</tr>
<tr>
<td style="text-align:right;">
4
</td>
<td style="text-align:right;">
7
</td>
<td style="text-align:right;">
0.0122
</td>
<td style="text-align:right;">
0.0123
</td>
</tr>
<tr>
<td style="text-align:right;">
5
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0.0000
</td>
<td style="text-align:right;">
0.0023
</td>
</tr>
<tr>
<td style="text-align:right;">
6
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0.0000
</td>
<td style="text-align:right;">
0.0004
</td>
</tr>
<tr>
<td style="text-align:right;">
7
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
0.0017
</td>
<td style="text-align:right;">
0.0000
</td>
</tr>
</tbody>
</table></div>
</div>
<div id="PoissonBinomial" class="section level3" number="7.7.3">
<h3>
<span class="header-section-number">7.7.3</span> Relationship to the binomial distribution<a class="anchor" aria-label="anchor" href="#PoissonBinomial"><i class="fas fa-link"></i></a>
</h3>
<p>Given the derivation of the Poisson distribution (Sect. <a href="DiscreteDistributions.html#PoissonDerivation">7.7.1</a>), a relationship between the Poisson and binomial distributions should come as no surprise.</p>
<p>Computing binomial probabilities is tedious if the number of trials <span class="math inline">\(n\)</span> is very large and the probability of success in a single trial is very small.
For example, consider <span class="math inline">\(X\sim \text{Bin}(n = 2000, p = 0.005)\)</span>; the probability function is
<span class="math display">\[
   p_X(x) = \binom{2000}{x}(0.005)^x 0.995^{2000 - x}.
\]</span>
Using the PMF, computing some probabilities, such as <span class="math inline">\(\Pr(X &gt; 101)\)</span>, is tedious.
(Try computing <span class="math inline">\(\binom{2000}{102}\)</span> on your calculator, for example.)
However, the Poisson distribution can be used to <em>approximate</em> this probability.</p>
<p>Set the Poisson mean to equal the binomial mean (that is, <span class="math inline">\(\lambda = \mu = np\)</span>).
Since <span class="math inline">\(\operatorname{E}[Y] = \operatorname{var}[Y] = \lambda\)</span> for the Poisson distribution, this means the variance is also set to <span class="math inline">\(\sigma^2 = np\)</span>.
Of course, the binomial mean is <span class="math inline">\(np(1 - p)\)</span>, so this can only be (approximately) true if <span class="math inline">\(p\)</span> is close to zero (and so <span class="math inline">\(1 - p\approx 1\)</span>).</p>
<p>A general guideline is that the Poisson distribution can be used to approximate the binomial when <span class="math inline">\(n\)</span> is large (recall, the derivation was for the case <span class="math inline">\(n\to\infty\)</span>), <span class="math inline">\(p\)</span> is small and <span class="math inline">\(np \le 7\)</span>.</p>
<div class="linkBox link">
<p>If the random variable <span class="math inline">\(X\)</span> has the binomial distribution <span class="math inline">\(X \sim \text{Bin}(n, p)\)</span>, the probability function can be approximated by the Poisson distribution <span class="math inline">\(X \sim \text{Pois}(\lambda)\)</span>, where <span class="math inline">\(\lambda = np\)</span>.
The approximation is good when <span class="math inline">\(n\)</span> is large, <span class="math inline">\(p\)</span> is small, and <span class="math inline">\(np\le 7\)</span>.</p>
</div>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:BinomialPoisson"></span>
<img src="07-SpecificDiscrete_files/figure-html/BinomialPoisson-1.png" alt="The Poisson distribution is an excellent approximation to the binomial distribution when $p$ is small and $n$ is large. The binomial probability function is shown using empty circles; the Poisson probability function using crosses." width="80%"><p class="caption">
FIGURE 7.7: The Poisson distribution is an excellent approximation to the binomial distribution when <span class="math inline">\(p\)</span> is small and <span class="math inline">\(n\)</span> is large. The binomial probability function is shown using empty circles; the Poisson probability function using crosses.
</p>
</div>
</div>
<div id="PoissonExtensions" class="section level3" number="7.7.4">
<h3>
<span class="header-section-number">7.7.4</span> Extensions<a class="anchor" aria-label="anchor" href="#PoissonExtensions"><i class="fas fa-link"></i></a>
</h3>
<p>The Poisson distribution is commonly used to model independent counts.
However, sometimes these counts explicitly <em>exclude</em> a count of zero.
For example, consider modelling the number of nights patients spend in hospital; patients must spend at least one night to be in the data.
Then, the probability functions can be expressed as
<span class="math display">\[
   p_X(x; \lambda) = \frac{\exp(-\lambda) \lambda^{x - 1}}{(x - 1)!}\quad   \text{for $x = 1, 2, 3, \dots$}
\]</span>
where the parameter is <span class="math inline">\(\lambda &gt; 0\)</span>.
This is called the <em>zero-truncated Poisson distribution</em>.
In this case, <span class="math inline">\(\operatorname{E}[X] = \lambda + 1\)</span> and <span class="math inline">\(\operatorname{var}[X] = \lambda\)</span> (Ex. <a href="DiscreteDistributions.html#exr:PoissonZeroTruncated">7.22</a>).</p>
<p>In some cases, the random variable is a count, but has an upper limit on the possible number of counts.
This is the <em>truncated Poisson distribution</em>.</p>
<p>Other situations exist where the the proportions of zeros exceed the proportions expected by the Poisson distribution (e.g., <span class="math inline">\(\exp(-\lambda)\)</span>), but the Poisson distributions seems to otherwise be suitable.
In these situations, the <em>zero-inflated Poisson distribution</em> may be suitable.
</p>
</div>
</div>
<div id="HypergeometricDistribution" class="section level2" number="7.8">
<h2>
<span class="header-section-number">7.8</span> Hypergeometric distribution<a class="anchor" aria-label="anchor" href="#HypergeometricDistribution"><i class="fas fa-link"></i></a>
</h2>
<div id="HypergeometricDerivation" class="section level3" number="7.8.1">
<h3>
<span class="header-section-number">7.8.1</span> Derivation of a hypergeometric distribution<a class="anchor" aria-label="anchor" href="#HypergeometricDerivation"><i class="fas fa-link"></i></a>
</h3>
<p>
When the selection of items a fixed number of times is done <em>with</em> replacement, the probability of an item being selected stays the same and the binomial distribution can be used.
However, when the selection of items is done <em>without</em> replacement, the trials are not independent, making the binomial model unsuitable.
In these situations, a hypergeometric distribution is appropriate.</p>
<p>Consider a simple example.
A bag contains six light-coloured balls and four dark-coloured balls (Fig. <a href="DiscreteDistributions.html#fig:HypergeometricHelp">7.8</a>).
The variable of interest, say <span class="math inline">\(X\)</span>, is the number of light-coloured balls drawn in three random selections from the bag, without replacing the balls.
Since the balls are not replaced, <span class="math inline">\(\Pr(\text{draw a light-coloured ball})\)</span> is not constant, and so the binomial distribution cannot be used.</p>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:HypergeometricHelp"></span>
<img src="07-SpecificDiscrete_files/figure-html/HypergeometricHelp-1.png" alt="Drawing balls from a bag." width="75%"><p class="caption">
FIGURE 7.8: Drawing balls from a bag.
</p>
</div>
<p>The probabilities can be computed, however, using the counting ideas from Chap. <a href="ChapterProbability.html#Probability">2.4</a>.
There are a total of <span class="math inline">\(\binom{10}{3}\)</span> ways of selecting a sample of size <span class="math inline">\(k = 3\)</span> from the bag.
Consider the case <span class="math inline">\(X = 0\)</span>.
The number of ways of drawing no light-coloured balls is <span class="math inline">\(\binom{6}{0}\)</span> and the number of ways of drawing the three dark-coloured balls in <span class="math inline">\(\binom{4}{3}\)</span>, so the probability is
<span class="math display">\[
   \Pr(X = 0) = \frac{\binom{6}{0}\binom{4}{3}}{\binom{10}{3}} \approx 0.00833.
\]</span>
Likewise, the number of ways to draw one light-coloured ball (and hence two dark-coloured balls; Fig. <a href="DiscreteDistributions.html#fig:HypergeometricHelp">7.8</a>) is <span class="math inline">\(\binom{6}{1}\times \binom{4}{2}\)</span>, so
<span class="math display">\[
   \Pr(X = 1) = \frac{ \binom{6}{1}\times \binom{4}{2} }{\binom{10}{3}}.
\]</span>
Similarly,
<span class="math display">\[
   \Pr(X = 2) =\frac{ \binom{6}{2}\times \binom{4}{1} }{\binom{10}{3}}
   \quad\text{and}\quad
   \Pr(X = 3) =\frac{ \binom{6}{3}\times \binom{4}{0} }{\binom{10}{3}}.
\]</span></p>
</div>
<div id="HypergeometricDefinition" class="section level3" number="7.8.2">
<h3>
<span class="header-section-number">7.8.2</span> Definition and properties<a class="anchor" aria-label="anchor" href="#HypergeometricDefinition"><i class="fas fa-link"></i></a>
</h3>
<p>In general, suppose a bag contains <span class="math inline">\(N\)</span> balls, and <span class="math inline">\(m\)</span> of them are light-coloured and <span class="math inline">\(n\)</span> and <em>not</em> light-coloured; then <span class="math inline">\(N = m + n\)</span>.
Suppose a sample of size <span class="math inline">\(k\)</span> is drawn from the bag <em>without replacement</em>; then the probability of finding <span class="math inline">\(x\)</span> light-coloured balls in the sample of size <span class="math inline">\(k\)</span> is
<span class="math display">\[
   \Pr(X = x)
   = \frac{ \binom{m}{x}{ \binom{n}{k - x}}}{\binom{N}{k}}
   = \frac{ \binom{m}{x}{ \binom{n}{k - x}}}{\binom{m + n}{k}}
\]</span>
where <span class="math inline">\(X\)</span> is the number of light-coloured balls in sample of size <span class="math inline">\(k\)</span>.
(In the example, <span class="math inline">\(k = 3\)</span>, <span class="math inline">\(m = 6\)</span> and <span class="math inline">\(N = 10\)</span>.)</p>
<p>In the formula, <span class="math inline">\(\binom{m}{x}\)</span> is the number of ways of selecting <span class="math inline">\(x\)</span> light-coloured balls from the <span class="math inline">\(m\)</span> light-coloured balls in the bag; <span class="math inline">\(\binom{n}{k - x}\)</span> is the number of ways of selecting all the remaining <span class="math inline">\(k - x\)</span> to be the other colour (and there are <span class="math inline">\(n = N - m\)</span> of those in the bag); and <span class="math inline">\(\binom{N}{k}\)</span> is the number of ways of selecting a sample of size <span class="math inline">\(k\)</span> if there are <span class="math inline">\(N\)</span> balls in the bag in total.</p>
<div class="definition">
<p><span id="def:HypergeometricDistribution" class="definition"><strong>Definition 7.14  (Hypergeometric distribution) </strong></span>Consider a set of <span class="math inline">\(N = m + n\)</span> items of which <span class="math inline">\(m\)</span> are of one kind (call them ‘successes’) and other <span class="math inline">\(n = N - m\)</span> are of another kind (call them ‘failures’).
We are interested in the probability of <span class="math inline">\(x\)</span> successes in <span class="math inline">\(k\)</span> trials, when the selection (or drawing) is made <em>without</em> replacement.
Then the random variable <span class="math inline">\(X\)</span> is said to have a <em>hypergeometric distribution</em> with probability function
<span class="math display" id="eq:HypergeometricPMF">\[\begin{equation}
   p_X(x; n, m, k) = \frac{ \binom{m}{x}\binom{n}{k - x}}{\binom{m + n}{k}}
   \tag{7.12}
\end{equation}\]</span>
where <span class="math inline">\(\max(0, k - n) \le x \le \min(n, m)\)</span>.</p>
</div>
<p>The distribution function is complicated and is not given.
The following are the basic properties of the hypergeometric distribution.</p>
<div class="theorem">
<p><span id="thm:HypergeomatricDistributionProperties" class="theorem"><strong>Theorem 7.9  (Hypergeometric distribution properties) </strong></span>If <span class="math inline">\(X\)</span> has a hypergeometric distribution with probability function <a href="DiscreteDistributions.html#eq:HypergeometricPMF">(7.12)</a>, then (writing <span class="math inline">\(N = m + n\)</span>)</p>
<ol style="list-style-type: decimal">
<li>
<span class="math inline">\(\operatorname{E}[X] = km/N\)</span>.</li>
<li>
<span class="math inline">\(\displaystyle
\operatorname{var}[X] = k \left(\frac{m}{N}\right)\left(\frac{N - k}{N - 1}\right)\left(1 - \frac{m}{N}\right)\)</span>.</li>
</ol>
</div>
<p>The moment-generating function is difficult and will not be considered.</p>
<div class="softwareBox software">
<p>The four <strong>R</strong> functions for working with the hypergeometric distribution functions have the form <code>[dpqr]hyper(m, n, k)</code>, where <code>k</code><span class="math inline">\({} = k\)</span>, <code>m</code><span class="math inline">\({} = m\)</span> and <code>m</code><span class="math inline">\({} + {}\)</span><code>n</code><span class="math inline">\({} = N\)</span>, so that <code>n</code><span class="math inline">\({}={}\)</span><code>N</code><span class="math inline">\({}-{}\)</span><code>m</code> (see Appendix <a href="UseRDistributions.html#UseRDistributions">D</a>).</p>
</div>
<div class="linkBox link">
<p>If the population is much larger than the sample size (that is, <span class="math inline">\(N\)</span> is much larger than <span class="math inline">\(k\)</span>), then the probability of a success will be approximately constant, and the binomial distribution can be used to give approximate probabilities.</p>
</div>
<p>Consider the example at the start of this section.
The probability of drawing a light-coloured ball initially is <span class="math inline">\(6/10 = 0.6\)</span>, and the probability that the next ball is light-coloured is <span class="math inline">\(5/9 = 0.556\)</span>.
But suppose there are <span class="math inline">\(10\,000\)</span> balls in the bag, of which $6,000 $are light-coloured.
The probability of drawing a light-coloured ball initially is <span class="math inline">\(6000 / 10,000 = 0.6\)</span>, and the probability that the next ball is light-coloured becomes <span class="math inline">\(5999/9999 = 0.59996\)</span>; the probability is almost the same.
In this case, we might consider using the binomial distribution with <span class="math inline">\(p\approx 0.6\)</span>.</p>
<p>In general, if <span class="math inline">\(N\)</span> is much larger than <span class="math inline">\(k\)</span>, the population proportion then will be approximately <span class="math inline">\(p\approx m/N\)</span>, and so <span class="math inline">\(1 - p \approx (N - m)/N\)</span>.
Using this information,
<span class="math display">\[
   \operatorname{E}[X] = k \times (m/N) \approx kp
\]</span>
and
<span class="math display">\[\begin{align*}
   \operatorname{var}[X]
   &amp;= k\left(\frac{m}{N}\right)\left(1 - \frac{m}{N}\right)\left(\frac{N - k}{N - 1}\right)\\
   &amp;\approx k\left( p \right ) \left( 1 - p \right) \left(1 \right ) \\
   &amp;= k p (1 - p),
\end{align*}\]</span>
which correspond to the mean and variance of the binomial distribution.</p>
<div class="example">
<p><span id="exm:Mice" class="example"><strong>Example 7.11  (Mice) </strong></span>Twenty mice are available to be used in an experiment; seven of the mice are female and <span class="math inline">\(13\)</span> are male.
Five mice are required and will be sacrificed.
What is the probability that more than three of the mice are males?</p>
<p>Let <span class="math inline">\(X\)</span> be the number of male mice chosen in a sample of size <span class="math inline">\(k = 5\)</span>.
Then <span class="math inline">\(X\)</span> has a hypergeometric distribution (since mice are chosen without replacement) where <span class="math inline">\(N = 20\)</span>, <span class="math inline">\(k = 5\)</span>, <span class="math inline">\(m = 13\)</span> and <span class="math inline">\(n = 20 - 13 = 7\)</span>, and we seek
<span class="math display">\[\begin{align*}
   \Pr(X &gt; 3)
   &amp;= \Pr(X = 4) + \Pr(X = 5) \\
   &amp;= \frac{ \binom{13}{4} \binom{7}{1}}{ \binom{20}{5} } +
       \frac{ \binom{13}{5} \binom{7}{0}}{ \binom{20}{5} } \\
   &amp; \approx  0.3228 + 0.0830 = 0.4058.
\end{align*}\]</span>
The probability is about <span class="math inline">\(41\)</span>%:</p>
<div class="sourceCode" id="cb55"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/stats/Hypergeometric.html">dhyper</a></span><span class="op">(</span>x <span class="op">=</span> <span class="fl">4</span>, m <span class="op">=</span> <span class="fl">13</span>, n <span class="op">=</span> <span class="fl">7</span>, k <span class="op">=</span> <span class="fl">5</span><span class="op">)</span> <span class="op">+</span> </span>
<span>  <span class="fu"><a href="https://rdrr.io/r/stats/Hypergeometric.html">dhyper</a></span><span class="op">(</span>x <span class="op">=</span> <span class="fl">5</span>, m <span class="op">=</span> <span class="fl">13</span>, n <span class="op">=</span> <span class="fl">7</span>, k <span class="op">=</span> <span class="fl">5</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] 0.4058308</span></span></code></pre></div>
</div>
<p></p>
</div>
</div>
<div id="MultinomialDistribution" class="section level2" number="7.9">
<h2>
<span class="header-section-number">7.9</span> Multinomial distribution<a class="anchor" aria-label="anchor" href="#MultinomialDistribution"><i class="fas fa-link"></i></a>
</h2>
<p>
A specific example of a <em>discrete</em> multivariate distribution is the <em>multinomial distribution</em>, a generalization of the binomial distribution.</p>
<div class="definition">
<p><span id="def:MultinomialDistribution" class="definition"><strong>Definition 7.15  (Multinomial distribution) </strong></span>Consider an experiment with the the sample space partitioned as <span class="math inline">\(S = \{B_1, B_2, \ldots, B_k\}\)</span>.
Let <span class="math inline">\(p_i = \Pr(B_i), \ i = 1, 2,\ldots k\)</span> where <span class="math inline">\(\sum_{i = 1}^k p_i = 1\)</span>.
Suppose there are <span class="math inline">\(n\)</span> repetitions of the experiment in which <span class="math inline">\(p_i\)</span> is constant.
Let the random variable <span class="math inline">\(X_i\)</span> be the number of times (in the <span class="math inline">\(n\)</span> repetitions) that the event <span class="math inline">\(B_i\)</span> occurs.
In this situation, the random vector <span class="math inline">\((X_1, X_2, \dots, X_k)\)</span> is said to have a <em>multinomial distribution</em> with probability function
<span class="math display" id="eq:Multinomial">\[\begin{equation}
   \Pr(X_1 = x_1, X_2 = x_2, \ldots, X_k = x_k)
   = \frac{n!}{x_1! \, x_2! \ldots x_k!}
      p_1^{x_1}\, p_2^{x_2} \ldots p_k^{x_k},
   \tag{7.13}
\end{equation}\]</span>
where <span class="math inline">\(\mathcal{R}_X = \{(x_1, \ldots x_k) : x_i = 0,1,\ldots,n, \, i = 1, 2, \ldots k, \, \sum_{i = 1}^k x_i = n\}\)</span>.</p>
</div>
<p>The part of Eq. <a href="DiscreteDistributions.html#eq:Multinomial">(7.13)</a> involving factorials arises as the number of ways of arranging <span class="math inline">\(n\)</span> objects, <span class="math inline">\(x_1\)</span> of which are of the first kind, <span class="math inline">\(x_2\)</span> of which are of the second kind, etc.
The above distribution is <span class="math inline">\((k - 1)\)</span>-variate since <span class="math inline">\(x_k = n-\sum_{i = 1}^{k - 1}x_i\)</span>.
In particular if <span class="math inline">\(k = 2\)</span>, the multinomial distribution reduces to the binomial distribution which is a univariate distribution.</p>
<p><span class="math inline">\(X_i\)</span> is the number of times (out of <span class="math inline">\(n\)</span>) that the event <span class="math inline">\(B_i\)</span>, which has probability <span class="math inline">\(p_i\)</span>, occurs.
So the random variable <span class="math inline">\(X_i\)</span> clearly has a binomial distribution with parameters <span class="math inline">\(n\)</span> and <span class="math inline">\(p_i\)</span>.
We see then that the marginal probability distribution of one of the components of a multinomial distribution is a binomial distribution.</p>
<p>Notice that the distribution in Example <a href="ChapBivariate.html#exm:BVDiscreteTwoDice">4.5</a> is an example of a <em>trinomial distribution</em>.
The probabilities shown in Table <a href="ChapBivariate.html#tab:BVTwoDice">4.3</a> can be expressed algebraically as
<span class="math display">\[
   \Pr(X = x, Y = y)
   = \frac{2!}{x! \, y!(2 - x - y)!}
      \left(\frac{1}{6}\right)^x\left(\frac{1}{6}\right)^y\left(\frac{2}{3}\right)^ {2 - x - y}
\]</span>
for <span class="math inline">\(x, y = 0 , 1 , 2\)</span>; <span class="math inline">\(x + y \leq 2\)</span>.</p>
<p>The following are the basic properties of the multinomial distribution.</p>
<div class="theorem">
<p><span id="thm:MultinomialProperties" class="theorem"><strong>Theorem 7.10  (Multinomial distribution properties) </strong></span>Suppose <span class="math inline">\((X_1, X_2, \ldots, X_k)\)</span> has the multinomial distribution given in Def. <a href="DiscreteDistributions.html#def:MultinomialDistribution">7.15</a>.
Then for <span class="math inline">\(i = 1, 2, \ldots, k\)</span>:</p>
<ul>
<li>
<span class="math inline">\(\operatorname{E}[X_i] = np_i\)</span>.</li>
<li>
<span class="math inline">\(\operatorname{var}[X_i] = n p_i(1 - p_i)\)</span>.</li>
<li>
<span class="math inline">\(\operatorname{Cov}(X_i, X_j) = -n p_i p_j\)</span> for <span class="math inline">\(i \neq j\)</span>.</li>
</ul>
</div>
<div class="proof">
<p><span id="unlabeled-div-35" class="proof"><em>Proof</em>. </span>The first two results follow from the fact that <span class="math inline">\(X_i \sim \text{Bin}(n, p_i)\)</span>.</p>
<p>We will use <span class="math inline">\(x\)</span> for <span class="math inline">\(x_1\)</span> and <span class="math inline">\(y\)</span> for <span class="math inline">\(x_2\)</span> in the third for convenience.
Consider only the case <span class="math inline">\(k = 3\)</span>, and note that
<span class="math display">\[
   \sum_{(x, y) \in R}
   \frac{n!}{x! \, y! (n - x - y)!} p_1^x p_2^y (1 - p_1 - p_2)^{n - x - y} = 1.
\]</span>
Then, putting <span class="math inline">\(p_3 = 1 - p_1 - p_2\)</span>,
<span class="math display">\[\begin{align*}
  E(XY)
  &amp;= \sum_{(x, y)}xy \Pr(X = x, Y = y)\\
  &amp;= \sum_{(x, y)}\frac{n!}{(x - 1)!(y - 1)!(n - x - y)!} p_1^x  p_2^y p_3^{n - x - y}\\
  &amp;= n(n - 1) p_1 p_2\underbrace{\sum_{(x,y)}\frac{(n - 2)!}{(x - 1)!(y - 1)!(n - x - y)!} p_1^{x - 1} p_2^{y - 1}p_3^{n - x - y}}_{ = 1}.
\end{align*}\]</span>
So <span class="math inline">\(\operatorname{Cov}(X, Y) = n^2 p_1 p_2 - n p_1 p_2 - (n p_1)(n p_2) = -n p_1 p_2\)</span>.</p>
</div>
<div class="softwareBox software">
<p>The two <strong>R</strong> functions for working with the multinomial distribution functions have the form <code>[dr]multinom(size, prob)</code> where <code>size</code><span class="math inline">\({}= n\)</span> and <code>prob</code><span class="math inline">\({} = (p_1, p_2, \dots, p_k)\)</span> for <span class="math inline">\(k\)</span> categories (see App. <a href="UseRDistributions.html#UseRDistributions">D</a>).
Note that the functions <code>qmultinom()</code> and <code>pmultinom()</code> are not defined.</p>
</div>
<div class="example">
<p><span id="exm:Multinomial" class="example"><strong>Example 7.12  (Multinomial distribution) </strong></span>Suppose that the four basic blood groups O, A, B and AB are known to occur in the proportions <span class="math inline">\(9:8:2:1\)</span>.
Given a random sample of <span class="math inline">\(8\)</span> individuals, what is the probability that there will be <span class="math inline">\(3\)</span> each of Types O and A and <span class="math inline">\(1\)</span> each of Types B and AB?</p>
<p>The probabilities are <span class="math inline">\(p_1 = 0.45\)</span>, <span class="math inline">\(p_2 = 0.4\)</span>, <span class="math inline">\(p_3 = 0.1\)</span>, <span class="math inline">\(p_4 = 0.05\)</span>, and
<span class="math display">\[\begin{align*}
   \Pr(X_O = 3, X_A = 3, X_B = 1, X_{AB} = 1)
   &amp;= \frac{8!}{3!\,3!\,1!\,1!}(0.45)^3 (0.4)^3 (0.1)(.05)\\
   &amp;= 0.033.
\end{align*}\]</span>
In <strong>R</strong>:</p>
<div class="sourceCode" id="cb56"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/stats/Multinom.html">dmultinom</a></span><span class="op">(</span>x <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">3</span>, <span class="fl">3</span>, <span class="fl">1</span>, <span class="fl">1</span><span class="op">)</span>, </span>
<span>          size <span class="op">=</span> <span class="fl">8</span>, </span>
<span>          prob <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0.45</span>, <span class="fl">0.4</span>, <span class="fl">0.1</span>, <span class="fl">0.05</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] 0.0326592</span></span></code></pre></div>
</div>
<p></p>
</div>
<div id="OtherDiscreteDistributions" class="section level2" number="7.10">
<h2>
<span class="header-section-number">7.10</span> Other notable discrete distributions<a class="anchor" aria-label="anchor" href="#OtherDiscreteDistributions"><i class="fas fa-link"></i></a>
</h2>
<p>The distributions discussed in this chapter are standard and commonly-used discrete distributions.
The <em>discrete uniform distribution</em> is used for equally-likely outcomes (like coin tosses and rolls of a die).
The <em>Bernoulli distribution</em> is used to model single dichotomous (e.g., yes/no; true/false) outcomes.
These two distributions are nit often used in practice, but the basis for developing other distributions.</p>
<p>The <em>binomial distribution</em> is used to model the number of successes in a fixed number of Bernoulli repetitions.
The <em>geometric distribution</em> is used to model the number of Bernoulli trials until the first success.
The <em>negative binomial distribution</em> can be used to model the number of Bernoulli trials until the <span class="math inline">\(r\)</span>th success, but it also has mider uses for modelling count data (with no theoretical upper limit); it has greater flexibility for modelling counts than the Poisson distribution.
The <em>Poisson distribution</em> is commonly used to model count data (with no theoretical upper limit).</p>
<p>The <em>hypergeometric distribution</em> is used to model the number of successes in a sample of size <span class="math inline">\(n\)</span> n drawn <em>without replacement</em> from population size of size <span class="math inline">\(N\)</span>, with <span class="math inline">\(K\)</span> successes.
The <em>multinomial distribution</em> is used to model counts of outcomes in <span class="math inline">\(n\)</span> independent categorical trials (whereas the binomial is used for two independent categorical trials).</p>
<p>Countless other useful discrete distributions exist; we mention a few.
The <em>Zeta distribution</em> (or sometimes, the <em>Zipf distribution</em>), defined for <span class="math inline">\(x = 1, 2, \dots\)</span>, is often used in linguistics, and for heavy-tailed counts.</p>
<p>While the Poisson distribution is commonly used to model counts, two important variations to the Poisson distribution exist.
The <em>zero-truncated Poisson distribution</em>, with range space <span class="math inline">\(\mathcal{R}_X = \{1, 2, \dots\}\)</span>, is used to model counts where counts of zero are impossible (i.e., the number of night of hospital stay for in-patients).
In contrast, the <em>zero-inflated Poisson distribution</em>, with range space <span class="math inline">\(\mathcal{R}_X = \{0, 1, 2, \dots\}\)</span>, is used to model counts where the proportion of zeros is higher than expected when using the Poisson distribution.</p>
</div>
<div id="SimulationDiscrete" class="section level2" number="7.11">
<h2>
<span class="header-section-number">7.11</span> Simulation<a class="anchor" aria-label="anchor" href="#SimulationDiscrete"><i class="fas fa-link"></i></a>
</h2>
<p></p>
<p>Simulation was introduced in Sect. <a href="ChapterProbability.html#ProbStatisticalComputing">2.11</a> as a computer-based tool for helping understand and model specific scenarios.
Probability distributions are usually crucial in simulations for modelling phenomena.
Simulation effectively connects theory to real-world modelling.
Many application (finance, epidemiology, engineering, AI) use simulation to study uncertainty in complex situations.
This book provides the building blocks for those more complex situations.</p>
<p>Distributions can be used to <em>simulate</em> practical situations, using random numbers generated from the distributions.
In <strong>R</strong>, these function start with the letter <code>r</code>; for example, to generate three random numbers from a Poisson distribution, use <code><a href="https://rdrr.io/r/stats/Poisson.html">rpois()</a></code>:</p>
<div class="sourceCode" id="cb57"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/stats/Poisson.html">rpois</a></span><span class="op">(</span><span class="fl">3</span>, <span class="co"># Generate three random numbers...</span></span>
<span>      lambda <span class="op">=</span> <span class="fl">4</span><span class="op">)</span> <span class="co"># ... with mean = 4</span></span>
<span><span class="co">#&gt; [1] 2 4 4</span></span></code></pre></div>
<p>Consider a study of insects where a females lays <span class="math inline">\(E\)</span> eggs, only some of which are fertile (based on <span class="citation">Ito (<a href="references.html#ref-ito1977model">1977</a>)</span>).
Suppose <span class="math inline">\(E \sim \text{Pois}(8.5)\)</span>, and the probability that any given egg is fertile is <span class="math inline">\(p = 0.6\)</span> (and assume the fertility of the eggs from any one female are independent).</p>
<p>The fertility for one females can be modelled in <strong>R</strong>:</p>
<div class="sourceCode" id="cb58"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># One female</span></span>
<span><span class="va">NumberOfEggs</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Poisson.html">rpois</a></span><span class="op">(</span><span class="fl">1</span>, </span>
<span>                      lambda <span class="op">=</span> <span class="fl">8.5</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># How many eggs are fertile?</span></span>
<span><span class="va">NumberFertileEggs</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Binomial.html">rbinom</a></span><span class="op">(</span><span class="fl">1</span>, <span class="co"># Only one random number for this one female</span></span>
<span>                            size <span class="op">=</span> <span class="va">NumberOfEggs</span>,</span>
<span>                            prob <span class="op">=</span> <span class="fl">0.6</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/cat.html">cat</a></span><span class="op">(</span><span class="st">"Number of eggs:"</span>, <span class="va">NumberOfEggs</span>,</span>
<span>    <span class="st">"| Number of fertile eggs:"</span>, <span class="va">NumberFertileEggs</span>, <span class="st">"\n"</span><span class="op">)</span></span>
<span><span class="co">#&gt; Number of eggs: 5 | Number of fertile eggs: 3</span></span></code></pre></div>
<p>Since random numbers are being generated, each simulation will produce different results, so we repeat this over many simulations:</p>
<div class="sourceCode" id="cb59"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Many simulations</span></span>
<span></span>
<span><span class="va">numberSims</span> <span class="op">&lt;-</span> <span class="fl">5000</span></span>
<span></span>
<span><span class="va">NumberOfEggs</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/array.html">array</a></span><span class="op">(</span> dim <span class="op">=</span> <span class="va">numberSims</span> <span class="op">)</span></span>
<span><span class="va">NumberFertileEggs</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/array.html">array</a></span><span class="op">(</span> dim <span class="op">=</span> <span class="va">numberSims</span> <span class="op">)</span></span>
<span></span>
<span><span class="kw">for</span> <span class="op">(</span><span class="va">i</span> <span class="kw">in</span> <span class="op">(</span><span class="fl">1</span><span class="op">:</span><span class="va">numberSims</span><span class="op">)</span><span class="op">)</span><span class="op">{</span> </span>
<span>  <span class="va">NumberOfEggs</span><span class="op">[</span><span class="va">i</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Poisson.html">rpois</a></span><span class="op">(</span><span class="fl">1</span>, lambda <span class="op">=</span> <span class="fl">8.5</span><span class="op">)</span></span>
<span>  </span>
<span>  <span class="co"># How many fertile?</span></span>
<span>  <span class="va">NumberFertileEggs</span><span class="op">[</span><span class="va">i</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Binomial.html">rbinom</a></span><span class="op">(</span><span class="fl">1</span>, <span class="co"># Only one random number/female</span></span>
<span>                                 size <span class="op">=</span> <span class="va">NumberOfEggs</span><span class="op">[</span><span class="va">i</span><span class="op">]</span>,</span>
<span>                                 prob <span class="op">=</span> <span class="fl">0.6</span><span class="op">)</span></span>
<span><span class="op">}</span></span></code></pre></div>
<p>We can then draw a histogram of the number of fertile eggs laid by females (Fig. <a href="DiscreteDistributions.html#fig:EggsSimulation">7.9</a>):</p>
<div class="sourceCode" id="cb60"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/graphics/hist.html">hist</a></span><span class="op">(</span><span class="va">NumberFertileEggs</span>,</span>
<span>     las <span class="op">=</span> <span class="fl">1</span>, <span class="co"># Horizontal tick mark labels</span></span>
<span>     xlab <span class="op">=</span> <span class="st">"Number of fertile eggs"</span>,</span>
<span>     ylab <span class="op">=</span> <span class="st">"Count"</span>,</span>
<span>     main <span class="op">=</span> <span class="st">"Number of fertile eggs:\n5000 simulations"</span><span class="op">)</span></span></code></pre></div>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:EggsSimulation"></span>
<img src="07-SpecificDiscrete_files/figure-html/EggsSimulation-1.png" alt="The number of fertile eggs laid, over $5\,000$\ simulations." width="56%"><p class="caption">
FIGURE 7.9: The number of fertile eggs laid, over <span class="math inline">\(5\,000\)</span> simulations.
</p>
</div>
<p>Using this information, questions can be answered about the number of fertile eggs layed.
For example:</p>
<div class="sourceCode" id="cb61"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span> <span class="va">NumberFertileEggs</span> <span class="op">&gt;</span> <span class="fl">6</span><span class="op">)</span> <span class="op">/</span> <span class="va">numberSims</span></span>
<span><span class="co">#&gt; [1] 0.2512</span></span></code></pre></div>
<p>The chance that a female lays more than <span class="math inline">\(6\)</span> fertile eggs is about 25.1%.
The mean and variance of the number of fertile eggs laid is:</p>
<div class="sourceCode" id="cb62"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span> <span class="va">NumberFertileEggs</span> <span class="op">)</span></span>
<span><span class="co">#&gt; [1] 5.1074</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/stats/cor.html">var</a></span><span class="op">(</span> <span class="va">NumberFertileEggs</span> <span class="op">)</span></span>
<span><span class="co">#&gt; [1] 4.97166</span></span></code></pre></div>
</div>
<div id="DiscreteExercises" class="section level2" number="7.12">
<h2>
<span class="header-section-number">7.12</span> Exercises<a class="anchor" aria-label="anchor" href="#DiscreteExercises"><i class="fas fa-link"></i></a>
</h2>
<p>Selected answers appear in Sect. <a href="selected-solutions.html#AnswersChapDiscreteDistributions">E.7</a>.</p>
<div class="exercise">
<p><span id="exr:BinChange" class="exercise"><strong>Exercise 7.1  </strong></span>If <span class="math inline">\(X \sim \text{Bin}(n, 1 - p)\)</span>, show that <span class="math inline">\((n - X) \sim \text{Bin}(n, p)\)</span>.</p>
</div>
<div class="exercise">
<p><span id="exr:SaltIntake" class="exercise"><strong>Exercise 7.2  </strong></span>A study by <span class="citation">Sutherland et al. (<a href="references.html#ref-data:Sutherland:SaltIntake">2012</a>)</span> found that about <span class="math inline">\(30\)</span>% of Britons ‘generally added salt’ at the dinner table.</p>
<ol style="list-style-type: decimal">
<li>In a group of <span class="math inline">\(25\)</span> Britons, what is the probability that at least <span class="math inline">\(10\)</span> generally added salt?</li>
<li>In a group of <span class="math inline">\(25\)</span> Britons, what is the probability that no more than <span class="math inline">\(9\)</span> generally added salt?</li>
<li>In a group of <span class="math inline">\(25\)</span> Britons, what is the probability that between <span class="math inline">\(5\)</span> and <span class="math inline">\(10\)</span> (inclusive) generally added salt?</li>
<li>What is the probability that <span class="math inline">\(6\)</span> Britons would need to be selected to find one that generally adds salt?</li>
<li>What is the probability that at least <span class="math inline">\(8\)</span> Britons would need to be selected to find the first that generally add salt?</li>
<li>What is the probability that at least <span class="math inline">\(8\)</span> Britons would need to be selected to find three that generally add salt?</li>
<li>What assumptions are being made in the above calculations?</li>
</ol>
</div>
<div class="exercise">
<p><span id="exr:Placebos" class="exercise"><strong>Exercise 7.3  </strong></span>A study by <span class="citation">Loyeung et al. (<a href="references.html#ref-loyeung2018experimental">2018</a>)</span> examined whether people could identify potential placebos.
The <span class="math inline">\(81\)</span> subjects were each presented with five different supplements, and asked to select which <em>one</em> was the legitimate herbal supplement based on the taste (the rest were placebos).</p>
<ol style="list-style-type: decimal">
<li>What is the probability that more than <span class="math inline">\(15\)</span> will correctly identify the legitimate supplement?</li>
<li>What is the probability that at least <span class="math inline">\(12\)</span> correctly identify the legitimate supplement?</li>
<li>What is the probability that the first person to identify the legitimate supplement is the third person tested?</li>
<li>What is the probability that the fifth person to identify the legitimate supplement is the <span class="math inline">\(10\)</span>th person tested?</li>
<li>In the study, <span class="math inline">\(50\)</span> people correctly selected the true herbal supplement.
What does this suggest?</li>
</ol>
</div>
<div class="exercise">
<p><span id="exr:NoisyMiners" class="exercise"><strong>Exercise 7.4  </strong></span>A study by <span class="citation">Maron (<a href="references.html#ref-data:Maron:eucthreshold">2007</a>)</span> used statistical modelling to show that the mean number of noisy miners (a type of bird) in sites with about <span class="math inline">\(15\)</span> eucalypts per two hectares was about <span class="math inline">\(3\)</span>.</p>
<ol style="list-style-type: decimal">
<li>In a two hectare site with <span class="math inline">\(15\)</span> eucalypts, what is the probability of observing no noisy miners?</li>
<li>In a two hectare site with <span class="math inline">\(15\)</span> eucalypts, what is the probability of observing more than <span class="math inline">\(5\)</span> noisy miners?</li>
<li>In a four hectare site with <span class="math inline">\(30\)</span> eucalypts, what is the probability of observing two noisy miners?</li>
</ol>
</div>
<div class="exercise">
<p><span id="exr:DisaggregationPoisson" class="exercise"><strong>Exercise 7.5  </strong></span>In a study of rainfall disaggregation (extracting small-scale rainfall features from large-scale measurements), the number of non-overlapping rainfall events per day at Katherine was modelled using a Poisson distribution (for <span class="math inline">\(x = 0, 1, 2, 3, \dots\)</span>) with <span class="math inline">\(\lambda = 2.5\)</span> in summer and <span class="math inline">\(\lambda = 1.9\)</span> in winter <span class="citation">(<a href="references.html#ref-connolly1998daily">Connolly, Schirmer, and Dunn 1998</a>)</span>.</p>
<p>Denote the number of rainfall events in summer as <span class="math inline">\(S\)</span>, and in winter as <span class="math inline">\(W\)</span>.</p>
<ol style="list-style-type: decimal">
<li>Plot the two distributions, and compare summer and winter (on the same graph).
Compare and comment.</li>
<li>What is the probability of more than <span class="math inline">\(3\)</span> rainfall events per day in winter?</li>
<li>What is the probability of more than <span class="math inline">\(3\)</span> rainfall events per day in summer?</li>
<li>Describe what is meant by the statement <span class="math inline">\(\Pr(S &gt; 3 \mid S &gt; 1)\)</span>, and compute the probability.</li>
<li>Describe what is meant by the statement <span class="math inline">\(\Pr(W &gt; 2 \mid W &gt; 1)\)</span>, and compute the probability.</li>
</ol>
</div>
<div class="exercise">
<p><span id="exr:UnknownSize" class="exercise"><strong>Exercise 7.6  </strong></span>Consider a population of animals of a certain species of unknown size <span class="math inline">\(N\)</span> <span class="citation">(<a href="references.html#ref-romesburg1979fitting">Romesburg and Marshall 1979</a>)</span>.
A certain number of animals in an area are trapped and tagged, then released.
At a later point, more animals are trapped, and the number tagged is noted.</p>
<p>Define <span class="math inline">\(p\)</span> as the probability that an animal is captured zero times during the study, and <span class="math inline">\(n_x\)</span> as the number of animals captured <span class="math inline">\(x\)</span> times (for <span class="math inline">\(x = 0, 1, 2, \dots\)</span>).
(<span class="math inline">\(n_0\)</span>, which we write as <span class="math inline">\(N\)</span>, is unknown.)
The study consist of <span class="math inline">\(s\)</span> trapping events, so we have <span class="math inline">\(n_1\)</span>, <span class="math inline">\(n_2, \dots n_s\)</span> where <span class="math inline">\(s\)</span> is sufficiently ‘large’ that the truncation is negligible.
Then,
<span class="math display" id="eq:GeomCapture">\[\begin{equation}
   n_x = N p (1 - p)^x \quad \text{for $x = 0, 1, 2, \dots$}.
   \tag{7.14}
\end{equation}\]</span>
While <span class="math inline">\(p\)</span> and <span class="math inline">\(N\)</span> are both unknown, the value of <span class="math inline">\(N\)</span> (effectively, the population size) is of interest.</p>
<ol style="list-style-type: decimal">
<li>Explain <em>how</em> <a href="DiscreteDistributions.html#eq:GeomCapture">(7.14)</a> arises from understanding the problem.</li>
<li>Take logarithms of both sides of of <a href="DiscreteDistributions.html#eq:GeomCapture">(7.14)</a>, and hence write this equation in the form of a linear regression equation <span class="math inline">\(\hat{y} = \beta_0 + \beta_1 x\)</span>.</li>
<li>Using the regression equation, identify <em>how</em> to estimate <span class="math inline">\(p\)</span> from knowing an estimate of the slope of the regression line (<span class="math inline">\(\beta_1\)</span>), and then how to estimate <span class="math inline">\(N\)</span> from the <span class="math inline">\(y\)</span>-intercept (<span class="math inline">\(\beta_0\)</span>).</li>
<li>Use the data in Table <a href="DiscreteDistributions.html#tab:RabbitPop">7.3</a>, from a study of rabbits in an area Michigan <span class="citation">(<a href="references.html#ref-eberhardt1963problems">Eberhardt, Peterle, and Schofield 1963</a>)</span>, to estimate the rabbit population.</li>
</ol>
<p>(<strong>Hint:</strong> To fit a regression line in <strong>R</strong>, use <code><a href="https://rdrr.io/r/stats/lm.html">lm()</a></code>; for example, to fit the regression line <span class="math inline">\(\hat{y} = \beta_0 + \beta_1 x\)</span>, use <code>lm(y ~ x)</code>.)</p>
<div class="inline-table"><table class="table table-sm">
<caption>
<span id="tab:RabbitPop">TABLE 7.3: </span>The number of rabbits <span class="math inline">\(n_x\)</span> at each trapping who had been previously trapped.
</caption>
<tbody>
<tr>
<td style="text-align:right;">
<span class="math inline">\(x\)</span>
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
2
</td>
<td style="text-align:right;">
3
</td>
<td style="text-align:right;">
4
</td>
<td style="text-align:right;">
5
</td>
<td style="text-align:right;">
6
</td>
</tr>
<tr>
<td style="text-align:right;">
<span class="math inline">\(n_x\)</span>
</td>
<td style="text-align:right;">
247
</td>
<td style="text-align:right;">
63
</td>
<td style="text-align:right;">
20
</td>
<td style="text-align:right;">
4
</td>
<td style="text-align:right;">
2
</td>
<td style="text-align:right;">
1
</td>
</tr>
</tbody>
</table></div>
</div>
<div class="exercise">
<p><span id="exr:NigerianWater" class="exercise"><strong>Exercise 7.7  </strong></span>A Nigerian study of using solar energy to disinfect water (SODIS) modelled the number of days exposure needed to disinfect the water <span class="citation">(<a href="references.html#ref-nwankwo2022solar">Nwankwo and Attama 2022</a>)</span>.
The threshold for disinfection was a single day recording <span class="math inline">\(4\)</span> kWh/m<sup><span class="math inline">\(2\)</span></sup> daily cumulative solar irradiance.
Define <span class="math inline">\(p\)</span> as the probability that a single day records more than this threshold.</p>
<ol style="list-style-type: decimal">
<li>Suppose <span class="math inline">\(p = 0.5\)</span>.
How many days exposure would be needed, on average, to achieve disinfection?</li>
<li>Suppose <span class="math inline">\(p = 0.25\)</span>.
How many days exposure would be needed, on average, to achieve disinfection?</li>
<li>If <span class="math inline">\(p = 0.25\)</span>, what is the variance of the number of days needed to achieve disinfection?</li>
<li>If <span class="math inline">\(p = 0.25\)</span>, what is the probability that disinfection will be achieved in three days or fewer?</li>
</ol>
</div>
<div class="exercise">
<p><span id="exr:unlabeled-div-36" class="exercise"><strong>Exercise 7.8  </strong></span>A negative binomial distribution was used to model the number of parasites on feral cats on on Kerguelen Island <span class="citation">(<a href="references.html#ref-hwang2016estimating">Hwang, Huggins, and Stoklosa 2016</a>)</span>.
The model used is parameterised so that <span class="math inline">\(\mu = 8.7\)</span> and <span class="math inline">\(k = 0.4\)</span>, where <span class="math inline">\(\operatorname{var}[X] = \mu + \mu^2/k\)</span>.</p>
<ol style="list-style-type: decimal">
<li>Use the above information to determine the negative binomial parameters used for the parameterisation in Sect. <a href="DiscreteDistributions.html#def:GeometricDistributionALT">7.9</a>.</li>
<li>Determine the probability that a feral cat has more than <span class="math inline">\(10\)</span> parasites.</li>
<li>The cats with the largest <span class="math inline">\(10\)</span>% of parasites have how many parasites?</li>
</ol>
</div>
<div class="exercise">
<p><span id="exr:unlabeled-div-37" class="exercise"><strong>Exercise 7.9  </strong></span>A study investigating bacterial sample preparation procedures for single-cell studies <span class="citation">(<a href="references.html#ref-koyama2016bacterial">Koyama et al. 2016</a>)</span> studied, among other bacteria, <em>E. coli 110</em>.
The number of bacteria in 2<span class="math inline">\(\mu\)</span>L samples was modelled using a Poisson distribution with <span class="math inline">\(\lambda = 1.04\)</span>.</p>
<ol style="list-style-type: decimal">
<li>What is the probability that a sample has more than <span class="math inline">\(4\)</span> bacteria?</li>
<li>What is the probability that a sample has more than <span class="math inline">\(4\)</span> bacteria, given it has bacteria?</li>
<li>Would you expect the negative binomial distribution to fit the data better than the Poisson?</li>
</ol>
</div>
<div class="exercise">
<p><span id="exr:unlabeled-div-38" class="exercise"><strong>Exercise 7.10  </strong></span>A study of accidents in coal mines <span class="citation">(<a href="references.html#ref-sari2009stochastic">Sari et al. 2009</a>)</span> used a Poisson distribution with <span class="math inline">\(\lambda = 12.87\)</span>.</p>
<ol style="list-style-type: decimal">
<li>Plot the distribution.</li>
<li>Compute the probability of more than one accident per day in February.</li>
</ol>
</div>
<div class="exercise">
<p><span id="exr:unlabeled-div-39" class="exercise"><strong>Exercise 7.11  </strong></span>In a study of heat spells <span class="citation">(<a href="references.html#ref-furrer2010statistical">Furrer et al. 2010</a>)</span> examined three cities.
In Paris, a ‘hot’ day was defined as a day with a maximum over <span class="math inline">\(27\)</span><sup><span class="math inline">\(\circ\)</span></sup>C; in Phoenix, a ‘hot’ day was defined as a day with a maximum over <span class="math inline">\(40.8\)</span><sup><span class="math inline">\(\circ\)</span></sup>C.</p>
<p>The length of a heat spell <span class="math inline">\(X\)</span> was modelled using a geometric distribution, with <span class="math inline">\(p = 0.40\)</span> in Paris, and <span class="math inline">\(\lambda = 0.24\)</span> in Phoenix.</p>
<ol style="list-style-type: decimal">
<li>On the same graph, plot both probability functions.</li>
<li>For each city, what is the probability that a heat spell last longer than a week?</li>
<li>For each city, what is the probability that a heat spell last longer than a week, given it has lasted two days?</li>
</ol>
</div>
<div class="exercise">
<p><span id="exr:unlabeled-div-40" class="exercise"><strong>Exercise 7.12  </strong></span>A study of crashes at intersections <span class="citation">(<a href="references.html#ref-lord2005poisson">Lord, Washington, and Ivan 2005</a>)</span> examined high risk intersections, and modelled the number of crashes with a Poisson distribution using <span class="math inline">\(\lambda = 11.5\)</span> per day.</p>
<ol style="list-style-type: decimal">
<li>What proportion of such intersection would be expected to have zero crashes?</li>
<li>What proportion of such intersection would be expected to have more than five crashes?</li>
</ol>
</div>
<div class="exercise">
<p><span id="exr:LayDate" class="exercise"><strong>Exercise 7.13  </strong></span>A <em>negative binomial</em> distribution was used to model the day on which eggs were layed by glaucous-winged gulls <span class="citation">(<a href="references.html#ref-zador2006balancing">Zador, Piatt, and Punt 2006</a>)</span>.
The authors used a different parameterisation of the negative binomial distribution, with the two parameters <span class="math inline">\(\mu\)</span> (the mean) and <span class="math inline">\(\phi\)</span> (the overdispersion parameter).
(In <strong>R</strong>, these are called <code>mu</code> and <code>size</code> respectively when calling <code><a href="https://rdrr.io/r/stats/NegBinomial.html">dnbinom()</a></code> and friends.)
The <em>expected</em> ‘lay date’ (from Day <span class="math inline">\(0\)</span>) was <span class="math inline">\(23.0\)</span> in 1999, and <span class="math inline">\(19.5\)</span> in 2000;
the overdispersion parameter was <span class="math inline">\(\phi = 20.6\)</span> in 1999 and <span class="math inline">\(\phi = 8.9\)</span> in 2000.</p>
<ol style="list-style-type: decimal">
<li>Using this information, plot the probability function for both years (on the same graph), and comment.</li>
<li>For both years, compute the probability that the lay date exceeds <span class="math inline">\(30\)</span>, and comment.</li>
<li>For both years, find the lay date for the earliest <span class="math inline">\(15\)</span>% of birds.</li>
<li>The data in Table <a href="DiscreteDistributions.html#tab:ClutchSize">7.4</a> shows the clutch size (number of eggs) found in the birds’ nest.
Compute the mean and standard deviation of the number of eggs per clutch.</li>
</ol>
</div>
<div class="inline-table"><table class="table table-sm">
<caption>
<span id="tab:ClutchSize">TABLE 7.4: </span>The numbers of eggs per clutch.
</caption>
<thead><tr>
<th style="text-align:left;">
</th>
<th style="text-align:right;">
1 egg
</th>
<th style="text-align:right;">
2 eggs
</th>
<th style="text-align:right;">
3 eggs
</th>
</tr></thead>
<tbody><tr>
<td style="text-align:left;font-weight: bold;">
Number of clutches
</td>
<td style="text-align:right;font-weight: bold;">
9
</td>
<td style="text-align:right;font-weight: bold;">
29
</td>
<td style="text-align:right;font-weight: bold;">
199
</td>
</tr></tbody>
</table></div>
<div class="exercise">
<p><span id="exr:NegativeBinomialALT" class="exercise"><strong>Exercise 7.14  </strong></span>The negative binomial distribution was defined in Sect <a href="DiscreteDistributions.html#NegativeBinomialDistribution">7.6</a> for the random variable <span class="math inline">\(X\)</span>, which represented the number of failures until the <span class="math inline">\(r\)</span>th success.
An alternative parameterisation is to define the random variable <span class="math inline">\(Y\)</span> as the number of trials necessary to obtain <span class="math inline">\(r\)</span> successes.</p>
<ol style="list-style-type: decimal">
<li>Define the range space for <span class="math inline">\(Y\)</span>; explain.</li>
<li>Deduce the probability function for <span class="math inline">\(Y\)</span>.</li>
<li>Determine the mean, variance and MGF of <span class="math inline">\(Y\)</span>, using the results already available for <span class="math inline">\(X\)</span>.</li>
</ol>
</div>
<div class="exercise">
<p><span id="exr:PoissonTypos" class="exercise"><strong>Exercise 7.15  </strong></span>Suppose typos made by a typist on a typing test form a <em>Poisson process</em> with the mean rate <span class="math inline">\(2.5\)</span> typos per minute and that the test lasts five minutes.</p>
<ol style="list-style-type: decimal">
<li>Determine the probability that the typist makes exactly <span class="math inline">\(10\)</span> errors during the test.</li>
<li>Determine the probability that the typist makes exactly <span class="math inline">\(6\)</span> errors during the first 3 minutes, and exactly <span class="math inline">\(4\)</span> errors during the last <span class="math inline">\(2\)</span> minutes of the test.</li>
<li>Determine the probability that the typist makes exactly <span class="math inline">\(6\)</span> errors during the first 3 minutes, and exactly <span class="math inline">\(6\)</span> during the last <span class="math inline">\(3\)</span> minutes of the test.
(Note: These times overlap.)</li>
</ol>
</div>
<div class="exercise">
<p><span id="exr:RiverDepth" class="exercise"><strong>Exercise 7.16  </strong></span>The depth of a river varies from the ‘normal’ level, say <span class="math inline">\(Y\)</span> (in metres), a specific location with a PDF given by <span class="math inline">\(f(y) = 1/4\)</span> for <span class="math inline">\(-2 \le y \le 2\)</span>.</p>
<ol style="list-style-type: decimal">
<li>Find the probability that <span class="math inline">\(Y\)</span> is greater than <span class="math inline">\(1\,\text{m}\)</span>.</li>
<li>If readings on four different days are taken what is the probability that exactly two are greater than <span class="math inline">\(1\,\text{m}\)</span>?</li>
</ol>
</div>
<div class="exercise">
<p><span id="exr:QueuingPois" class="exercise"><strong>Exercise 7.17  </strong></span>Poisson distributions are used in <em>queuing theory</em> to model the formation of queues.
Suppose that a certain queue is modelled with a Poisson distribution with a mean of <span class="math inline">\(0.5\)</span> arrivals per minute.</p>
<ol style="list-style-type: decimal">
<li>Use <strong>R</strong> to simulate the arrivals from 8AM to 9AM (for one simulation, plot the queue length after each minute).
Produce <span class="math inline">\(100\)</span> simulations, and hence compute the mean and standard deviation of people in the queue at 9AM.</li>
<li>Suppose a server begins work at 8:30AM serving customers (e.g., removing them from the queue) at the rate of <span class="math inline">\(0.7\)</span> per minute.
Again, use <strong>R</strong> to simulate the arrivals for <span class="math inline">\(60\,\text{mins}\)</span> (for one simulation, plot the queue length after each minute).
Produce <span class="math inline">\(100\)</span> simulations, and hence compute the mean and standard deviation of people in the queue at 9AM.</li>
<li>Suppose one server begins work at 8:30AM as above, and another servers begins at 8:45; together the two can serve customers (e.g., removing them from the queue) at the rate of <span class="math inline">\(1.3\)</span> per minute.
Again, use <strong>R</strong> to simulate the arrivals for 60 minutes (for one simulation, plot the queue length after each minute).
Produce <span class="math inline">\(100\)</span> simulations, and hence compute the mean and standard deviation of people in the queue at 9AM.</li>
</ol>
</div>
<div class="exercise">
<p><span id="exr:PoissonEqualValues" class="exercise"><strong>Exercise 7.18  </strong></span>For the Poisson distribution in <a href="DiscreteDistributions.html#eq:PoissonPMF">(7.11)</a>, determine the values of <span class="math inline">\(\lambda\)</span> such that <span class="math inline">\(\Pr(X) = \Pr(X + 1)\)</span>.</p>
</div>
<div class="exercise">
<p><span id="exr:HypergeometricFPC" class="exercise"><strong>Exercise 7.19  </strong></span>Consider the mean and variance for the hypergeometric distribution.
In this exercise, write <span class="math inline">\(p = m/N\)</span>.</p>
<ol style="list-style-type: decimal">
<li>Show that the mean of the hypergeometric and binomial distributions are the same.</li>
<li>Show that the variance of the hypergeometric and binomial distributions are by connected by the <em>Finite Population Correction</em> factor (seen later in Def. <a href="SamplingDistributions.html#def:FPC">12.4</a>): <span class="math inline">\((N - k)/(N - 1)\)</span>.</li>
</ol>
</div>
<div class="exercise">
<p><span id="exr:DiscreteUniformVar" class="exercise"><strong>Exercise 7.20  </strong></span>In this exercise, we find the variance of the discrete uniform distribution, using the definitions of <span class="math inline">\(Y\)</span> and <span class="math inline">\(X\)</span> in Sect. <a href="DiscreteDistributions.html#DiscreteUniform">7.2</a>.</p>
<ol style="list-style-type: decimal">
<li>First find <span class="math inline">\(\operatorname{E}[Y^2]\)</span>
</li>
<li>Then find <span class="math inline">\(\operatorname{var}[X]\)</span>.</li>
</ol>
</div>
<div class="exercise">
<p><span id="exr:GeometricPropertiesProof" class="exercise"><strong>Exercise 7.21  </strong></span>Prove the results in Theorem <a href="DiscreteDistributions.html#thm:GeometricProperties">7.4</a>, by first finding the MGF.</p>
</div>
<div class="exercise">
<p><span id="exr:PoissonZeroTruncated" class="exercise"><strong>Exercise 7.22  </strong></span>Prove these results from Sect. <a href="DiscreteDistributions.html#PoissonExtensions">7.7.4</a>: <span class="math inline">\(\operatorname{E}[X] = \lambda + 1\)</span> and <span class="math inline">\(\operatorname{var}[X] = \lambda\)</span> for the zero-truncated Poisson distribution.</p>
</div>
<div class="exercise">
<p><span id="exr:GammaSpecificValues" class="exercise"><strong>Exercise 7.23  </strong></span>Prove this result from Theorem <a href="DiscreteDistributions.html#thm:GammaFunctionProperties">7.7</a>: <span class="math inline">\(\Gamma(1) = \Gamma(2) = 1\)</span>.</p>
</div>

</div>
</div>
<hr>
<div class="footer"><span style="color: gray; font-size:0.7em">Peter K. Dunn, 2024: <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a></span></div>
  <div class="chapter-nav">
<div class="prev"><a href="ChapterTransformations.html"><span class="header-section-number">6</span> Transformations of random variables</a></div>
<div class="next"><a href="ContinuousDistributions.html"><span class="header-section-number">8</span> Standard continuous distributions</a></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page"><h2>On this page</h2>
      <ul class="nav navbar-nav">
<li><a class="nav-link" href="#DiscreteDistributions"><span class="header-section-number">7</span> Standard discrete distributions</a></li>
<li><a class="nav-link" href="#introduction-1"><span class="header-section-number">7.1</span> Introduction</a></li>
<li><a class="nav-link" href="#DiscreteUniform"><span class="header-section-number">7.2</span> Discrete uniform distribution</a></li>
<li><a class="nav-link" href="#BernoulliDistribution"><span class="header-section-number">7.3</span> Bernoulli distribution</a></li>
<li>
<a class="nav-link" href="#BinomialDistribution"><span class="header-section-number">7.4</span> Binomial distribution</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#BinomialDerivation"><span class="header-section-number">7.4.1</span> Derivation of a binomial distribution</a></li>
<li><a class="nav-link" href="#BinomialDefinition"><span class="header-section-number">7.4.2</span> Definition and properties</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#GeometricDistribution"><span class="header-section-number">7.5</span> Geometric distribution</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#GeometricDerivation"><span class="header-section-number">7.5.1</span> Derivation of a geometric distribution</a></li>
<li><a class="nav-link" href="#GeometricDefinition"><span class="header-section-number">7.5.2</span> Definition and properties</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#NegativeBinomialDistribution"><span class="header-section-number">7.6</span> Negative binomial distribution</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#NegBinStandard"><span class="header-section-number">7.6.1</span> Derivation: standard parameterisation</a></li>
<li><a class="nav-link" href="#NegBinAlternative"><span class="header-section-number">7.6.2</span> Definition and properties: standard parameterisation</a></li>
<li><a class="nav-link" href="#alternative-parameterisations"><span class="header-section-number">7.6.3</span> Alternative parameterisations</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#PoissonDistribution"><span class="header-section-number">7.7</span> Poisson distribution</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#PoissonDerivation"><span class="header-section-number">7.7.1</span> Derivation of Poisson distribution</a></li>
<li><a class="nav-link" href="#PoissonDefinition"><span class="header-section-number">7.7.2</span> Definition and properties</a></li>
<li><a class="nav-link" href="#PoissonBinomial"><span class="header-section-number">7.7.3</span> Relationship to the binomial distribution</a></li>
<li><a class="nav-link" href="#PoissonExtensions"><span class="header-section-number">7.7.4</span> Extensions</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#HypergeometricDistribution"><span class="header-section-number">7.8</span> Hypergeometric distribution</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#HypergeometricDerivation"><span class="header-section-number">7.8.1</span> Derivation of a hypergeometric distribution</a></li>
<li><a class="nav-link" href="#HypergeometricDefinition"><span class="header-section-number">7.8.2</span> Definition and properties</a></li>
</ul>
</li>
<li><a class="nav-link" href="#MultinomialDistribution"><span class="header-section-number">7.9</span> Multinomial distribution</a></li>
<li><a class="nav-link" href="#OtherDiscreteDistributions"><span class="header-section-number">7.10</span> Other notable discrete distributions</a></li>
<li><a class="nav-link" href="#SimulationDiscrete"><span class="header-section-number">7.11</span> Simulation</a></li>
<li><a class="nav-link" href="#DiscreteExercises"><span class="header-section-number">7.12</span> Exercises</a></li>
</ul>

      <div class="book-extra">
        <ul class="list-unstyled">
<li><a id="book-source" href="https://github.com/PeterKDunn/DistTheory/blob/main/07-SpecificDiscrete.Rmd">View source <i class="fab fa-github"></i></a></li>
          <li><a id="book-edit" href="https://github.com/PeterKDunn/DistTheory/edit/main/07-SpecificDiscrete.Rmd">Edit this page <i class="fab fa-github"></i></a></li>
        </ul>
</div>
    </nav>
</div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>The Theory of Statistical Distributions</strong>" was written by Peter K. Dunn. It was last built on Last updated: 2025-12-12.</p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script><script type="text/x-mathjax-config">const popovers = document.querySelectorAll('a.footnote-ref[data-toggle="popover"]');
for (let popover of popovers) {
  const div = document.createElement('div');
  div.setAttribute('style', 'position: absolute; top: 0, left:0; width:0, height:0, overflow: hidden; visibility: hidden;');
  div.innerHTML = popover.getAttribute('data-content');

  var has_math = div.querySelector("span.math");
  if (has_math) {
    document.body.appendChild(div);
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, div]);
    MathJax.Hub.Queue(function() {
      popover.setAttribute('data-content', div.innerHTML);
      document.body.removeChild(div);
    })
  }
}
</script>
</body>
</html>
