<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>14 Introduction to Bayesian statistics | The Theory of Statistical Distributions</title>
<meta name="author" content="Peter K. Dunn">
<meta name="description" content="On completion of this chapter you should be able to: explain the idea of Bayesian statistics. compute conditional, marginal, prior and posterior distributions in the context of Bayesian...">
<meta name="generator" content="bookdown 0.45 with bs4_book()">
<meta property="og:title" content="14 Introduction to Bayesian statistics | The Theory of Statistical Distributions">
<meta property="og:type" content="book">
<meta property="og:description" content="On completion of this chapter you should be able to: explain the idea of Bayesian statistics. compute conditional, marginal, prior and posterior distributions in the context of Bayesian...">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="14 Introduction to Bayesian statistics | The Theory of Statistical Distributions">
<meta name="twitter:description" content="On completion of this chapter you should be able to: explain the idea of Bayesian statistics. compute conditional, marginal, prior and posterior distributions in the context of Bayesian...">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.6.0/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.6.0/bootstrap.bundle.min.js"></script><script src="libs/bs3compat-0.9.0/transition.js"></script><script src="libs/bs3compat-0.9.0/tabs.js"></script><script src="libs/bs3compat-0.9.0/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><script src="libs/kePrint-0.0.1/kePrint.js"></script><link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet">
<script src="libs/htmlwidgets-1.6.4/htmlwidgets.js"></script><script src="libs/rglWebGL-binding-1.3.31/rglWebGL.js"></script><link href="libs/rglwidgetClass-1.3.31/rgl.css" rel="stylesheet">
<script src="libs/rglwidgetClass-1.3.31/rglClass.min.js"></script><script src="libs/CanvasMatrix4-1.3.31/CanvasMatrix.min.js"></script><link rel="shortcut icon" href="icons/iconmonstr-chart-1-240.png">
<script>
    document.addEventListener('DOMContentLoaded', function() {
      // Find all R code blocks that should be toggleable.
      // Our Lua filter adds the 'r-code-box' class to the code block.
      var codeBlocks = document.querySelectorAll('.r-code-box');

      codeBlocks.forEach(function(codeBlock) {
        // Create the button element
        var button = document.createElement('button');
        button.textContent = 'Show R Code'; // Initial text for the button
        button.className = 'code-toggle-button'; // Assign CSS class

        // Insert the button directly before the code block.
        // The codeBlock's parentNode is the div.figure-with-code container.
        // We insert the button as a sibling of the codeBlock within that container.
        codeBlock.parentNode.insertBefore(button, codeBlock);

        // Hide the code block initially by default.
        codeBlock.style.display = 'none';

        // Add a click event listener to the button
        button.addEventListener('click', function() {
          if (codeBlock.style.display === 'none') {
            codeBlock.style.display = 'block'; // Show the code block
            button.textContent = 'Hide R Code'; // Change button text
          } else {
            codeBlock.style.display = 'none'; // Hide the code block
            button.textContent = 'Show R Code'; // Change button text back
          }
        });
      });
    });
    </script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- CSS --><style type="text/css">
    
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  </style>
<style type="text/css">
    /* Used with Pandoc 2.11+ new --citeproc when CSL is used */
    div.csl-bib-body { }
    div.csl-entry {
      clear: both;
          margin-bottom: 0em;
        }
    .hanging div.csl-entry {
      margin-left:2em;
      text-indent:-2em;
    }
    div.csl-left-margin {
      min-width:2em;
      float:left;
    }
    div.csl-right-inline {
      margin-left:2em;
      padding-left:1em;
    }
    div.csl-indent {
      margin-left: 2em;
    }
  </style>
<link rel="stylesheet" href="bs4_style.css">
<link rel="stylesheet" href="css/style.css">
<link rel="stylesheet" href="css/columns.css">
<link rel="stylesheet" href="html/largerDie.css">
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="">The Theory of Statistical Distributions</a>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
</form>

      <nav aria-label="Table of contents"><h2>Table of contents</h2>
        <ul class="book-toc list-unstyled">
<li><a class="" href="index.html">Preface</a></li>
<li class="book-part">Theoretical foundations</li>
<li><a class="" href="ChapterSetTheory.html"><span class="header-section-number">1</span> Essentials of set theory</a></li>
<li><a class="" href="ChapterProbability.html"><span class="header-section-number">2</span> Probability</a></li>
<li><a class="" href="DistributionRandomVariables.html"><span class="header-section-number">3</span> Random variables and their distributions</a></li>
<li><a class="" href="ChapBivariate.html"><span class="header-section-number">4</span> Bivariate distributions</a></li>
<li><a class="" href="ChapExpectation.html"><span class="header-section-number">5</span> Mathematical expectation</a></li>
<li><a class="" href="ChapterTransformations.html"><span class="header-section-number">6</span> Transformations of random variables</a></li>
<li class="book-part">Standard univariate probability distributions</li>
<li><a class="" href="DiscreteDistributions.html"><span class="header-section-number">7</span> Standard discrete distributions</a></li>
<li><a class="" href="ContinuousDistributions.html"><span class="header-section-number">8</span> Standard continuous distributions</a></li>
<li><a class="" href="ChapterMixedDistributions.html"><span class="header-section-number">9</span> Mixed distributions</a></li>
<li class="book-part">Multivariate random variables and distributions*</li>
<li><a class="" href="ChapMultivariate.html"><span class="header-section-number">10</span> Multivariate distributions*</a></li>
<li><a class="" href="MultivariateExtensions.html"><span class="header-section-number">11</span> Expectations for multivariate distributions*</a></li>
<li class="book-part">Sampling distributions</li>
<li><a class="" href="SamplingDistributions.html"><span class="header-section-number">12</span> Describing samples</a></li>
<li><a class="" href="OrderStatisticsChapter.html"><span class="header-section-number">13</span> Order statistcs</a></li>
<li><a class="active" href="BayesianIntro.html"><span class="header-section-number">14</span> Introduction to Bayesian statistics</a></li>
<li class="book-part">Appendices</li>
<li><a class="" href="SymbolsUsed.html"><span class="header-section-number">A</span> Symbols used</a></li>
<li><a class="" href="UsefulSeries.html"><span class="header-section-number">B</span> Some useful series</a></li>
<li><a class="" href="ShortRIntro.html"><span class="header-section-number">C</span> Short R introduction</a></li>
<li><a class="" href="UseRDistributions.html"><span class="header-section-number">D</span> Using R with distributions</a></li>
<li><a class="" href="selected-solutions.html"><span class="header-section-number">E</span> Selected solutions</a></li>
<li><a class="" href="references.html"><span class="header-section-number">F</span> References</a></li>
</ul>

        <div class="book-extra">
          <p><a id="book-repo" href="https://github.com/PeterKDunn/DistTheory">View book source <i class="fab fa-github"></i></a></p>
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="BayesianIntro" class="section level1" number="14">
<h1>
<span class="header-section-number">14</span> Introduction to Bayesian statistics<a class="anchor" aria-label="anchor" href="#BayesianIntro"><i class="fas fa-link"></i></a>
</h1>
<div class="objectivesBox objectives">
<p>On completion of this chapter you should be able to:</p>
<ul>
<li>explain the idea of Bayesian statistics.</li>
<li>compute conditional, marginal, prior and posterior distributions in the context of Bayesian statistics.</li>
<li>compute conjugate distributions and identify hyperparameters for Bayesian situations.</li>
</ul>
</div>
<div id="RandomParameters" class="section level2" number="14.1">
<h2>
<span class="header-section-number">14.1</span> Random parameters<a class="anchor" aria-label="anchor" href="#RandomParameters"><i class="fas fa-link"></i></a>
</h2>
<p>So far, parameters used to describe model or populations have been fixed values.
In some applications, though, the <em>parameters</em> themselves may be assumed to have distributions.
This idea is best introduced using examples.</p>
<div class="example">
<p><span id="exm:RandomPar1" class="example"><strong>Example 14.1  (Exam scores) </strong></span>The score achieved by students on an exam is approximately normal with mean <span class="math inline">\(\theta\)</span>.
The score achieved by any specific student <span class="math inline">\(X\)</span> can be modelled with this normal distribution, using this mean <span class="math inline">\(\theta\)</span>.
The mean <span class="math inline">\(\theta\)</span> is not constant, but varies (due to the specific questions that appear on the exam, the amount of study completed by individual students, etc.), and follows an approximately normal distribution (with mean 60 and standard deviation 10).</p>
</div>
<div class="example">
<p><span id="exm:RandomPar2" class="example"><strong>Example 14.2  (Vehicle crashes) </strong></span>The number of motor vehicle crashes per day occur in a town vary according to a Poisson distribution with mean <span class="math inline">\(\theta\)</span>.
The number of crashes on a specific day <span class="math inline">\(X\)</span> can be modelled with this distribution, but the mean <span class="math inline">\(\theta\)</span> is 10 per day when fine, and 20 per day when wet.
About 10% of days are wet.</p>
</div>
<div class="example">
<p><span id="exm:RandomPar3" class="example"><strong>Example 14.3  (Goal kicking) </strong></span>The probability that a rugby league goal-kicker lands a goal is <span class="math inline">\(\theta\)</span>.
This number of kicks landed <span class="math inline">\(X\)</span> can be modelled using a binomial distribution with probability <span class="math inline">\(\theta\)</span>, but the probability of a successful kick <span class="math inline">\(\theta\)</span> may vary (depending on the conditions, where the kick is taken from, who is taking the kick, and so on), such that <span class="math inline">\(\theta \sim \text{Beta}(4, 4)\)</span>.</p>
</div>
<p>In each of these examples, a distribution <span class="math inline">\(f_{X}(x; \theta)\)</span> describes the data for some given value of <span class="math inline">\(\Theta\)</span>.
However, the value of <span class="math inline">\(\Theta\)</span> itself has a distribution, <span class="math inline">\(f_\Theta(\theta)\)</span>, so the actual distribution <span class="math inline">\(f_{X}(x; \theta)\)</span> depends in <em>which</em> value of <span class="math inline">\(\Theta\)</span> is realised.
Hence, the distribution <span class="math inline">\(f_X(x; \theta)\)</span> is effectively a <em>conditional</em> distribution <span class="math inline">\(f_{x|\Theta}(x \mid \Theta)\)</span>, and the parameter <span class="math inline">\(\Theta\)</span> is effectively a random variable.</p>
<p>The <em>marginal</em> distribution of <span class="math inline">\(X\)</span> (i.e., <span class="math inline">\(f_X(x)\)</span>) can be found from Def. <a href="ChapBivariate.html#def:BVContMarginal">4.6</a> if <span class="math inline">\(X\)</span> and <span class="math inline">\(\Theta\)</span> are continuous:</p>
<p><span class="math display" id="eq:marg1">\[\begin{equation}
   f_X(x)
   = \int f_{X \mid \Theta}(x, \theta)f_\Theta(\theta)\,d\theta.
   \tag{14.1}
\end{equation}\]</span>
For <span class="math inline">\(X\)</span> and <span class="math inline">\(\Theta\)</span> discrete, Def. <a href="ChapBivariate.html#def:BVDiscreteMarginal">4.5</a> yields</p>
<p><span class="math display" id="eq:marg2">\[\begin{equation}
   p_X(x)
   = \sum_\theta p_{X \mid \Theta}(x, \theta)p_\Theta(\theta).
   \tag{14.2}
\end{equation}\]</span>
The mixed cases can be dealt with similarly:</p>
<p><span class="math display" id="eq:marg3">\[\begin{equation}
   f_X(x)
   = \sum_\theta f_{X \mid \Theta}(x, \theta)p_\Theta(\theta)
   \tag{14.3}
\end{equation}\]</span>
for <span class="math inline">\(X\)</span> continuous and <span class="math inline">\(\Theta\)</span> discrete, and</p>
<p><span class="math display" id="eq:marg4">\[\begin{equation}
   p_X(x)
   = \int p_{X \mid \Theta}(x, \theta)f_\Theta(\theta)\,d\theta
   \tag{14.4}
\end{equation}\]</span>
for <span class="math inline">\(X\)</span> discrete and <span class="math inline">\(\Theta\)</span> continuous.</p>
<p>Only in special cases will these marginal distributions have a standard distribution or closed form.
The parameters that describe the distribution of the parameter <span class="math inline">\(\Theta\)</span> are called <em>hyperparameters</em>.</p>
<div class="example">
<p><span id="exm:RandomParExam" class="example"><strong>Example 14.4  (Exam scores) </strong></span>(Follows Example <a href="BayesianIntro.html#exm:RandomPar1">14.1</a>.)
The score <span class="math inline">\(X\)</span> achieved by a student on an exam is normal with mean <span class="math inline">\(\Theta\)</span> and standard deviation <span class="math inline">\(\sigma = 5\)</span>; that is, for some <em>given</em> value of <span class="math inline">\(\Theta\)</span>, we have <span class="math inline">\(X \sim N(\Theta, 5^2)\)</span>.
The mean <span class="math inline">\(\Theta\)</span> varies from student to student with a normal distribution, with two <em>hyperparameters</em>: the mean 60 and standard deviation 10; that is, <span class="math inline">\(\Theta \sim N(60, 10^2)\)</span>.
From <a href="BayesianIntro.html#eq:marg1">(14.1)</a>, the marginal PDF of <span class="math inline">\(X\)</span> is
<span class="math display" id="eq:exam2">\[\begin{align}
  f_X(x)
  &amp;= \int f_{X \mid \Theta}(x, \theta)f_\Theta(\theta)\,d\theta\notag\\
  &amp;= \int_{-\infty}^\infty \frac{1}{5\sqrt{2\pi}}
     \exp\left[ -\frac{1}{2}\left(\frac{x - \theta}{5}\right)^2 \right]
     \frac{1}{10\sqrt{2\pi}}
  \exp\left[ -\frac{1}{2}\left(\frac{\theta - 60}{10}\right)^2 \right]\,d\theta\notag\\
  &amp;= \frac{1}{11.18\sqrt{2\pi}}
     \exp\left[-\frac{1}{2}\left(\frac{x - 60}{11.18}\right)^2\right]
     \tag{14.5}
\end{align}\]</span>
after considerable algebra involving completing the square.
Then, <span class="math inline">\(X\sim N(60, 11.18^2)\)</span> (see Fig. <a href="BayesianIntro.html#fig:RandomMarginal">14.1</a>, left panel; note that <span class="math inline">\(11.18 = \sqrt{5^2 + 10^2}\)</span>.)</p>
<p>This describes the distribution of <span class="math inline">\(X\)</span>, taking into account the various values of <span class="math inline">\(\Theta\)</span> that are possible (as described by <span class="math inline">\(f_\Theta(\theta)\)</span>.)</p>
</div>
<div class="example">
<p><span id="exm:RandomParAccidents" class="example"><strong>Example 14.5  (Vehicle crashes) </strong></span>(Follows Example <a href="BayesianIntro.html#exm:RandomPar2">14.2</a>.)
Let <span class="math inline">\(X\)</span> be the number of motor vehicle crashes per day.
Then <span class="math inline">\(X \sim \text{Pois}(\Theta)\)</span> for some given value of <span class="math inline">\(\Theta\)</span>.
However, the mean parameter <span class="math inline">\(\Theta\)</span> has the distribution by <span class="math inline">\(\Pr(\Theta = 10) = 0.9\)</span> and <span class="math inline">\(\Pr(\Theta = 20) = 0.1\)</span>, or
<span class="math display">\[
   p_\Theta(\theta) = 0.1^{(\theta - 10)/10} 0.9^{(20 - \theta)/10}\quad\text{for $\theta = 10, 20$}.
\]</span>
From <a href="BayesianIntro.html#eq:marg2">(14.2)</a>, the marginal pf of <span class="math inline">\(X\)</span> is
<span class="math display" id="eq:RandomParAccidents">\[\begin{align}
  p_X(x)
  &amp;= \sum_\theta p_{X \mid \Theta}(x, \theta)p_\Theta(\theta)\notag\\
  &amp;= 0.9 \frac{e^{-10}10^x}{x!} + 0.1 \frac{e^{-20}{20^x}}{x!}\quad\text{for $x = 0, 1, 2, \dots$}.
  \tag{14.6}
\end{align}\]</span>
See Fig. <a href="BayesianIntro.html#fig:RandomMarginal">14.1</a> (centre panel).</p>
</div>
<div class="example">
<p><span id="exm:RandomParGoalKicking" class="example"><strong>Example 14.6  (Goal kicking) </strong></span>(Follows Example <a href="BayesianIntro.html#exm:RandomPar3">14.3</a>.)
Suppose the goal-kicker takes 10 kicks at goal in a match; the number of successful kicks <span class="math inline">\(X\)</span> can be be modelled by the binomial distribution <span class="math inline">\(\text{Bin}(n = 10, p = \Theta)\)</span>, where the probability <span class="math inline">\(\Theta\)</span> has a <span class="math inline">\(\text{Beta}(4, 4)\)</span> distribution.
From <a href="BayesianIntro.html#eq:marg4">(14.4)</a>, the marginal pf of <span class="math inline">\(X\)</span> is</p>
<p><span class="math display">\[\begin{align}
  p_X(x)
  &amp;= \int p_{X \mid \Theta}(x, \theta)f_\Theta(\theta)\,d\theta\notag\\
  &amp;= \int_0^1 \binom{10}{x}\theta^x(1 - \theta)^{10 - x}
     \frac{\Gamma(8)}{\Gamma(4)\,\Gamma(4)} \theta^3(1 - \theta)^3\,d\theta\notag\\
  &amp;= \frac{\Gamma(8)}{\Gamma(4)\,\Gamma(4)}\binom{10}{x}\int_0^1 \theta^{x + 3}(1 - \theta)^{10 - x + 3}\,d\theta\notag\\
\end{align}\]</span>
where the integral can be simplified by seeing it is a Beta function (see Equation <a href="ContinuousDistributions.html#eq:BetaFunction">(8.12)</a>) with parameters <span class="math inline">\(x + 4\)</span> and <span class="math inline">\(14 - x\)</span>, so that <span class="math inline">\(\int_0^1 \theta^{x + 3}(1 - \theta)^{10 - x + 3}\,d\theta = \Gamma(x - 4)\Gamma(14 - x)/\Gamma(18)\)</span>.
Then:
<span class="math display" id="eq:goal2">\[
  p_X(x)
  = \frac{7!\, 10!\, (x + 3)!\, (13 - x)!}{3!^2\, 17!\, x!\, (10 - x)!}\quad\text{for $x = 0 , 1 , 2,\dots, 10$}.
   \tag{14.7}
\]</span>
See Fig. <a href="BayesianIntro.html#fig:RandomMarginal">14.1</a> (right panel).</p>
</div>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:RandomMarginal"></span>
<img src="14-Bayesian_files/figure-html/RandomMarginal-1.png" alt="Marginal distributions" width="100%"><p class="caption">
FIGURE 14.1: Marginal distributions
</p>
</div>
</div>
<div id="BayesTheoremRevisited" class="section level2" number="14.2">
<h2>
<span class="header-section-number">14.2</span> Bayes’ theorem revisited<a class="anchor" aria-label="anchor" href="#BayesTheoremRevisited"><i class="fas fa-link"></i></a>
</h2>
<p>Recall Bayes’ theorem in Sect. <a href="ChapterProbability.html#BayesTheorem">2.10.6</a>.
Let <span class="math inline">\(E\)</span> be an event, with <span class="math inline">\(H_1, \ldots, H_n\)</span> a sequence of mutually exclusive and exhaustive events partitioning the sample space.
Then</p>
<p><span class="math display">\[\begin{equation}
  \Pr(H_n \mid E )
  = \frac{\Pr(H_n) \Pr(E \mid H_n) }{\Pr(E)}
  = \frac{\Pr(H_n) \Pr(E \mid H_n) }{\sum_m \Pr(H_m ) \Pr(E \mid H_m )}
\end{equation}\]</span>
assuming that <span class="math inline">\(\Pr(E) \neq 0\)</span>.
Bayes’ theorem extends to random variables.</p>
<p>Suppose that <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are discrete random variables, where <span class="math inline">\(p_Y(y)\)</span> is the pf for <span class="math inline">\(Y\)</span> and <span class="math inline">\(p_{X \mid Y}(x,y)\)</span> is the conditional PMF for <span class="math inline">\(X\)</span> given <span class="math inline">\(Y\)</span>.
Then the conditional PMF of <span class="math inline">\(Y\)</span> given <span class="math inline">\(X\)</span> is</p>
<p><span class="math display" id="eq:Bayesdis">\[\begin{align}
  p_{Y \mid X}(y, x)
  &amp;= \frac{p_{X, Y}(x, y)}{p_X(x)} \\
  &amp;= \frac{p_{X \mid Y}(x, y)p_Y(y)}{\sum_y p_{X \mid Y}(x, y)p_Y(y)}.
  \tag{14.8}
\end{align}\]</span></p>
<p>In their most usual form, the above equations for the combinations of continuous and discrete random variables produce these results:</p>
<p><span class="math display" id="eq:Bayesctsdis">\[\begin{align}
   f_{Y \mid X}(y, x)
   &amp;= \frac{f_{X, Y}(x, y)}{f_X(x)}
    = \frac{f_{X \mid Y}(x, y)f_Y(y)}{\int_y f_{X \mid Y}(x, y)f_Y(y)\,dy}
    = \frac{f_{X \mid Y}(x, y)f_Y(y)}{f_X(x)};
   \tag{14.9}\\
  p_{Y \mid X}(y, x)
  &amp;= \frac{f_{X, Y}(x, y)}{p_X(x)}
   = \frac{f_{X \mid Y}(x, y)p_Y(y)}{\sum_y f_{X \mid Y}(x, y)p_Y(y)}
   = \frac{f_{X \mid Y}(x, y)p_Y(y)}{p_X(x)};
  \tag{14.10}\\
  f_{Y \mid X}(y, x)
  &amp;= \frac{f_{X, Y}(x, y)}{p_X(x)}
   = \frac{p_{X \mid Y}(x, y)f_Y(y)}{\int_y p_{X \mid Y}(x, y)f_Y(y)\,dy}
   = \frac{p_{X \mid Y}(x, y)f_Y(y)}{p_X(x)}.
  \tag{14.11}
\end{align}\]</span>
<!-- \begin{align} -->
<!--    f_{Y \mid X}(y, x) -->
<!--    &= \frac{f_{X, Y}(x, y)}{f_X(x)} -->
<!--    = \frac{f_{X \mid Y}(x, y)f_Y(y)}{\int_y f_{X \mid Y}(x, y)f_Y(y)\,dy}; -->
<!--    (\#eq:Bayescts)\\ -->
<!--   p_{Y \mid X}(y, x) -->
<!--   &= \frac{f_{X, Y}(x, y)}{p_X(x)} -->
<!--   = \frac{f_{X \mid Y}(x, y)p_Y(y)}{\sum_y f_{X \mid Y}(x, y)p_Y(y)}, -->
<!--   (\#eq:Bayesdiscts) -->
<!--   \quad \text{and}\\ -->
<!--   f_{Y \mid X}(y, x) -->
<!--   &= \frac{f_{X, Y}(x, y)}{p_X(x)} -->
<!--   = \frac{p_{X \mid Y}(x, y)f_Y(y)}{\int_y p_{X \mid Y}(x, y)f_Y(y)\,dy}. -->
<!--   (\#eq:Bayesctsdis) -->
<!-- \end{align} --></p>
<p>In all cases <a href="BayesianIntro.html#eq:Bayesdis">(14.8)</a> to <a href="BayesianIntro.html#eq:Bayesctsdis">(14.11)</a>, the left-hand side shows that the distribution is a function of <span class="math inline">\(y\)</span>, given the observed data <span class="math inline">\(X\)</span>.
The <em>denominator</em> of the right-hand side is only a function of the observed data <span class="math inline">\(X\)</span>, and contains no unknowns (so is effectively a normalising constant whose value ensures that the expression is a valid probability function).</p>
<p>Because of this, sometimes these expressions in <a href="BayesianIntro.html#eq:Bayesdis">(14.8)</a> to <a href="BayesianIntro.html#eq:Bayesctsdis">(14.11)</a> are written as proportionalities rather than equalities by omitting the denominator (the normalising constant); also, write <span class="math inline">\(\Theta\)</span> as the parameter of interest:</p>
<p><span class="math display">\[\begin{align*}
   f_{\Theta|X}(\theta \mid x)      &amp;\propto f_{X|\Theta}(x\mid \theta)      \times f_\Theta(\theta)\\
   \text{Posterior distn} &amp;\propto \text{Sampling distn} \times \text{Prior distn}.
\end{align*}\]</span>
Here:</p>
<ul>
<li>The <em>prior distribution</em>: This is the distribution of the parameter <span class="math inline">\(\Theta\)</span> that is assumed before the data <span class="math inline">\(X\)</span> are observed, perhaps based on past experience.</li>
<li>The <em>sampling distribution</em> or the <em>likelihood function</em>: This describes the distribution from which the data are assumed to come.</li>
<li>The <em>posterior distribution</em>: This describes the distribution of the parameter <span class="math inline">\(\Theta\)</span>, after the data <span class="math inline">\(X\)</span> is observed.</li>
</ul>
<p>The distribution for <span class="math inline">\(\Theta\)</span> is assumed <em>prior</em> to seeing the data (i.e., <span class="math inline">\(f_\Theta(\theta)\)</span>), and is then adjusted with the knowledge of the data <span class="math inline">\(X\)</span> to produce the <em>conditional</em> (or <em>posterior</em>) distribution <span class="math inline">\(f_{\Theta|X}(\theta\mid x)\)</span>.</p>
<p>In this context:</p>
<ul>
<li>
<span class="math inline">\(\Theta\)</span> is a parameter of interest.</li>
<li>The original, or <em>unconditional</em> distribution <span class="math inline">\(f_\Theta(\theta)\)</span> models our prior beliefs about the parameter <span class="math inline">\(\theta\)</span>.
This is called the <em>prior</em> distribution (modelling our beliefs about the parameter of the distribution <em>prior</em> to seeing the data).</li>
<li>We then observe the data <span class="math inline">\(X\)</span>.</li>
<li>Using this extra available information, the description of <span class="math inline">\(\theta\)</span> is updated.
This is the <em>conditional</em> distribution <span class="math inline">\(f_{\Theta \mid X}(\theta, x)\)</span>, called the <em>posterior</em> distribution in this context.</li>
</ul>
<div class="example">
<p><span id="exm:ExamScores3" class="example"><strong>Example 14.7  (Exam scores) </strong></span>(Follows Examples <a href="BayesianIntro.html#exm:RandomPar1">14.1</a> and <a href="BayesianIntro.html#exm:RandomParExam">14.4</a>.)
The conditional PDF of the mean score <span class="math inline">\(\Theta\)</span>, given an observed exam score <span class="math inline">\(X = x\)</span>, follows directly from <a href="BayesianIntro.html#eq:Bayescts">(14.9)</a> on substituting <a href="BayesianIntro.html#eq:exam2">(14.5)</a>:</p>
<p><span class="math display" id="eq:exam3">\[\begin{align}
  f_{\Theta \mid X}(\theta, x)
  &amp;= \frac{f_{X|\Theta}(x, \theta)f_\Theta(\theta)}{f_X(x)}\notag\\
  &amp;= \frac{\frac{1}{5\sqrt{2\pi}}
  \exp\left[ -\frac{1}{2}\left(\frac{x - \theta}{5}\right)^2 \right]
  \frac{1}{10\sqrt{2\pi}}
  \exp\left[ -\frac{1}{2}\left(\frac{\theta - 60}{10}\right)^2 \right]}
  {\frac{1}{11.18\sqrt{2\pi}}
  \exp\left[ -\frac{1}{2}\left(\frac{x - 60}{11.18}\right)^2 \right]}\notag\\
  &amp;=\frac{1}{4.472\sqrt{2\pi}}
  \exp\left[ -\frac{1}{2}\left(\frac{\theta - (0.8x + 12)}{4.472}\right)^2 \right]
   \tag{14.12}
\end{align}\]</span>
after some algebraic manipulation; i.e., <span class="math inline">\(\Theta | (X = x)\sim N(0.8x + 12, 4.472^2)\)</span>.</p>
<p>This distribution derived above is for the mean score, <em>given</em> an observed value of <span class="math inline">\(x\)</span>.
Thus we have:</p>
<ul>
<li>The initial (<em>prior</em>) distribution about the mean score (Example <a href="BayesianIntro.html#exm:RandomParExam">14.4</a>): <span class="math inline">\(\Theta \sim N(60, 10^2)\)</span>.</li>
<li>After seeing an exam score <span class="math inline">\(x\)</span>, the adjusted (<em>posterior</em>) distribution of the mean score is <span class="math inline">\(\Theta\mid X\sim N(0.8x + 12, 4.472^2)\)</span>.</li>
</ul>
<p>Consider the implications:</p>
<ul>
<li>If observed exam score is <span class="math inline">\(x = 60\)</span> (i.e., equal to the prior belief about the mean), then there is no change to the prior mean: <span class="math inline">\(0.8\times 60 + 12 = 60\)</span>.
That is, the mean of the prior distribution is 60, and the data don’t provide any evidence to change this, so the mean of the <em>posterior</em> distribution is also 60.
Note, however, that the variance is <em>smaller</em>, as our confidence in that value of the parameter has increased.</li>
<li>If observed exam score is <em>less</em> than <span class="math inline">\(60\)</span> (e.g., <span class="math inline">\(x = 50\)</span>), the original expectation is adjusted <em>down</em> from the original assumption of a mean of 60, since <span class="math inline">\(0.8\times 50 + 12 = 52\)</span>.</li>
<li>If observed exam score is greater than <span class="math inline">\(60\)</span> (e.g., <span class="math inline">\(x = 70\)</span>), the original expectation is adjusted <em>up</em> from the original assumption of a mean of 60, since <span class="math inline">\(0.8\times 70 + 12 = 68\)</span>.</li>
</ul>
<p>See Fig. <a href="BayesianIntro.html#fig:PosteriorExam">14.2</a>.
In other words, our <em>prior</em> belief of the mean score is adjusted in light of the observed evidence to produce the <em>posterior</em> distribution.
The extra observations adds to the confidence of the estimate (i.e., reduced variance).</p>
</div>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:PosteriorExam"></span>
<img src="14-Bayesian_files/figure-html/PosteriorExam-1.png" alt="Exam scores: The prior distribution for the mean $\Theta$ (left panel) and the posterior distribution when $x = 50$ centre panel) and when $x = 70$ (right panel). The dashed grey lines in the two right plots shows the prior distribution." width="100%"><p class="caption">
FIGURE 14.2: Exam scores: The prior distribution for the mean <span class="math inline">\(\Theta\)</span> (left panel) and the posterior distribution when <span class="math inline">\(x = 50\)</span> centre panel) and when <span class="math inline">\(x = 70\)</span> (right panel). The dashed grey lines in the two right plots shows the prior distribution.
</p>
</div>
<div class="example">
<p><span id="exm:Accidents3" class="example"><strong>Example 14.8  (Vehicle crashes) </strong></span>(Follows Example <a href="BayesianIntro.html#exm:RandomPar1">14.1</a> and <a href="BayesianIntro.html#exm:RandomParAccidents">14.5</a>.)
From <a href="BayesianIntro.html#eq:RandomParAccidents">(14.6)</a> and <a href="BayesianIntro.html#eq:Bayescts">(14.9)</a>, the conditional pf of the mean number of accidents per day <span class="math inline">\(\Theta\)</span> given <span class="math inline">\(X\)</span> is
<span class="math display" id="eq:accidents3">\[\begin{align}
  p_{\Theta \mid X}(\theta, x)
  &amp;= \frac{f_{X \mid \Theta}(x, \theta)f_\Theta(\theta)}{p_X(x)}\notag\\
  &amp;= \begin{cases}
        \displaystyle \frac{0.9\frac{e^{-10}10^x}{x!}} {0.9 \frac{e^{-10}10^x}{x!} + 0.1 \frac{e^{-20}{20^x}}{x!}} &amp; \text{for $\theta = 10$}\\[6pt]
        \displaystyle \frac{0.1\frac{e^{-20}20^x}{x!}}
     {0.9 \frac{e^{-10}10^x}{x!}+ 0.1 \frac{e^{-20}{20^x}}{x!}} &amp; \text{for $\theta = 20$}.
  \end{cases}\notag\\
  &amp;= \begin{cases}
        \displaystyle \frac{0.9 e^{-10}10^x} {0.9 e^{-10}10^x + 0.1 e^{-20}{20^x}} &amp; \text{for $\theta = 10$}\\[6pt]
        \displaystyle \frac{0.1 e^{-20}20^x}
     {0.9 e^{-10}10^x + 0.1 e^{-20}{20^x}} &amp; \text{for $\theta = 20$}.
   \tag{14.13}
  \end{cases}
\end{align}\]</span></p>
<p>So the <em>prior</em> belief about <span class="math inline">\(\Theta\)</span> is that
<span class="math display">\[
  p_\Theta(\theta) =
  \begin{cases}
    0.9 &amp; \text{for $\theta = 10$ (i.e., fine)};\\
    0.1 &amp; \text{for $\theta = 20$ (i.e, wet)}.
  \end{cases}
\]</span>
After observing the number of crashes, the prior belief is adjusted accordingly to produce the <em>posterior</em> distribution (Eq. <a href="BayesianIntro.html#eq:accidents3">(14.13)</a>); i.e.:</p>
<ul>
<li><p>If we observe <span class="math inline">\(x = 8\)</span> crashes, the prior belief is adjusted to
<span class="math display">\[
p_{\Theta\mid X}(\theta \mid X) =
\begin{cases}
   0.9987 &amp; \text{for $\theta = 10$ (i.e., fine)};\\
   0.0013 &amp; \text{for $\theta = 20$ (i.e., wet)}.
\end{cases}
\]</span></p></li>
<li><p>If we observe <span class="math inline">\(x = 17\)</span> crashes, the prior belief is adjusted to
<span class="math display">\[
p_{\Theta|X}(m \mid \theta) =
\begin{cases}
   0.602 &amp; \text{for $\theta = 10$ (i.e., fine)};\\
   0.398 &amp; \text{for $\theta = 20$ (i.e., wet)}.
\end{cases}
\]</span>
See Fig. <a href="BayesianIntro.html#fig:PostPosteriorAccidents">14.3</a>.</p></li>
</ul>
</div>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:PostPosteriorAccidents"></span>
<img src="14-Bayesian_files/figure-html/PostPosteriorAccidents-1.png" alt="Vehicle crashes: The prior distribution for $\Theta$ (left panel) and the posterior distribution when $x = 8$ crashes (centre panel) and when $x = 17$ crashes (right panel). The grey crosses in the two right plots shows the prior distribution." width="100%"><p class="caption">
FIGURE 14.3: Vehicle crashes: The prior distribution for <span class="math inline">\(\Theta\)</span> (left panel) and the posterior distribution when <span class="math inline">\(x = 8\)</span> crashes (centre panel) and when <span class="math inline">\(x = 17\)</span> crashes (right panel). The grey crosses in the two right plots shows the prior distribution.
</p>
</div>
<div class="example">
<p><span id="exm:GoalKicking3" class="example"><strong>Example 14.9  (Goal kicking) </strong></span>(Follows Example <a href="BayesianIntro.html#exm:RandomPar3">14.3</a> and <a href="BayesianIntro.html#exm:RandomParGoalKicking">14.6</a>.)
From <a href="BayesianIntro.html#eq:marg4">(14.4)</a>, the conditional PDF of <span class="math inline">\(\Theta\)</span> given <span class="math inline">\(X\)</span> is
<span class="math display">\[
  f_{\Theta \mid X}(\theta, x)
  = \frac{p_{X \mid \Theta}(x, \theta) f_\Theta(\theta)}{p_X(x)}
\]</span>
where <span class="math inline">\(X \mid \theta \sim \text{Bin}(n = 10, p = \theta)\)</span> so that
<span class="math display">\[
  p(X \mid\Theta) = \binom{10}{x}\theta^x (1 - \theta)^{10 - x},
\]</span>
and where <span class="math inline">\(p_X(x)\)</span> is given in <a href="BayesianIntro.html#eq:goal2">(14.7)</a>, and <span class="math inline">\(\Theta \sim \text{Beta}(4, 4)\)</span>.
Putting the pieces together:
<span class="math display" id="eq:goal3">\[\begin{align}
  f_{\Theta \mid X}(\theta, x)
  &amp;= \frac{ \binom{10}{x}\theta^x (1 - \theta)^{10 - x}
     \frac{\Gamma(8)}{\Gamma(4)\Gamma(4)} \theta^3 (1 - \theta)^3}
          {\frac{7!\, 10!\, (x + 3)!\, (13 - x)!}{3!^2\, 17!\, x!\, (10 - x)!} } \\
  &amp;= \frac{17!\theta^{x + 3} (1 - \theta)^{13 - x}}{(x + 3)! (13 - x)!}
   \tag{14.14}
\end{align}\]</span>
which corresponds to a <em>posterior</em> distribution for <span class="math inline">\(\Theta\)</span> of <span class="math inline">\(\text{Beta}(4 + x, 14 - x)\)</span>.
In contrast, the <em>prior</em> distribution for <span class="math inline">\(\Theta\)</span> was <span class="math inline">\(\text{Beta}(4, 4)\)</span>.</p>
<p>After observing the 10 kicks, the prior belief is adjusted accordingly:</p>
<ul>
<li>If the number of kicks made is <span class="math inline">\(5/10 = 0.5\)</span>, the prior belief is adjusted to <span class="math inline">\(\text{Beta}(9, 9)\)</span>, but the mean remains as <span class="math inline">\(9/(9 + 9) = 0.5\)</span>;</li>
<li>If the number of kicks made is <span class="math inline">\(8/10 = 0.8\)</span>, the prior belief is adjusted upwards; <span class="math inline">\(\text{Beta}(12, 6)\)</span> where the mean becomes <span class="math inline">\(12/(12 + 6) = 0.67\)</span>;</li>
<li>If the number of kicks made is <span class="math inline">\(1/10 = 0.1\)</span>, the prior belief is adjusted downwards: <span class="math inline">\(\text{Beta}(5, 13)\)</span> where the mean becomes <span class="math inline">\(5/(5 + 13) = 0.28\)</span>.</li>
</ul>
<p>See Fig. <a href="BayesianIntro.html#fig:PostPosteriorKicks">14.4</a>.</p>
</div>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:PostPosteriorKicks"></span>
<img src="14-Bayesian_files/figure-html/PostPosteriorKicks-1.png" alt="Goal kicking: The prior distribution for $\theta$ (left panel) and the posterior distribution when $x = 8$ (i.e., lands 8 kicks out of 10; centre panel) and when $x = 1$ (i.e., lands 1 kick out of 10; right panel). The grey vertical lines correspond to the distribution means. The dashed grey lines in the two right plots show the prior distribution." width="100%"><p class="caption">
FIGURE 14.4: Goal kicking: The prior distribution for <span class="math inline">\(\theta\)</span> (left panel) and the posterior distribution when <span class="math inline">\(x = 8\)</span> (i.e., lands 8 kicks out of 10; centre panel) and when <span class="math inline">\(x = 1\)</span> (i.e., lands 1 kick out of 10; right panel). The grey vertical lines correspond to the distribution means. The dashed grey lines in the two right plots show the prior distribution.
</p>
</div>
<p>Applying meaning to the conditional distribution of parameters, such as those in <a href="BayesianIntro.html#eq:exam3">(14.12)</a>, <a href="BayesianIntro.html#eq:accidents3">(14.13)</a> and <a href="BayesianIntro.html#eq:goal3">(14.14)</a>, has historically been controversial in statistics.
However, the idea of <em>modifying</em> a parameter of a distribution based on new information has proven constructive, and is the basis of an important branch of statistics called <em>Bayesian statistics</em>.</p>
<p>The algebra involved in deriving posterior distributions is, except in special cases (Table <a href="BayesianIntro.html#tab:PriorPosteriorTable">14.1</a>), intractable and so numerical methods are necessary (including simulation techniques).
If the prior and posterior distributions are the same type of distribution (e.g., both normal distribution), they are called <em>conjugate distributions</em>.
The prior and posterior distribution combinations in <a href="BayesianIntro.html#tab:PriorPosteriorTable">14.1</a>) and conjugate distributions.</p>
<div class="inline-table"><table class="table" style="font-size: 11px; margin-left: auto; margin-right: auto;">
<caption style="font-size: initial !important;">
<span id="tab:PriorPosteriorTable">TABLE 14.1: </span>Conjugate distributions for random parameters: The sampling distribution, <span class="math inline">\(f(X\mid\Theta)\)</span>; the prior distribution <span class="math inline">\(f(\Theta)\)</span>; and the posterior distribution <span class="math inline">\(f(\Theta\mid X\)</span>)
</caption>
<thead>
<tr>
<th style="border-bottom:hidden;padding-bottom:0; padding-left:3px;padding-right:3px;text-align: left; font-weight: bold; " colspan="1">
<div style="">
Sampling
</div>
</th>
<th style="border-bottom:hidden;padding-bottom:0; padding-left:3px;padding-right:3px;text-align: left; font-weight: bold; " colspan="1">
<div style="">
Prior
</div>
</th>
<th style="border-bottom:hidden;padding-bottom:0; padding-left:3px;padding-right:3px;text-align: left; font-weight: bold; " colspan="1">
<div style="">
Posterior
</div>
</th>
</tr>
<tr>
<th style="text-align:left;font-weight: bold;">
distribution <span class="math inline">\(f(X\mid\Theta)\)</span>
</th>
<th style="text-align:left;font-weight: bold;">
distribution <span class="math inline">\(f(\Theta)\)</span>
</th>
<th style="text-align:left;font-weight: bold;">
distribution <span class="math inline">\(f(\Theta\mid X\)</span>)
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
<span class="math inline">\(X \sim \text{Bin}(m, p)\)</span>
</td>
<td style="text-align:left;">
<span class="math inline">\(p \sim \text{Beta}(a, b)\)</span>
</td>
<td style="text-align:left;">
<span class="math inline">\(p \mid X \sim \text{Beta}(a + \sum x, b + mn - \sum x)\)</span>
</td>
</tr>
<tr>
<td style="text-align:left;">
<span class="math inline">\(X \sim \text{Gam}(\alpha, \beta)\)</span>
</td>
<td style="text-align:left;">
<span class="math inline">\(\beta \sim \text{Gam}(\alpha_0, \beta_0)\)</span>
</td>
<td style="text-align:left;">
<span class="math inline">\(\beta \mid X \sim \text{Gam}(\alpha_0 + n\alpha, \beta_0 + \sum x)\)</span>
</td>
</tr>
<tr>
<td style="text-align:left;">
<span class="math inline">\(X \sim \text{Pois}(\lambda)\)</span>
</td>
<td style="text-align:left;">
<span class="math inline">\(\lambda \sim \text{Gam}(\alpha_0, \beta_0)\)</span>
</td>
<td style="text-align:left;">
<span class="math inline">\(\lambda \mid X \sim \text{Gam}(\alpha_0 + \sum x, \beta_0 + n)\)</span>
</td>
</tr>
<tr>
<td style="text-align:left;">
<span class="math inline">\(X \sim N(\mu, \sigma^2)\)</span>
</td>
<td style="text-align:left;">
<span class="math inline">\(\mu \sim N(\mu_0, \sigma^2_0)\)</span>
</td>
<td style="text-align:left;">
<span class="math inline">\(\mu \mid X \sim N(\mu_P, \sigma_P^2)\)</span>
</td>
</tr>
<tr>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
where <span class="math inline">\(\mu_P = \displaystyle \frac{1}{1/\sigma_0^2 + n/\sigma^{2}} \left(\frac{\mu _0}{\sigma _0^2} + \frac{\sum_{i = 1}^n x_{i}}{\sigma^2}\right)\)</span>
</td>
</tr>
<tr>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
and <span class="math inline">\(\sigma_P^2 = \left( \frac{1}{\sigma_0^2} + \frac{n}{\sigma^2} \right)^{-1}\)</span>
</td>
</tr>
</tbody>
</table></div>
</div>
<div id="statistical-computing" class="section level2" number="14.3">
<h2>
<span class="header-section-number">14.3</span> Statistical computing<a class="anchor" aria-label="anchor" href="#statistical-computing"><i class="fas fa-link"></i></a>
</h2>
</div>
<div id="ExercisesBayesian" class="section level2" number="14.4">
<h2>
<span class="header-section-number">14.4</span> Exercises<a class="anchor" aria-label="anchor" href="#ExercisesBayesian"><i class="fas fa-link"></i></a>
</h2>
<p>Selected answers appear in Sect. <a href="selected-solutions.html#AnswersChapBayesianIntro">E.14</a>.</p>
<div class="exercise">
<p><span id="exr:Reproduction" class="exercise"><strong>Exercise 14.1  </strong></span>In studies of human reproduction <span class="citation">(<a href="references.html#ref-paul2005testing">Paul 2005</a>)</span>, the number of menstrual cycles until pregnancy is denoted <span class="math inline">\(Y\)</span>, where <span class="math inline">\(Y\)</span> is often modelled using a geometric distribution: <span class="math inline">\(Y\sim \text{Geom}(\theta)\)</span>, where <span class="math inline">\(\theta\)</span> is the pre-cycle conception probability.
<span class="math inline">\(\theta\)</span> is usually considered constant for each couple, but can vary between couples with a beta distribution: <span class="math inline">\(\theta\sim \text{Beta}(m, n)\)</span>.</p>
<ol style="list-style-type: decimal">
<li>Determine the <em>posterior</em> probability <span class="math inline">\(f(\theta\mid y)\)</span>, and identify the distribution in terms of known distributions (including parameters).
(The normalising constant is not needed.)</li>
<li>Determine the <em>prior</em> probability <span class="math inline">\(\theta\)</span> if <span class="math inline">\(m = 1.2\)</span> and <span class="math inline">\(n = 2\)</span>, and hence the expected number of cycles till pregnancy.</li>
<li>Determine the <em>posterior</em> probability <span class="math inline">\(\theta\)</span> if <span class="math inline">\(m = 1.2\)</span> and <span class="math inline">\(n = 2\)</span>, and a couple takes three cycles to fall pregnant.</li>
</ol>
</div>

</div>
</div>



<hr>
<div class="footer"><span style="color: gray; font-size:0.7em">Peter K. Dunn, 2024: <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a></span></div>
  <div class="chapter-nav">
<div class="prev"><a href="OrderStatisticsChapter.html"><span class="header-section-number">13</span> Order statistcs</a></div>
<div class="next"><a href="SymbolsUsed.html"><span class="header-section-number">A</span> Symbols used</a></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page"><h2>On this page</h2>
      <ul class="nav navbar-nav">
<li><a class="nav-link" href="#BayesianIntro"><span class="header-section-number">14</span> Introduction to Bayesian statistics</a></li>
<li><a class="nav-link" href="#RandomParameters"><span class="header-section-number">14.1</span> Random parameters</a></li>
<li><a class="nav-link" href="#BayesTheoremRevisited"><span class="header-section-number">14.2</span> Bayes’ theorem revisited</a></li>
<li><a class="nav-link" href="#statistical-computing"><span class="header-section-number">14.3</span> Statistical computing</a></li>
<li><a class="nav-link" href="#ExercisesBayesian"><span class="header-section-number">14.4</span> Exercises</a></li>
</ul>

      <div class="book-extra">
        <ul class="list-unstyled">
<li><a id="book-source" href="https://github.com/PeterKDunn/DistTheory/blob/main/14-Bayesian.Rmd">View source <i class="fab fa-github"></i></a></li>
          <li><a id="book-edit" href="https://github.com/PeterKDunn/DistTheory/edit/main/14-Bayesian.Rmd">Edit this page <i class="fab fa-github"></i></a></li>
        </ul>
</div>
    </nav>
</div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>The Theory of Statistical Distributions</strong>" was written by Peter K. Dunn. It was last built on Last updated: 2025-12-12.</p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script><script type="text/x-mathjax-config">const popovers = document.querySelectorAll('a.footnote-ref[data-toggle="popover"]');
for (let popover of popovers) {
  const div = document.createElement('div');
  div.setAttribute('style', 'position: absolute; top: 0, left:0; width:0, height:0, overflow: hidden; visibility: hidden;');
  div.innerHTML = popover.getAttribute('data-content');

  var has_math = div.querySelector("span.math");
  if (has_math) {
    document.body.appendChild(div);
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, div]);
    MathJax.Hub.Queue(function() {
      popover.setAttribute('data-content', div.innerHTML);
      document.body.removeChild(div);
    })
  }
}
</script>
</body>
</html>
