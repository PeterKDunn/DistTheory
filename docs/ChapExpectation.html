<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>5 Mathematical expectation | The Theory of Statistical Distributions</title>
<meta name="author" content="Peter K. Dunn">
<meta name="description" content="Upon completion of this chapter, you should be able to: understand the concept and definition of mathematical expectation. compute the expectations of a random variable, functions of a random...">
<meta name="generator" content="bookdown 0.45 with bs4_book()">
<meta property="og:title" content="5 Mathematical expectation | The Theory of Statistical Distributions">
<meta property="og:type" content="book">
<meta property="og:description" content="Upon completion of this chapter, you should be able to: understand the concept and definition of mathematical expectation. compute the expectations of a random variable, functions of a random...">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="5 Mathematical expectation | The Theory of Statistical Distributions">
<meta name="twitter:description" content="Upon completion of this chapter, you should be able to: understand the concept and definition of mathematical expectation. compute the expectations of a random variable, functions of a random...">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.6.0/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.6.0/bootstrap.bundle.min.js"></script><script src="libs/bs3compat-0.9.0/transition.js"></script><script src="libs/bs3compat-0.9.0/tabs.js"></script><script src="libs/bs3compat-0.9.0/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><script src="libs/kePrint-0.0.1/kePrint.js"></script><link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet">
<script src="libs/htmlwidgets-1.6.4/htmlwidgets.js"></script><script src="libs/rglWebGL-binding-1.3.31/rglWebGL.js"></script><link href="libs/rglwidgetClass-1.3.31/rgl.css" rel="stylesheet">
<script src="libs/rglwidgetClass-1.3.31/rglClass.min.js"></script><script src="libs/CanvasMatrix4-1.3.31/CanvasMatrix.min.js"></script><link rel="shortcut icon" href="icons/iconmonstr-chart-1-240.png">
<script>
    document.addEventListener('DOMContentLoaded', function() {
      // Find all R code blocks that should be toggleable.
      // Our Lua filter adds the 'r-code-box' class to the code block.
      var codeBlocks = document.querySelectorAll('.r-code-box');

      codeBlocks.forEach(function(codeBlock) {
        // Create the button element
        var button = document.createElement('button');
        button.textContent = 'Show R Code'; // Initial text for the button
        button.className = 'code-toggle-button'; // Assign CSS class

        // Insert the button directly before the code block.
        // The codeBlock's parentNode is the div.figure-with-code container.
        // We insert the button as a sibling of the codeBlock within that container.
        codeBlock.parentNode.insertBefore(button, codeBlock);

        // Hide the code block initially by default.
        codeBlock.style.display = 'none';

        // Add a click event listener to the button
        button.addEventListener('click', function() {
          if (codeBlock.style.display === 'none') {
            codeBlock.style.display = 'block'; // Show the code block
            button.textContent = 'Hide R Code'; // Change button text
          } else {
            codeBlock.style.display = 'none'; // Hide the code block
            button.textContent = 'Show R Code'; // Change button text back
          }
        });
      });
    });
    </script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- CSS --><style type="text/css">
    
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  </style>
<style type="text/css">
    /* Used with Pandoc 2.11+ new --citeproc when CSL is used */
    div.csl-bib-body { }
    div.csl-entry {
      clear: both;
          margin-bottom: 0em;
        }
    .hanging div.csl-entry {
      margin-left:2em;
      text-indent:-2em;
    }
    div.csl-left-margin {
      min-width:2em;
      float:left;
    }
    div.csl-right-inline {
      margin-left:2em;
      padding-left:1em;
    }
    div.csl-indent {
      margin-left: 2em;
    }
  </style>
<link rel="stylesheet" href="bs4_style.css">
<link rel="stylesheet" href="css/style.css">
<link rel="stylesheet" href="css/columns.css">
<link rel="stylesheet" href="html/largerDie.css">
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="">The Theory of Statistical Distributions</a>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
</form>

      <nav aria-label="Table of contents"><h2>Table of contents</h2>
        <ul class="book-toc list-unstyled">
<li><a class="" href="index.html">Preface</a></li>
<li class="book-part">Theoretical foundations</li>
<li><a class="" href="ChapterSetTheory.html"><span class="header-section-number">1</span> Essentials of set theory</a></li>
<li><a class="" href="ChapterProbability.html"><span class="header-section-number">2</span> Probability</a></li>
<li><a class="" href="DistributionRandomVariables.html"><span class="header-section-number">3</span> Random variables and their distributions</a></li>
<li><a class="" href="ChapBivariate.html"><span class="header-section-number">4</span> Bivariate distributions</a></li>
<li><a class="active" href="ChapExpectation.html"><span class="header-section-number">5</span> Mathematical expectation</a></li>
<li><a class="" href="ChapterTransformations.html"><span class="header-section-number">6</span> Transformations of random variables</a></li>
<li class="book-part">Standard univariate probability distributions</li>
<li><a class="" href="DiscreteDistributions.html"><span class="header-section-number">7</span> Standard discrete distributions</a></li>
<li><a class="" href="ContinuousDistributions.html"><span class="header-section-number">8</span> Standard continuous distributions</a></li>
<li><a class="" href="ChapterMixedDistributions.html"><span class="header-section-number">9</span> Mixed distributions</a></li>
<li class="book-part">Multivariate random variables and distributions*</li>
<li><a class="" href="ChapMultivariate.html"><span class="header-section-number">10</span> Multivariate distributions*</a></li>
<li><a class="" href="MultivariateExtensions.html"><span class="header-section-number">11</span> Expectations for multivariate distributions*</a></li>
<li class="book-part">Sampling distributions</li>
<li><a class="" href="SamplingDistributions.html"><span class="header-section-number">12</span> Describing samples</a></li>
<li><a class="" href="OrderStatisticsChapter.html"><span class="header-section-number">13</span> Order statistcs</a></li>
<li><a class="" href="BayesianIntro.html"><span class="header-section-number">14</span> Introduction to Bayesian statistics</a></li>
<li class="book-part">Appendices</li>
<li><a class="" href="SymbolsUsed.html"><span class="header-section-number">A</span> Symbols used</a></li>
<li><a class="" href="UsefulSeries.html"><span class="header-section-number">B</span> Some useful series</a></li>
<li><a class="" href="ShortRIntro.html"><span class="header-section-number">C</span> Short R introduction</a></li>
<li><a class="" href="UseRDistributions.html"><span class="header-section-number">D</span> Using R with distributions</a></li>
<li><a class="" href="selected-solutions.html"><span class="header-section-number">E</span> Selected solutions</a></li>
<li><a class="" href="references.html"><span class="header-section-number">F</span> References</a></li>
</ul>

        <div class="book-extra">
          <p><a id="book-repo" href="https://github.com/PeterKDunn/DistTheory">View book source <i class="fab fa-github"></i></a></p>
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="ChapExpectation" class="section level1" number="5">
<h1>
<span class="header-section-number">5</span> Mathematical expectation<a class="anchor" aria-label="anchor" href="#ChapExpectation"><i class="fas fa-link"></i></a>
</h1>
<div class="objectivesBox objectives">
<p>Upon completion of this chapter, you should be able to:</p>
<ul>
<li>understand the concept and definition of mathematical expectation.</li>
<li>compute the expectations of a random variable, functions of a random variable and linear functions of a random variable.</li>
<li>compute the variance and other higher moments of a random variable.</li>
<li>derive the moment-generating function of a random variable and linear functions of a random variable.</li>
<li>find the moments of a random variable from the moment-generating function.</li>
<li>state and use Tchebysheff’s inequality.</li>
</ul>
</div>
<div id="ExpectedValue" class="section level2" number="5.1">
<h2>
<span class="header-section-number">5.1</span> Expected values<a class="anchor" aria-label="anchor" href="#ExpectedValue"><i class="fas fa-link"></i></a>
</h2>
<p>Because random variables are <em>random</em>, knowing the outcome on any one realisation of the random process is not possible.
Instead, we can talk about what we might <em>expect</em> to happen, or what might happen <em>on average</em>.</p>
<p>This is the idea of <em>mathematical expectation</em>.
In more usual terms, the mathematical expression of the probability distribution of a random variable is the <em>mean</em> of the random variable.
Mathematical expectation goes far beyond just computing means, but we begin here as the idea of a <em>mean</em> is easily understood.</p>
<p>The definition looks different in detail for discrete, continuous and mixed random variables, but the intention is the same.</p>
<div class="definition">
<p><span id="def:Expectation" class="definition"><strong>Definition 5.1  (Expectation) </strong></span>The <em>expectation</em> or <em>expected value</em> (or <em>mean</em>) of a random variable <span class="math inline">\(X\)</span> is written <span class="math inline">\(\operatorname{E}[X]\)</span> (or <span class="math inline">\(\mu\)</span>, or <span class="math inline">\(\mu_X\)</span> to distinguish between random variables).</p>
<p>For a <em>discrete</em> random variable <span class="math inline">\(X\)</span> with PMF <span class="math inline">\(p_X(x)\)</span>, the expected value is
<span class="math display">\[
   \operatorname{E}[X] =
        \sum_{x\in \mathcal{R}_X} x\, p_X(x).
\]</span>
For a <em>continuous</em> random variable <span class="math inline">\(X\)</span> with PDF <span class="math inline">\(f_X(x)\)</span>, the expected value is
<span class="math display">\[
   \operatorname{E}[X] =
        \int_{-\infty}^\infty x\, f_X(x).
\]</span></p>
</div>
<p>For a <em>mixed</em> random variable <span class="math inline">\(X\)</span>, the expected value is a combination of the two above results, for the discrete and continuous components of <span class="math inline">\(\mathcal{R}_X\)</span>; that is,
<span class="math display">\[
   \operatorname{E}[X] =
     \sum_{x_i} x_i \, p_X(x_i) + \int_{-\infty}^\infty x \, f_X(x) \, dx,
\]</span>
where <span class="math inline">\(p_X(x)\)</span> is the probability mass function over discrete points <span class="math inline">\(x_i\in \mathcal{R}_X\)</span>, and <span class="math inline">\(f_X(x)\)</span> is the probability density function (PDF) over the continuous regions of <span class="math inline">\(x\in \mathcal{R}_X\)</span> with PDF <span class="math inline">\(f_X(x)\)</span>.</p>
<div class="importantBox important">
<p>In the rest of this chapter, the case of mixed random variable <span class="math inline">\(X\)</span> will not be explicitly discussed; however, the results remain a combination of the discrete case for the discrete points in <span class="math inline">\(\mathcal{R}_X\)</span> and the continuous case for the continuous component of <span class="math inline">\(\mathcal{R}_X\)</span>.</p>
</div>
<p>Effectively <span class="math inline">\(\operatorname{E}[X]\)</span> is a weighted average of the points in <span class="math inline">\(\mathcal{R}_X\)</span>, the weights being the probabilities for each value of <span class="math inline">\(x\in \mathcal{R}_X\)</span> in the discrete case and probability densities in the continuous case.</p>
<div class="example">
<p><span id="exm:ExpectationDiscrete" class="example"><strong>Example 5.1  (Expectation for discrete variables) </strong></span>Consider the discrete random variable <span class="math inline">\(U\)</span> with probability function
<span class="math display">\[
   p_U(u) = \begin{cases}
               (u^2 + 1)/5 &amp; \text{for $u = -1, 0, 1$};\\
               0 &amp; \text{elsewhere},
           \end{cases}
\]</span>
so that <span class="math inline">\(\mathcal{R}_U = \{-1, 0, 1\}\)</span>.
The expected value of <span class="math inline">\(U\)</span> is
<span class="math display">\[\begin{align*}
   \operatorname{E}[U]
   &amp;= \sum_{\mathcal{R}_U} u\, p_U(u) \\
   &amp;= \sum_{\mathcal{R}_U} u \times\left( \frac{u^2 + 1}{5} \right) \\
   &amp;= \left( -1 \times \frac{(-1)^2 + 1}{5} \right ) +
       \left( 0 \times \frac{(0)^2  + 1}{5} \right ) +
       \left( 1 \times \frac{(1)^2  + 1}{5} \right ) \\
   &amp;= -2/5 + 0 + 2/5 = 0.
\end{align*}\]</span>
The expected value of <span class="math inline">\(U\)</span> is <span class="math inline">\(\operatorname{E}[U] = 0\)</span>.</p>
</div>
<div class="example">
<p><span id="exm:ExpectationContinuousX" class="example"><strong>Example 5.2  (Expectation for continuous variables) </strong></span>Consider a continuous random variable <span class="math inline">\(X\)</span> with PDF
<span class="math display">\[
   f_X(x) = \begin{cases}
               x/4 &amp; \text{for $1 &lt; x &lt; 3$};\\
               0 &amp; \text{elsewhere}.
            \end{cases}
\]</span>
The expected value of <span class="math inline">\(X\)</span> is
<span class="math display">\[\begin{align*}
   \operatorname{E}[X]
   &amp;= \int_{-\infty}^\infty x\, f_X(x) \, dx
    = \int_1^3 x(x/4)\, dx\\
   &amp;= \left.\frac{1}{12} x^3\right|_1^3 = 13/6.
\end{align*}\]</span>
The expected value of <span class="math inline">\(X\)</span> is <span class="math inline">\(\operatorname{E}[X] = 13/6\)</span>.</p>
</div>
<div class="example">
<p><span id="exm:ExpectationMixedX" class="example"><strong>Example 5.3  (Expectation for mixed variables) </strong></span>Consider a continuous random variable <span class="math inline">\(W\)</span> with probability function
<span class="math display">\[
  f_W(w) =
  \begin{cases}
     1/2       &amp; \text{for $w = 0$};\\
     \exp(-2w) &amp; \text{for $w &gt; 0$};\\
     0         &amp; \text{elsewhere},
  \end{cases}
\]</span>
so that <span class="math inline">\(p_W(w) = 1/2\)</span> for <span class="math inline">\(w = 0\)</span>, and <span class="math inline">\(f_W(w) = \exp(-2w)\)</span> for <span class="math inline">\(w &gt; 0\)</span>.
The expected value of <span class="math inline">\(W\)</span> is
<span class="math display">\[\begin{align*}
   \operatorname{E}[W]
   &amp;= \overbrace{\sum_{w = 0} w\, p_W(w)}^{\text{Discrete component}} \quad + \quad \overbrace{\int_{-\infty}^\infty w\, f_W(w)\, dw}^{\text{Continuous component}}\\
   &amp;= \sum_{w = 0} 0\times (1/2)\quad + \quad \int_{0}^\infty w\times \exp(-2w)\, dw\\
   &amp;= 0 \quad + \quad 1/4\\
   &amp;= 1/4.
\end{align*}\]</span>
The expected value of <span class="math inline">\(W\)</span> is <span class="math inline">\(\operatorname{E}[W] = 1/4\)</span>.</p>
</div>
<div class="example">
<p><span id="exm:ExpectationCoinOnce" class="example"><strong>Example 5.4  (Expectation for a coin toss) </strong></span>Consider tossing a coin <em>once</em> and counting the number of tails.
Let this random variable be <span class="math inline">\(T\)</span>.
The probability function is
<span class="math display">\[
   p_T(t) = \begin{cases}
               0.5 &amp; \text{for $t = 0$ or $t = 1$};\\
               0   &amp; \text{otherwise.}
            \end{cases}
\]</span>
The expected value of <span class="math inline">\(T\)</span> is
<span class="math display">\[\begin{align*}
   \operatorname{E}[T]
   &amp;= \sum_{i = 1}^2 t\, p_T(t)\\
   &amp;= \Pr(T = 0) \times 0 \quad + \quad \Pr(T = 1) \times 1\\
   &amp;= (0.5 \times 0) \qquad + \qquad (0.5 \times 1) = 0.5.
\end{align*}\]</span>
Of course, <span class="math inline">\(0.5\)</span> tails can never actually be observed in practice on one toss.
But it would be silly to round up (or down) and say that the expected number of tails on one toss of a coin is one (or zero).
The expected value of <span class="math inline">\(0.5\)</span> simply means that over a large number of repetitions of this random process, a tail is <em>expected</em> to occur in half of those repetitions.</p>
</div>
<div class="example">
<p><span id="exm:InfiniteMean" class="example"><strong>Example 5.5  (Mean not defined) </strong></span>Consider the distribution of <span class="math inline">\(Z\)</span>, with the probability density function
<span class="math display">\[
   f_Z(z) =
   \begin{cases}
      z^{-2} &amp; \text{for $z \ge 1$};\\
      0      &amp; \text{elsewhere}
   \end{cases}
\]</span>
as in Fig. <a href="#fig:NoMean"><strong>??</strong></a>.
The expected value of <span class="math inline">\(Z\)</span> is
<span class="math display">\[
   \operatorname{E}[Z] = \int_1^{-\infty} z \frac{1}{z^2}\, dz = \int_1^\infty \frac{1}{z} = -\log z \Big|_1^\infty.
\]</span>
However, <span class="math inline">\(\displaystyle\lim_{z\to\infty}\, -\log z \to \infty\)</span>.
The expected value of <span class="math inline">\(\operatorname{E}[Z]\)</span> is undefined.</p>
</div>
<!-- SHOW CODE AND PLOT -->
<!-- ******************* LATEX: image + code display together ******************* -->
<!-- Read in the code -->
<!-- ******************* HTML: image, then code display ******************* -->
<div class="figure">
<span style="display:block;" id="fig:unnamed-chunk-7"></span>
<img src="05-Expectation_files/figure-html/unnamed-chunk-7-1.png" alt="The probability function for the random variable $Z$. The mean is not defined." width="100%"><p class="caption">
FIGURE 5.1: The probability function for the random variable <span class="math inline">\(Z\)</span>. The mean is not defined.
</p>
</div>
<details><summary>
Show R code
</summary><pre><code class="r">


# Define values of z &gt; 1 to plot over
z &lt;- seq(1, 6, length.out = 100)

# Plot for z &gt; 1
plot(x = z, y = z^(-2),
     type = "l", lwd = 2, las = 1,
     xlim = c(0, 6), ylim = c(-0.025, 1),
     xlab = expression( italic(z)),
     ylab = "Density",
     main = expression(paste(The~probability~"function for"~italic(Z))))

# Line for x = 0 to x = 1
lines( x = c(-1, 1),
       y = c(0, 0),
       lwd = 2) ### lwd = 2: Thicker line width

# Dotted vertical line
abline(v = 1, lty = 2, ### lty = 2: means 'dotted lines' 
       col = "grey") 

# Show open point
points(x = 1, y = 0,
       pch = 1) ### pch = 1: open circle


</code></pre>
</details>
</div>
<div id="ExpectationFunction" class="section level2" number="5.2">
<h2>
<span class="header-section-number">5.2</span> Expectation of a function of a random variable<a class="anchor" aria-label="anchor" href="#ExpectationFunction"><i class="fas fa-link"></i></a>
</h2>
<p>
While the mean can be expressed in terms of mathematical expectation, mathematical expectation is a more general concept.</p>
<p>Let <span class="math inline">\(X\)</span> be a discrete random variable with a probability function <span class="math inline">\(p_X(x)\)</span>, or a continuous random variable with PDF <span class="math inline">\(f_X(x)\)</span>.
Also assume <span class="math inline">\(g(X)\)</span> is a real-valued function of <span class="math inline">\(X\)</span>.
We can then define the expected value of <span class="math inline">\(g(X)\)</span>.</p>
<div class="definition">
<p><span id="def:ExpectationFunction" class="definition"><strong>Definition 5.2  (Expectation for function of a random variable) </strong></span>The <em>expected value</em> of some function <span class="math inline">\(g(\cdot)\)</span> of a random variable <span class="math inline">\(X\)</span> is written <span class="math inline">\(\operatorname{E}[ g(X)]\)</span>.</p>
<p>For a <em>discrete</em> random variable <span class="math inline">\(X\)</span> wth PMF <span class="math inline">\(p_X(x)\)</span>, the expected value of <span class="math inline">\(g(X)\)</span> is
<span class="math display">\[
   \operatorname{E}\big[g(X)\big)] = \sum_{x\in \mathcal{R}_X} g(x)\, p_X(x).
\]</span>
For a <em>continuous</em> random variable <span class="math inline">\(X\)</span> wth PDF <span class="math inline">\(f_X(x)\)</span>, the expected value of <span class="math inline">\(g(X)\)</span> is
<span class="math display">\[
   \operatorname{E}\big[g(X)\big] = \int_{-\infty}^\infty g(x)\, f_X(x)\,dx.
\]</span></p>
</div>
<div class="example">
<p><span id="exm:ExpectationDiscreteFnX" class="example"><strong>Example 5.6  (Expectation for a function of a discrete variable) </strong></span>Consider the discrete random variable <span class="math inline">\(U\)</span> with probability function shown in Example <a href="ChapExpectation.html#exm:ExpectationDiscrete">5.1</a>:
<span class="math display">\[
   p_U(u) = \begin{cases}
               (u^2 + 1)/5 &amp; \text{for $u = -1, 0, 1$};\\
               0 &amp; \text{elsewhere}.
           \end{cases}
\]</span>
Since <span class="math inline">\(\mathcal{R}_U = \{-1, 0, 1\}\)</span>, then <span class="math inline">\(\mathcal{R}_V = \{ (-1)^2, 0^2, 1^2\} = \{0, 1\}\)</span>.
Then expected value of <span class="math inline">\(V = U^2\)</span>, where <span class="math inline">\(g(U) = U^2\)</span>, is
<span class="math display">\[\begin{align*}
   \operatorname{E}[V] = \operatorname{E}[g(U)]
   &amp;= \sum_{\mathcal{R}_U} g(u)\, p_U(u) \\
   &amp;= \left( (-1)^2 \times \frac{(-1)^2 + 1}{5} \right ) +
       \left( 0^2 \times \frac{(0)^2  + 1}{5} \right ) +
       \left( 1^2 \times \frac{(1)^2  + 1}{5} \right ) \\
   &amp;= 2/5 + 0 + 2/5 = 4/5.
\end{align*}\]</span>
The expected value of <span class="math inline">\(V = U^2\)</span> is <span class="math inline">\(\operatorname{E}[V] = 4/5\)</span>.</p>
</div>
<div class="example">
<p><span id="exm:ExpectationContinuousXFnX" class="example"><strong>Example 5.7  (Expectation for a function of a continuous variable) </strong></span>Consider the continuous random variable <span class="math inline">\(X\)</span> with probability density function shown in Example <a href="ChapExpectation.html#exm:ExpectationContinuousX">5.2</a>:
<span class="math display">\[
   f_X(x) = \begin{cases}
               x/4 &amp; \text{for $1 &lt; x &lt; 3$};\\
               0 &amp; \text{elsewhere}.
            \end{cases}
\]</span>
The expected value of <span class="math inline">\(Y = \sqrt{X}\)</span>, where <span class="math inline">\(g(X) = \sqrt{X}\)</span>, is
<span class="math display">\[\begin{align*}
   \operatorname{E}[Y] = \operatorname{E}[ g(X) ]
   &amp;= \int_{-\infty}^\infty g(x)\, f_X(x) \, dx\\
   &amp;= \int_1^3 \sqrt{x}\times \frac{x}{4}\, dx\\
   &amp;= \frac{9\sqrt{3} - 1}{10}\approx 1.458...
\end{align*}\]</span>
The expected value of <span class="math inline">\(Y = \sqrt{X}\)</span> is <span class="math inline">\(\operatorname{E}[Y] = (9\sqrt{3} - 1)/10\)</span>.</p>
</div>
<p>Importantly, the expectation operator is a <em>linear operator</em>, as stated below.</p>
<div class="theorem">
<p><span id="thm:ExpectationLinear" class="theorem"><strong>Theorem 5.1  (Expectation properties) </strong></span>For any random variable <span class="math inline">\(X\)</span> and constants <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span>,
<span class="math display">\[
   \operatorname{E}[aX + b] = a\operatorname{E}[X] + b.
\]</span></p>
</div>
<div class="proof">
<p><span id="unlabeled-div-12" class="proof"><em>Proof</em>. </span>Assume <span class="math inline">\(X\)</span> is a discrete random variable with probability function <span class="math inline">\(p_X(x)\)</span>.
By Def. <a href="ChapExpectation.html#def:ExpectationFunction">5.2</a> with <span class="math inline">\(g(X) = aX + b\)</span>,
<span class="math display">\[
   \operatorname{E}[aX + b] = \sum_x (ax + b)\, p_X(x) = a\sum_x p_X(x) + \sum_x b\, p_X(x) = a\operatorname{E}[X] + b,
\]</span>
using that <span class="math inline">\(\sum_x p_X(x) = 1\)</span>.
(The proof in the continuous case is similar, but the probability function is a PDF and integrals replace summations.)</p>
</div>
<div class="example">
<p><span id="exm:ExpectationFunctionY" class="example"><strong>Example 5.8  (Expectation of a function of a random variable) </strong></span>Consider the random variable <span class="math inline">\(Z = 2X\)</span> where <span class="math inline">\(X\)</span> is defined in Example <a href="ChapExpectation.html#exm:ExpectationContinuousX">5.2</a>.
Using Theorem <a href="ChapExpectation.html#thm:ExpectationLinear">5.1</a> with <span class="math inline">\(a = 2\)</span> and <span class="math inline">\(b = 0\)</span>, the value of <span class="math inline">\(\operatorname{E}[Z]\)</span> is
<span class="math display">\[
      \operatorname{E}[Z] = \operatorname{E}[2X] = 2\operatorname{E}[X] = 2 \times 13/6 = 13/3.
\]</span></p>
</div>
<p></p>
</div>
<div id="VarianceStdDev" class="section level2" number="5.3">
<h2>
<span class="header-section-number">5.3</span> The variance and standard deviation<a class="anchor" aria-label="anchor" href="#VarianceStdDev"><i class="fas fa-link"></i></a>
</h2>
<p>
Apart from the mean, the most important description of a random variable is the <em>variability</em>: quantifying how the values of the random variable are dispersed.
The most important measure of variability is the <em>variance</em>.</p>
<p>The <em>variance</em> of a random variable is a measure of the variability of a random variable.
(More correct is to say ‘the variance of the <em>distribution</em> of the random variable’ rather than ‘variance of a random variable’, but this language is commonly used.)
A small variance means the observations are nearly the same (i.e., small variation); a large variance means they are quite different.
The <em>variance</em> can be expressed as a function of a random variable.</p>
<div class="definition">
<p><span id="def:Variance" class="definition"><strong>Definition 5.3  (Variance) </strong></span>The <em>variance</em> of a random variable <span class="math inline">\(X\)</span> (or, of the distribution of <span class="math inline">\(X\)</span>) is
<span class="math display">\[
   \operatorname{var}[X]  = \operatorname{E}\big[(X - \mu)^2\big]
\]</span>
where <span class="math inline">\(\mu = \operatorname{E}[X]\)</span>.
The variance of <span class="math inline">\(X\)</span> is commonly denoted by <span class="math inline">\(\sigma^2\)</span>, or <span class="math inline">\(\sigma^2_X\)</span> if distinguishing among variables is needed.</p>
</div>
<p>The variance is the <em>expected value</em> of the squared distance of the values of the random variable from the mean, weighted by the probability function.
The unit of measurement for variance is the original unit of measurement <em>squared</em>.
That is, if <span class="math inline">\(X\)</span> is measured in metres, the variance of <span class="math inline">\(X\)</span> is in <span class="math inline">\(\text{metres}^2\)</span>.</p>
<p>Describing the variability in terms of the original units is more natural, by taking the square root of the variance.</p>
<div class="definition">
<p><span id="def:StandardDeviation" class="definition"><strong>Definition 5.4  (Standard deviation) </strong></span>The <em>standard deviation</em> of a random variable <span class="math inline">\(X\)</span> is defined as the <em>positive</em> square root of the variance (denoted by <span class="math inline">\(\sigma\)</span>); i.e.,
<span class="math display">\[
   \text{sd}[X] = \sigma = +\sqrt{\operatorname{var}[X]}
\]</span></p>
</div>
<p>The variance is less popular than the standard deviation in practice to describe variability.
In theoretical work, however, the variance is easier to work with than standard deviation (due to the square root), and the variance, rather than standard deviation, features in many results in theoretical statistics.</p>
<div class="example">
<p><span id="exm:VarianceDice" class="example"><strong>Example 5.9  (Variance for a die toss) </strong></span>Suppose a fair die is tossed, and <span class="math inline">\(X\)</span> denotes the number of points showing.
Then <span class="math inline">\(\Pr(X = x) =  1/6\)</span> for <span class="math inline">\(x = 1, 2, 3, 4, 5, 6\)</span> and
<span class="math display">\[
   \mu = \operatorname{E}[X] = \sum_S x\,\Pr(X = x) = (1 + 2 + 3 + 4 + 5 + 6 )/6 = 7/2.
\]</span>
The variance of <span class="math inline">\(X\)</span> is then
<span class="math display">\[\begin{align*}
   \sigma^2
   &amp;= \operatorname{var}[X] = \sum (X - \mu)^2 \Pr(X = x)\\
   &amp;= \frac{1}{6}\left[ \left(1 - \frac{7}{2}\right)^2 + \left(2 - \frac{7}{2}\right)^2 + \dots + \left(6 - \frac{7}{2}\right)^2 \right] = \frac{70}{24}.
\end{align*}\]</span>
The standard deviation is then <span class="math inline">\(\sigma = \sqrt{70/24} = 1.71\)</span>.</p>
</div>
<p>An important result is the <em>computational formula for variance</em>, which is usually easier to use in practice than the formula given in Definition] <a href="ChapExpectation.html#def:Variance">5.3</a>.</p>
<div class="theorem">
<p><span id="thm:VarianceComputational" class="theorem"><strong>Theorem 5.2  (Computational formula for variance) </strong></span>For any random variable <span class="math inline">\(X\)</span>,
<span class="math display">\[
   \operatorname{var}[X] = \operatorname{E}[X^2] - \operatorname{E}[X]^2.
\]</span></p>
</div>
<div class="proof">
<p><span id="unlabeled-div-13" class="proof"><em>Proof</em>. </span>Let <span class="math inline">\(\operatorname{E}[X] = \mu\)</span>, then (using the properties of expectation in Theorem <a href="ChapExpectation.html#thm:ExpectationLinear">5.1</a>):
<span class="math display">\[\begin{align*}
\operatorname{var}[X]
   = \operatorname{E}\left[(X - \mu)^2\right]
   &amp;= \operatorname{E}[X^2 - 2X\mu + \mu^2] \\
   &amp;= \operatorname{E}[X^2] - \operatorname{E}[2X\mu] + \operatorname{E}[\mu^2]\quad\text{(since $\operatorname{E}[\cdot]$ is a linear operator)}\\
   &amp;= \operatorname{E}[X^2] - 2\mu\operatorname{E}[X] + \mu^2\\
   &amp;= \operatorname{E}[X^2] - 2\mu^2 + \mu^2 \\
   &amp;= \operatorname{E}[X^2] - \mu^2 \\
   &amp;= \operatorname{E}[X^2] - \operatorname{E}[X]^2.
\end{align*}\]</span></p>
</div>
<p>This formula is often easier to use to compute <span class="math inline">\(\operatorname{var}[X]\)</span> than using the definition directly.</p>
<div class="example">
<p><span id="exm:VarianceDice2" class="example"><strong>Example 5.10  (Variance for a die toss) </strong></span>Consider Example <a href="ChapExpectation.html#exm:VarianceDice">5.9</a> again.
Then
<span class="math display">\[\begin{align*}
  \operatorname{E}[X^2] = \sum_S x^2 \Pr(X = x)
  &amp;= \frac{1}{6}[1^2 + 2^2 + 3^2 + 4^2 = 5^2 + 6^2]\\
  &amp;= 91/6,
\end{align*}\]</span>
and so <span class="math inline">\(\operatorname{var}[X] = 91/6 - (7/2)^2 = 70/24\)</span>, as before.</p>
</div>
<div class="example">
<p><span id="exm:VarianceComputational" class="example"><strong>Example 5.11  (Variance using computational formula) </strong></span>Consider the continuous random variable <span class="math inline">\(X\)</span> with PDF
<span class="math display">\[
   f_X(x) = \begin{cases}
            3x(2 - x)/4  &amp; \text{for $0 &lt; x &lt; 2$};\\
            0 &amp; \text{elsewhere}.
            \end{cases}
\]</span>
The variance of <span class="math inline">\(X\)</span> can be computed in two ways: using <span class="math inline">\(\operatorname{var}[X] = \operatorname{E}[(X - \mu)^2]\)</span> or using the computational formula.
The expected value of <span class="math inline">\(X\)</span> is
<span class="math display">\[
   \operatorname{E}[X] = \int_0^2 x\times 3x(2 - x)4\, dx = 1.
\]</span>
To use the computational formula, also find
<span class="math display">\[
   \operatorname{E}[X^2] = \frac{6}{5},
\]</span>
and so <span class="math inline">\(\operatorname{var}[X] = \operatorname{E}[X^2] - \operatorname{E}[X]^2 = 1/5\)</span>.</p>
<p>Using the definition,
<span class="math display">\[\begin{align*}
   \operatorname{var}[X]
   = \operatorname{E}\big[(X - \operatorname{E}[X])^2\big]
   &amp;= \operatorname{E}\big[(X - 1)^2\big]\\
   &amp;= \int_0^2 (x - 1)^2 \times 3x(2 - x)/4\,dx = 1/5.
\end{align*}\]</span>
Both methods give the same answer of course, and both methods require initial computation of <span class="math inline">\(\operatorname{E}[X]\)</span>.</p>
</div>
<p>The variance represents the expected value of the squared distance of the values of the random variable from the mean.
The variance is never negative, and is only zero when all the values of the random variable are identical (that is, there <em>is</em> no variation).</p>
<p>If most of the probability lies near the mean, the dispersion will be small; if the probability is spread out over a considerable range the dispersion will be large.</p>
<div class="example">
<p><span id="exm:InfiniteVar" class="example"><strong>Example 5.12  (Variance does not exist) </strong></span>
In Example <a href="ChapExpectation.html#exm:InfiniteMean">5.5</a>, <span class="math inline">\(\operatorname{E}[X]\)</span> was not defined.
For that reason, the variance is also undefined, since computing the variance relies on having a finite value for <span class="math inline">\(\operatorname{E}[X]\)</span>.</p>
</div>
<div class="theorem">
<p><span id="thm:VarianceLinear" class="theorem"><strong>Theorem 5.3  (Variance properties) </strong></span>For any random variable <span class="math inline">\(X\)</span> and constants <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span>,
<span class="math display">\[
   \operatorname{var}[aX + b] = a^2\operatorname{var}[X].
\]</span></p>
</div>
<div class="proof">
<p><span id="unlabeled-div-14" class="proof"><em>Proof</em>. </span>Using the computational formula for the variance:
<span class="math display">\[\begin{align*}
   \operatorname{var}[aX + b]
   &amp;= \operatorname{E}[ (aX + b)^2 ] - \left[\operatorname{E}[aX + b] \right] ^2\\
   &amp;= \operatorname{E}[a^2 X + 2abX + b^2] + (a\mu+b)^2\\
   &amp;= a^2 \operatorname{E}[X^2] - a^2\mu^2\\
   &amp;= a^2 \operatorname{var}[X].
\end{align*}\]</span></p>
</div>
<p>The special case <span class="math inline">\(a = 0\)</span> is instructive: <span class="math inline">\(\operatorname{var}[b] = 0\)</span> when <span class="math inline">\(b\)</span> is constant; that is, a constant has <em>zero</em> variation, as expected.</p>
<div class="example">
<p><span id="exm:VarancenFunctionY" class="example"><strong>Example 5.13  (Variance of a function of a random variable) </strong></span>Consider the random variable <span class="math inline">\(Y = 4 - 2X\)</span> where <span class="math inline">\(\operatorname{E}[X] = 1\)</span> and <span class="math inline">\(\operatorname{var}[X] = 3\)</span>.
Then:
<span class="math display">\[\begin{align*}
  \operatorname{E}[Y]
  &amp;= \operatorname{E}[4 - 2X] = 4 - 2\times\operatorname{E}[X] = 2;\\
  \operatorname{var}[Y]
  &amp;= \operatorname{var}[4 - 2X] = (-2)^2\operatorname{var}[X] = 12.
\end{align*}\]</span></p>
</div>
<p></p>
</div>
<div id="HigherMoments" class="section level2" number="5.4">
<h2>
<span class="header-section-number">5.4</span> Higher moments<a class="anchor" aria-label="anchor" href="#HigherMoments"><i class="fas fa-link"></i></a>
</h2>
<div id="RawCentralMoments" class="section level3" number="5.4.1">
<h3>
<span class="header-section-number">5.4.1</span> Raw and central moments<a class="anchor" aria-label="anchor" href="#RawCentralMoments"><i class="fas fa-link"></i></a>
</h3>
<p>The ideas of a mean and a variance can be generalised.
The mean is a special case of a ‘raw moment’, and the variance is a special case of a ‘central moment’.</p>
<div class="definition">
<p><span id="def:RawMoments" class="definition"><strong>Definition 5.5  (Raw moments) </strong></span>The <em><span class="math inline">\(r\)</span>th raw moment</em>, or <em><span class="math inline">\(r\)</span>th moment about the origin</em>, of a random variable <span class="math inline">\(X\)</span> (where <span class="math inline">\(r\)</span> is a positive integer) is denoted <span class="math inline">\(\mu'_r\)</span> and defined as <span class="math inline">\(\mu'_r = \operatorname{E}[X^r]\)</span>.</p>
<p>For a discrete random variable <span class="math inline">\(X\)</span>, the <span class="math inline">\(r\)</span>th moment about the origin is
<span class="math display">\[
  \mu'_r = \operatorname{E}[X^r] = \sum_X x^r\, p_X(x).
\]</span>
For a continuous random variable <span class="math inline">\(X\)</span>, the <span class="math inline">\(r\)</span>th moment about the origin is
<span class="math display">\[
   \mu'_r = \operatorname{E}[X^r] =  \int_{-\infty}^\infty x^r\, f_X(x)
\]</span></p>
</div>
<div class="definition">
<p><span id="def:CentralMoments" class="definition"><strong>Definition 5.6  (Central moments) </strong></span>The <em><span class="math inline">\(r\)</span>th central moment</em>, or <em><span class="math inline">\(r\)</span>th moment about the mean</em> (where <span class="math inline">\(r\)</span> is a positive integer), is denoted <span class="math inline">\(\mu_r\)</span> and defined as <span class="math inline">\(\mu_r = \operatorname{E}[(X - \mu)^r]\)</span>.</p>
<p>For a discrete random variable <span class="math inline">\(X\)</span>, the <span class="math inline">\(r\)</span>th central moment is
<span class="math display">\[
   \mu_r = \operatorname{E}[(X - \mu)^r] = \sum_x (x - \mu)^r\, p_X(x).
\]</span>
For a continuous random variable <span class="math inline">\(X\)</span>, the <span class="math inline">\(r\)</span>th central moment is
<span class="math display">\[
   \mu_r = \operatorname{E}\big[(X - \mu)^r\big] = \int_{-\infty}^{\infty} (x - \mu)^r\, f_X(x).
\]</span></p>
</div>
<p>From these definitions:</p>
<ul>
<li>the mean <span class="math inline">\(\mu'_1 = \mu\)</span> is the <em>first raw moment</em>;</li>
<li>
<span class="math inline">\(\mu'_2 = \operatorname{E}[X^2]\)</span> is the <em>second raw moment</em>; and</li>
<li>the variance <span class="math inline">\(\mu_2 = \sigma^2\)</span> is the <em>second central moment</em>.</li>
</ul>
</div>
<div id="Skewness" class="section level3" number="5.4.2">
<h3>
<span class="header-section-number">5.4.2</span> Skewness<a class="anchor" aria-label="anchor" href="#Skewness"><i class="fas fa-link"></i></a>
</h3>
<p>
Higher moments also exist that describe other features of a random variable.
The third central moment is related to <em>skewness</em>, a measure the asymmetry of a distribution.</p>
<div class="definition">
<p><span id="def:Symmetry" class="definition"><strong>Definition 5.7  (Symmetry) </strong></span>The distribution of <span class="math inline">\(X\)</span> is said to be <em>symmetric</em> if, for all <span class="math inline">\(x\in \mathcal{R}_X\)</span>,</p>
<ul>
<li>
<span class="math inline">\(p_X(\mu + x) = p_X(\mu - x)\)</span> for a discrete random variable <span class="math inline">\(X\)</span> with PMF <span class="math inline">\(p_X(x)\)</span>, or</li>
<li>
<span class="math inline">\(f_X(\mu + x) = f_X(\mu - x)\)</span> for a continuous random variable <span class="math inline">\(X\)</span> with PDF <span class="math inline">\(f_X(x)\)</span>,</li>
</ul>
<p>where <span class="math inline">\(\mu = \operatorname{E}[X]\)</span> is the mean of <span class="math inline">\(X\)</span>.</p>
</div>
<p>For a symmetric distribution, the odd central moments are zero (Exercise <a href="ChapExpectation.html#exr:SkewDiscreteCentralMomentsZero">5.19</a>).
This suggests that the odd central moments (such as the third central moment) can be used to measure the <em>asymmetry</em> of a distribution.</p>
<p>However, rather than using the third central moment explicitly, by applying Def. <a href="ChapExpectation.html#def:CentralMoments">5.6</a>, finding the appropriate expected value of a <em>normalised</em> version of the random variable (i.e., with mean zero and variance one) is preferred.
That is, the definition of skewness finds the appropriate expected value of <span class="math inline">\((X - \mu)/\sigma\)</span> rather than of <span class="math inline">\(X\)</span> directly.
This means that the value of the skewness for the random variable <span class="math inline">\(X\)</span> is unaffected by a linear transformation of the type <span class="math inline">\(Y = aX + b\)</span> (for constants <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span>).</p>
<div class="definition">
<p><span id="def:Skewness" class="definition"><strong>Definition 5.8  (Skewness) </strong></span>The <em>skewness</em> of the distribution of a random variable <span class="math inline">\(X\)</span> with mean <span class="math inline">\(\operatorname{E}[X] = \mu\)</span> and variance <span class="math inline">\(\operatorname{var}[X] = \sigma^2\)</span> is defined as
<span class="math display" id="eq:Skewness">\[\begin{align}
  \text{skewness} = \gamma_1
  &amp;= \operatorname{E}\left[\left(\frac{X-\mu}{\sigma}\right)^3\right]\notag\\
  &amp;= \frac{\mu_3}{(\sigma^2)^{3/2}}
   = \frac{\mu_3}{\mu_2^{3/2}}.
  \tag{5.1}
\end{align}\]</span></p>
</div>
<p>If <span class="math inline">\(\gamma_1 &gt; 0\)</span> we say the distribution is positively (or right) skewed, and it is ‘stretched’ in the positive (negative) direction.
Similarly, if <span class="math inline">\(\gamma_1 &lt; 0\)</span> we say the distribution is negatively (or left) skewed.
The distribution is symmetric if <span class="math inline">\(\gamma_1 = 0\)</span>.
For a symmetric distribution, the mean is also a median of the distribution, as these results show.</p>
<div class="example">
<p><span id="exm:SkewnessPlots" class="example"><strong>Example 5.14  (Skewness) </strong></span>Figure <a href="ChapExpectation.html#fig:SkewnessPlots">5.2</a> shows examples of right-skewed (left panels), symmetric (centre panels) and left-skewed (right panels) distributions, for both a continuous random variable (top panels) and a discrete random variable (bottom panels).</p>
<p>(The top distributions are all beta distributions; the bottom distributions are all binomial distributions.)</p>
</div>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:SkewnessPlots"></span>
<img src="05-Expectation_files/figure-html/SkewnessPlots-1.png" alt="Examples of right-skewed (left panels), symmetric (centre panels) and left-skewed (right panels) distributions. Top: continuous random variable. Bottom: discrete random variable." width="100%"><p class="caption">
FIGURE 5.2: Examples of right-skewed (left panels), symmetric (centre panels) and left-skewed (right panels) distributions. Top: continuous random variable. Bottom: discrete random variable.
</p>
</div>
<div class="example">
<p><span id="exm:SkewnessCont" class="example"><strong>Example 5.15  (Skewness) </strong></span>Consider the random variable <span class="math inline">\(X\)</span> in Example <a href="ChapExpectation.html#exm:VarianceComputational">5.11</a>, where <span class="math inline">\(f_X(x) = x(2 - x)\)</span> for <span class="math inline">\(0 &lt; x &lt; 2\)</span>.
From that example, <span class="math inline">\(\operatorname{E}[X] = \mu'_1 = 1\)</span> and <span class="math inline">\(\operatorname{E}[X^2] = \mu_2 = 6/5\)</span>.
Then,
<span class="math display">\[
   \mu_3 = \int_0^2 (x - 1)^3\times 3x(2 - x)/4 \,dx = 0,
\]</span>
so that the skewness in Eq. <a href="ChapExpectation.html#eq:Skewness">(5.1)</a> will be zero.
This is expected, since the distribution is symmetric (Fig. <a href="ChapExpectation.html#fig:VarianceComputationalPDF">5.3</a>).</p>
</div>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:VarianceComputationalPDF"></span>
<img src="05-Expectation_files/figure-html/VarianceComputationalPDF-1.png" alt="The probability density function for\ $X$." width="60%"><p class="caption">
FIGURE 5.3: The probability density function for <span class="math inline">\(X\)</span>.
</p>
</div>
<div class="example">
<p><span id="exm:SkewnessDiscrete" class="example"><strong>Example 5.16  (Skewness) </strong></span>Consider the random variable <span class="math inline">\(Y\)</span> with PMF
<span class="math display">\[
   p_Y(y) =
   \begin{cases}
      0.2 &amp; \text{for $y = 5$};\\
      0.3 &amp; \text{for $y = 6$};\\
      0.5 &amp; \text{for $y = 7$};\\
      0   &amp; \text{elsewhere}.
    \end{cases}
\]</span>
Then
<span class="math display">\[
   \mu'_1 = \operatorname{E}[Y] = (5\times 0.2) + (6\times 0.3) + (7\times 0.5) = 6.3.
\]</span>
Likewise,
<span class="math display">\[\begin{align*}
   \mu_2
   &amp;= \operatorname{E}\big[(y - 6.3)^2 \big]
    = (5 - 6.3)^2\times 0.2 + (6 - 6.3)^2\times 0.3 + (7 - 6.3)^2\times 0.5
    = 0.61;\quad{\text{and}}\\
   \mu_3
   &amp;= \operatorname{E}\big[(y - 6.3)^3  \big]
    = (5 - 6.3)^3\times 0.2 + (6 - 6.3)^3\times 0.3 + (7 - 6.3)^3\times 0.5
    = -0.276.
\end{align*}\]</span>
Hence, the skewness is
<span class="math display">\[
   \gamma_1 = \frac{\mu_3}{\mu_2^{3/2}} = \frac{-0.276}{0.61^{3/2}} =  -0.579\dots,
\]</span>
so the distribution has slight negative skewness.</p>
</div>
<p></p>
</div>
<div id="Kurtosis" class="section level3" number="5.4.3">
<h3>
<span class="header-section-number">5.4.3</span> Kurtosis<a class="anchor" aria-label="anchor" href="#Kurtosis"><i class="fas fa-link"></i></a>
</h3>
<p>
Another description of a distribution is <em>kurtosis</em>, which measures the heaviness of the tails in a distribution; that is, how much of the probability of the random variable <span class="math inline">\(X\)</span> is concentrated in the extremes values of <span class="math inline">\(X\)</span>.
This is related to the fourth central moment.
Again, finding the appropriate expected value of a <em>normalised</em> version of the random variable (i.e., with mean zero and variance one) is preferred.
That is, the definition of kurtosis finds the appropriate expected value of <span class="math inline">\((X - \mu)/\sigma\)</span> rather than of <span class="math inline">\(X\)</span> directly.</p>
<div class="definition">
<p><span id="def:Kurtosis" class="definition"><strong>Definition 5.9  (Kurtosis) </strong></span>The <em>kurtosis</em> of a random variable <span class="math inline">\(X\)</span> with mean <span class="math inline">\(\mu = \operatorname{E}[X]\)</span> and variance <span class="math inline">\(\sigma^2 = \operatorname{var}[X]\)</span> is defined as
<span class="math display">\[
  \text{kurtosis}
  = \operatorname{E}\left[\left(\frac{X-\mu}{\sigma}\right)^4\right]
  = \frac{\mu_4}{\mu^2_2}.
\]</span>
The <em>excess kurtosis</em> of the distribution of a random variable is defined as
<span class="math display">\[\begin{equation*}
     \gamma_2 = \frac{\mu_4}{\mu^2_2} - 3.
\end{equation*}\]</span>
The <em>excess kurtosis</em> definition defines the excess kurtosis compared to a bell-shaped (normal distribution), which has an excess kurtosis of zero.</p>
</div>
<div class="importantBox important">
<p>Excess kurtosis is so commonly used that is often just called ‘kurtosis’.</p>
</div>
<p>One way to understand kurtosis (from <span class="citation">Moors (<a href="references.html#ref-moors1986meaning">1986</a>)</span>) is to first define <span class="math inline">\(Z = (X - \mu)/\sigma\)</span>; then the kurtosis is, from Def. <a href="ChapExpectation.html#def:Kurtosis">5.9</a>, <span class="math inline">\(\operatorname{E}[Z^4]\)</span>, and <span class="math inline">\(\operatorname{E}[Z] = 0\)</span> and <span class="math inline">\(\operatorname{var}[X] = 1\)</span>.
Also observe that since <span class="math inline">\(\operatorname{var}[X] =  \operatorname{E}[X^2] - \operatorname{E}[X]^2\)</span> (the definition of variance), we can write
<span class="math display" id="eq:VarianceRearranged">\[\begin{equation}
   \operatorname{E}[X^2] = \operatorname{var}[X] + \operatorname{E}[X]^2.
   \tag{5.2}
\end{equation}\]</span>
Then, the kurtosis is
<span class="math display">\[\begin{align*}
  \operatorname{E}[Z^4]
  &amp;= \operatorname{var}[Z^2] + \operatorname{E}[Z^2]^2\quad\text{(using Eq.~(\ref{eq:VarianceRearranged}))}\\
  &amp;= \operatorname{var}[Z^2] +
     \left\{ \operatorname{var}[Z] + \operatorname{E}[Z]^2\right\}^2\quad\text{(using Eq.~(\ref{eq:VarianceRearranged}) again)}\\
  &amp;= \operatorname{var}[Z^2] + (1 + 0)^2 \\
  &amp;= \operatorname{var}[Z^2] + 1.
\end{align*}\]</span>
Thus, the kurtosis is related to the variance of <span class="math inline">\(Z^2\)</span> (not <span class="math inline">\(Z\)</span>) about the mean.
That is, kurtosis emphasises focuses on the probability function in the extremes of the random variable.</p>
<p>Large values of kurtosis corresponds to greater proportion of the distribution in the tails.
Then (see Fig. <a href="ChapExpectation.html#fig:KurtosisPlots">5.4</a>):</p>
<ul>
<li>distributions with <em>negative</em> excess kurtosis are called <em>platykurtic</em>.
These distribution have fewer, or less extreme, observations in the tail compared to the normal distribution (‘thinner tails’).
Examples include the Bernoulli distribution (Sect. <a href="DiscreteDistributions.html#BernoulliDistribution">7.3</a>).</li>
<li>distributions with <em>positive</em> excess kurtosis are called <em>leptokurtic</em>.
These distribution have more, or more extreme, observations in the tail compared to the normal distribution (‘fatter tails’).
Examples include the exponential distribution (Sect. <a href="ContinuousDistributions.html#ExponentialDistribution">8.4</a>) and Poisson distributions (Sect. <a href="DiscreteDistributions.html#PoissonDistribution">7.7</a>).</li>
<li>distributions with <em>zero</em> excess kurtosis are called <em>mesokurtic</em>.
The normal distribution (Sect. <a href="ContinuousDistributions.html#Normal">8.3</a>) is the obvious example.</li>
</ul>
<div class="figure">
<span style="display:block;" id="fig:KurtosisPlots"></span>
<img src="05-Expectation_files/figure-html/KurtosisPlots-1.png" alt="Kurtosis for three distributions plotted from $x = -3$ to $x = +3$; all plots have mean of\ $0$, variance of\ $1$ and are symmetric. The grey line shows the middle distribution as a reference, with $\gamma_1 = 0$ (zero excess kurtosis)." width="100%"><p class="caption">
FIGURE 5.4: Kurtosis for three distributions plotted from <span class="math inline">\(x = -3\)</span> to <span class="math inline">\(x = +3\)</span>; all plots have mean of <span class="math inline">\(0\)</span>, variance of <span class="math inline">\(1\)</span> and are symmetric. The grey line shows the middle distribution as a reference, with <span class="math inline">\(\gamma_1 = 0\)</span> (zero excess kurtosis).
</p>
</div>
<div class="example">
<p><span id="exm:SkewWind" class="example"><strong>Example 5.17  (Uses of skewness and kurtosis) </strong></span><span class="citation">Monypenny and Middleton (<a href="references.html#ref-monypenny1998analysiswinds">1998b</a>)</span> and <span class="citation">Monypenny and Middleton (<a href="references.html#ref-monypenny1998analysisgust">1998a</a>)</span> use the skewness and kurtosis to analyse wind gusts at Sydney airport.</p>
</div>
<div class="example">
<p><span id="exm:SkewPricing" class="example"><strong>Example 5.18  (Uses of skewness and kurtosis) </strong></span><span class="citation">Galagedera, Henry, and Silvapulle (<a href="references.html#ref-galagedera2002conditional">2002</a>)</span> used higher moments in a capital analysis pricing model for Australian stock returns.</p>
</div>
<div class="example">
<p><span id="exm:SkewDiscrete" class="example"><strong>Example 5.19  (Skewness and kurtosis) </strong></span>Consider the discrete random variable <span class="math inline">\(U\)</span> from Example <a href="ChapExpectation.html#exm:ExpectationDiscrete">5.1</a>.
The raw moments are
<span class="math display">\[\begin{align*}
   \mu'_r = \operatorname{E}[U^r]
   &amp;= \sum_{u = -1, 0, 1} u^r \frac{u^2 + 1}{5} \\
   &amp;= (-1)^r \frac{ (-1)^2 + 1}{5} +
       (0)^r \frac{ (0)^2 + 1}{5} +
       (1)^r \frac{ (1)^2 + 1}{5} \\
   &amp;= \frac{2(-1)^r}{5} + 0 + \frac{2}{5} \\
   &amp;= \frac{2}{5}[ (-1)^r + 1]
\end{align*}\]</span>
for the <span class="math inline">\(r\)</span>th raw moment.
Then,
<span class="math display">\[\begin{align*}
   \operatorname{E}[X]   &amp;= \mu'_1 = \frac{2}{5}[ (-1)^1 + 1 ] = 0;\\
   \operatorname{E}[X^2] &amp;= \mu'_2 = \frac{2}{5}[ (-1)^2 + 1 ] = 4/5;\\
   \operatorname{E}[X^3] &amp;= \mu'_1 = \frac{2}{5}[ (-1)^3 + 1 ] = 0;\\
   \operatorname{E}[X^4] &amp;= \mu'_2 = \frac{2}{5}[ (-1)^4 + 1 ] = 4/5.
\end{align*}\]</span>
Since <span class="math inline">\(\operatorname{E}[U] = 0\)</span>, then the <span class="math inline">\(r\)</span>th central and raw moments are the same: <span class="math inline">\(\mu'_r = \mu_r\)</span>.
Notice that once the initial computations to find <span class="math inline">\(\mu'_r\)</span> are complete, the evaluation of any raw moment is simple.</p>
<p>The skewness is
<span class="math display">\[
   \gamma_1 = \frac{\mu_3}{\mu_2^{3/2}} = \frac{0}{(4/5)^{3/2}} = 0,
\]</span>
so the distribution is symmetric.
The excess kurtosis is
<span class="math display">\[
   \gamma_2 = \frac{\mu_4}{\mu_2^2} -3 = \frac{4/5}{(4/5)^2} -3 = -7/4,
\]</span>
so the distribution is platykurtic.</p>
</div>
<p></p>
</div>
</div>
<div id="MGF" class="section level2" number="5.5">
<h2>
<span class="header-section-number">5.5</span> Moment-generating functions<a class="anchor" aria-label="anchor" href="#MGF"><i class="fas fa-link"></i></a>
</h2>
<p></p>
<div id="MGFIntroduction" class="section level3" number="5.5.1">
<h3>
<span class="header-section-number">5.5.1</span> Introduction<a class="anchor" aria-label="anchor" href="#MGFIntroduction"><i class="fas fa-link"></i></a>
</h3>
<p>By themselves, the mean, variance, skewness and kurtosis do not completely describe a distribution; many different distributions can be found having a given mean, variance, skewness and kurtosis.
However, in general, <em>all</em> the moments of a distribution together define the distribution.
This leads to the idea of a <em>moment-generating function</em>.</p>
<p>Suppose I asked you to draw the probability density function of a random variable <span class="math inline">\(X\)</span>, with <span class="math inline">\(\operatorname{E}[X] = 2\)</span>.
Any of the six distributions in Fig. <a href="ChapExpectation.html#fig:SixDistributions">5.5</a> meet this (first moment) criterion, so this information is not sufficient to uniquely define a distribution.</p>
<p>So, suppose a second criterion is added: in addition, we require <span class="math inline">\(\operatorname{var}[X] = 1\)</span>.
Any of the first five distributions in Fig. <a href="ChapExpectation.html#fig:SixDistributions">5.5</a> meet these two criteria (based on the first two moments), so again this information is not sufficient to uniquely define a distribution.</p>
<p>Suppose a third criterion is added: the distribution must be symmetric.
Any of the top four distributions in Fig. <a href="ChapExpectation.html#fig:SixDistributions">5.5</a> meet these three criteria (based on the first three moments); again, this information is not sufficient to uniquely define a distribution.</p>
<p>Suppose a fourth criterion is added: the distribution must have zero excess kurtosis.
Either of the top two distributions in Fig. <a href="ChapExpectation.html#fig:SixDistributions">5.5</a> meet these four criteria (based on the first four moments); again, this information is not sufficient to uniquely define a distribution.</p>
<p>In general, <em>all</em> the moments of a distribution are needed to uniquely define a distribution.
However, computing all (or even many) moments of a distribution is usually very tedious.
For this reason, the <em>moment generating function</em> (or MGF) is now introduced, a function that encapsulates <em>all</em> the moments of a distribution.</p>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:SixDistributions"></span>
<img src="05-Expectation_files/figure-html/SixDistributions-1.png" alt="Six distributions, all with mean 1 and variance 1. The top four are also symmetric (i.e., $\gamma_1 = 0$); the top two also have zero excess kurtosis (i.e., $\gamma_2=0$)." width="90%"><p class="caption">
FIGURE 5.5: Six distributions, all with mean 1 and variance 1. The top four are also symmetric (i.e., <span class="math inline">\(\gamma_1 = 0\)</span>); the top two also have zero excess kurtosis (i.e., <span class="math inline">\(\gamma_2=0\)</span>).
</p>
</div>
</div>
<div id="MGFDefinition" class="section level3" number="5.5.2">
<h3>
<span class="header-section-number">5.5.2</span> Definition<a class="anchor" aria-label="anchor" href="#MGFDefinition"><i class="fas fa-link"></i></a>
</h3>
<p>So far, the distribution of a random variable has been described using a probability function or a distribution function.
Sometimes, however, working with a different representation is useful (for example, see Sect. <a href="ChapterTransformations.html#TransformationMoments">6.4</a>).</p>
<p>In this section, the <em>moment-generating function</em> is used to represent the distribution of the probabilities of a random variable.
As the name suggests, this function can be used to generate <em>any</em> moment of a distribution.
Other uses of the moment-generating function are seen later (see Sect. <a href="ChapterTransformations.html#TransformationMoments">6.4</a>).</p>
<div class="definition">
<p><span id="def:MGF" class="definition"><strong>Definition 5.10  (Moment-generating function (MGF)) </strong></span>The <em>moment-generating function</em> (or MGF)
<span class="math inline">\(M_X(t)\)</span> of the random variable <span class="math inline">\(X\)</span> defined over a range <span class="math inline">\(\mathcal{R}_X\)</span> is denoted <span class="math inline">\(M_X(t)\)</span>, and defined as
<span class="math display">\[
   \operatorname{E}\big[\exp(tX)\big],
\]</span>
provided the expectation exists for values of <span class="math inline">\(t\)</span> in some interval that includes <span class="math inline">\(t = 0\)</span>.</p>
<p>When <span class="math inline">\(X\)</span> is a discrete random variable,
<span class="math display">\[
  M_X(t)  = \operatorname{E}\big[\exp(tX)\big] = \sum_{x\in \mathcal{R}_X} \exp(tx)\, p_X(x).
\]</span>
When <span class="math inline">\(X\)</span> is a continuous random variable,
<span class="math display">\[
  M_X(t)  = \operatorname{E}\big[\exp(tX)\big] = \int_{-\infty}^\infty \exp(tx)\, f_X(x).
\]</span></p>
</div>
<p>The MGF may not always exist (that is, converge to a finite value) for all values of <span class="math inline">\(t\)</span>, so the MGF may not be defined for all values of <span class="math inline">\(t\)</span>.
Note that the MGF always exists for <span class="math inline">\(t = 0\)</span>; in fact <span class="math inline">\(M_X(0) = 1\)</span>.</p>
<p>Provided the MGF is defined for some values of <span class="math inline">\(t\)</span> <em>other</em> than zero, it <em>uniquely</em> defines a probability distribution, and we can use it to easily generate the moments of the distribution, as described in Theorem <a href="ChapExpectation.html#thm:Moments">5.4</a>.</p>
<div class="linkBox link">
<p>Moment-generating functions are related to Laplace transformations.</p>
</div>
<div class="example">
<p><span id="exm:MGF" class="example"><strong>Example 5.20  (Moment-generating function) </strong></span>Consider the random variable <span class="math inline">\(Y\)</span> with PDF
<span class="math display">\[
   f_Y(y) =
   \begin{cases}
      \exp(-y) &amp; \text{for $y &gt; 0$};\\
      0        &amp; \text{elsewhere.}
   \end{cases}
\]</span>
The MGF is
<span class="math display">\[\begin{align*}
   M_Y(t)
    = \operatorname{E}[\exp(tY)]
   &amp;= \int_0^\infty \exp(ty)\,\exp(-y)\, dy \\
   &amp;= \int_0^\infty \exp\{ y(t-1) \}\, dy \\
   &amp;= (1 - t)^{-1}
\end{align*}\]</span>
provided <span class="math inline">\(t - 1 &lt; 0\)</span>; that is, for <span class="math inline">\(t &lt; 1\)</span> (which includes <span class="math inline">\(t = 0\)</span>).
If <span class="math inline">\(t &gt; 1\)</span>, the integral does not converge.
For example, if <span class="math inline">\(t = 2\)</span>,
<span class="math display">\[
   \left. \frac{1}{2 - 1} \exp(y)\right|_{y = 0}^{y = \infty} = \exp(0) - \lim_{y\to\infty} \exp(y)
\]</span>
which does not converge.</p>
</div>
<div class="example">
<p><span id="exm:MGFDice" class="example"><strong>Example 5.21  (MGF for die rolls) </strong></span>Consider the PMF of <span class="math inline">\(X\)</span>, the outcome of tossing a fair die (Example <a href="ChapExpectation.html#exm:VarianceDice">5.9</a>).
The MGF of <span class="math inline">\(X\)</span> is
<span class="math display">\[\begin{align*}
   M_X(t)
   &amp;= \operatorname{E}[\exp(tX)] = \sum_{x = 1}^6 \exp(tx)\, p_X(x)\\
   &amp;= \frac{1}{6}\left(e^t + e^{2t} + e^{3t} + e^{4t} + e^{5t} + e^{6t}\right),
\end{align*}\]</span>
which exists for all values of <span class="math inline">\(t\)</span>.</p>
</div>
<div class="example">
<p><span id="exm:MGFDoesNotExist" class="example"><strong>Example 5.22  (MGF does not exist) </strong></span>Consider the Cauchy distribution with the PDF
<span class="math display">\[
   f_X(x) = \frac{1}{\pi(1 + x^2)},
\]</span>
defined over <span class="math inline">\(x\in\mathbb{R}\)</span>.
The moment generating function is
<span class="math display">\[
  \operatorname{E}[\exp(tX)]
  = \int_{-\infty}^{\infty} e^{tx}\frac{1}{\pi(1 + x^2)}\,dx.
\]</span>
Consider the integrand <span class="math inline">\(\exp(tx)/\big(\pi(1 + x^2)\big)\)</span>.
This integrand does not converge unless <span class="math inline">\(t = 0\)</span>.</p>
<p>For example, consider <span class="math inline">\(t &gt; 0\)</span>: as <span class="math inline">\(x\to\infty\)</span>, we see <span class="math inline">\(\exp(tx)\to\infty\)</span>, while <span class="math inline">\(1/(1 + x^2)\to 0\)</span> quite slowly; the integrand diverges (see Fig. <a href="ChapExpectation.html#fig:MGFdiverges">5.6</a> (left panel) for an example when <span class="math inline">\(t = 1\)</span>).
Now consider <span class="math inline">\(t &lt; 0\)</span>: as <span class="math inline">\(x\to-\infty\)</span>, we see <span class="math inline">\(\exp(tx)\to\infty\)</span>, while <span class="math inline">\(1/(1 + x^2)\to 0\)</span> quite slowly; the integrand again diverges (see Fig. <a href="ChapExpectation.html#fig:MGFdiverges">5.6</a> (right panel) for an example when <span class="math inline">\(t = -1\)</span>).</p>
<p>The integral only converges for <span class="math inline">\(t = 0\)</span>.
The definition for the MGF states that the MGF exists ‘provided the expectation exists for values of <span class="math inline">\(t\)</span> in some interval that includes <span class="math inline">\(t = 0\)</span>’.
This is not the case: the integral exists <em>only</em> for <span class="math inline">\(t = 0\)</span>.
The MGF does not exist for the Cauchy distribution.</p>
</div>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:MGFdiverges"></span>
<img src="05-Expectation_files/figure-html/MGFdiverges-1.png" alt="The MGF cannot be computed, as the integrand diverges for $t &gt; 0$ (left panel) and for $t &lt; 0$ (right panel)." width="100%"><p class="caption">
FIGURE 5.6: The MGF cannot be computed, as the integrand diverges for <span class="math inline">\(t &gt; 0\)</span> (left panel) and for <span class="math inline">\(t &lt; 0\)</span> (right panel).
</p>
</div>
</div>
<div id="MGFMoments" class="section level3" number="5.5.3">
<h3>
<span class="header-section-number">5.5.3</span> Using the MGF to generate moments<a class="anchor" aria-label="anchor" href="#MGFMoments"><i class="fas fa-link"></i></a>
</h3>
<p>Replacing <span class="math inline">\(\exp(xt)\)</span> by its series expansion (App. <a href="UsefulSeries.html#UsefulSeries">B</a>) in the definition of the MGF for a discrete random variable <span class="math inline">\(X\)</span> gives
<span class="math display">\[\begin{align*}
     M_X(t)
     &amp; = {\sum_x} \left(1 + xt + \frac{x^2t^2}{2!} + \dots\right) \Pr(X = x)\\
     &amp; = 1 + \mu'_1t + \mu'_2 \frac{t^2}{2!} +\mu'_3 \frac{t^3}{3!} + \dots
\end{align*}\]</span>
Then, the <span class="math inline">\(r\)</span>th moment of a distribution about the origin is seen to be the coefficient of <span class="math inline">\(t^r/r!\)</span> in the series expansion of <span class="math inline">\(M_X(t)\)</span>:
<span class="math display">\[\begin{align*}
     \frac{d M_X(t)}{dt}
     &amp; = \sum_x x\,e^{xt}\Pr(X = x)\\
     \frac{d^2 M_X(t)}{dt^2}
     &amp; = \sum_x x^2\,e^{xt} \Pr(X = x),
\end{align*}\]</span>
and, in general, for each positive integer <span class="math inline">\(r\)</span>:
<span class="math display">\[
   \frac{d^r M_X(t)}{dt^r} = \sum_x x^re^{xt}\Pr(X = x).
\]</span>
On setting <span class="math inline">\(t = 0\)</span>,
<span class="math display">\[\begin{align*}
\left.\frac{d M_X(t)}{dt}\right|_{t = 0} &amp;= \operatorname{E}[X]\\
\left.\frac{d^2M_X(t)}{dt^2}\right|_{t = 0} &amp;= \operatorname{E}[X^2].
\end{align*}\]</span>
(The notation to the left means to evaluate the expression at <span class="math inline">\(t = 0\)</span>.)
In general, for each positive integer <span class="math inline">\(r\)</span>,
<span class="math display">\[\begin{equation}
  \left.\frac{d^r M_X(t)}{dt^r}\right|_{t = 0} = \operatorname{E}[X^r].
\end{equation}\]</span>
(Sometimes, <span class="math inline">\(d^r M_X(t)/dt^r\)</span> evaluated at <span class="math inline">\(t = 0\)</span> is written as <span class="math inline">\(M^{(r)}(0)\)</span> for brevity.)
This result is summarised in the following theorem.</p>
<div class="theorem">
<p><span id="thm:Moments" class="theorem"><strong>Theorem 5.4  (Moments) </strong></span>The <span class="math inline">\(r\)</span>th moment <span class="math inline">\(\mu'_r\)</span> of the distribution of the random variable <span class="math inline">\(X\)</span> about the origin is given by either</p>
<ol style="list-style-type: decimal">
<li>the coefficient of <span class="math inline">\(t^r/r!,  r = 1, 2, 3,\dots\)</span> in the power series expansion of <span class="math inline">\(M_X(t)\)</span>; or</li>
<li>
<span class="math inline">\(\displaystyle \mu'_r = \left.\frac{d^rM(t)}{dt^r}\right|_{t = 0}\)</span> where <span class="math inline">\(M_X(t)\)</span> is the MGF of <span class="math inline">\(X\)</span>.</li>
</ol>
</div>
<div class="example">
<p><span id="exm:MeanVarMGF" class="example"><strong>Example 5.23  (Mean and variance from a MGF) </strong></span>Continuing Example <a href="ChapExpectation.html#exm:MGF">5.20</a>, the mean and variance of <span class="math inline">\(Y\)</span> can be found from the MGF.
To find the mean, first find
<span class="math display">\[
   \frac{d}{dt}M_Y(t) = (1 - t)^{-2}.
\]</span>
Setting <span class="math inline">\(t = 0\)</span> gives the mean as <span class="math inline">\(\operatorname{E}[Y] = 1\)</span>.
Likewise,
<span class="math display">\[
   \frac{d^2}{dt^2}M_Y(t) = 2(1 - t)^{-3}.
\]</span>
Setting <span class="math inline">\(t = 0\)</span> gives <span class="math inline">\(\operatorname{E}[Y^2] = 2\)</span>.
The variance is therefore <span class="math inline">\(\operatorname{var}[Y] = 2 - 1^2 = 1\)</span>.</p>
<p>Once the moment-generating function has been computed, raw moments can be computed using
<span class="math display">\[
   \operatorname{E}[Y^r] = \mu'_r = \left.\frac{d^r}{dt^r} M_Y(t)\right|_{t = 0}.
\]</span></p>
</div>
</div>
<div id="some-useful-results" class="section level3" number="5.5.4">
<h3>
<span class="header-section-number">5.5.4</span> Some useful results<a class="anchor" aria-label="anchor" href="#some-useful-results"><i class="fas fa-link"></i></a>
</h3>
<p>The moment-generating function can be used to derive the distribution of a function of a random variable (see Sect. <a href="ChapterTransformations.html#TransformationMoments">6.4</a>).
The following theorems are valuable for this task.</p>
<div class="theorem">
<p><span id="thm:MGFLinear" class="theorem"><strong>Theorem 5.5  (MGF of linear combinations) </strong></span>If the random variable <span class="math inline">\(X\)</span> has MGF <span class="math inline">\(M_X(t)\)</span> and <span class="math inline">\(Y = aX + b\)</span> where <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> are constants, then the MGF of <span class="math inline">\(Y\)</span> is
<span class="math display">\[
   M_Y(t) = \operatorname{E}\big[\exp\{t(aX + b)\}\big] = \exp(bt) M_X(at).
\]</span></p>
</div>
<div class="theorem">
<p><span id="thm:MGFIndependent" class="theorem"><strong>Theorem 5.6  (MGF of independent rvs) </strong></span>If <span class="math inline">\(X_1\)</span>, <span class="math inline">\(X_2\)</span>, <span class="math inline">\(\dots\)</span>, <span class="math inline">\(X_n\)</span> are <span class="math inline">\(n\)</span> independent random variables, where <span class="math inline">\(X_i\)</span> has MGF <span class="math inline">\(M_{X_i}(t)\)</span>, then the MGF of <span class="math inline">\(Y = X_1 + X_2 + \cdots X_n\)</span> is
<span class="math display">\[
   M_Y(t) = \prod_{i = 1}^n M_{X_i}(t).
\]</span></p>
</div>
<div class="proof">
<p><span id="unlabeled-div-15" class="proof"><em>Proof</em>. </span>The proofs are left as an exercise.</p>
</div>
<p>Note that in the special case when all the random variables are independently and identically distributed in Theorem <a href="ChapExpectation.html#thm:MGFLinear">5.5</a>, we have
<span class="math display">\[
   M_Y(t) = [M_{X_i}(t)]^n.
\]</span></p>
<div class="example">
<p><span id="exm:MGFLinearCombinations" class="example"><strong>Example 5.24  (MGF of linear combinations) </strong></span>Consider the random variable <span class="math inline">\(X\)</span> with pf
<span class="math display">\[
   p_X(x) = 2(1/3)^x \qquad \text{for $x = 1, 2, 3, \dots$}
\]</span>
and zero elsewhere.
The MGF of <span class="math inline">\(X\)</span> is
<span class="math display">\[\begin{align*}
   M_X(t)
   &amp;= \sum_{x: p(x) &gt; 0} \exp(tx)\, p_X(x) \\
   &amp;= \sum_{x = 1}^\infty \exp(tx)\, 2(1/3)^x \\
   &amp;= 2\sum_{x = 1}^\infty (\exp(t)/3)^x \\
   &amp;= 2\left\{ \frac{\exp(t)}{3} + \left(\frac{\exp(t)}{3}\right)^2
   + \left(\frac{\exp(t)}{3}\right)^3 + \dots\right\} \\
   &amp;= \frac{2\exp(t)}{3 - \exp(t)}
\end{align*}\]</span>
where <span class="math inline">\(\sum_{y = 1}^\infty a^y = a/(1 - a)\)</span> for <span class="math inline">\(a &lt; 1\)</span> has been used (App. <a href="UsefulSeries.html#UsefulSeries">B</a>); here <span class="math inline">\(a = \exp(t)/3\)</span>.</p>
<p>Next consider finding the MGF of <span class="math inline">\(Y = (X - 2)/3\)</span>.
From Theorem <a href="ChapExpectation.html#thm:MGFLinear">5.5</a> with <span class="math inline">\(a = 1/3\)</span> and <span class="math inline">\(b = -2/3\)</span>,
<span class="math display">\[
   M_Y(t)
   = \exp(-2t/3) M_X(t/3)
   = \frac{2\exp\{(-t)/3\}}{3 - \exp(t/3)}.
\]</span>
In practice, rather than identify <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> and remember Theorem <a href="ChapExpectation.html#thm:MGFLinear">5.5</a>, problems like this are best solved directly from the definition of the MGF:
<span class="math display">\[\begin{align*}
   M_Y(t)
    = \operatorname{E}[\exp(tY)]
   &amp;= \operatorname{E}[\exp\{t(X - 2)/3\}]\\
   &amp;= \operatorname{E}[\exp\{tX/3 - 2t/3\}]\\
   &amp;= \exp(-2t/3) M_X(t/3) \\
   &amp;= \frac{2\exp\{(-t)/3\}}{3 - \exp(t/3)}.
\end{align*}\]</span></p>
</div>
</div>
<div id="DistributionFromMGF" class="section level3" number="5.5.5">
<h3>
<span class="header-section-number">5.5.5</span> Determining the distribution from the MGF<a class="anchor" aria-label="anchor" href="#DistributionFromMGF"><i class="fas fa-link"></i></a>
</h3>
<p>The MGF (if it exists) completely determines the distribution of a random variable hence, given a MGF, deducing the probability function should be possible.
For some distributions, the PDF cannot be written in closed form (so the PDF can <em>only</em> be evaluated numerically; for example, see Sect. <a href="ChapterMixedDistributions.html#TweedieModels">9.4</a>), but the MGF is relatively simple to write down.</p>
<p>For a <em>discrete</em> random variable <span class="math inline">\(X\)</span>, the MGF is defined as
<span class="math display" id="eq:MGFtoPDFdiscrete">\[\begin{equation}
   M_X(t)
   = \operatorname{E}[\exp(tX)]
   = \sum_X e^{tx} p_X(x)
   \tag{5.3}
\end{equation}\]</span>
for <span class="math inline">\(X\)</span> discrete with PMF <span class="math inline">\(p_X(x)\)</span>.
This can be expressed as
<span class="math display">\[\begin{align*}
   M_X(t)
   &amp;= \exp(t x_1) p_X(x_1) + \exp(t x_2)p_X(x_2) + \dots\\
   &amp;= \exp(t x_1) \Pr(X = x_1) + \exp(t x_2)\Pr(X = x_2) + \dots\\
\end{align*}\]</span>
and so the probability function of <span class="math inline">\(Y\)</span> can be deduced from the MGF.</p>
<div class="example">
<p><span id="exm:DistFromMGF" class="example"><strong>Example 5.25  (Distribution from the MGF) </strong></span>Suppose a discrete random variable <span class="math inline">\(D\)</span> has the MGF
<span class="math display">\[
   M_D(t) = \frac{1}{3} \exp(2t) + \frac{1}{6}\exp(3t) + \frac{1}{12}\exp(6t)
   + \frac{5}{12}\exp(7t).
\]</span>
Then, by the definition of the MGF in the discrete case given above, the coefficient of <span class="math inline">\(t\)</span> in the exponential indicates values of <span class="math inline">\(D\)</span>, and the coefficient indicates the probability of that value of <span class="math inline">\(Y\)</span>:
<span class="math display">\[\begin{align*}
   M_D(t)
   &amp;= \overbrace{\frac{1}{3} \exp(2t)}^{D = 2} + \overbrace{\frac{1}{6}\exp(3t)}^{D = 3} +
       \overbrace{\frac{1}{12}\exp(6t)}^{D = 6} + \overbrace{\frac{5}{12}\exp(7t)}^{D = 7}\\
   &amp;= \Pr(D = 2)\exp(2t) + \Pr(D = 3)\exp(3t) + \\
   &amp; \quad \Pr(D = 6)\exp(6t) + \Pr(D = 7)\exp(7t).
\end{align*}\]</span>
So the PMF is
<span class="math display">\[
   p_D(d) =
   \begin{cases}
      1/3 &amp; \text{for $d=2$}\\
      1/6 &amp; \text{for $d=3$}\\
      1/12 &amp; \text{for $d=6$}\\
      5/12 &amp; \text{for $d=7$}\\
      0 &amp; \text{otherwise}
   \end{cases}
\]</span>
(Of course, it is easy to check by computing the MGF for <span class="math inline">\(D\)</span> from the pf found above; you should get the original MGF.)</p>
</div>
<p>Sometimes, using the results in App. <a href="UsefulSeries.html#UsefulSeries">B</a> can be helpful.</p>
<div class="example">
<p><span id="exm:DistFromMGF2" class="example"><strong>Example 5.26  (Distribution from the MGF) </strong></span>Consider the MGF
<span class="math display">\[
   M_X(t) = \frac{\exp(t)}{3 - 2\exp(t)}.
\]</span>
To find the corresponding probability function, one approach is to write the MGF as
<span class="math display">\[
   M_X(t) = \frac{\exp(t)/3}{1 - 2\exp(t)/3}.
\]</span>
This is the sum of a geometric series (Eq. <a href="UsefulSeries.html#eq:SumGeometricInfinite">(B.5)</a>):
<span class="math display">\[
   a + ar + ar^2 + \ldots + ar^{n - 1}
   \rightarrow \frac{a}{1 - r} \text{ as $n  \rightarrow  \infty$},
\]</span>
where <span class="math inline">\(a = \exp(t)/3\)</span> and <span class="math inline">\(r = 2\exp(t)/3\)</span>.
Hence the MGF can be expressed as
<span class="math display">\[
   \frac{1}{3}\exp(t) +
   \frac{1}{3}\left(\frac{2}{3}\right) \exp(2t) +
   \frac{1}{3}\left(\frac{2}{3}\right)^2 \exp(3t) + \dots
\]</span>
so that the probability function can be deduced as
<span class="math display">\[\begin{align*}
   \Pr(X = 1) &amp;= \frac{1}{3};\\
   \Pr(X = 2) &amp;= \frac{1}{3}\left(\frac{2}{3}\right);\\
   \Pr(X = 3) &amp;= \frac{1}{3}\left(\frac{2}{3}\right)^2,
\end{align*}\]</span>
or, in general,
<span class="math display">\[
   p_x(x) = \frac{1}{3}\left( \frac{2}{3}\right)^{x - 1}\quad\text{for $x = 1, 2, 3, \dots$}.
\]</span>
(Later, this will be identified as a <a href="DiscreteDistributions.html#GeometricDistribution">geometric distribution</a>.)</p>
</div>
<p>For a <em>continuous</em> random variable <span class="math inline">\(X\)</span>, the approach is more involved.
Suppose the continuous random variable <span class="math inline">\(X\)</span> has the MGF <span class="math inline">\(M_X(t)\)</span>.
Then the probability density function is (see <span class="citation">Abramowitz and Stegun (<a href="references.html#ref-abramowitz1964handbook">1964</a>)</span>, 26.1.10)
<span class="math display" id="eq:MGFtoPDFcontinuous">\[\begin{equation}
   f_X(x) =
   \frac{1}{2\pi} \int_{-\infty}^{\infty} M_X(it) \exp(-itx)\, dt,
   \tag{5.4}
\end{equation}\]</span>
where <span class="math inline">\(i = \sqrt{-1}\)</span>.</p>
<div class="example">
<p><span id="exm:DistFromMGFContinuous" class="example"><strong>Example 5.27  (Distribution from the MGF) </strong></span>Consider the MGF for a continuous random variable <span class="math inline">\(X\)</span> such that <span class="math inline">\(M_X(t) = \exp(t^2/2)\)</span>, for <span class="math inline">\(y\in\mathbb{R}\)</span> and <span class="math inline">\(t\in\mathbb{R}\)</span>.
Then, <span class="math inline">\(M_X(it) = \exp\left( (it)^2/2 \right) = \exp(-t^2/2)\)</span>.
Using Eq. <a href="ChapExpectation.html#eq:MGFtoPDFcontinuous">(5.4)</a>, the PDF is:
<span class="math display">\[\begin{align*}
f_X(x)
&amp;= \frac{1}{2\pi} \int_{-\infty}^{\infty} \exp(-t^2/2) \exp(-itx)\, dt,\\
&amp;= \frac{1}{2\pi} \int_{-\infty}^{\infty} \exp(-t^2/2)\left[ \cos(-tx) + i\sin(-itx)\right]\, dt,
\end{align*}\]</span>
since <span class="math inline">\(x\in\mathbb{R}\)</span> and <span class="math inline">\(t\in\mathbb{R}\)</span>.
Extracting just the real components:
<span class="math display">\[\begin{align*}
f_X(x)
&amp;= \frac{1}{2\pi} \int_{-\infty}^{\infty} \exp(-t^2/2) \cos(-tx) \, dt\\
&amp;= \frac{1}{2\pi} \left( \sqrt{2\pi} \exp( -x^2/2 ) \right)
  = \frac{1}{ \sqrt{2\pi} } \exp( -x^2/2 ),
\end{align*}\]</span>
which will later be identified as a <a href="ContinuousDistributions.html#StandardNormal">normal distribution</a> with a mean of zero, and standard deviation of one.</p>
</div>
<p>In practice, using Eq. <a href="ChapExpectation.html#eq:MGFtoPDFcontinuous">(5.4)</a> can become tedious or intractable for producing a closed form expression for the PDF.
However, Eq. <a href="ChapExpectation.html#eq:MGFtoPDFcontinuous">(5.4)</a> has been used to compute numerical values of the probability density function.
For example, <span class="citation">Dunn and Smyth (<a href="references.html#ref-mypapers:Dunn:fourier">2008</a>)</span> used Eq. <a href="ChapExpectation.html#eq:MGFtoPDFcontinuous">(5.4)</a> to evaluate the <em>Tweedie distributions</em> <span class="citation">(<a href="references.html#ref-mypapers:DunnSmyth:IWSM:2001">Dunn and Smyth 2001</a>)</span>, for which (in general) the PDF has no closed form, but does have a simple MGF.
</p>
</div>
</div>
<div id="Tchebysheff" class="section level2" number="5.6">
<h2>
<span class="header-section-number">5.6</span> Tchebysheff’s inequality<a class="anchor" aria-label="anchor" href="#Tchebysheff"><i class="fas fa-link"></i></a>
</h2>
<p>Tchebysheff’s inequality applies to any probability distribution, and is sometimes useful in theoretical work or to provide bounds on probabilities.</p>
<div class="theorem">
<p><span id="thm:Tchebysheff" class="theorem"><strong>Theorem 5.7  (Tchebysheff's theorem) </strong></span>Let <span class="math inline">\(X\)</span> be a random variable with finite mean <span class="math inline">\(\mu\)</span> and variance <span class="math inline">\(\sigma^2\)</span>.
Then for any positive <span class="math inline">\(k\)</span>,
<span class="math display" id="eq:Tchebysheff">\[\begin{equation}
   \Pr\big(|X - \mu| \geq k\sigma \big)\leq \frac{1}{k^2}
   \tag{5.5}
\end{equation}\]</span>
or, equivalently
<span class="math display">\[\begin{equation}
   \Pr\big(|X - \mu| &lt; k\sigma \big)\geq 1 - \frac{1}{k^2}.
\end{equation}\]</span></p>
</div>
<div class="proof">
<p><span id="unlabeled-div-16" class="proof"><em>Proof</em>. </span>The proof for the continuous case only is given.
Let <span class="math inline">\(X\)</span> be continuous with PDF <span class="math inline">\(f(x)\)</span>.
For some <span class="math inline">\(c &gt; 0\)</span>, then
<span class="math display">\[\begin{align*}
     \sigma^2
     &amp; = \int^\infty_{-\infty} (x - \mu )^2f(x)\,dx\\
     &amp; = \int^{\mu -\sqrt{c}}_{-\infty} (x - \mu )^2f(x)\, dx +
         \int^{\mu + \sqrt{c}}_{\mu-\sqrt{c}}(x - \mu )^2f(x)\,dx +
         \int^\infty_{\mu + \sqrt{c}}(x - \mu)^2f(x)\,dx\\
     &amp; \geq \int^{\mu -\sqrt{c}}_{-\infty} (x - \mu )^2f(x)\,dx +
       \int^\infty_{\mu + \sqrt{c}}(x - \mu )^2f(x)\,dx,
\end{align*}\]</span>
since the second integral is non-negative.
Now <span class="math inline">\((x - \mu )^2 \geq c\)</span> if <span class="math inline">\(x \leq \mu -\sqrt{c}\)</span> or <span class="math inline">\(x\geq \mu + \sqrt{c}\)</span>.
So in both the remaining integrals above, replace <span class="math inline">\((x - \mu )^2\)</span> by <span class="math inline">\(c\)</span> without altering the direction of the inequality:
<span class="math display">\[\begin{align*}
  \sigma^2
  &amp;\geq  c \int^{\mu -\sqrt{c}}_{-\infty} f(x)\,dx + c\int^\infty_{\mu + \sqrt{c}}f(x)\,dx\\
  &amp;=  c\,\Pr(X \leq \mu - \sqrt{c}\,) + c\,\Pr(X \geq \mu + \sqrt{c}\,)\\
  &amp;=  c\,\Pr(|X - \mu| \geq \sqrt{c}\,).
\end{align*}\]</span>
Putting <span class="math inline">\(\sqrt{c} = k\sigma\)</span>, Eq. <a href="ChapExpectation.html#eq:Tchebysheff">(5.5)</a> is obtained.</p>
</div>
<p>With the probability function or PDF of a random variable <span class="math inline">\(X\)</span>, then <span class="math inline">\(\operatorname{E}[X]\)</span> and <span class="math inline">\(\operatorname{var}[X]\)</span> can be found, but the converse is not true.
That is, from a knowledge of <span class="math inline">\(\operatorname{E}[X]\)</span> and <span class="math inline">\(\operatorname{var}[X]\)</span> we cannot reconstruct the probability distribution of <span class="math inline">\(X\)</span> and hence cannot compute probabilities such as <span class="math inline">\(\Pr(|X - \mu| \geq k\sigma)\)</span>.
Nonetheless, using Tchebysheff’s inequality we can find a useful <em>bound</em> to either the probability outside or inside of <span class="math inline">\(\mu \pm k\sigma\)</span>.</p>
</div>
<div id="ExpectationsBivariate" class="section level2" number="5.7">
<h2>
<span class="header-section-number">5.7</span> Mathematical expectation for bivariate distributions<a class="anchor" aria-label="anchor" href="#ExpectationsBivariate"><i class="fas fa-link"></i></a>
</h2>
<div id="ExpectationsBivariateFunctions" class="section level3" number="5.7.1">
<h3>
<span class="header-section-number">5.7.1</span> Expected values of a bivariate function<a class="anchor" aria-label="anchor" href="#ExpectationsBivariateFunctions"><i class="fas fa-link"></i></a>
</h3>
<p>In a manner analogous to the univariate case, the expectation of functions of two random variables can be given.</p>
<div class="definition">
<p><span id="def:BVExpectation" class="definition"><strong>Definition 5.11  (Expectation for bivariate distributions) </strong></span>Let <span class="math inline">\((X, Y)\)</span> be a <span class="math inline">\(2\)</span>-dimensional random variable and let <span class="math inline">\(u(X, Y)\)</span> be a function of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>.</p>
<p>For a <em>discrete</em> bivariate distribution with probability mass function <span class="math inline">\(p_{X, Y}(x, y)\)</span> defined over <span class="math inline">\((x, y) \in R\)</span>, the <em>expectation</em> or <em>expected value</em> of <span class="math inline">\(\operatorname{E}[u(X, Y)]\)</span> is
<span class="math display">\[
   \operatorname{E}[u(X, Y)]
   = \mathop{\sum\sum}_{(x, y)\in R} u(x, y)\, p_{X, Y}(x, y).
\]</span>
For a <em>continuous</em> bivariate distribution with probability density function <span class="math inline">\(f_{X, Y}(x, y)\)</span> defined over <span class="math inline">\((x, y) \in R\)</span>, the <em>expectation</em> or <em>expected value</em> of <span class="math inline">\(\operatorname{E}[u(X, Y)]\)</span> is
<span class="math display">\[
   \operatorname{E}[u(X, Y)]
   = \mathop{\sum\sum}_{(x, y)\in R} u(x, y)\, p_{X, Y}(x, y).
\]</span></p>
</div>
<p>This definition can be extended to the expectation of a function of any number of random variables.</p>
<div class="example">
<p><span id="exm:ExpectationFunctionTwoDiscrete" class="example"><strong>Example 5.28  (Expectation of function of two rvs (discrete)) </strong></span>Consider the joint distribution of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> in Example <a href="ChapBivariate.html#exm:BVDiscrete3">4.4</a>.
Determine <span class="math inline">\(\operatorname{E}[X + Y]\)</span>; i.e., the mean of the number of heads plus the number showing on the die.</p>
<p>From Def. <a href="ChapExpectation.html#def:BVExpectation">5.11</a>, write <span class="math inline">\(u(X, Y) = X + Y\)</span> and so
<span class="math display">\[\begin{align*}
   \operatorname{E}[X + Y]
   &amp;= \sum_{x = 0}^2 \sum_{y = 1}^6 (x + y)\, p_{X, Y}(x, y)\\
   &amp;= 1\times(1/24) + 2\times(1/24) + \dots + 6\times(1/24)\\
   &amp; \qquad + 2\times(1/12) + 3\times(1/12) + \dots + 7\times(1/12)\\
   &amp; \qquad + 3\times(1/24) + 4\times(1/24) + \dots + 8\times(1/24)\\
   &amp;= 21/24 + 27/12 + 33/24\\
   &amp;= 4.5.
\end{align*}\]</span>
The answer is just <span class="math inline">\(\operatorname{E}[X] + \operatorname{E}[Y] = 1 + 3.5 = 4.5\)</span>.
This is no coincidence, as we see from Theorem <a href="ChapExpectation.html#thm:ExpTwoRV">5.8</a>.</p>
</div>
<div class="example">
<p><span id="exm:ExpectationFunctionTwoContinuous" class="example"><strong>Example 5.29  (Expectation of function of two rvs (continuous)) </strong></span>Consider Example <a href="ChapBivariate.html#exm:BVDiscreteBank">4.6</a>.
To determine <span class="math inline">\(\operatorname{E}[XY]\)</span>, write <span class="math inline">\(u(X, Y) = XY\)</span> and proceed:
<span class="math display">\[
  \operatorname{E}[XY]
   = \frac{6}{5} \int_0^1\int_0^1 xy(x + y^2)\,dx\,dy
   = \frac7{20}.
\]</span>
Unlike the previous example, an alternative simple calculation based on <span class="math inline">\(\operatorname{E}[X]\)</span> and <span class="math inline">\(\operatorname{E}[Y]\)</span> is not possible, since <span class="math inline">\(\operatorname{E}[XY]\neq\operatorname{E}[X] \operatorname{E}[Y]\)</span> in general.</p>
</div>
<div class="theorem">
<p><span id="thm:ExpTwoRV" class="theorem"><strong>Theorem 5.8  (Expectations of two rvs) </strong></span>If <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are any random variables, and <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> are any constants, then
<span class="math display">\[
   \operatorname{E}[aX + bY] = a\operatorname{E}[X] + b\operatorname{E}[Y].
\]</span></p>
</div>
<p>This theorem is no surprise after seeing Theorem <a href="ChapExpectation.html#thm:ExpectationLinear">5.1</a>, but is powerful and useful.
The proof given here is for the discrete case; the continuous case is analogous.</p>
<div class="proof">
<p><span id="unlabeled-div-17" class="proof"><em>Proof</em>. </span><span class="math display">\[\begin{align*}
  \operatorname{E}[aX + bY]
  &amp;= \mathop{\sum\sum}_{(x, y) \in R}(ax + by) \, p_{X, Y}(x, y), \text{ by definition}\\
  &amp;= \sum_x \sum_y ax\, p_{X, Y}(x, y) + \sum_x \sum_y by\, p_{X, Y}(x, y)\\
  &amp;= a\sum_x x\sum_y p_{X, Y}(x, y) + b\sum_y y\sum_x p_{X, Y}(x, y)\\
  &amp;= a\sum_x x \Pr(X = x) + b\sum_y y \Pr(Y = y)\\
  &amp;= a\operatorname{E}[X] + b\operatorname{E}[Y].
\end{align*}\]</span></p>
</div>
<p>This result is true whether or not <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are independent.
Theorem <a href="ChapExpectation.html#thm:ExpTwoRV">5.8</a> naturally generalises to the expected value of a <em>linear combination of random variables</em> (see Theorem <a href="MultivariateExtensions.html#thm:ExpLinear">11.1</a>).</p>
</div>
<div id="moments-of-a-bivariate-distribution-covariance" class="section level3" number="5.7.2">
<h3>
<span class="header-section-number">5.7.2</span> Moments of a bivariate distribution: covariance<a class="anchor" aria-label="anchor" href="#moments-of-a-bivariate-distribution-covariance"><i class="fas fa-link"></i></a>
</h3>
<p>The idea of a moment in the univariate case naturally extends to the bivariate case.
Hence, define <span class="math inline">\(\mu'_{rs} = \operatorname{E}[X^r Y^s]\)</span> or <span class="math inline">\(\mu_{rs} = \operatorname{E}\big[(X - \mu_X)^r (Y - \mu_Y)^s\big]\)</span> as the raw and central moments for a bivariate distribution.</p>
<p>The most important of these moments is the covariance.</p>
<div class="definition">
<p><span id="def:BVCovariance" class="definition"><strong>Definition 5.12  (Covariance) </strong></span>The <em>covariance</em> of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> is defined as
<span class="math display">\[
   \operatorname{Cov}(X, Y) = \operatorname{E}[(X - \mu_X)(Y - \mu_Y)].
\]</span>
When <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are discrete,
<span class="math display">\[
   \operatorname{Cov}(X, Y) =
   \sum_{x} \sum_{y} (x - \mu_X)(y - \mu_Y)\, p_{X, Y}(x, y).
\]</span>
When <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are continuous,
<span class="math display">\[
   \operatorname{Cov}(X, Y) =
   \int_{-\infty}^\infty\!\int_{-\infty}^\infty (x - \mu_X)(y - \mu_Y)\, f_{X, Y}(x, y)\, dx\, dy.
\]</span></p>
</div>
<p>The covariance is a measure of how <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> vary jointly, in the sense that a positive covariance indicates that ‘on average’ <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> increase (or decrease) together whereas a negative covariance indicates that `on average’ as <span class="math inline">\(X\)</span> increases and <span class="math inline">\(Y\)</span> decreases (and vice versa).
We say that covariance is a measure of <em>linear dependence</em>.</p>
<p>Covariance is best evaluated from the computational formula.</p>
<div class="theorem">
<p><span id="thm:Covariance" class="theorem"><strong>Theorem 5.9  (Covariance) </strong></span>For any random variables <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>,
<span class="math display">\[
   \operatorname{Cov}(X, Y)
   =
   \operatorname{E}[XY] - \operatorname{E}[X]\operatorname{E}[Y].
\]</span></p>
</div>
<div class="proof">
<p><span id="unlabeled-div-18" class="proof"><em>Proof</em>. </span>The proof uses Theorems <a href="ChapExpectation.html#thm:ExpectationLinear">5.1</a> and <a href="ChapExpectation.html#thm:ExpTwoRV">5.8</a>.
<span class="math display">\[\begin{align*}
   \operatorname{Cov}(X, Y)
   &amp;= \operatorname{E}\big[ (X - \mu_X)(Y-\mu_Y)\big] \\
   &amp;= \operatorname{E}[ XY - \mu_X Y - \mu_Y X + \mu_X\mu_Y] \\
   &amp;= \operatorname{E}[ XY ] - \mu_X\operatorname{E}[Y] - \mu_Y\operatorname{E}[X] +  \mu_X \mu_Y \\
   &amp;= \operatorname{E}[ XY ] - \mu_X\mu_Y - \mu_Y\mu_X +  \mu_X \mu_Y \\
   &amp;= \operatorname{E}[ XY ] - \mu_X \mu_Y.
\end{align*}\]</span></p>
</div>
<p>Computing the covariance is tedious: <span class="math inline">\(\operatorname{E}[X]\)</span>, <span class="math inline">\(\operatorname{E}[Y]\)</span>, <span class="math inline">\(\operatorname{E}[XY]\)</span> need to be computed, and so the joint and marginal distributions of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are needed.</p>
<p>Covariance has units given by the product of the units of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>.
For example, if <span class="math inline">\(X\)</span> is measured in metres and <span class="math inline">\(Y\)</span> is measured in seconds then <span class="math inline">\(\operatorname{Cov}(XY)\)</span> has the units metre–seconds.
To compare the strength of covariation amongst pairs of random variables, a unitless measure is useful.
Correlation does this by scaling the covariance in terms of the standard deviations of the individual variables.</p>
<div class="definition">
<p><span id="def:Correlation" class="definition"><strong>Definition 5.13  (Correlation) </strong></span>The <em>correlation coefficient</em> between the random variables <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> is denoted by <span class="math inline">\(\text{Corr}(X, Y)\)</span> or <span class="math inline">\(\rho_{X, Y}\)</span> and is defined as
<span class="math display">\[
   \rho_{X, Y}
   = \frac{\operatorname{Cov}(X, Y)}{\sqrt{ \operatorname{var}[X]\operatorname{var}[Y]}}
   = \frac{\sigma_{X, Y}}{\sigma_X \sigma_Y}.
\]</span></p>
</div>
<p>If there is no confusion over which random variables are involved, we write <span class="math inline">\(\rho\)</span> rather than <span class="math inline">\(\rho_{XY}\)</span>.
It can be shown that <span class="math inline">\(-1 \leq \rho \leq 1\)</span>.</p>
<div class="example">
<p><span id="exm:CorrCoefDiscrete" class="example"><strong>Example 5.30  (Correlation coefficient (discrete rvs)) </strong></span>Consider two discrete random variables <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> with the joint pf given in Table <a href="ChapExpectation.html#tab:Joint3">5.1</a>.
To compute the correlation coefficient, the following steps are required.</p>
<ul>
<li>
<span class="math inline">\(\text{Corr}(X, Y) = \operatorname{Cov}(X, Y)/\sqrt{ \operatorname{var}[X]\operatorname{var}[Y]}\)</span>, so <span class="math inline">\(\operatorname{var}[X]\)</span>, <span class="math inline">\(\operatorname{var}[Y]\)</span> must be computed;</li>
<li>To find <span class="math inline">\(\operatorname{var}[X]\)</span> and <span class="math inline">\(\operatorname{var}[Y]\)</span>, <span class="math inline">\(\operatorname{E}[X]\)</span> and <span class="math inline">\(\operatorname{E}[X^2]\)</span>, <span class="math inline">\(\operatorname{E}[Y]\)</span> and <span class="math inline">\(\operatorname{E}[Y^2]\)</span> are needed, so the marginal probability functions of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are needed.</li>
</ul>
<p>So first, the marginal pfs are
<span class="math display">\[
   p_X(x) = \sum_{y = -1, 1} p_{X, Y}(x, y) =
      \begin{cases}
          7/24 &amp; \text{for $x = 0$};\\
          8/24 &amp; \text{for $x = 1$};\\
          9/24 &amp; \text{for $x = 2$};\\
          0 &amp; \text{otherwise}
      \end{cases}
\]</span>
and
<span class="math display">\[
   p_Y(y) = \sum_{x = 0}^2 p_{X, Y}(x, y) =
      \begin{cases}
          1/2 &amp; \text{for $y = -1$};\\
          1/2 &amp; \text{for $y = 1$};\\
          0 &amp; \text{otherwise.}
      \end{cases}
\]</span>
Then,
<span class="math display">\[\begin{align*}
   \operatorname{E}[X]   &amp;= (7/24 \times 0) + (8/24 \times 1) + (9/24\times 2) = 26/24;\\
   \operatorname{E}[X^2] &amp;= (7/24 \times 0^2) + (8/24 \times 1^2) + (9/24\times 2^2) = 44/24;\\
   \operatorname{E}[Y]   &amp;= (1/2 \times -1) + (1/2 \times 1) = 0;\\
   \operatorname{E}[Y^2] &amp;= (1/2 \times (-1)^2) + (1/2 \times 1^2) = 1,
\end{align*}\]</span>
giving <span class="math inline">\(\operatorname{var}[X] = 44/24 - (26/24)^2 = 0.6597222\)</span> and <span class="math inline">\(\operatorname{var}[Y] = 1 - 0^2 = 1\)</span>.
Then,
<span class="math display">\[\begin{align*}
   \operatorname{E}[XY] &amp;= \sum_x\sum_y xy\,p_{X,Y}(x,y) \\
    &amp;= (0\times -1 \times 1/8)  + (0\times 1 \times 1/6) + \cdots + (2\times 1 \times 1/4) \\
    &amp;= 1/12.
\end{align*}\]</span>
Hence,
<span class="math display">\[
   \operatorname{Cov}(X,Y)
   = \operatorname{E}[XY] - \operatorname{E}[X] \operatorname{E}[Y]
   = \frac{1}{12} - \left(\frac{26}{24}\times 0\right) = 1/12,
\]</span>
and
<span class="math display">\[
   \text{Corr}(X,Y)
   = \frac{ \operatorname{Cov}(X,Y)}{\sqrt{ \operatorname{var}[X]\operatorname{var}[Y] } }
   = \frac{1/12}{\sqrt{0.6597222 \times 1}}
   = 0.1025978,
\]</span>
so the correlation coefficient is about <span class="math inline">\(0.10\)</span>, and a small positive linear association exists between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>.</p>
</div>
<div class="inline-table"><table class="table table-sm">
<caption>
<span id="tab:Joint3">TABLE 5.1: </span>A bivariate discrete probability function.
</caption>
<thead><tr>
<th style="text-align:right;font-weight: bold;">
</th>
<th style="text-align:right;font-weight: bold;">
<span class="math inline">\(x = 0\)</span>
</th>
<th style="text-align:right;font-weight: bold;">
<span class="math inline">\(x = 1\)</span>
</th>
<th style="text-align:right;font-weight: bold;">
<span class="math inline">\(x = 2\)</span>
</th>
<th style="text-align:right;font-weight: bold;">
Total
</th>
</tr></thead>
<tbody>
<tr>
<td style="text-align:right;font-weight: bold;">
<span class="math inline">\(y = -1\)</span>
</td>
<td style="text-align:right;">
<span class="math inline">\(1/8\)</span>
</td>
<td style="text-align:right;">
<span class="math inline">\(1/4\)</span>
</td>
<td style="text-align:right;">
<span class="math inline">\(1/8\)</span>
</td>
<td style="text-align:right;">
<span class="math inline">\(1/2\)</span>
</td>
</tr>
<tr>
<td style="text-align:right;font-weight: bold;">
<span class="math inline">\(y = +1\)</span>
</td>
<td style="text-align:right;">
<span class="math inline">\(1/6\)</span>
</td>
<td style="text-align:right;">
<span class="math inline">\(1/12\)</span>
</td>
<td style="text-align:right;">
<span class="math inline">\(1/4\)</span>
</td>
<td style="text-align:right;">
<span class="math inline">\(1/2\)</span>
</td>
</tr>
<tr>
<td style="text-align:right;font-weight: bold;">
Total
</td>
<td style="text-align:right;">
<span class="math inline">\(7/24\)</span>
</td>
<td style="text-align:right;">
<span class="math inline">\(1/3\)</span>
</td>
<td style="text-align:right;">
<span class="math inline">\(3/8\)</span>
</td>
<td style="text-align:right;">
<span class="math inline">\(1\)</span>
</td>
</tr>
</tbody>
</table></div>
</div>
<div id="properties-of-covariance-and-correlation" class="section level3" number="5.7.3">
<h3>
<span class="header-section-number">5.7.3</span> Properties of covariance and correlation<a class="anchor" aria-label="anchor" href="#properties-of-covariance-and-correlation"><i class="fas fa-link"></i></a>
</h3>
<ul>
<li>The correlation has no units.</li>
<li>The covariance has units; if <span class="math inline">\(X\)</span> is measured in kilograms and <span class="math inline">\(Y\)</span> in centimetres, then the units of the covariance are kg-cm.</li>
<li>If the units of measurements change, the numerical value of the covariance changes, but the numerical value of the correlation stays the same.
(For example, if <span class="math inline">\(X\)</span> is changed from kilograms to grams, the numerical value of the correlation will not change in value, but the numerical values of covariance will change.)</li>
<li>The correlation is a number between <span class="math inline">\(-1\)</span> and <span class="math inline">\(1\)</span> (inclusive).
When the correlation coefficient (or covariance) is negative, a <em>negative linear relationship</em> is said to exist between the two variables.
Likewise, when the correlation coefficient (or covariance) is positive, a <em>positive linear relationship</em> is said to exist between the two variables.</li>
<li>When the correlation coefficient (or covariance) is zero, no <em>linear</em> dependence is said to exist.</li>
</ul>
<div class="theorem">
<p><span id="thm:CovarianceProperties" class="theorem"><strong>Theorem 5.10  (Properties of the covariance) </strong></span>For random variables <span class="math inline">\(X\)</span>, <span class="math inline">\(Y\)</span> and <span class="math inline">\(Z\)</span>, and constants <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span>:</p>
<ul>
<li>
<span class="math inline">\(\operatorname{Cov}(X, Y) = \operatorname{Cov}(Y, X)\)</span>.</li>
<li>
<span class="math inline">\(\operatorname{Cov}(aX,bY) = ab\,\operatorname{Cov}(X, Y)\)</span>.</li>
<li>
<span class="math inline">\(\operatorname{var}[aX + bY] = a^2\operatorname{var}[X] + b^2\operatorname{var}[Y] + ab\,\operatorname{Cov}(X, Y)\)</span>.</li>
<li>If <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are independent, then <span class="math inline">\(\operatorname{E}[XY] = \operatorname{E}[X]\operatorname{E}[Y]\)</span> and hence <span class="math inline">\(\operatorname{Cov}(X,Y) = 0\)</span>.</li>
<li>
<span class="math inline">\(\operatorname{Cov}(X, Y) = 0\)</span> does not imply <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are independent, except for the special case of the bivariate normal distribution.</li>
</ul>
</div>
<p>A zero correlation coefficient in an indication of no <em>linear</em> dependence only.
A relationship may still exist between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> even if the correlation is zero.</p>
<div class="example">
<p><span id="exm:LinearDependence" class="example"><strong>Example 5.31  (Linear dependence and correlation) </strong></span>Consider <span class="math inline">\(X\)</span> with the pf:</p>
<div class="inline-table"><table class="table table-sm">
<thead><tr class="header">
<th><span class="math inline">\(x\)</span></th>
<th><span class="math inline">\(-1\)</span></th>
<th><span class="math inline">\(0\)</span></th>
<th><span class="math inline">\(1\)</span></th>
</tr></thead>
<tbody><tr class="odd">
<td><span class="math inline">\(p_{X}(x)\)</span></td>
<td><span class="math inline">\(1/3\)</span></td>
<td><span class="math inline">\(1/3\)</span></td>
<td><span class="math inline">\(1/3\)</span></td>
</tr></tbody>
</table></div>
<p>Then, define <span class="math inline">\(Y\)</span> to be explicitly related to <span class="math inline">\(X\)</span>: <span class="math inline">\(Y = X^2\)</span>.
So, we <em>know</em> a relationship exists between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> (but the relationship is non-linear).
The joint probability function for <span class="math inline">\((X, Y)\)</span> is shown in Table <a href="ChapExpectation.html#tab:JointRship">5.2</a>.
Then
<span class="math display">\[\begin{equation*}
   \operatorname{Cov}(X, Y)
   = \operatorname{E}[X, Y] - \operatorname{E}[X]\operatorname{E}[Y]
   = 0 - 0\times 2/3 = 0
\end{equation*}\]</span>
so <span class="math inline">\(\text{Corr}(X, Y) = 0\)</span>.
But <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are <em>certainly related</em>, because <span class="math inline">\(Y\)</span> was explicitly defined as a function of <span class="math inline">\(X\)</span>.</p>
<p>Since the correlation is a measure of the strength of the <em>linear</em> relationship between two random variables, a correlation of zero simply is indication of no <em>linear</em> relationship between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>.
(As is the case in this example, there may be a different relationship between the variables, but no linear relationship.)</p>
</div>
<div class="inline-table"><table class="table table-sm">
<caption>
<span id="tab:JointRship">TABLE 5.2: </span>A bivariate discrete probability function.
</caption>
<thead><tr>
<th style="text-align:right;font-weight: bold;">
</th>
<th style="text-align:right;font-weight: bold;">
<span class="math inline">\(x = -1\)</span>
</th>
<th style="text-align:right;font-weight: bold;">
<span class="math inline">\(x = 0\)</span>
</th>
<th style="text-align:right;font-weight: bold;">
<span class="math inline">\(x = 1\)</span>
</th>
<th style="text-align:right;font-weight: bold;">
Total
</th>
</tr></thead>
<tbody>
<tr>
<td style="text-align:right;font-weight: bold;">
<span class="math inline">\(y = 0\)</span>
</td>
<td style="text-align:right;">
<span class="math inline">\(0\)</span>
</td>
<td style="text-align:right;">
<span class="math inline">\(1/3\)</span>
</td>
<td style="text-align:right;">
<span class="math inline">\(0\)</span>
</td>
<td style="text-align:right;">
<span class="math inline">\(1/3\)</span>
</td>
</tr>
<tr>
<td style="text-align:right;font-weight: bold;">
<span class="math inline">\(y = 1\)</span>
</td>
<td style="text-align:right;">
<span class="math inline">\(1/3\)</span>
</td>
<td style="text-align:right;">
<span class="math inline">\(0\)</span>
</td>
<td style="text-align:right;">
<span class="math inline">\(1/3\)</span>
</td>
<td style="text-align:right;">
<span class="math inline">\(2/3\)</span>
</td>
</tr>
<tr>
<td style="text-align:right;font-weight: bold;">
Total
</td>
<td style="text-align:right;">
<span class="math inline">\(1/3\)</span>
</td>
<td style="text-align:right;">
<span class="math inline">\(1/3\)</span>
</td>
<td style="text-align:right;">
<span class="math inline">\(1/3\)</span>
</td>
<td style="text-align:right;">
<span class="math inline">\(1\)</span>
</td>
</tr>
</tbody>
</table></div>
</div>
<div id="ConditionalExpectation" class="section level3" number="5.7.4">
<h3>
<span class="header-section-number">5.7.4</span> Conditional expectations<a class="anchor" aria-label="anchor" href="#ConditionalExpectation"><i class="fas fa-link"></i></a>
</h3>
<p>Conditional expectations are simply expectations computed from a conditional distribution.</p>
<p>The conditional mean is the expected value computed from a conditional distribution.</p>
<div class="definition">
<p><span id="def:ConditionalExpectation" class="definition"><strong>Definition 5.14  (Conditional expectation) </strong></span>The <em>conditional expected value</em> or <em>conditional mean</em> of a random variable <span class="math inline">\(X\)</span> for given <span class="math inline">\(Y = y\)</span> is denoted by <span class="math inline">\(\operatorname{E}[X \mid Y = y]\)</span>.</p>
<p>If the conditional distribution is discrete with probability mass function <span class="math inline">\(p_{X\mid Y}(x\mid y)\)</span>, then
<span class="math display">\[
  \operatorname{E}[X \mid Y = y] =
  \displaystyle \sum_{x} x \cdot p_{X\mid Y}(x\mid y).
\]</span>
If the conditional distribution is continuous with probability density function <span class="math inline">\(f_{X\mid Y}(x\mid y)\)</span>, then
<span class="math display">\[
  \operatorname{E}[X \mid Y = y] =
  \int_{-\infty}^\infty x \cdot f_{X\mid Y}(x\mid y)\, dx.
\]</span></p>
</div>
<p><span class="math inline">\(\operatorname{E}[X \mid Y = y]\)</span> is typically denoted <span class="math inline">\(\mu_{X \mid Y = y}\)</span>.</p>
<div class="example">
<p><span id="exm:ConditionalMean" class="example"><strong>Example 5.32  (Conditional mean (continuous)) </strong></span>Consider the two random variables <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> with joint PDF
<span class="math display">\[
   f_{X, Y}(x, y) =
      \begin{cases}
         \frac{3}{5}(x + xy + y^2) &amp; \text{for $0 &lt; x &lt; 1$ and $-1 &lt; y &lt; 1$};\\
         0 &amp; \text{otherwise.}
      \end{cases}
\]</span>
To find <span class="math inline">\(f_{Y \mid X = x}(y\mid x)\)</span>, first <span class="math inline">\(f_X(x)\)</span> is needed:
<span class="math display">\[
   f_X(x) = \int_{-1}^1 f_{X,Y}(x,y) dy = \frac{3}{15}(6x + 2)
\]</span>
for <span class="math inline">\(0 &lt; x &lt; 1\)</span>.
Then,
<span class="math display">\[
   f_{Y \mid X = x}(y \mid x)
   = \frac{ f_{X, Y}(x, y)}{ f_X(x) }
   = \frac{3(x + xy + y^2)}{6x + 2}
\]</span>
for <span class="math inline">\(-1 &lt; y &lt; 1\)</span> and given <span class="math inline">\(0 &lt; x &lt; 1\)</span>.
The expected value of <span class="math inline">\(Y\)</span> given <span class="math inline">\(X = x\)</span> is then
<span class="math display">\[
  \operatorname{E}[Y\mid X = x]
   = \frac{x}{3x + 1}.
\]</span>
This expression indicates that the conditional expected value of <span class="math inline">\(Y\)</span> depends on the given value of <span class="math inline">\(X\)</span>; for example,
<span class="math display">\[\begin{align*}
   \operatorname{E}[Y\mid X = 0]   &amp;= 0;\\
   \operatorname{E}[Y\mid X = 0.5] &amp;= 0.2;\\
   \operatorname{E}[Y\mid X = 1]   &amp;= 1/4.
\end{align*}\]</span>
Since <span class="math inline">\(\operatorname{E}[Y\mid X = x]\)</span> depends on the value of <span class="math inline">\(X\)</span>, this means <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are <em>not</em> independent.</p>
</div>
<p>The conditional variance is the variance computed from a conditional distribution.</p>
<div class="definition">
<p><span id="def:ConditionalVariance" class="definition"><strong>Definition 5.15  (Conditional variance) </strong></span>The <em>conditional variance</em> of a random variable <span class="math inline">\(X\)</span> for given <span class="math inline">\(Y = y\)</span> is denoted by <span class="math inline">\(\operatorname{var}[X \mid Y = y]\)</span>.</p>
<p>If the conditional distribution is discrete with probability mass function <span class="math inline">\(p_{X\mid Y}(x\mid y)\)</span>, then
<span class="math display">\[
  \operatorname{var}[X \mid Y = y] =
  \displaystyle \sum_{x} (x - \mu_{X\mid y})^2\, p_{X\mid Y}(x\mid y),
\]</span>
where <span class="math inline">\(\mu_{X \mid y}\)</span> is the <em>conditional mean</em> of <span class="math inline">\(X\)</span> given <span class="math inline">\(Y = y\)</span>.</p>
<p>If the conditional distribution is continuous with probability density function <span class="math inline">\(f_{X\mid Y}(x\mid y)\)</span>, then
<span class="math display">\[
  \operatorname{var}[X \mid Y = y] =
  \int_{-\infty}^\infty (x - \mu_{X\mid y})^2\, f_{X\mid Y}(x\mid y)\, dx.
\]</span>
where <span class="math inline">\(\mu_{X \mid y}\)</span> is the <em>conditional mean</em> of <span class="math inline">\(X\)</span> given <span class="math inline">\(Y = y\)</span>.</p>
</div>
<p>For brevity, <span class="math inline">\(\operatorname{var}[X \mid Y = y]\)</span> is often denoted <span class="math inline">\(\sigma^2_{X \mid Y = y}\)</span>.</p>
<div class="example">
<p><span id="exm:ConditionVariance" class="example"><strong>Example 5.33  (Conditional variance (continuous)) </strong></span>Refer to Example <a href="ChapExpectation.html#exm:ConditionalMean">5.32</a>.
The conditional variance of <span class="math inline">\(Y\)</span> given <span class="math inline">\(X = x\)</span> can be found by first computing <span class="math inline">\(\operatorname{E}[Y^2\mid X = x]\)</span>:
<span class="math display">\[\begin{align*}
   \operatorname{E}[Y^2\mid X = x]
   &amp;= \int_{-1}^1 y^2 f_{Y\mid X = x}(y\mid x)\,dy \\
   &amp;= \frac{3}{6x + 2} \int_{-1}^1 y^2 (x + xy + y^2)\, dy \\
   &amp;= \frac{5x + 3}{5(3x + 1)}.
\end{align*}\]</span>
So the conditional variance is
<span class="math display">\[\begin{align*}
   \operatorname{var}[Y\mid X = x]
   &amp;= \operatorname{E}[Y^2\mid X = x] - \left( \operatorname{E}[Y\mid X = x] \right)^2 \\
   &amp;= \frac{5x+3}{5(3x + 1)} - \left( \frac{x}{3x + 1}\right)^2 \\
   &amp;= \frac{10x^2 + 14x + 3}{5(3x + 1)^2}
\end{align*}\]</span>
for given <span class="math inline">\(0 &lt; x &lt; 1\)</span>.
Hence the variance of <span class="math inline">\(Y\)</span> depends on the value of <span class="math inline">\(X\)</span> that is given; for example,
<span class="math display">\[\begin{align*}
   \operatorname{var}[Y\mid X = 0]   &amp;= 3/5 = 0.6\\
   \operatorname{var}[Y\mid X = 0.5] &amp;= \frac{10\times (0.5)^2 + (14\times0.5) + 3}{5(3\times0.5 + 1)^2} = 0.4\\
   \operatorname{var}[Y\mid X = 1]   &amp;= 27/80 = 0.3375.
\end{align*}\]</span></p>
</div>
<p>In general, to compute the conditional variance of <span class="math inline">\(X\mid Y = y\)</span> given a joint probability function, the following steps are required.</p>
<ul>
<li>Find the marginal distribution of <span class="math inline">\(Y\)</span>.</li>
<li>Use this to compute the conditional probability function <span class="math inline">\(p_{X \mid Y = y}(x \mid y) = p_{X, Y}(x, y)/p_{X}(x)\)</span>.</li>
<li>Find the conditional mean <span class="math inline">\(\operatorname{E}[X \mid Y = y]\)</span>.</li>
<li>Find the conditional second raw moment <span class="math inline">\(\operatorname{E}[X^2 \mid Y = y]\)</span>.</li>
<li>Finally, compute <span class="math inline">\(\operatorname{var}[X\mid Y = y] = \operatorname{E}[X^2\mid Y=y] - (\operatorname{E}[X\mid Y=y])^2\)</span>.</li>
</ul>
<div class="example">
<p><span id="exm:ConditionalVar" class="example"><strong>Example 5.34  (Conditional variance (discrete)) </strong></span>Two discrete random variables <span class="math inline">\(U\)</span> and <span class="math inline">\(V\)</span> have the joint probability function given in Table <a href="ChapExpectation.html#tab:CondVar">5.3</a>.
To find the conditional variance of <span class="math inline">\(V\)</span> given <span class="math inline">\(U = 11\)</span>, use the steps above.</p>
<p>First, find the marginal distribution of <span class="math inline">\(U\)</span>:
<span class="math display">\[
      p_U(u) = \begin{cases}
         4/9 &amp; \text{for $u = 10$};\\
         7/18 &amp; \text{for $u = 11$};\\
         1/6 &amp; \text{for $u = 12$};\\
         0 &amp; \text{otherwise.}\\
         \end{cases}
\]</span>
Secondly, compute the conditional probability function:
<span class="math display">\[\begin{align*}
      p_{V\mid U = 11}(v \mid u = 11)
      &amp;= p_{U, V}(u,v)/p_{U}(u = 11) \\
      &amp;= \begin{cases}
             \frac{1/18}{7/18} = 1/7 &amp; \text{if $v = 0$};\\
             \frac{1/3}{7/18}  = 6/7 &amp; \text{if $v = 1$}
          \end{cases}
   \end{align*}\]</span>
using <span class="math inline">\(p_U(u = 11) = 7/18\)</span> from the Step 1.</p>
<p>Thirdly, find the conditional mean:
<span class="math display">\[
  \operatorname{E}[V\mid U = 11]
  = \sum_v v p_{V\mid U}(v\mid u = 11)
  = \left(\frac{1}{7}\times 0\right) + \left(\frac{6}{7}\times 1\right)  
  = 6/7.
\]</span>
Fourthly, find the conditional second raw moment:
<span class="math display">\[
  \operatorname{E}[V^2\mid U]
  = \sum_v v^2 p_{V\mid U}(v\mid u = 11)
  = \left(\frac{1}{7}\times 0^2\right) + \left(\frac{6}{7}\times 1^2\right)  
  = 6/7.
\]</span>
Finally, compute:
<span class="math display">\[\begin{align*}
  \operatorname{var}[V\mid U = 11]
  &amp;= \operatorname{E}[V\mid U = 11] - (\operatorname{E}[V\mid U = 11])^2\\
  &amp;= (6/7) - (6/7)^2\\
  &amp;\approx  0.1224.
\end{align*}\]</span></p>
</div>
<div class="inline-table"><table class="table table-sm">
<caption>
<span id="tab:CondVar">TABLE 5.3: </span>A bivariate discrete probability function.
</caption>
<thead><tr>
<th style="text-align:right;font-weight: bold;">
</th>
<th style="text-align:right;font-weight: bold;">
<span class="math inline">\(u = 10\)</span>
</th>
<th style="text-align:right;font-weight: bold;">
<span class="math inline">\(u = 11\)</span>
</th>
<th style="text-align:right;font-weight: bold;">
<span class="math inline">\(u = 12\)</span>
</th>
<th style="text-align:right;font-weight: bold;">
Total
</th>
</tr></thead>
<tbody>
<tr>
<td style="text-align:right;font-weight: bold;">
<span class="math inline">\(v = 0\)</span>
</td>
<td style="text-align:right;">
<span class="math inline">\(1/9\)</span>
</td>
<td style="text-align:right;">
<span class="math inline">\(1/18\)</span>
</td>
<td style="text-align:right;">
<span class="math inline">\(1/6\)</span>
</td>
<td style="text-align:right;">
<span class="math inline">\(1/3\)</span>
</td>
</tr>
<tr>
<td style="text-align:right;font-weight: bold;">
<span class="math inline">\(v = 1\)</span>
</td>
<td style="text-align:right;">
<span class="math inline">\(1/3\)</span>
</td>
<td style="text-align:right;">
<span class="math inline">\(1/3\)</span>
</td>
<td style="text-align:right;">
<span class="math inline">\(0\)</span>
</td>
<td style="text-align:right;">
<span class="math inline">\(2/3\)</span>
</td>
</tr>
<tr>
<td style="text-align:right;font-weight: bold;">
Total
</td>
<td style="text-align:right;">
<span class="math inline">\(4/9\)</span>
</td>
<td style="text-align:right;">
<span class="math inline">\(7/18\)</span>
</td>
<td style="text-align:right;">
<span class="math inline">\(1/6\)</span>
</td>
<td style="text-align:right;">
<span class="math inline">\(1\)</span>
</td>
</tr>
</tbody>
</table></div>
</div>
</div>
<div id="numerical-approaches" class="section level2" number="5.8">
<h2>
<span class="header-section-number">5.8</span> Numerical approaches<a class="anchor" aria-label="anchor" href="#numerical-approaches"><i class="fas fa-link"></i></a>
</h2>
<p>Some distributions cannot be written in closed form (i.e., a neat function of standard mathematical functions), which makes (for example) evaluation of probability and computation of means difficult.
In these cases, numerical methods, such as numerical integration, may be needed.</p>
<p>Consider the random variable <span class="math inline">\(X\)</span> with density function
<span class="math display">\[
  f_X(x) = \frac{c}{\sqrt{x}}\exp( -x - \sqrt{2x})\quad \text{for $x &gt; 0$}.
\]</span>
for some normalising constant <span class="math inline">\(c\)</span>.
This density function has no closed form, and the value of <span class="math inline">\(c\)</span> cannot be found via integration.
However, the value of <span class="math inline">\(c\)</span> could be found using numerical integration in <strong>R</strong> using <code><a href="https://rdrr.io/r/stats/integrate.html">integrate()</a></code>:</p>
<div class="sourceCode" id="cb42"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co">### Define the function, without the constant term</span></span>
<span><span class="va">g</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">x</span><span class="op">)</span><span class="op">{</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/base/ifelse.html">ifelse</a></span><span class="op">(</span><span class="va">x</span> <span class="op">&gt;</span> <span class="fl">0</span>, </span>
<span>         <span class="va">x</span><span class="op">^</span><span class="op">(</span><span class="op">-</span><span class="fl">0.5</span><span class="op">)</span> <span class="op">*</span> <span class="fu"><a href="https://rdrr.io/r/base/Log.html">exp</a></span><span class="op">(</span><span class="op">-</span><span class="va">x</span> <span class="op">-</span> <span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">sqrt</a></span><span class="op">(</span><span class="fl">2</span> <span class="op">*</span> <span class="va">x</span><span class="op">)</span><span class="op">)</span>, <span class="co"># When x &gt; 0</span></span>
<span>         <span class="fl">0</span><span class="op">)</span>                                <span class="co"># When x &lt;= 0</span></span>
<span><span class="op">}</span></span>
<span></span>
<span><span class="co">### Integrate between 0 and infinity:</span></span>
<span><span class="co">#\n adds a "new line"</span></span>
<span><span class="va">Const</span> <span class="op">&lt;-</span> <span class="fl">1</span> <span class="op">/</span> <span class="fu"><a href="https://rdrr.io/r/stats/integrate.html">integrate</a></span><span class="op">(</span><span class="va">g</span>, </span>
<span>                       lower <span class="op">=</span> <span class="fl">0</span>, </span>
<span>                       upper <span class="op">=</span> <span class="cn">Inf</span><span class="op">)</span><span class="op">$</span><span class="va">value</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/cat.html">cat</a></span><span class="op">(</span><span class="st">"The value of  c  is about"</span>, <span class="fu"><a href="https://rdrr.io/r/base/Round.html">round</a></span><span class="op">(</span><span class="va">Const</span>, <span class="fl">4</span><span class="op">)</span>, <span class="st">"\n"</span><span class="op">)</span></span>
<span><span class="co">#&gt; The value of  c  is about 1.0784</span></span>
<span></span>
<span><span class="co"># So now define f(x):</span></span>
<span><span class="va">f</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">x</span><span class="op">)</span><span class="op">{</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/base/ifelse.html">ifelse</a></span><span class="op">(</span><span class="va">x</span> <span class="op">&gt;</span> <span class="fl">0</span>, </span>
<span>         <span class="va">x</span><span class="op">^</span><span class="op">(</span><span class="op">-</span><span class="fl">0.5</span><span class="op">)</span> <span class="op">*</span> <span class="fu"><a href="https://rdrr.io/r/base/Log.html">exp</a></span><span class="op">(</span><span class="op">-</span><span class="va">x</span> <span class="op">-</span> <span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">sqrt</a></span><span class="op">(</span><span class="fl">2</span> <span class="op">*</span> <span class="va">x</span><span class="op">)</span><span class="op">)</span> <span class="op">*</span> <span class="va">Const</span>,  <span class="co"># When x &gt; 0</span></span>
<span>         <span class="fl">0</span><span class="op">)</span>                                         <span class="co"># When x &lt;= 0</span></span>
<span><span class="op">}</span></span></code></pre></div>
<p>That is, <span class="math inline">\(c = 1.0784\dots\)</span>.
The distribution function can even be found:</p>
<div class="sourceCode" id="cb43"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="cn">F</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">x</span><span class="op">)</span><span class="op">{</span></span>
<span>  <span class="cn">F</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/array.html">array</a></span><span class="op">(</span>dim <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/length.html">length</a></span><span class="op">(</span><span class="va">x</span><span class="op">)</span> <span class="op">)</span></span>
<span>  <span class="kw">for</span> <span class="op">(</span><span class="va">i</span> <span class="kw">in</span> <span class="fl">1</span><span class="op">:</span><span class="fu"><a href="https://rdrr.io/r/base/length.html">length</a></span><span class="op">(</span><span class="va">x</span><span class="op">)</span><span class="op">)</span><span class="op">{</span></span>
<span>    <span class="cn">F</span><span class="op">[</span><span class="va">i</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/integrate.html">integrate</a></span><span class="op">(</span> <span class="va">f</span>, </span>
<span>                       lower <span class="op">=</span> <span class="fl">0</span>,</span>
<span>                       upper <span class="op">=</span> <span class="va">x</span><span class="op">[</span><span class="va">i</span><span class="op">]</span><span class="op">)</span><span class="op">$</span><span class="va">value</span></span>
<span>  <span class="op">}</span></span>
<span>  <span class="kw"><a href="https://rdrr.io/r/base/function.html">return</a></span><span class="op">(</span><span class="cn">F</span><span class="op">)</span></span>
<span><span class="op">}</span></span></code></pre></div>
<p>So now the density and the distribution function can be plotted (Fig. <a href="ChapExpectation.html#fig:plotOddFnFig">5.7</a>):</p>
<div class="sourceCode" id="cb44"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co">### Make room for two plots side-by-side</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/par.html">par</a></span><span class="op">(</span> mfrow <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1</span>, <span class="fl">2</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="co">### Evaluate over these values of x</span></span>
<span><span class="va">x</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/seq.html">seq</a></span><span class="op">(</span><span class="fl">0.01</span>, <span class="fl">2</span>, </span>
<span>         length <span class="op">=</span> <span class="fl">1000</span><span class="op">)</span></span>
<span></span>
<span><span class="co">### Now plot</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span> <span class="fu">f</span><span class="op">(</span><span class="va">x</span><span class="op">)</span> <span class="op">~</span> <span class="va">x</span>,</span>
<span>      type <span class="op">=</span> <span class="st">"l"</span>,</span>
<span>      las <span class="op">=</span> <span class="fl">1</span>,</span>
<span>      lwd <span class="op">=</span> <span class="fl">3</span>,</span>
<span>      main <span class="op">=</span> <span class="st">"Density function"</span>,</span>
<span>      xlab <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/expression.html">expression</a></span><span class="op">(</span><span class="fu">italic</span><span class="op">(</span><span class="va">x</span><span class="op">)</span><span class="op">)</span>,</span>
<span>      ylab <span class="op">=</span> <span class="st">"Density fn."</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span> <span class="fu"><a href="https://rdrr.io/r/base/logical.html">F</a></span><span class="op">(</span><span class="va">x</span><span class="op">)</span> <span class="op">~</span> <span class="va">x</span>,</span>
<span>      type <span class="op">=</span> <span class="st">"l"</span>,</span>
<span>      las <span class="op">=</span> <span class="fl">1</span>,</span>
<span>      lwd <span class="op">=</span> <span class="fl">3</span>,</span>
<span>      main <span class="op">=</span> <span class="st">"Distribution function"</span>,</span>
<span>      xlab <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/expression.html">expression</a></span><span class="op">(</span><span class="fu">italic</span><span class="op">(</span><span class="va">x</span><span class="op">)</span><span class="op">)</span>,</span>
<span>      ylab <span class="op">=</span> <span class="st">"Distribution fn."</span><span class="op">)</span></span></code></pre></div>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:plotOddFnFig"></span>
<img src="05-Expectation_files/figure-html/plotOddFnFig-1.png" alt="The density function (left) and distribution function (right) of a distribution that cannot be expressed in closed form." width="100%"><p class="caption">
FIGURE 5.7: The density function (left) and distribution function (right) of a distribution that cannot be expressed in closed form.
</p>
</div>
<p>Probabilities can then be computed; for example, we can find <span class="math inline">\(\Pr(X &gt; 0.5)\)</span>:</p>
<div class="sourceCode" id="cb45"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/stats/integrate.html">integrate</a></span><span class="op">(</span><span class="va">f</span>,</span>
<span>          lower <span class="op">=</span> <span class="fl">0.5</span>,</span>
<span>          upper <span class="op">=</span> <span class="cn">Inf</span><span class="op">)</span></span>
<span><span class="co">#&gt; 0.1433935 with absolute error &lt; 4.4e-06</span></span></code></pre></div>
<p>And the expected value of <span class="math inline">\(X\)</span>, <span class="math inline">\(\operatorname{E}[X]\)</span>, can be found:</p>
<div class="sourceCode" id="cb46"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">f_Expected</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">x</span><span class="op">)</span><span class="op">{</span></span>
<span>  <span class="va">x</span> <span class="op">*</span> <span class="fu">f</span><span class="op">(</span><span class="va">x</span><span class="op">)</span></span>
<span><span class="op">}</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/stats/integrate.html">integrate</a></span><span class="op">(</span><span class="va">f_Expected</span>, </span>
<span>          lower <span class="op">=</span> <span class="fl">0</span>,</span>
<span>          upper <span class="op">=</span> <span class="cn">Inf</span><span class="op">)</span></span>
<span><span class="co">#&gt; 0.2374324 with absolute error &lt; 2.1e-07</span></span></code></pre></div>
</div>
<div id="ExercisesChapExpectation" class="section level2" number="5.9">
<h2>
<span class="header-section-number">5.9</span> Exercises<a class="anchor" aria-label="anchor" href="#ExercisesChapExpectation"><i class="fas fa-link"></i></a>
</h2>
<p>Selected answers appear in Sect. <a href="selected-solutions.html#AnswersChapExpectation">E.5</a>.</p>
<div class="exercise">
<p><span id="exr:C3ContA" class="exercise"><strong>Exercise 5.1  </strong></span>The PDF for the random variable <span class="math inline">\(Y\)</span> is defined as
<span class="math display">\[
   f_Y(y) = \begin{cases}
               2y + k &amp; \text{for $1\le y \le 2$};\\
               0      &amp; \text{elsewhere}.
            \end{cases}
\]</span></p>
<ol style="list-style-type: decimal">
<li>Find the value of <span class="math inline">\(k\)</span>.</li>
<li>Plot the PDF of <span class="math inline">\(Y\)</span>.</li>
<li>Compute <span class="math inline">\(\operatorname{E}[Y]\)</span>.</li>
<li>Compute <span class="math inline">\(\operatorname{var}[Y]\)</span>.</li>
<li>Compute <span class="math inline">\(\Pr(Y &gt; 1.5)\)</span>.</li>
</ol>
</div>
<div class="exercise">
<p><span id="exr:C3ContA2" class="exercise"><strong>Exercise 5.2  </strong></span>The PDF for the random variable <span class="math inline">\(X\)</span> is defined as
<span class="math display">\[
   f_Y(y) = \begin{cases}
               2(x + 1)/ 3 &amp; \text{for $-1\le x \le 0$};\\
               (2 - x)/ 3  &amp; \text{for $0\le x \le 2$};\\
               0           &amp; \text{elsewhere}.
            \end{cases}
\]</span></p>
<ol style="list-style-type: decimal">
<li>Plot the PDF of <span class="math inline">\(X\)</span>.</li>
<li>Compute <span class="math inline">\(\operatorname{E}[X]\)</span>.</li>
<li>Compute <span class="math inline">\(\operatorname{var}[X]\)</span>.</li>
<li>Compute <span class="math inline">\(\Pr(X &gt; 0)\)</span>.</li>
</ol>
</div>
<div class="exercise">
<p><span id="exr:Discontinuous" class="exercise"><strong>Exercise 5.3  </strong></span>The random variable <span class="math inline">\(T\)</span> has the density function
<span class="math display">\[
  f_T(t) =
  \begin{cases}
    k         &amp; \text{for $0 &lt; t &lt; 1$};\\
    2k(2 - t) &amp; \text{for $1 &lt; t &lt; 2$}.
  \end{cases}
\]</span></p>
<ol style="list-style-type: decimal">
<li>Plot the PDF of <span class="math inline">\(T\)</span>.</li>
<li>Compute <span class="math inline">\(\operatorname{E}[T]\)</span>.</li>
<li>Compute <span class="math inline">\(\operatorname{var}[T]\)</span>.</li>
<li>Find and plot the distribution function of <span class="math inline">\(T\)</span>.</li>
<li>Compute <span class="math inline">\(\Pr(T \le 1)\)</span>.</li>
</ol>
</div>
<div class="exercise">
<p><span id="exr:Discontinuous2" class="exercise"><strong>Exercise 5.4  </strong></span>The random variable <span class="math inline">\(Z\)</span> has the density function
<span class="math display">\[
  f_Z(z) =
  \begin{cases}
    1 - z  &amp; \text{for $0 &lt; z &lt; 1$};\\
    2 + z  &amp; \text{for $2 &lt; z &lt; 3$};\\
    0      &amp; \text{elsewhere}.
  \end{cases}
\]</span></p>
<ol style="list-style-type: decimal">
<li>Plot the PDF of <span class="math inline">\(Z\)</span>.</li>
<li>Compute <span class="math inline">\(\operatorname{E}[Z]\)</span>.</li>
<li>Compute <span class="math inline">\(\operatorname{var}[Z]\)</span>.</li>
<li>Find and plot the distribution function of <span class="math inline">\(Z\)</span>.</li>
<li>Compute <span class="math inline">\(\Pr(Z &gt; 2 \mid Z &gt; 1)\)</span>.</li>
</ol>
</div>
<div class="exercise">
<p><span id="exr:C3DiscreteA" class="exercise"><strong>Exercise 5.5  </strong></span>The PMF for the random variable <span class="math inline">\(D\)</span> is defined as
<span class="math display">\[
   p_D(d) =
   \begin{cases}
      1/2 &amp; \text{for $d = 1$};\\
      1/4 &amp; \text{for $d = 2$};\\
      k   &amp; \text{for $d = 3$};\\
      0 &amp; \text{otherwise},
   \end{cases}
\]</span>
for a constant <span class="math inline">\(k\)</span>.</p>
<ol style="list-style-type: decimal">
<li>Plot the probability mass function.</li>
<li>Compute the mean and variance of <span class="math inline">\(D\)</span>.</li>
<li>Find the MGF for <span class="math inline">\(D\)</span>.</li>
<li>Compute the mean and variance of <span class="math inline">\(D\)</span> from the MGF.</li>
<li>Compute <span class="math inline">\(\Pr(D &lt; 3)\)</span>.</li>
</ol>
</div>
<div class="exercise">
<p><span id="exr:C3DiscreteA2" class="exercise"><strong>Exercise 5.6  </strong></span>The PMF for the random variable <span class="math inline">\(Z\)</span> is defined as
<span class="math display">\[
   p_Z(z) =
   \begin{cases}
      c/z^2 &amp; \text{for $z = 1, 2, 3, 4$};\\
      0 &amp; \text{otherwise},
   \end{cases}
\]</span>
for a constant <span class="math inline">\(c\)</span>.</p>
<ol style="list-style-type: decimal">
<li>Plot the probability mass function.</li>
<li>Compute the mean and variance of <span class="math inline">\(Z\)</span>.</li>
<li>Find the MGF for <span class="math inline">\(Z\)</span>.</li>
<li>Compute the mean and variance of <span class="math inline">\(Z\)</span> from the MGF.</li>
<li>Compute <span class="math inline">\(\Pr(Z \ge 2)\)</span>.</li>
</ol>
</div>
<div class="exercise">
<p><span id="exr:C3DiscreteB" class="exercise"><strong>Exercise 5.7  </strong></span>The MGF of the discrete random variable <span class="math inline">\(Z\)</span> is
<span class="math display">\[
   M_Z(t) = [0.3\exp(t) + 0.7]^2.
\]</span></p>
<ol style="list-style-type: decimal">
<li>Compute the mean and variance of <span class="math inline">\(Z\)</span>.</li>
<li>Find the probability function of <span class="math inline">\(Z\)</span>.</li>
</ol>
</div>
<div class="exercise">
<p><span id="exr:C3DiscreteB2" class="exercise"><strong>Exercise 5.8  </strong></span>The MGF of the discrete random variable <span class="math inline">\(W\)</span> is
<span class="math display">\[
   M_W(t) = \frac{p}{1 - (1 - p)\exp(w)}\quad\text{for $t &lt; -\log(1 - p)$}.
\]</span></p>
<ol style="list-style-type: decimal">
<li>Compute the mean and variance of <span class="math inline">\(W\)</span>.</li>
<li>Find the probability mass function of <span class="math inline">\(W\)</span>.</li>
</ol>
</div>
<div class="exercise">
<p><span id="exr:AandBsimple" class="exercise"><strong>Exercise 5.9  </strong></span>The random variable <span class="math inline">\(A\)</span> has mean <span class="math inline">\(13\)</span> and variance <span class="math inline">\(5\)</span>.
The random variable <span class="math inline">\(B\)</span> has mean <span class="math inline">\(4\)</span> and variance <span class="math inline">\(2\)</span>.
Assuming <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are independent, find:</p>
<ol style="list-style-type: decimal">
<li>
<span class="math inline">\(\operatorname{E}[A + B]\)</span>.</li>
<li>
<span class="math inline">\(\operatorname{var}[A + B]\)</span>.</li>
<li>
<span class="math inline">\(\operatorname{E}[2A - 3B]\)</span>.</li>
<li>
<span class="math inline">\(\operatorname{var}[2A - 3B]\)</span>.</li>
</ol>
</div>
<div class="exercise">
<p><span id="exr:AandBsimple2" class="exercise"><strong>Exercise 5.10  </strong></span>Repeat Exercise <a href="ChapExpectation.html#exr:AandBsimple">5.9</a>, but with <span class="math inline">\(\operatorname{Cov}(A, B) = 0.2\)</span>.</p>
</div>
<div class="exercise">
<p><span id="exr:C3MGFA" class="exercise"><strong>Exercise 5.11  </strong></span>The MGF of <span class="math inline">\(G\)</span> is <span class="math inline">\(M_G(t) = (1 - \beta t)^{-\alpha}\)</span> (where <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span> are constants).
Find the mean and variance of <span class="math inline">\(G\)</span>.</p>
</div>
<div class="exercise">
<p><span id="exr:C3Moments" class="exercise"><strong>Exercise 5.12  </strong></span>Suppose that the PDF of <span class="math inline">\(X\)</span> is
<span class="math display">\[
   f_X(x) = \begin{cases}
               2(1 - x) &amp; \text{for $0 &lt; x &lt; 1$};\\
               0 &amp; \text{otherwise}.
            \end{cases}
\]</span></p>
<ol style="list-style-type: decimal">
<li>Find the <span class="math inline">\(r\)</span>th raw moment of <span class="math inline">\(X\)</span>.</li>
<li>Find the <span class="math inline">\(r\)</span>th <em>central</em> moment of <span class="math inline">\(X\)</span>.</li>
<li>Find <span class="math inline">\(\operatorname{E}\big[(X + 3)^2\big]\)</span> using the previous answer.</li>
<li>Find the value of the skewness <span class="math inline">\(\gamma_1\)</span> using the previous results.</li>
<li>Find the value of the excess kurtosis <span class="math inline">\(\gamma_2\)</span> using the previous results.</li>
<li>Find the variance of <span class="math inline">\(X\)</span>.</li>
</ol>
</div>
<div class="exercise">
<p><span id="exr:C3Moments2" class="exercise"><strong>Exercise 5.13  </strong></span>Suppose that the PDF of <span class="math inline">\(X\)</span> is
<span class="math display">\[
   f_X(x) = \begin{cases}
               1/x^2 &amp; \text{for $1 &lt; x &lt; \infty$};\\
               0     &amp; \text{otherwise}.
            \end{cases}
\]</span></p>
<ol style="list-style-type: decimal">
<li>Find the <span class="math inline">\(r\)</span>th <em>raw</em> moment of <span class="math inline">\(X\)</span>.</li>
<li>Find the <span class="math inline">\(r\)</span>th <em>central</em> moment of <span class="math inline">\(X\)</span>.</li>
<li>Find <span class="math inline">\(\operatorname{E}\big[(X -1)^2\big]\)</span> using the previous results.</li>
<li>Find the value of the skewness <span class="math inline">\(\gamma_1\)</span> using the previous results.</li>
<li>Find the value of the excess kurtosis <span class="math inline">\(\gamma_2\)</span> using the previous results.</li>
<li>Find the variance of <span class="math inline">\(X\)</span>.</li>
</ol>
</div>
<div class="exercise">
<p><span id="exr:MGFcont1" class="exercise"><strong>Exercise 5.14  </strong></span>Find the MGF for the continuous random variable <span class="math inline">\(Y\)</span> with probability density function
<span class="math display">\[
  f_X(x) = 1/2\quad\text{for $3 &lt; x &lt; 5$}.
\]</span></p>
</div>
<div class="exercise">
<p><span id="exr:MGFcont2" class="exercise"><strong>Exercise 5.15  </strong></span>Find the MGF for the continuous random variable <span class="math inline">\(R\)</span> with probability density function
<span class="math display">\[
  f_R(r) = 6 r (1 - r) \quad\text{for $0 &lt; r &lt; 1$}.
\]</span></p>
</div>
<div class="exercise">
<p><span id="exr:C3NoMean" class="exercise"><strong>Exercise 5.16  </strong></span>Consider the PDF
<span class="math display">\[
  f_Y(y) = \frac{2}{y^2}\qquad y\ge 2.
\]</span></p>
<ol style="list-style-type: decimal">
<li>Show that the mean of the distribution is not defined.</li>
<li>Show that the variance does not exist.</li>
<li>Plot the probability density function over a suitable range.</li>
<li>Plot the distribution function over a suitable range.</li>
<li>Determine the median of the distribution.</li>
<li>Determine the interquartile range of the distribution.
(The interquartile range is a measure of spread, and is calculated as the difference between the third quartile and the first quartile.
The first quartile is the value below which <span class="math inline">\(25\)</span>% of the data lie; the third quartile is the value below which <span class="math inline">\(75\)</span>% of the data lie.)</li>
<li>Find <span class="math inline">\(\Pr(Y &gt; 4 \mid Y &gt; 3)\)</span>.</li>
</ol>
</div>
<div class="exercise">
<p><span id="exr:C3Cauchy" class="exercise"><strong>Exercise 5.17  </strong></span>The Cauchy distribution has the PDF
<span class="math display" id="eq:CauchyPDF">\[\begin{equation}
   f = \frac{1}{\pi(1 + x^2)}\quad\text{for $x\in\mathbb{R}$}.
   \tag{5.6}
\end{equation}\]</span></p>
<ol style="list-style-type: decimal">
<li>Use <strong>R</strong> to draw the probability density function.</li>
<li>Compute the distribution function for <span class="math inline">\(X\)</span>.
Again, use <strong>R</strong> to draw the function.</li>
<li>Show that the mean of the Cauchy distribution is not defined.</li>
<li>Find the variance and the mode of the Cauchy distribution.</li>
</ol>
</div>
<div class="exercise">
<p><span id="exr:C3Exponential" class="exercise"><strong>Exercise 5.18  </strong></span>The exponential distribution has the PDF
<span class="math display">\[
   f_Y(y) = \frac{1}{\lambda}\exp( -y/\lambda)
\]</span>
(for <span class="math inline">\(\lambda &gt; 0\)</span>) for <span class="math inline">\(y &gt; 0\)</span> and is zero elsewhere.</p>
<ol style="list-style-type: decimal">
<li>Determine the moment-generating function of <span class="math inline">\(Y\)</span>.</li>
<li>Use the moment-generating function to compute the mean and variance of the exponential distribution.</li>
</ol>
</div>
<div class="exercise">
<p><span id="exr:SkewDiscreteCentralMomentsZero" class="exercise"><strong>Exercise 5.19  </strong></span>Prove that for a continuous random variable <span class="math inline">\(X\)</span> which has a distribution that is symmetric about <span class="math inline">\(0\)</span> then <span class="math inline">\(M_X(t) = M_{-X}(t)\)</span>.
Hence prove that for such a random variable, all odd moments about the origin are zero.</p>
</div>
<!-- :::{.exercise} -->
<!-- INFINITE MEAN? -->
<!-- * Cauchy: $f = \frac{1}{\pi(1+ x^2}$ for $x\in\mathbb{R}$ -->
<!-- * Cont: $f = x^{-2}$. USED ABOVE -->
<!-- * Pareto distn: $f = \frac{\alpha}{x^{\alpha + 1}}$ for $\alpha > 0$. $E[X]$ DNE when $) < \alpha < 1$. -->
<!-- * Discrete $p = \frac{6}{\pi^2}\frac{1}{(n + 1)^2}$ for $n\ge 0$) -->
<!-- * Or St Peterburg paradox (https://math.stackexchange.com/questions/239288/infinite-expected-value-of-a-random-variable):  -->
<!--   * Throw a coin until it lands tails. -->
<!--   * You win $2n$ dollars, where $n$ is the number of heads. -->
<!--   * The expected value funcion of your payment (let's name it $X$): $E[X] = \infty$ -->
<!-- ::: -->
<div class="exercise">
<p><span id="exr:Givena" class="exercise"><strong>Exercise 5.20  </strong></span>The continuous random variable <span class="math inline">\(X\)</span> is defined with the probability density function
<span class="math display">\[
  f_X(x) = \frac{x + a + 1}{2(2 + a)}\quad \text{for $0 \le x \le 2$}
\]</span>
for some real value <span class="math inline">\(a\)</span>.</p>
<ol style="list-style-type: decimal">
<li>Find the possible values for <span class="math inline">\(a\)</span> such that <span class="math inline">\(f_X(x)\)</span> is a valid probability function.</li>
<li>If <span class="math inline">\(\operatorname{E}[X] = 4/3\)</span>, find the values of <span class="math inline">\(a\)</span> such that <span class="math inline">\(f_X(x)\)</span> is a valid probability function.</li>
</ol>
</div>
<div class="exercise">
<p><span id="exr:GivenEandVara" class="exercise"><strong>Exercise 5.21  </strong></span>The continuous random variable <span class="math inline">\(X\)</span> is defined with the probability density function
<span class="math display">\[
  f_X(x) = x^{2a} - x^a + 7/6\quad \text{for $0 \le x \le 1$}
\]</span>
for some real value <span class="math inline">\(a\)</span>.</p>
<ol style="list-style-type: decimal">
<li>Find the possible values for <span class="math inline">\(a\)</span> such that <span class="math inline">\(f_X(x)\)</span> is a valid probability function.</li>
<li>If <span class="math inline">\(\operatorname{E}[X] &gt; 1/2\)</span>, find the values of <span class="math inline">\(a\)</span> such that <span class="math inline">\(f_X(x)\)</span> is a valid probability function.</li>
</ol>
</div>
<div class="exercise">
<p><span id="exr:TriangularHospitalExp" class="exercise"><strong>Exercise 5.22  </strong></span>(This exercise follows from Exercise <a href="DistributionRandomVariables.html#exr:TriangularHospital">3.16</a>.)
In a study modelling waiting times at a hospital <span class="citation">(<a href="references.html#ref-khadem2008evaluating">Khadem et al. 2008</a>)</span>, patients are classified into one of three categories:</p>
<ul>
<li>Red: Critically ill or injured patients.</li>
<li>Yellow: Moderately ill or injured patients.</li>
<li>Green: Minimally injured or uninjured patients.</li>
</ul>
<p>For ‘Yellow’ patients, the service time of doctors are modelled using a triangular distribution, with a minimum at <span class="math inline">\(3.5\,\text{mins}\)</span>, a maximum at <span class="math inline">\(30.5\,\text{mins}\)</span> and a mode at <span class="math inline">\(5\,\text{mins}\)</span>.</p>
<ol style="list-style-type: decimal">
<li>Compute the mean of the service times.</li>
<li>Compute the variance of the service times.</li>
</ol>
</div>
<div class="exercise">
<p><span id="exr:YouFriendInLine2" class="exercise"><strong>Exercise 5.23  </strong></span>(This Exercise follows Ex. <a href="DistributionRandomVariables.html#exr:YouFriendInLine">3.18</a>.)
Five people, including you and a friend, line up at random.
The random variable <span class="math inline">\(X\)</span> denotes the number of people between yourself and your friend.</p>
<p>Use the probability function of <span class="math inline">\(X\)</span> found in Ex. <a href="DistributionRandomVariables.html#exr:YouFriendInLine">3.18</a>, and find the mean number of people between you and your friend.
Simulate this in <strong>R</strong> to confirm your answer.</p>
</div>
<div class="exercise">
<p><span id="exr:CharFunction" class="exercise"><strong>Exercise 5.24  </strong></span>The <em>characteristic function</em> of a random variable <span class="math inline">\(X\)</span>, denoted <span class="math inline">\(\varphi(t)\)</span>, is defined as <span class="math inline">\(\varphi_X(t) = \operatorname{E}[\exp(i t X)]\)</span>, where <span class="math inline">\(i = \sqrt{-1}\)</span>.
Unlike the MGF, the characteristic function is <em>always</em> defined, so is sometimes preferred over the MGF.</p>
<ol style="list-style-type: decimal">
<li>Show that <span class="math inline">\(M_X(t) = \varphi_X(-it)\)</span>.</li>
<li>Show that the mean of a random variable <span class="math inline">\(X\)</span> is given by <span class="math inline">\(-i\varphi'(0)\)</span> (where, as before, the notation means to compute the derivative of <span class="math inline">\(\varphi(t)\)</span> with respect to <span class="math inline">\(t\)</span>, and evaluate at <span class="math inline">\(t = 0\)</span>).</li>
</ol>
</div>
<div class="exercise">
<p><span id="exr:GeometricGeneratingFn" class="exercise"><strong>Exercise 5.25  </strong></span>App. <a href="UsefulSeries.html#InfiniteSeriesLimits">B.2</a> will prove useful.</p>
<ol style="list-style-type: decimal">
<li>Write down the first three terms and general term of the expansion of <span class="math inline">\((1 - a)^{-1}\)</span>.</li>
<li>Write down the first three terms and general term of the expansion of <span class="math inline">\(\operatorname{E}\left[(1 - tX)^{-1}\right]\)</span>.</li>
<li>Suppose <span class="math inline">\(\mathcal{R}_X(t) = \operatorname{E}\left [(1 - tX)^{-1} \right]\)</span>, called the geometric generating function of <span class="math inline">\(X\)</span>.
Suppose the random variable <span class="math inline">\(Y\)</span> has a uniform distribution on <span class="math inline">\((0, 1)\)</span>; i.e., <span class="math inline">\(f_Y(y) = 1\)</span> for <span class="math inline">\(0 &lt; y &lt; 1\)</span>.
Determine the geometric generating function of <span class="math inline">\(Y\)</span> from the definition of the expected value.
Your answer will involve a term <span class="math inline">\(\log(1 - t)\)</span>.</li>
<li>Using the answer in Part 3, expand the term <span class="math inline">\(\log(1 - t)\)</span> by writing in terms of the infinite series.</li>
<li>Equate the two series expansions in Part 2 and Part 4 to determine an expression for <span class="math inline">\(\operatorname{E}[Y^n]\)</span>, <span class="math inline">\(n = 1, 2, 3,\dots\)</span>.</li>
</ol>
</div>
<div class="exercise">
<p><span id="exr:FindkGiveVar" class="exercise"><strong>Exercise 5.26  </strong></span>Suppose the random variable <span class="math inline">\(X\)</span> is defined as
<span class="math display">\[
   f_X(x) = k (3x^2 + 4)\quad\text{for $-c &lt; x &lt; c$},
\]</span>
and is zero elsewhere.
Solve for <span class="math inline">\(c\)</span> and <span class="math inline">\(k\)</span> if <span class="math inline">\(\operatorname{var}[X] = 28/15\)</span>.
(Hint: Make sure you use the properties of the given probability distribution before embarking on complicated expressions!)</p>
</div>
<div class="exercise">
<p><span id="exr:FindkGiveVar2" class="exercise"><strong>Exercise 5.27  </strong></span>Suppose the random variable <span class="math inline">\(Y\)</span> is defined as
<span class="math display">\[
   f_Y(y) =
   \begin{cases}
     c          &amp; \text{for $0 &lt; y &lt; 1$}\\
     k(y - 4)/3 &amp; \text{for $1 &lt; y &lt; 4$};\\
     0          &amp; \text{elsewhere}.
  \end{cases}
\]</span></p>
<ol style="list-style-type: decimal">
<li>What values of <span class="math inline">\(c\)</span> and <span class="math inline">\(k\)</span> are possible?</li>
<li>If <span class="math inline">\(c = k\)</span>, what are the values of <span class="math inline">\(c\)</span> and <span class="math inline">\(k\)</span>?</li>
<li>If <span class="math inline">\(k = 2\)</span>, what is the value of <span class="math inline">\(c\)</span>?</li>
</ol>
</div>
<div class="exercise">
<p><span id="exr:FindBits" class="exercise"><strong>Exercise 5.28  </strong></span>Suppose the random variable <span class="math inline">\(Y\)</span> is defined as
<span class="math display">\[
   f_Y(y) =
   \begin{cases}
     \exp(-y^2)    &amp; \text{for $0 &lt; y &lt; k$}\\
     0             &amp; \text{elsewhere},
  \end{cases}
\]</span>
and <span class="math inline">\(\operatorname{E}[Y] = 1/2\)</span>.
What is the value of <span class="math inline">\(k\)</span>?</p>
</div>
<div class="exercise">
<p><span id="exr:FindBits2" class="exercise"><strong>Exercise 5.29  </strong></span>Suppose the random variable <span class="math inline">\(X\)</span> is defined as
<span class="math display">\[
   f_X(x) =
   \begin{cases}
     x^r     &amp; \text{for $0 &lt; x &lt; 5$}\\
     0       &amp; \text{elsewhere},
  \end{cases}
\]</span>
and <span class="math inline">\(\operatorname{E}[X] = 625\)</span>.
What is the value of <span class="math inline">\(r\)</span>?</p>
</div>
<div class="exercise">
<p><span id="exr:BenfordsLawMean" class="exercise"><strong>Exercise 5.30  </strong></span>Benford’s law (also see Exercise <a href="DistributionRandomVariables.html#exr:BenfordsLaw">3.24</a>) describes the distribution of the leading digits of numbers that span many orders of magnitudes (e.g., lengths of rivers) as
<span class="math display">\[
   p_D(d) = \log_{10}\left( \frac{d + 1}{d} \right) \quad\text{for $d\in\{1, 2, \dots 9\}$},
\]</span>
Find the mean of <span class="math inline">\(D\)</span> (i.e., the mean leading digit).</p>
</div>
<div class="exercise">
<p><span id="exr:vonMises" class="exercise"><strong>Exercise 5.31  </strong></span>The <em>von Mises</em> distribution is used to model angular data.
The probability function is
<span class="math display">\[
  p_Y(y) = k \exp\{ \lambda\cos(y - \mu) \}
\]</span>
for <span class="math inline">\(0\le y &lt; 2\pi\)</span>, <span class="math inline">\(0 \le \mu &lt; 2\pi\)</span> where <span class="math inline">\(\mu\)</span> is the mean, and with <span class="math inline">\(\lambda &gt; 0\)</span>.</p>
<ol style="list-style-type: decimal">
<li>Show that the constant <span class="math inline">\(k\)</span> is a function of <span class="math inline">\(\lambda\)</span> only.</li>
<li>Find the median of the distribution.</li>
<li>Using <strong>R</strong>, numerically integrate to find the value <span class="math inline">\(k\)</span> when <span class="math inline">\(\mu = \pi/2\)</span> and <span class="math inline">\(\lambda = 1\)</span>.</li>
<li>The distribution function has no closed form.
Use <strong>R</strong> to plot the distribution function for <span class="math inline">\(\mu = \pi/2\)</span> with <span class="math inline">\(\lambda = 1\)</span>.</li>
</ol>
</div>
<div class="exercise">
<p><span id="exr:inverseGaussian" class="exercise"><strong>Exercise 5.32  </strong></span>The <em>inverse Gaussian</em> distribution has the PDF
<span class="math display">\[
   P_Y(y) = \frac{1}{\sqrt{2\pi y^3\phi}} \exp\left\{ -\frac{1}{2\phi} \frac{(y - \mu)^2}{y\mu^2}\right\}
\]</span>
for <span class="math inline">\(y &gt; 0\)</span>, <span class="math inline">\(\mu &gt; 0\)</span> and <span class="math inline">\(\phi &gt; 0\)</span>.</p>
<ol style="list-style-type: decimal">
<li>Plot the distribution for <span class="math inline">\(\mu = 1\)</span> for various values of <span class="math inline">\(\phi\)</span>; comment.</li>
<li>The MGF is
<span class="math display">\[
M_Y(t) = \exp\left\{ \frac{\lambda}{\mu} \left( 1 - \sqrt{1 - \frac{2\mu^2 t}{\lambda}} \right) \right\}.
\]</span>
Use the MGF to deduce the mean and variance of the inverse Gaussian distribution.</li>
</ol>
</div>
<div class="exercise">
<p><span id="exr:W3inverse" class="exercise"><strong>Exercise 5.33  </strong></span>Consider the random variable <span class="math inline">\(W\)</span> such that
<span class="math display">\[
   f_W(w) = \frac{c}{w^3}\quad\text{for $w &gt; c$.}
\]</span></p>
<ol style="list-style-type: decimal">
<li>Find the value of <span class="math inline">\(c\)</span>.</li>
<li>Find <span class="math inline">\(\operatorname{E}[W]\)</span>.</li>
<li>Find <span class="math inline">\(\operatorname{var}[W]\)</span>.</li>
</ol>
</div>
<div class="exercise">
<p><span id="exr:Pareto" class="exercise"><strong>Exercise 5.34  </strong></span>The <em>Pareto</em> distribution has the distribution function
<span class="math display">\[
   F_X(x) =
   \begin{cases}
      1 - \left(\frac{k}{x}\right)^\alpha &amp; \text{for $x &gt; k$}\\
      0                                  &amp; \text{elesewhere},
   \end{cases}
\]</span>
for <span class="math inline">\(\alpha &gt; 0\)</span> and parameter <span class="math inline">\(k\)</span>,</p>
<ol style="list-style-type: decimal">
<li>What values of <span class="math inline">\(k\)</span> are possible?</li>
<li>Find the density function for the Pareto distribution.</li>
<li>Compute the mean and variance for the Pareto distribution.</li>
<li>Find the mode of the Pareto distribution.</li>
<li>Plot the distribution for <span class="math inline">\(\alpha = 3\)</span> and <span class="math inline">\(k = 3\)</span>.</li>
<li>For <span class="math inline">\(\alpha = 3\)</span> and <span class="math inline">\(k = 3\)</span>, compute <span class="math inline">\(\Pr(X &gt; 4 \mid X &lt; 5)\)</span>.</li>
<li>The Pareto distribution is often used to model incomes.
For example, the “<span class="math inline">\(80\)</span>-–<span class="math inline">\(20\)</span> rule” states that <span class="math inline">\(20\)</span>% of people receive <span class="math inline">\(80\)</span>% of all income (and, further, that <span class="math inline">\(20\)</span>% of the highest-earning <span class="math inline">\(20\)</span>% receive <span class="math inline">\(80\)</span>% of that <span class="math inline">\(80\)</span>%).
Find the value of <span class="math inline">\(\alpha\)</span> for which this rule holds.</li>
</ol>
</div>
<div class="exercise">
<p><span id="exr:unlabeled-div-19" class="exercise"><strong>Exercise 5.35  </strong></span>A <em>mixture distribution</em> is a mixture of two or more univariate distributions.
For example, the heights of all adults may follow a mixture distribution: one normal distribution for adult females, and another for adult males.
For a set of probability functions <span class="math inline">\(p^{(i)}_X(x)\)</span> for <span class="math inline">\(i = 1, 2, \dots n\)</span> and a set of weights <span class="math inline">\(w_i\)</span> such that <span class="math inline">\(\sum w_i = 1\)</span> and <span class="math inline">\(w_i \ge 0\)</span> for all <span class="math inline">\(i\)</span>, the mixture distribution <span class="math inline">\(f_X(x)\)</span> is
<span class="math display">\[
   f_X(x) = \sum_{i = 1}^n w_i p^{(i)}_X(x).
\]</span></p>
<ol style="list-style-type: decimal">
<li>Compute the distribution function for <span class="math inline">\(p_X(x)\)</span>.</li>
<li>Compute the mean and variance of <span class="math inline">\(f_X(x)\)</span>.</li>
<li>Consider the case where <span class="math inline">\(p^{(i)}_X(x)\)</span> has a normal distribution for <span class="math inline">\(i = 1, 2, 3\)</span>, where the means are <span class="math inline">\(-1\)</span>, <span class="math inline">\(2\)</span>, and <span class="math inline">\(4\)</span> respectively, and the variances are <span class="math inline">\(1\)</span>, <span class="math inline">\(1\)</span> and <span class="math inline">\(4\)</span> respectively.
Plot the probability density function of <span class="math inline">\(f_X(x)\)</span> for various instructive values of the weights.</li>
<li>Suppose heights of female adults have a normal distribution with mean <span class="math inline">\(163\,\text{cm}\)</span> and a standard deviation of <span class="math inline">\(5\,\text{cm}\)</span>, and adult males have heights with a mean of <span class="math inline">\(175\,\text{cm}\)</span> with standard deviation of <span class="math inline">\(7\,\text{cm}\)</span>, and constitute <span class="math inline">\(48\)</span>% of the population <span class="citation">(<a href="references.html#ref-data:ABS1995:MeasureUp">Australian Bureau of Statistics 1995</a>)</span>.
Deduce and plot the probability density of heights of adult Australians.</li>
</ol>
</div>
<!-- Soliton distribution -->
<div class="exercise">
<p><span id="exr:SolitonDist" class="exercise"><strong>Exercise 5.36  </strong></span>Consider the distribution such that
<span class="math display">\[
   p_X(x) =
   \begin{cases}
      1/K                     &amp; \text{for $x = 1$};\\
      1/\left(x(x - 1)\right) &amp; \text{for $x = 2, 3, \dots, K$};\\
      0                       &amp; \text{elsewhere}
   \end{cases}
\]</span>
for <span class="math inline">\(K &gt; 2\)</span>.</p>
<ol style="list-style-type: decimal">
<li>Find the mean and variance of <span class="math inline">\(X\)</span> (as well as possible).</li>
<li>Plot the distribution for various values of <span class="math inline">\(K\)</span>.</li>
<li>For <span class="math inline">\(K = 6\)</span>, determine the MGF.</li>
</ol>
</div>
<div class="exercise">
<p><span id="exr:unlabeled-div-20" class="exercise"><strong>Exercise 5.37  </strong></span>The random variable <span class="math inline">\(V\)</span> has the PMF
<span class="math display">\[
   f_V(v) = (1 - p)^{v - 1} p\quad\text{for $v = 1, 2, \dots$},
\]</span>
and zero elsewhere.</p>
<ol style="list-style-type: decimal">
<li>Show that this is a valid PMF.</li>
<li>Find <span class="math inline">\(\operatorname{E}[V]\)</span>.</li>
<li>Find <span class="math inline">\(\operatorname{var}[V]\)</span>.</li>
</ol>
</div>
<div class="exercise">
<p><span id="exr:TwoDiceAreRolled" class="exercise"><strong>Exercise 5.38  </strong></span>Two dice are rolled.
Deduce the PMF for the absolute difference between the two numbers that appear uppermost.</p>
</div>
<div class="exercise">
<p><span id="exr:unlabeled-div-21" class="exercise"><strong>Exercise 5.39  </strong></span>The random variable <span class="math inline">\(Y\)</span> has the PMF
<span class="math display">\[
   p_Y(y) = \frac{e^{-\lambda}\lambda^y}{y!}\quad\text{for $y = 0, 1, 2, \dots$},
\]</span>
where <span class="math inline">\(\lambda &gt; 0\)</span>.
Find the MGF of <span class="math inline">\(Y\)</span>, and hence show that <span class="math inline">\(\operatorname{E}[Y] = \operatorname{var}[Y]\)</span>.</p>
</div>
<div class="exercise">
<p><span id="exr:InvertMGFcontinuous" class="exercise"><strong>Exercise 5.40  </strong></span>In practice, some distributions cannot be written in closed form, but can be given by writing their moment-generating function.
To evaluate the density then requires an infinite summation or an infinite integral.
Given a moment-generating function <span class="math inline">\(M(t)\)</span>, the probability density function can be reconstructed numerically from
the integral using the <em>inversion formula</em> in Eq. <a href="ChapExpectation.html#eq:MGFtoPDFcontinuous">(5.4)</a>.</p>
<p>The evaluation of the integral generally requires advanced numerical techniques.
In this question, we just consider the exponential distribution as a simple example to demonstrate the use of the inversion formula.</p>
<ol style="list-style-type: decimal">
<li>Write down the expression in Eq. <a href="ChapExpectation.html#eq:MGFtoPDFcontinuous">(5.4)</a> in the case of the <em>exponential distribution</em>, for which <span class="math inline">\(M_X(t) = \lambda/(\lambda - t)\)</span> for <span class="math inline">\(t &lt; \lambda\)</span>.</li>
<li>Only the real part of the integral is needed.
Extract the real parts of this expression, and simplify the integrand.
(The integrand is the expression to be integrated.)</li>
<li>Plot the integrand from the last part from <span class="math inline">\(t = -50\)</span> to <span class="math inline">\(t = 2\)</span> in the case <span class="math inline">\(\lambda = 2\)</span> and <span class="math inline">\(x = 1\)</span>.</li>
</ol>
</div>
<div class="exercise">
<p><span id="exr:unlabeled-div-22" class="exercise"><strong>Exercise 5.41  </strong></span>The density function for the random variable <span class="math inline">\(X\)</span> is given as
<span class="math display">\[
   f(x) =  x e^{-x} \quad\text{for $x &gt; 0$}.
\]</span></p>
<ol style="list-style-type: decimal">
<li>Determine the moment-generating function (MGF) of <span class="math inline">\(X\)</span>.</li>
<li>Use the MGF to verify that <span class="math inline">\(\operatorname{E}[X] = \operatorname{var}[X]\)</span>.</li>
<li>Suppose that <span class="math inline">\(Y = 1 - X\)</span>.
Determine <span class="math inline">\(\operatorname{E}[Y]\)</span>.</li>
</ol>
</div>
<div class="exercise">
<p><span id="exr:GumbelOslo" class="exercise"><strong>Exercise 5.42  </strong></span>The <em>Gumbel distribution</em> has the cumulative distribution function
<span class="math display" id="eq:Gumbel">\[\begin{equation}
   F(x; \mu, \beta) = \exp\left[ -\exp\left( -\frac{x - \mu}{\sigma}\right)\right]
   \tag{5.7}
\end{equation}\]</span>
(for <span class="math inline">\(\mu &gt; 0\)</span> and <span class="math inline">\(\sigma &gt; 0\)</span>) and is often used to model extremes values (such as the distribution of the maximum river height).</p>
<ol style="list-style-type: decimal">
<li>Deduce the probability function for the Gumbel distribution.</li>
<li>Plot the Gumbel distribution for a variety of parameters.</li>
<li>The maximum daily precipitation (in mm) in Oslo, Norway, is well-modelled using a Gumbel distribution with <span class="math inline">\(\mu = 2.6\,\text{mm}\)</span> and <span class="math inline">\(\sigma = 1.86\,\text{mm}\)</span>.
Draw this distribution, and explain what it means.</li>
</ol>
</div>
<div class="exercise">
<p><span id="exr:Continuous2" class="exercise"><strong>Exercise 5.43  </strong></span>The density function for the random variable <span class="math inline">\(X\)</span> is given as
<span class="math display">\[
   f(x) =  x e^{-x} \quad\text{for $x &gt; 0$}.
\]</span></p>
<ol style="list-style-type: decimal">
<li>Determine <span class="math inline">\(\operatorname{E}[X]\)</span>.</li>
<li>Verify that <span class="math inline">\(\operatorname{E}[X] = \operatorname{var}[X]\)</span>.</li>
</ol>
</div>
<div class="exercise">
<p><span id="exr:PooledTest2" class="exercise"><strong>Exercise 5.44  </strong></span>(This exercise follows from Ex. <a href="DistributionRandomVariables.html#exr:PooledTest">3.22</a>.)
To detect disease in a population through a blood test, usually every individual is tested.
If the disease is uncommon, however, an alternative method is often more efficient.</p>
<p>In the alternative method (called a <em>pooled test</em>), blood from <span class="math inline">\(n\)</span> individuals is combined, and one test is conducted.
If the test returns a negative result, then none of the <span class="math inline">\(n\)</span> people have the disease; if the test returns a positive result, all <span class="math inline">\(n\)</span> individuals are then tested individually to identify which individual(s) have the disease.</p>
<p>Suppose a disease occurs in an unknown proportion of people <span class="math inline">\(p\)</span> of people.
Let <span class="math inline">\(X\)</span> be the number of tests to be performed for a group of <span class="math inline">\(n\)</span> individuals.</p>
<ol style="list-style-type: decimal">
<li>What is the expected number of tests needed in a group of <span class="math inline">\(n\)</span> people using the pooled method?</li>
<li>What is the variance of the number of tests needed in a group of <span class="math inline">\(n\)</span> people using the pooled method?</li>
<li>Explain what happens to the mean and variance as <span class="math inline">\(p \to 1\)</span> and as <span class="math inline">\(p \to 0\)</span>, and how these results make sense <em>in the context of the question</em>
</li>
<li>If pooling was <em>not</em> used with a group of <span class="math inline">\(n\)</span> people, the number of tests would be <span class="math inline">\(n\)</span>: one for each person.
Deduce an expression for the value of <span class="math inline">\(p\)</span> for which the expected number of tests using the pooled approach <em>exceeds</em> the non-pooled approach.</li>
<li>Produce a well-labelled plot showing the expected number of tests that are <em>saved</em> by using the pooled method when <span class="math inline">\(p = 0.1\)</span> for values of <span class="math inline">\(n\)</span> from <span class="math inline">\(2\)</span> to <span class="math inline">\(10\)</span>, and comment on what this shows practically.</li>
<li>Suppose a test costs $<span class="math inline">\(15\)</span>.
What is the expected cost-saving for using the pooled-testing method with <span class="math inline">\(n = 4\)</span> and <span class="math inline">\(p = 0.1\)</span>, if <span class="math inline">\(200\)</span> people must be tested?</li>
</ol>
</div>
<div class="exercise">
<p><span id="exr:RunningTotal" class="exercise"><strong>Exercise 5.45  </strong></span>Consider rolling a fair, six-sided die.
The ‘running total’ is the total of all the numbers rolled on the die.</p>
<ol style="list-style-type: decimal">
<li>Find the probability mass function for <span class="math inline">\(R\)</span>, the number of rolls needed to obtain a running total of <span class="math inline">\(3\)</span> or more.</li>
<li>Find the expected number of rolls until the running total reaches <span class="math inline">\(3\)</span> or more.</li>
</ol>
</div>
<div class="exercise">
<p><span id="exr:MeanAbsDev" class="exercise"><strong>Exercise 5.46  </strong></span>Besides the variance, an alternative measure of variation is the <em>mean absolute deviation</em> (MAD), defined as <span class="math inline">\(\operatorname{E}[\,|X - \mu|\,]\)</span>.</p>
<p>Consider the fair die described in Example <a href="ChapExpectation.html#exm:VarianceDice">5.9</a>.</p>
<ol style="list-style-type: decimal">
<li>Find <span class="math inline">\(\operatorname{E}[X]\)</span>.</li>
<li>Find <span class="math inline">\(\operatorname{MAD}[X]\)</span> using the above definition.</li>
</ol>
</div>
<div class="exercise">
<p><span id="exr:exp4" class="exercise"><strong>Exercise 5.47  </strong></span>Suppose a random variable <span class="math inline">\(W\)</span> has the probability density function
<span class="math display">\[
  f_W(w) = K \, \exp(-w^4)\quad\text{for $w\in \mathbb{R}$,}
\]</span>
for some normalising constant <span class="math inline">\(K\)</span>.</p>
<ol style="list-style-type: decimal">
<li>Using a computer, determine a value for <span class="math inline">\(K\)</span>.</li>
<li>Plot the density function and the distribution function of <span class="math inline">\(W\)</span>.</li>
<li>Using a computer, find the mean of the distribution.</li>
<li>Using a computer, find the variance of the distribution.</li>
<li>Using a computer, find <span class="math inline">\(\Pr(W &lt; 1 \mid W &gt; -1)\)</span>.</li>
</ol>
</div>
<!-- Beta prime distribution\index{Beta prime distribution}-->
<!-- f\ $p$ has a beta distribution, the odds $p/(1 - p)$ has beta prime distribution)) -->
<div class="exercise">
<p><span id="exr:BetaPrime" class="exercise"><strong>Exercise 5.48  </strong></span>Suppose a random variable <span class="math inline">\(Y\)</span> has the probability density function
<span class="math display">\[
  f_Y(y) = k\, y^{\alpha - 1} (1 + y)^{-\alpha-\beta}\quad\text{for $y &gt; 0$,}
\]</span>
for some normalising constant <span class="math inline">\(k\)</span>, where <span class="math inline">\(\alpha &gt; 0\)</span> and <span class="math inline">\(\beta &gt; 0\)</span>.</p>
<ol style="list-style-type: decimal">
<li>Using a computer, determine a value for <span class="math inline">\(k\)</span> when <span class="math inline">\(\alpha = 0.5\)</span> and <span class="math inline">\(\beta = 2.5\)</span>.</li>
<li>Plot the density function and the distribution function of <span class="math inline">\(Y\)</span>.</li>
<li>Using a computer, find the mean of the distribution.
(Compare to the theoretical mean of <span class="math inline">\(\operatorname{E}[Y] = \alpha/(\beta - 1)\)</span> provided <span class="math inline">\(\beta &gt; 1\)</span>.)</li>
<li>Using a computer, find the variance of the distribution.</li>
<li>Using a computer, find <span class="math inline">\(\Pr(Y &lt; 1)\)</span>.</li>
</ol>
<p>The mean is not defined if <span class="math inline">\(\beta &lt; 1\)</span>.</p>
<ol start="6" style="list-style-type: decimal">
<li>What happens if you use a computer to produce the density function for <span class="math inline">\(\alpha = 0.5\)</span> and <span class="math inline">\(\beta = 0.5\)</span>?</li>
<li>What does the simulation suggest for the value of <span class="math inline">\(\operatorname{E}[Y]\)</span> for <span class="math inline">\(\alpha = 0.5\)</span> and <span class="math inline">\(\beta = 0.5\)</span>?</li>
</ol>
</div>
<div class="exercise">
<p><span id="exr:KurtosisNegative" class="exercise"><strong>Exercise 5.49  </strong></span>Consider the random variable <span class="math inline">\(X\)</span> with the probability density function
<span class="math display">\[
  f_X(x) = 3 x^2/2\quad\text{for $-1 &lt; x &lt; 1$}
\]</span>
and is zero elsewhere.</p>
<ol style="list-style-type: decimal">
<li>Plot the PDF.</li>
<li>Compute the mean and the variance.</li>
<li>Without computation, determine the skewness.</li>
<li>Compute the kurtosis, and then show that the excess kurtosis is a negative value.</li>
<li>The text states that distributions with <em>negative</em> excess kurtosis (<em>platykurtic</em> distributions)
have fewer, or less extreme, observations in the tail compared to the normal distribution.
Explain why this distribution has a negative kurtosis.</li>
</ol>
</div>
<div class="exercise">
<p><span id="exr:KurtosisNegative2" class="exercise"><strong>Exercise 5.50  </strong></span>Consider the random variable <span class="math inline">\(X\)</span> with the probability density function
<span class="math display">\[
  f_X(x) = x/2 \quad\text{for $0 &lt; x &lt; 2$}
\]</span>
and is zero elsewhere.</p>
<ol style="list-style-type: decimal">
<li>Plot the PDF.</li>
<li>Find an expression for the <span class="math inline">\(r\)</span>th raw moment.</li>
<li>Compute the mean and the variance.</li>
<li>Compute the skewness, and explain what this value means.</li>
<li>Compute the kurtosis, and explain what this value means.</li>
</ol>
</div>

</div>
</div>
<hr>
<div class="footer"><span style="color: gray; font-size:0.7em">Peter K. Dunn, 2024: <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a></span></div>
  <div class="chapter-nav">
<div class="prev"><a href="ChapBivariate.html"><span class="header-section-number">4</span> Bivariate distributions</a></div>
<div class="next"><a href="ChapterTransformations.html"><span class="header-section-number">6</span> Transformations of random variables</a></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page"><h2>On this page</h2>
      <ul class="nav navbar-nav">
<li><a class="nav-link" href="#ChapExpectation"><span class="header-section-number">5</span> Mathematical expectation</a></li>
<li><a class="nav-link" href="#ExpectedValue"><span class="header-section-number">5.1</span> Expected values</a></li>
<li><a class="nav-link" href="#ExpectationFunction"><span class="header-section-number">5.2</span> Expectation of a function of a random variable</a></li>
<li><a class="nav-link" href="#VarianceStdDev"><span class="header-section-number">5.3</span> The variance and standard deviation</a></li>
<li>
<a class="nav-link" href="#HigherMoments"><span class="header-section-number">5.4</span> Higher moments</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#RawCentralMoments"><span class="header-section-number">5.4.1</span> Raw and central moments</a></li>
<li><a class="nav-link" href="#Skewness"><span class="header-section-number">5.4.2</span> Skewness</a></li>
<li><a class="nav-link" href="#Kurtosis"><span class="header-section-number">5.4.3</span> Kurtosis</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#MGF"><span class="header-section-number">5.5</span> Moment-generating functions</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#MGFIntroduction"><span class="header-section-number">5.5.1</span> Introduction</a></li>
<li><a class="nav-link" href="#MGFDefinition"><span class="header-section-number">5.5.2</span> Definition</a></li>
<li><a class="nav-link" href="#MGFMoments"><span class="header-section-number">5.5.3</span> Using the MGF to generate moments</a></li>
<li><a class="nav-link" href="#some-useful-results"><span class="header-section-number">5.5.4</span> Some useful results</a></li>
<li><a class="nav-link" href="#DistributionFromMGF"><span class="header-section-number">5.5.5</span> Determining the distribution from the MGF</a></li>
</ul>
</li>
<li><a class="nav-link" href="#Tchebysheff"><span class="header-section-number">5.6</span> Tchebysheff’s inequality</a></li>
<li>
<a class="nav-link" href="#ExpectationsBivariate"><span class="header-section-number">5.7</span> Mathematical expectation for bivariate distributions</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#ExpectationsBivariateFunctions"><span class="header-section-number">5.7.1</span> Expected values of a bivariate function</a></li>
<li><a class="nav-link" href="#moments-of-a-bivariate-distribution-covariance"><span class="header-section-number">5.7.2</span> Moments of a bivariate distribution: covariance</a></li>
<li><a class="nav-link" href="#properties-of-covariance-and-correlation"><span class="header-section-number">5.7.3</span> Properties of covariance and correlation</a></li>
<li><a class="nav-link" href="#ConditionalExpectation"><span class="header-section-number">5.7.4</span> Conditional expectations</a></li>
</ul>
</li>
<li><a class="nav-link" href="#numerical-approaches"><span class="header-section-number">5.8</span> Numerical approaches</a></li>
<li><a class="nav-link" href="#ExercisesChapExpectation"><span class="header-section-number">5.9</span> Exercises</a></li>
</ul>

      <div class="book-extra">
        <ul class="list-unstyled">
<li><a id="book-source" href="https://github.com/PeterKDunn/DistTheory/blob/main/05-Expectation.Rmd">View source <i class="fab fa-github"></i></a></li>
          <li><a id="book-edit" href="https://github.com/PeterKDunn/DistTheory/edit/main/05-Expectation.Rmd">Edit this page <i class="fab fa-github"></i></a></li>
        </ul>
</div>
    </nav>
</div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>The Theory of Statistical Distributions</strong>" was written by Peter K. Dunn. It was last built on Last updated: 2025-12-12.</p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script><script type="text/x-mathjax-config">const popovers = document.querySelectorAll('a.footnote-ref[data-toggle="popover"]');
for (let popover of popovers) {
  const div = document.createElement('div');
  div.setAttribute('style', 'position: absolute; top: 0, left:0; width:0, height:0, overflow: hidden; visibility: hidden;');
  div.innerHTML = popover.getAttribute('data-content');

  var has_math = div.querySelector("span.math");
  if (has_math) {
    document.body.appendChild(div);
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, div]);
    MathJax.Hub.Queue(function() {
      popover.setAttribute('data-content', div.innerHTML);
      document.body.removeChild(div);
    })
  }
}
</script>
</body>
</html>
