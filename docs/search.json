[{"path":"index.html","id":"preface","chapter":"Preface","heading":"Preface","text":"book introduction theory statistical probability distributions.","code":""},{"path":"index.html","id":"statistical-software","chapter":"Preface","heading":"Statistical software","text":"book can read without relying specific statistical software, though sometimes R code (R Core Team 2025) included demonstrate ideas, discuss simulation.","code":""},{"path":"index.html","id":"callouts-used-on-this-book","chapter":"Preface","heading":"Callouts used on this book","text":"callouts used book meanings; example:chunks introduce objectives chapters book.chunks highlight common mistakes warnings, particular concept using formula.chunks offer helpful information.chunks refer text relevant using R statistical software.chunks explain certain concepts distributions linked.chunks give questions think .","code":""},{"path":"index.html","id":"who-can-use-this-book","chapter":"Preface","heading":"Who can use this book?","text":"textbook free anyone use:\ncharge students, instructors institutions.Although essential, email author (explaining textbook used, using textbook, thoughts textbook) appreciated: pdunn2 <> usc.edu.au.","code":""},{"path":"index.html","id":"how-this-book-was-made","chapter":"Preface","heading":"How this book was made","text":"book made using R (R Core Team 2025), bookdown package (Y. Xie 2025a), based Markdown syntax, using knitr (Y. Xie 2025b).","code":""},{"path":"index.html","id":"learning-outcomes","chapter":"Preface","heading":"Learning Outcomes","text":"book, learn :apply rules, theorems concepts compute probabilities.describe probability distribution functions.apply concepts mathematical expectation.apply work standard discrete continuous distributions, including computing probabilities.apply work bivariate multivariate distributions.determine probability functions distribution functions transformed radon variables.apply work standard sampling distributions.use R produce relevant plots compute probabilities.","code":""},{"path":"index.html","id":"how-to-cite-this-book","chapter":"Preface","heading":"How to cite this book","text":"Peter K. Dunn (2024). theory statistical distributions.\nhttps://bookdown.org/pkaldunn/DistTheoryThe CC -NC-SA 4.0 licence applied textbook.\nPeter K. Dunn\nSippy Downs, Australia\n","code":""},{"path":"ChapterSetTheory.html","id":"ChapterSetTheory","chapter":"1 Essentials of set theory","heading":"1 Essentials of set theory","text":"UpOn completion chapter able :define sets, define elements sets.define cardinality sets.perform basic set operations.","code":""},{"path":"ChapterSetTheory.html","id":"Introduction","chapter":"1 Essentials of set theory","heading":"1.1 Introduction","text":"Imagine life deterministic!\ncommute gym, university work always take exactly length time; weather predictions always accurate; know exactly lotto numbers drawn weekend…However, life full unpredictable variation.Variation may unpredictable, patterns still emerge.\nmay know next toss coin produce… see pattern long run: Head appears half time.Probability one tools used describe understand unpredictability.\nDistribution theory describing patterns unpredictability using probability.\nStatistics data collection extracting information data, using probability distribution theory.fields study, situations exist certain almost impossible, probability necessary:chance particular share price crash next month?odds medical patient suffer dangerous side-effect?likely dam overflow next year?chance finding rare bird species given forest?answer questions, framework needed: concepts like probability need defining, notation theory required.\ntools important modelling real-world phenomena, also providing firm mathematical foundation theory statistics.chapter covers concept probability, introduces notation definitions, develops theory useful working probabilities.","code":""},{"path":"ChapterSetTheory.html","id":"Sets","chapter":"1 Essentials of set theory","heading":"1.2 Sets","text":"basis probability working sets, first define.Definition 1.1  (Sets) set well-defined, unordered collection distinct elements, usually denoted using capital letter: \\(\\).use ‘distinct’ means elements repeated; notice order elements set relevant.\nphrase ‘well-defined’ means must clear whether given element set (‘belongs set’) set (‘belong set’).elements can interpreted widely, can refer letters, digits, names, suits packof cards, numbers, words, animals, people, machine components, plants… even sets.Definition 1.2  (Elements) Distinct elements set \\(\\) usually denoted lower-case letters, shown belong set enclosing braces:\n\\[\n   = \\{ a_1, a_2, a_3, a_4\\}.\n\\]Example 1.1  (Sets) two sets equal, since order elements matter:\n\\[\\begin{align*}\n  &= \\{\\text{Brisbane}, \\text{Sydney}, \\text{Hobart}, \\text{Melbourne}, \\text{Adelaide}, \\text{Perth}\\};\\\\\n  B &= \\{\\text{Hobart}, \\text{Melbourne}, \\text{Perth, }\\text{Brisbane}, \\text{Sydney}, \\text{Adelaide}\\}.\n\\end{align*}\\]\nsets contain collection six distinct elements (‘Australian state capital cities’).\ncan write \\(= B\\).contrast,\n\\[\n  C = \\{\\text{Brisbane}, \\text{Sydney}, \\text{Hobart}, \\text{Sydney}, \\text{Hobart}, \\text{Hobart} \\}\n\\]\nset, since elements distinct; ‘Sydney’ ‘Hobart’ repeated.Example 1.2  (Examples sets) define set \\(M\\) :\n\\[\n  M = \\{\\text{Echidnas}, \\text{Platypuses}\\},\n\\]\n\\(M\\) set known monotremes.\nSet \\(M\\) two elements.define set \\(E\\) \n\\[\n   E = \\{1, 3, 5, 7, 9\\},\n\\]\nset odd digits.\nSet \\(E\\) five elements., define set \\(W\\) :\n\\[\n  W = \\{ M, E\\} = \\{ (\\text{Echidnas}, \\text{Platypuses}), (1, 3, 5, 7, 9)\\}.\n\\]\nset \\(W\\) contains two elements: two sets \\(M\\)  \\(E\\).symbol ‘\\(\\\\)’ used denote element member set; symbol ‘\\(\\notin\\)’ used denote element member set.\nelement \\(\\) set \\(\\), write \\(\\\\), say ‘\\(\\) element set \\(\\)’.Example 1.3  (Elements sets) set \\(\\) Example 1.1, write\n\\[\n  \\text{`Hobart'} \\\n\\]\nindicate ‘Hobart’ member set \\(\\).\nSimilarly, write\n\\[\n  \\text{`Canberra'} \\notin \n\\]\nindicate ‘Canberra’ member set \\(\\).","code":""},{"path":"ChapterSetTheory.html","id":"DescribingElementsOfSets","chapter":"1 Essentials of set theory","heading":"1.3 Defining the elements of sets","text":"elements sets can defined various ways:enumerating (listing) individual elements;describing common elements set, using description stating pattern; orby using rule.Whichever method used, given element must clearly set set.Example 1.4  (Elements set) Set \\(\\) Example 1.1 elements listed.\nelements also described defining set \\(\\) ‘set capital cities Australian states’.‘Canberra’ element set, since Canberra Australia state capital.Example 1.5  (Defining elements set) Define set \\(L\\) ‘set books local library two authors’.\nListing elements set difficult, definition clearly allows us distinguish book set book set.elements set described using rule.","code":""},{"path":"ChapterSetTheory.html","id":"SpecialSets","chapter":"1 Essentials of set theory","heading":"1.4 Specific sets","text":"specific sets useful often used, given names /symbols represent .set elements given special name: empty set.Definition 1.3  (Empty set) null set, empty set, zero elements, denoted \\(\\varnothing\\).Notice set denoted \\(\\varnothing\\), number elements  \\(0\\).contrast, universal set set elements consideration, specific context.Definition 1.4  (Universal set) universal set set elements considered specific context.\noften denoted \\(S\\), \\(U\\) \\(\\Omega\\).practice, universal almost always defined (assumed).\nTheoretically, true, avoid difficulties like *Russell’s paradox’.Russell’s paradox, discovered Bertrand Russell  1901, famous set-theory paradox.Consider set \\(R\\), whose elements sets members .\n \\(R\\) member ?Consider two options:\\(R\\) member (, \\(R \\R\\)).\ndefinition  \\(R\\) (set sets members ),  \\(R\\) member , satisfies condition member .\nleads contradiction: assuming \\(R\\) member leads conclusion  \\(R\\) member .R member (, \\(R \\notin R\\)).\n \\(R\\) member , meets criterion included  \\(R\\).\nTherefore, \\(R\\) must member .\nalso leads contradiction: assuming \\(R\\) member leads conclusion  \\(R\\) member .cases lead contradiction.\nset \\(R\\) allowed exist, leads breakdown set theory.\nprevent paradox, modern set theory (ZFC) places restriction sets can formed (though discuss limits).Example 1.6  (Null set) Consider rolling standard, fair six-sided die, observing uppermost face.\nsix options possible, context define universal set \n\\[\n  U = \\{1, 2, 3, 4, 5, 6\\}.\n\\]\n, set \\(B\\) rolls greater  \\(4\\) contain two elements\n\\[\n  B = \\{5, 6\\}.\n\\]\nAlso, set \\(C\\) rolls greater  \\(10\\) contain elements, empty set\n\\[\n  C = \\varnothing.\n\\]\nNotice write \\(C = \\{\\varnothing\\}\\); symbol \\(\\varnothing\\) represents set (elements).\nwrite\n\\[\n  D = {\\varnothing},\n\\]\nset \\(D\\) one element, element empty set.special sets numbers useful define:Natural (counting) numbers denoted \\(\\mathbb{N}\\): \\(\\{1, 2, 3, \\dots\\}\\).Integers denoted \\(\\mathbb{Z}\\): \\(\\{\\ldots, -2, 1, 0, 1, 2, 3, \\dots\\}\\).Rational numbers denoted \\(\\mathbb{Q}\\).Real numbers denoted \\(\\mathbb{R}\\).Positive real numbers denoted \\(\\mathbb{R}_{+}\\).Negative real numbers denoted \\(\\mathbb{R}_{-}\\).Complex numbers denoted \\(\\mathbb{C}\\).authors include \\(0\\) set natural numbers, .","code":""},{"path":"ChapterSetTheory.html","id":"FiniteInfiniteSets","chapter":"1 Essentials of set theory","heading":"1.5 Types of sets and cardinality","text":"Sets can :finite (informally: elements can counted; number elements finite);countable infinite (informally: elements counted principle… infinite number ); oruncountably infinite (informally: elements set counted).sets finite countably infinite, single element set called element sample point.\n, write \\(\\\\) denote element \\(\\) belongs set \\(\\).Example 1.7  (Defining elements set) Define set \\(\\) \n\\[\n   = \\left\\{ \\text{Countries whose names begin end letter `'}\\right\\}.\n\\]\nelements \\(\\) include, example, ‘Austria’, ‘Antigua Barbuda’ ‘Albania’.\n\\(\\) contains finite number elements.","code":""},{"path":"ChapterSetTheory.html","id":"FiniteSets","chapter":"1 Essentials of set theory","heading":"1.5.1 Finite sets","text":"number elements set can counted, set called finite set.\nwords, set finite exactly \\(n\\) distinct elements non-negative integer \\(n\\).\n\\(n = 0\\) (, set contains elements), set empty set.two sets \\(\\)  \\(B\\) Example 1.1 finite sets, contain finite number elements (\\(n = 6\\)).Example 1.8  (Defining elements finite sets) Define set \\(S\\) ‘set suits standard pack cards’.\n, \\(S = \\{\\clubsuit, \\spadesuit, \\heartsuit, \\diamondsuit\\}\\).\ncan write, example, \\(\\spadesuit \\S\\).elements set listed; set four elements.Example 1.9  (Defining elements finite sets) Define set \\(O\\) ‘set odd numbers  \\(0\\)  \\(100\\)’.\nAlternatively, elements set can defined giving pattern:\n\\[\n  O = \\{1, 3, 5, 7, 9, \\dots 95, 97, 99\\}\n\\]\nSet \\(O\\) \\(50\\) elements.Example 1.10  (Finite sets) Define set \\(B\\) ‘even numbers rolling single die’; ,\nB = {⚁, ⚃, ⚅}set \\(B\\) finite, three elements.","code":""},{"path":"ChapterSetTheory.html","id":"CountablyInfiniteSets","chapter":"1 Essentials of set theory","heading":"1.5.2 Countably infinite sets","text":"Sets can also countably infinite.\nset countably infinite distinct elements can counted listed principle, infinite number elements.Definition 1.5  (Countably infinite set) set called countably infinite elements can put one--one correspondence natural numbers\n\\[\n  N = \\{1, 2, 3, \\dots\\}.\n\\]\nwords, elements can listed enumerated infinite sequence \\(\\{a_1, a_2, a_3, \\dots\\}\\) without missing elements, element appears just .Example 1.11  (Countably infinite sets) Define \\(S\\) number rolls die needed roll \n⚅.\nnumber rolls needed \\(1\\) \\(2\\) \\(3\\)… theory, upper limit exists.\n, set \\(S\\) \n\\[\n  S = \\{1, 2, 3, 4, \\dots\\}\n\\]\nSet \\(S\\) countably infinite.Example 1.12  (Countably infinite sets) Define set \\(O\\) ‘set odd numbers’.\nelements set can described (odd numbers’), giving pattern:\n\\[\n  O = \\{1, 3, 5, 7, 9, \\dots\\}\n\\]\nSet \\(O\\) countably infinite.","code":""},{"path":"ChapterSetTheory.html","id":"InfiniteSets","chapter":"1 Essentials of set theory","heading":"1.5.3 Uncountably infinite sets","text":"sets described listing elements, even using infinite sequence.\ncalled uncountably infinite sets.\nUncountably infinite sets described listing elements.Definition 1.6  (Uncountably infinite sets) set called uncountably infinite elements set counted (.e., set contains infinite number elements), elements set listed sequence like \\(a_1\\), \\(a_2\\), \\(a_3\\), \\(\\dots\\) without missing elements.Example 1.13  (Uncountably infinite sets) set \\(R\\) refers positive real numbers, uncountably infinite set.\nelements \\(R\\) listed without missing elements.\ninstance, infinite number elements exists elements \\(0\\)  \\(0.0001\\).Set \\(R\\) uncountably infinite.Example 1.14  (Uncountably infinite set) Consider heights people.\ntwo heights \\(173\\,\\text{cm}\\) \\(174\\,\\text{cm}\\), infinite number heights exist:\n\\(173.1\\,\\text{cm}\\), \\(173.01\\,\\text{cm}\\), \\(173.001\\,\\text{cm}\\), \\(173.0001\\,\\text{cm}\\), .set heights uncountably infinite.Since elements uncountably infinite sets can listed—even pattern—elements uncountably infinite sets often concisely defined using set-builder notation.\n(Set-builder notation can used sets, especially useful uncountably infinite sets.)Set-builder notation usually format:\n\\[\\begin{align*}\n  &= \\{ x \\mid \\text{condition $x$ belong set}\\}, \\text{}\\\\\n  &= \\{ x :    \\text{condition $x$ belong set}\\}.\n\\end{align*}\\]\nnotation, \\(\\{\\dots\\}\\) means  \\(\\) set; \\(x\\) variable representing elements set \\(\\); colon vertical bar read ‘’, followed condition \\(x\\) belong set \\(\\).\nOften, notation left ‘’ defines universal set context.Example 1.15  (Set-builder notation) Consider set defined \n\\[\n   F = \\{s \\\\text{Names students certain course} \\mid \\text{students whose mark less $50\\%$}\\}.\n\\]\nread ‘\\(F\\) set student names certain course \\(s\\), elements \\(s\\) names students course mark less \\(50\\)%’.\n, \\(F\\) represents set students specific course scored less  \\(50\\)%.Example 1.16  (Set-builder notation) Consider set defined \n\\[\n   B = \\{x \\\\text{Animal species}: \\text{$x$ animal species two legs}\\}\n\\]\nread ‘\\(B\\) set animals species \\(x\\), elements \\(x\\) animal species two legs’.\nalso written\n\\[\n   B = \\{x : \\text{$x$ animal species two legs}\\}\n\\]Example 1.17  (Set-builder notation) Consider set defined \n\\[\n    V = \\{x \\\\text{letters alphabet} \\mid \\text{$x$ vowel}\\}.\n\\]\nread ‘\\(V\\) set letters  \\(x\\), elements \\(x\\) vowels’.Example 1.18  (Set-builder notation) Consider set defined \n\\[\n   P = \\{y \\\\mathbb{Z} : \\text{$y$ two-digit integer}\\}.\n\\]\nread ‘\\(P\\) set integers \\(y\\), elements \\(y\\) two-digit integers’.Example 1.19  (Throwing cricket ball) Consider throwing cricket ball.\nLet \\(D\\) set possible distances (metres) ball thrown; \n\\[\n   D = \\{ d \\\\mathbb{R} \\mid d \\ge 0 \\}.\n\\]\ncardinality \\(D\\) uncountably infinite.Example 1.20  (Uncountably infinite sets) Define \\(C\\) set ‘possible distances travelled cars Europe one year’.\nSet \\(C\\) uncountably infinite, can defined \n\\[\n  C = \\{c \\\\mathbb{R} \\mid c \\ge 0 \\}.\n\\]","code":""},{"path":"ChapterSetTheory.html","id":"Cardinality","chapter":"1 Essentials of set theory","heading":"1.5.4 Cardinality","text":"Cardinality refers number elements set.\ncardinality finite set easy describe principle: number elements set.\nnumber elements set finite cardinality denoted \\(||\\).Example 1.21  (Cardinality finite sets) set \\(S\\), defined Example 1.8, \n\\[\n  S = \\{\\clubsuit, \\spadesuit, \\heartsuit, \\diamondsuit\\}.\n\\]\nelements set listed; set four elements.\n, \\(|S| = 4\\).Countably infinite sets uncountably infinite sets infinite cardinality.Countably infinite sets uncountably infinite sets infinite cardinality, different infinite cardinalities.\nInformally, can write \\(|| = \\infty\\) countably infinite sets uncountably infinite sets.formally:\\(\\) countably infinite set \\(|| = \\aleph_0\\), \\(\\aleph_0\\) (‘aleph-null’) smallest infinite cardinality.\\(\\) uncountably infinite set \\(|| = \\mathfrak{c}\\) \\(\\mathfrak{c}\\) (‘cardinality continuum’) larger cardinality \\(\\aleph_0\\).\nSometimes \\(|\\mathbb{R}|\\) used place \\(\\mathfrak{c}\\).Georg Cantor showed \n\\[\n  \\mathfrak{c} = 2^{\\aleph_{0}} > \\aleph_0.\n\\]","code":""},{"path":"ChapterSetTheory.html","id":"RelationshipsBetweenSets","chapter":"1 Essentials of set theory","heading":"1.6 Operations on sets","text":"Sets can combined various ways, sometimes makes defining sets easier .\nVarious operations defined sets.Definition 1.7  (Set operations) Consider two sets \\(\\)  \\(B\\), universal set \\(S\\).\nfollowing operations defined.Intersection:\nintersection sets \\(\\) \\(B\\), written \\(\\cap B\\), set elements \\(\\) \\(B\\): \\(\\cap B = \\{x \\mid x\\\\text{ } x \\B\\}\\).Union:\nunion sets \\(\\) \\(B\\), written \\(\\cup B\\), combined set elements either \\(\\) \\(B\\), : \\(\\cup B = \\{x \\mid x\\\\text{ } x \\B\\}\\).Complement:\ncomplement set \\(\\), written \\(^c\\), elements  \\(S\\) set \\(\\): \\(^c = \\{x \\mid x\\notin \\}\\).Difference, set difference set subtraction:\ndifference sets \\(\\) set \\(B\\), written \\(\\setminus B\\) \\(- B\\), elements  \\(\\) set \\(B\\): \\(\\setminus B = \\{x \\mid x\\\\text{ } x \\notin B\\}\\).Cartesian product:\nCartesian product  \\(\\)  \\(B\\), written \\(\\times B\\), combination elements  \\(\\) elements  \\(B\\).\nexample, \\(= \\{ 1, 2\\}\\) \\(B = \\{10, 20\\}\\), \\(\\times B = \\{(1, 10), (1, 20), (2, 10), (2, 20) \\}\\).Subset:\\(\\) subset  \\(B\\), written \\(\\subseteq B\\), every elements  \\(\\) also element \\(B\\).\n(Set \\(B\\) may also elements  \\(\\).)\ndefinition requires \\(\\) equal smaller  \\(B\\).Proper subset:\ndefinition requires \\(B\\) least one element  \\(\\).Two sets whose intersection empty set (.e., \\(\\cap B = \\varnothing\\) two sets \\(\\)  \\(B\\)) sets called disjoint sets.visual representation Fig. 1.1 may help clarify definitions.\ndiagrams called Venn diagrams.aware variations notation!\nnotation complement event \\(\\) (everything event \\(\\)), denote  \\(^c\\), includes \\(\\overline{}\\)  \\('\\).\nFIGURE 1.1: Venn diagrams showing intersection, union, proper subset, complement set differences. rectangle represents universal set, \\(S\\).\nExample 1.22  (Relationships sets) Suppose define sets:\n\\[\\begin{align*}\n   C &= \\{ 0, 1, 2, 3, 4, 5\\};\\\\\n   D &= \\{ 4, 5, 6, 7\\};\\\\\n   E &= \\{ 6 \\}.\n\\end{align*}\\]\n:\n\\[\\begin{align*}\n   C \\cap D &= \\{ 4, 5\\};                   &          C \\cup D &= \\{ 0, 1, 2, 3, 4, 5, 6, 7\\};\\\\\n   E &\\D;                                &          E &\\notin C;\\\\\n   C \\cap E &= \\varnothing;                   &          D \\cap E &= D;\\\\\n   C\\setminus D &= \\{0, 1, 2, 3\\};          &          D\\setminus C &= \\{6, 7\\}.\n\\end{align*}\\]Example 1.23  (Relationships sets) Suppose define sets:\n\\[\\begin{align*}\n   U &= \\{\\text{digits}\\} = \\{ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9\\};\\\\\n   B &= \\{ 6, 7, 8, 9\\};\\\\\n   E &= \\{ 2, 4, 6, 8\\}.\n\\end{align*}\\]\n, \\(U\\) universal set, set digits,\n, example:\n\\[\\begin{align*}\n   U \\cap B &= \\{ 6, 7, 8, 9 \\} = B;        &          B \\cup E &= \\{2, 4, 6, 7, 8, 9\\};\\\\\n   B &\\U;                                &          U &\\notin B;\\\\\n   B \\cap E &= \\{6, 8\\};                    &          U \\cup E &= U;\\\\\n   B \\setminus E &= \\{7, 8, 9\\};            &          E\\setminus U &= \\varnothing.\n\\end{align*}\\]Example 1.24  (Relationships sets) Consider set \\(\\) defined using set-builder notation :\n\\[\n  = \\{ x  \\\\mathbb{R} \\mid \\sqrt{x^2 - 4} \\\\mathbb{R}\\}.\n\\]\nmeans set \\(\\) comprises real elements \\(x\\) values \\(\\sqrt{x^2 - 4}\\) real numbers; , \\(x^2 - 4 \\ge 0\\).\nvalues  \\(x\\) can defined \n\\[\n  = \\{ x \\\\mathbb{R} \\mid (-\\infty, -2] \\cup [2, \\infty)\\};\n\\]\nset \\(\\) contains real numbers less equal  \\(-2\\), real numbers greater equal  \\(2\\).Notice use brackets: using square brackets ‘\\([\\)’  ‘\\(]\\)’ standard showing indicated value included interval, round brackets ‘\\((\\)’  ‘\\()\\)’ standard showing indicated value included interval.Example 1.25  (Set operations) can define \\(\\mathbb{N}_0 = \\{ 0 \\} \\cup \\mathbb{N}\\).write \\(\\mathbb{Q} = \\{\\frac{m}{n}\\mid m\\\\mathbb{Z}, n \\\\mathbb{Z}, n\\ne 0\\}\\).can also write \\(\\mathbb{Z} \\\\mathbb{R}\\), \\(\\mathbb{R} \\notin \\mathbb{Z}\\).Example 1.26  (Relationships sets) Consider set \\(\\) defined using set-builder notation :\n\\[\n  = \\{ x  \\\\mathbb{R} \\mid \\sqrt{x^2 - 4} \\\\mathbb{R}\\}.\n\\]\nmeans set \\(\\) comprises real elements \\(x\\) values \\(\\sqrt{x^2 - 4}\\) real numbers; , \\(x^2 - 4 \\ge 0\\).\nmeans \n\\[\n  = \\{ (-\\infty, -2] \\cup [2, \\infty)\\};\n\\]\nset \\(\\) contains real numbers less equal  \\(-2\\), real numbers greater equal  \\(2\\).Notice use brackets: using square brackets ‘\\([\\)’ ‘\\(]\\)’ standard showiing indicated value included interval, round brackets ‘\\((\\)’ ‘\\()\\)’ standard showing indicated value included interval.Example 1.27  (Sets can two-dimensional) Sets one-dimensional.\ndefine set \\(P\\) \n\\[\n  P = \\{(x, y)\\\\mathbb{R}\\times\\mathbb{R}  \\mid (0 < x < 1)\\cap(0 < y < 1) \\},\n\\]\n\\(\\mathbb{R}\\times\\mathbb{R}\\) means ‘ordered pairs \\((x, y)\\)  \\(x\\)  \\(y\\) real numbers’.\n\\(P\\) defines unit square Cartesian axes.","code":""},{"path":"ChapterSetTheory.html","id":"SetAlgebra","chapter":"1 Essentials of set theory","heading":"1.7 Set algebra","text":"Set algebra many rules; provide .\nsets \\(\\), \\(B\\)  \\(C\\) defined universal set \\(S\\) rules defined.commutative laws :\\(\\cup B = B \\cup \\)\\(\\cap B = B \\cap \\).associative laws :\\(\\cup(B\\cup C) = (\\cup B)\\cup C\\).\\(\\cap(B\\cap C) = (\\cap B)\\cap C\\).distributive laws :\\(\\cap (B\\cup C) = (\\cap B)\\cup (\\cap C)\\).\\(\\cup (B\\cap C) = (\\cup B)\\cap (\\cup C)\\).de Morgan’s law :\\((\\cap B)^c = ^c \\cup B^c\\).\\((\\cup B)^c = ^c \\cap B^c\\). \nabsorptions laws :\\(\\cup (\\cap B) = \\).\\(\\cap (\\cup B) = \\).dominance laws:\\(\\cup S = S\\)\\(\\cap\\varnothing = \\varnothing\\).idempotent laws:\\(\\cup = \\).\\(\\cap = \\).Proof. give one example proof, one commutative laws: prove \\(\\cup B = B \\cup \\) (may seem ‘obvious’).show proposition, need show \n\\[\\begin{align}\n   (\\cup B) &\\subseteq B\\cup \\qquad\\text{\\emph{also} }\\\\\n   (B \\cup ) &\\subseteq \\cup B.\n\\end{align}\\]\nConsider first statement, consider element \\(x\\) \\(x\\(\\cup B)\\).\nmeans either:\\(x\\\\), case \\(x\\(B\\cup )\\); \\(x\\B\\), case \\(x\\(B\\cup )\\).either case, \\(x \\(B\\cup )\\) \\((\\cup B)\\subseteq B\\cup \\).proof second statement similar, hence \\(\\cup B = B \\cup \\).rules can used prove numerous properties sets.Example 1.28  (Using set algebra) Consider statement \\(\\cap (^c \\cup B)\\).\nsimplify, set algebra rules can used:\n\\[\\begin{align*}\n     \\cap (^c \\cup B)\n     &= (\\cap ^c) \\cup (\\cap B)\\qquad\\text{(distributive law)}\\\\\n     &= \\varnothing \\cup (\\cap B)\\qquad\\text{(definition $^c$)}\\\\\n     &= \\cap B.\\qquad\\text{(dominance law)}\n\\end{align*}\\]\nThus, \\(\\cap (^c \\cup B) = \\cap B\\).","code":""},{"path":"ChapterSetTheory.html","id":"StatisticalComputingWithR","chapter":"1 Essentials of set theory","heading":"1.8 Statistical computing with R","text":"","code":""},{"path":"ChapterSetTheory.html","id":"StatisticalComputingIntro","chapter":"1 Essentials of set theory","heading":"1.8.1 Using R","text":"Computers computer packages essential tools application statistics real problems.\nbook, statistical package R used, used illustrate various concepts help understand theory (Sect. 2.11.3).One way R can used easily compute probabilities specific distributions.\nAlso, R can used verify (prove) theoretical results obtained.\n, technique called computer simulation can used.Simulation can also used solve problems may difficult (impossible) obtain theoretical result.\nSometimes numerical solutions intractable analytical problems termed Monte Carlo simulation.introduce R showing operate sets; general information using R given Sect. C.\nGreater functionality working sets provided installing using R package sets.","code":""},{"path":"ChapterSetTheory.html","id":"SetsInRExample1","chapter":"1 Essentials of set theory","heading":"1.8.2 Simple example","text":"Consider rolling standard, fair six-sided die.\nDefine \\(\\) set numbers can rolled die, \\(B\\) set even numbers can rolled,  \\(C\\) numbers can rolled divisible three.simple example, R can used determine elements sets, union intersection sets.First, define sets:use R find intersection union:function setdiff() find difference two sets:function .element() determines given element contained given set:","code":"\nDieRolls <- 1:6 # Set A\nRollsEven <- seq(2, 6, by = 2) # Set B\nRollsDivisibleBy3 <- c(3, 6) # Set C\nRollsEven # Set B\n#> [1] 2 4 6\nRollsDivisibleBy3 # Set C\n#> [1] 3 6\nintersect(RollsEven, RollsDivisibleBy3)\n#> [1] 6\nunion(RollsEven, RollsDivisibleBy3)\n#> [1] 2 4 6 3\nsetdiff(RollsEven, RollsDivisibleBy3) # B \\ C\n#> [1] 2 4\nsetdiff(RollsDivisibleBy3, RollsEven) # C \\ B\n#> [1] 3\nis.element(RollsDivisibleBy3, RollsEven)\n#> [1] FALSE  TRUE\nis.element(RollsEven, RollsDivisibleBy3)\n#> [1] FALSE FALSE  TRUE"},{"path":"ChapterSetTheory.html","id":"SetsInRExample2","chapter":"1 Essentials of set theory","heading":"1.8.3 More involved example","text":"examples simple enough complete manually.\nHowever, power using computers obvious involved examples.Consider two sets:\n\\[\\begin{align*}\n  &= \\{\\text{Countries whose names `' \\emph{second} letter}\\};\\\\\n  B &= \\{\\text{Countries whose names end `'}\\}.\n\\end{align*}\\]R, names countries can found loading countries package (assuming installed):list country names R vector list_countries():First, find countries whose names second letter (assuming always lower case); , identify elements set \\(\\):, find countries whose names end letter (assuming always lower case); , identify elements set \\(B\\).\n, first need know long name country :can proceed similar second letter name:Now can find countries whose names second last letters finding intersection two sets:","code":"\nlibrary(\"countries\")\nhead( list_countries() ) # Show the first five values only\n#> [1] \"Afghanistan\"    \"Åland Islands\" \n#> [3] \"Albania\"        \"Algeria\"       \n#> [5] \"American Samoa\" \"Andorra\"\n# Find where, in the list of countries, a country name has an `a` in position 2:\n# (Assumes the letter is lower case)\nLocate_a_Second <-  which( substr(list_countries(),\n                                  start = 2, \n                                  stop = 2)\n                           == \"a\")\n\n# Now find the countries listed at those locations:\na_Second <- list_countries()[Locate_a_Second]\n\n# How many countries are there whose names have an `a` as second letter?\nlength(a_Second)\n#> [1] 57\n# Find the length of the name of each country in the list:\nLength_Country_Names <- nchar(list_countries()) # Length of each country name\n# Find where, in the list of countries, a country name has an A in last position:\nlocate_Ends_With_a <- which( substr(list_countries(),\n                                    start = Length_Country_Names,\n                                    stop = Length_Country_Names)\n                            == \"a\")\n\n# Now find the countries listed at those locations:\nEnds_With_a <- list_countries()[locate_Ends_With_a]\n\n# How many countries are there whose names end with an `a`?\nlength(Ends_With_a)\n#> [1] 79\na_Second_And_Last <- intersect(a_Second, \n                               Ends_With_a)\n\n# How many countries are there whose names have `a` as second and last letters?\nlength(a_Second_And_Last)\n#> [1] 18\n\n# What are these countries?\na_Second_And_Last\n#>  [1] \"Cambodia\"                                    \n#>  [2] \"Canada\"                                      \n#>  [3] \"Gambia\"                                      \n#>  [4] \"Jamaica\"                                     \n#>  [5] \"Latvia\"                                      \n#>  [6] \"Malaysia\"                                    \n#>  [7] \"Malta\"                                       \n#>  [8] \"Mauritania\"                                  \n#>  [9] \"Namibia\"                                     \n#> [10] \"Panama\"                                      \n#> [11] \"Papua New Guinea\"                            \n#> [12] \"Saint Helena, Ascension and Tristan da Cunha\"\n#> [13] \"Saint Lucia\"                                 \n#> [14] \"Samoa\"                                       \n#> [15] \"Saudi Arabia\"                                \n#> [16] \"Wallis and Futuna\"                           \n#> [17] \"Zambia\"                                      \n#> [18] \"Taiwan, Province of China\""},{"path":"ChapterSetTheory.html","id":"SetsExercises","chapter":"1 Essentials of set theory","heading":"1.9 Exercises","text":"Selected answers appear Sect. E.1.Exercise 1.1  Prove one idempotent laws Sect. 1.7.Exercise 1.2  Prove one dominance laws Sect. 1.7.Exercise 1.3  Consider set complex numbers \\(\\mathbb{C}\\) written form \\(+ bi\\).Use set-builder notation define set complex numbers terms set real numbers \\(\\mathbb{R}\\).Adapt notation define set purely imaginary numbers \\(\\mathbb{}\\).Exercise 1.4  Consider real solutions general quadratic equation \\(y = ax^2 + bx + c = 0\\) (\\(\\ne 0\\), since equation define quadratic).Using set-builder notation, define:universal set \\(U\\), defines quadratics.set \\(R\\) contains quadratics two equal real roots.set \\(Z\\) contains quadratics real roots.set \\(S\\), terms  \\(U\\), \\(R\\)  \\(z\\), contains quadratics exactly one real root.Exercise 1.5  Consider equation \\(y = \\log(x^2 - 1)\\) real values  \\(y\\).\nUsing set-builder notation, define:set values  \\(x\\)  \\(y\\) real values.set values  \\(x\\) \\(y > 1\\).set values  \\(x\\)  \\(y\\) value zero.Exercise 1.6  Define sets:\n\\[\\begin{align*}\n  S &= \\{\\clubsuit, \\spadesuit\\};\\\\\n  P &= \\{ \\text{Jack}, \\text{Queen}, \\text{King}, \\text{Ace}\\}\\quad \\text{(.e., picture cards)},\n\\end{align*}\\]\n \\(U\\) universal set cards pack.\nUsing set-builder notation, define:set \\(\\) contains black picture cards.set \\(B\\) contains red picture cards.set \\(C\\) contains red non-picture cards.Exercise 1.7  R, names US states contains built-dataset state.name.\nUsing dataset, use R :find set US states names begin  W.find set US states names begin North.find set US states names end .find set US states names end , start  W.find set US states names end , start  W.find set US states names end , start  W.Exercise 1.8  R, information US states (mostly) 1977 contained built-dataset state.x77.\nUsing dataset, use R :find set US states whose average life expectancy exceeds \\(70\\).find set US states whose area less  \\(500\\,000\\) square miles.find set US states whose illiteracy rate greater  \\(2\\)%.find set US states whose percent high-school graduates less  \\(50\\)%.find set US states whose illiteracy rate greater  \\(2\\)%, whose percent high-school graduates less  \\(50\\)%.find set US states whose illiteracy rate greater  \\(2\\)%, whose percent high-school graduates less  \\(50\\)%.(view help data, type ?state.x77 R.)Exercise 1.9  Using set-builder notation, define set \\(T\\) values  \\(x\\) \\(y = 1/x\\) real value.Exercise 1.10  Using set-builder notation, define set \\(L\\) values  \\(x\\) \\(y = \\tan x\\) real value.Exercise 1.11  Explain values contained set \\(G\\), defined \n\\[\n   G = \\{ x\\\\mathbb{R} \\mid x^2 < 4\\}\n\\]\ncardinality set \\(G\\)?Exercise 1.12  Explain values contained set \\(B\\), defined \n\\[\n   B = \\{x\\\\mathbb{R} \\mid \\lfloor x \\rfloor = x\\}.\n\\]\nexpression, \\(\\lfloor x \\rfloor\\) floor function, rounds value  \\(x\\) next lowest integer (example, \\(\\lfloor \\pi \\rfloor = 3\\)).cardinality set \\(B\\)?Exercise 1.13  Use set algebra show \\(\\setminus(\\cap B) = \\cap B^c\\).Exercise 1.14  Use set algebra show \\((\\setminus B)^c = ^c\\cup B^c\\).Exercise 1.15  Use set algebra show \\((\\cup B) \\cap (\\cup B^c) \\cap B = \\cap B\\).Exercise 1.16  Use set algebra show \\((\\cap B) \\cup (\\cap B^c) = \\).Exercise 1.17  Suppose set \\(C\\) defined \nC = {H, T}\nset results tossing coin.\nDefine set \\(D\\) shows results two tosses coin, ordered pair.\n(See Example 1.27.)Exercise 1.18  Define following two sets:\n\\[\n   S = \\{\\spadesuit, \\clubsuit, \\diamondsuit, \\heartsuit\\}\n   \\qquad\\text{}\\qquad\n   D = \\{2, 3, 4, \\dots, \\text{Jack}, \\text{Queen}, \\text{King}, \\text{Ace}\\}.\n\\]\nUse two sets define Set \\(C\\) ‘cards standard pack’, ordered pair.\n(See Example 1.27.)","code":""},{"path":"ChapterProbability.html","id":"ChapterProbability","chapter":"2 Probability","heading":"2 Probability","text":"UpOn completion chapter able :understand concepts probability, apply rules probability.define probability using different methods apply compute probabilities various situations.apply concepts conditional probability independence.differentiate mutually exclusive events independent events.apply Bayes’ Theorem.use combinations permutations compute probabilities various events involving counting problems.","code":""},{"path":"ChapterProbability.html","id":"IntroductionToProbability","chapter":"2 Probability","heading":"2.1 Introduction","text":"Probability way describing likely event occur.\nfoundation set theory allows idea probability developed, since probability relies heavily many ideas set theory.Building upon foundation, probability relates outcomes random processes (random experiments).Definition 2.1  (Random process) random process (random experiment) procedure :can repeated, theory, indefinitely essentially identical conditions; andhas well-defined outcomes; andthe outcome individual repetition unpredictable.Examples simple random processes include tossing coin, rolling die.\noutcome instance random process produces unknown, possible outcomes known.","code":""},{"path":"ChapterProbability.html","id":"SampleSpaces","chapter":"2 Probability","heading":"2.2 Sample spaces","text":"talking probability, universal set set possible outcomes can result random process, usually denoted \\(S\\), \\(\\Omega\\) \\(U\\).Definition 2.2  (Sample space) sample space (event space, outcome space) random process set possible outcomes random process, usually denoted \\(S\\), \\(\\Omega\\) \\(U\\) (‘universal set’).Example 2.1  (Sample space) Consider rolling die.\nsample space set possible outcomes:\n\\[\n  S = \\{ 1, 2, 3, 4, 5, 6\\}.\n\\]sets, sample space may finite, countably infinite, uncountably infinite.\nsample space finite countably infinite, sample space called discrete.\nsample space uncountably finite set, sample space called continuous.Example 2.2  (Discrete sample space) sample space Example 2.1 discrete.Example 2.3  (Continuous sample space) Consider height students.\nsample space continuous (see Example 1.14).Sample spaces can also mixture discrete continuous sample spaces.\nsample spaces, part sample space discrete, part continuous.\ncommon example discrete component refers  \\(0\\) continuous part refers positive real numbers \\(\\mathbb{R}\\).Example 2.4  (Mixed sample space) Consider random process observe rainfall recorded given day, \\(R\\).rain fall, rainfall recorded exactly \\(R = 0\\); discrete component.\nHowever, rain fall, exact amount recorded; teh continuous component.sample space \n\\[\n  S = \\{0\\}\\cup \\mathbb{R}.\n\\]\nsample space mixed.","code":""},{"path":"ChapterProbability.html","id":"Events","chapter":"2 Probability","heading":"2.3 Events","text":"","code":""},{"path":"ChapterProbability.html","id":"SimpleEvents","chapter":"2 Probability","heading":"2.3.1 Simple events","text":"sample space defines set possible outcomes, usually interested just elements sample space.\nEvents subsets sample space (hence also sets).Definition 2.3  (Event) event \\(E\\) subset \\(S\\), write \\(E \\subseteq S\\).definition, \\(S\\) event.\nsample space finite countable infinite set, event collection sample points.Example 2.5  (Events) Consider simple random process tossing coin twice.\nsample space set\n\\[S = \\{ (H, H), \\enskip(H, T), \\enskip (T, H), \\enskip (T, T)\\},\\]\n\nH\nrepresents tossing head \nT\nrepresents tossing tail, pair lists result two tosses order.can define event \\(\\) ‘tossing head second toss’, list elements:\n\\[= \\{ (H, H), \\enskip (T, H)\\};\\]\nnotice \\(\\subset S\\) (.e., \\(\\) proper subset  \\(S\\)).event \\(T\\), defined ‘set outcomes corresponding tossing three heads’, null empty set; sample points three heads.\n, \\(T = \\varnothing\\).Definition 2.4  (Simple (elementary) event) sample space finite countable infinite number elements, simple event (elementary event) event one sample point, decomposed smaller events.Example 2.6  (Simple events) Consider observing outcome single roll die (Example 2.1), sample space set possible outcomes:\n\\[\n  S = \\{ 1, 2, 3, 4, 5, 6\\}.\n\\]\nsix simple events :\n\\[\\begin{align*}\n   E_1 = \\{1\\}&\\quad \\text{(.e., roll 1)}; & E_2 = \\{2\\}:&\\quad \\text{(.e., roll 2)};\\\\\n   E_3 = \\{3\\}&\\quad \\text{(.e., roll 3)}; & E_2 = \\{4\\}:&\\quad \\text{(.e., roll 4)};\\\\\n   E_5 = \\{5\\}&\\quad \\text{(.e., roll 5)}; & E_2 = \\{6\\}:&\\quad \\text{(.e., roll 6)}.\n\\end{align*}\\]important concept occurrence event.Definition 2.5  (Occurrence) event \\(\\) occurs particular trial random process outcome trial element subset \\(\\) random process.","code":""},{"path":"ChapterProbability.html","id":"CompoundEvents","chapter":"2 Probability","heading":"2.3.2 Compound events","text":"Simple events usually great interest; events interest usually contains many elements sample space.\ncalled compound events.Definition 2.6  (Compound event) collection simple events sometimes called compound event.Since compound events, like events, sets, operations existing sets (Sect. \n1.6) can used define compound events.Example 2.7  (Simple compound events) Consider observing outcome single roll die, shown Example 2.6.Define event \\(T\\) ‘numbers divisible  \\(3\\)’ event \\(D\\) ‘numbers divisible  \\(2\\)’.\n\\(T\\) \\(D\\) compound events:\n\\[\n   T =\\{E_3, E_6\\} = \\{3, 6 \\}\n   \\quad\n   \\text{}\n   \\quad\n   D = \\{E_2, E_4, E_6\\} = \\{2, 4, 6 \\}.\n\\]set operations Sect. 1.6 apply events, events sets.\nHowever, different language usually used, indicate events real outcomes, whereas sets describing structures widely abstractly (Table 2.1).\nexample, ‘disjoint’ used sets (Sect. 1.6), whereas ‘mutually exclusive’ used referring events.Definition 2.7  (Mutually exclusive) Events \\(\\) \\(B\\) mutually exclusive , , \\(\\cap B = \\varnothing\\); , outcomes common.\n, Events \\(\\) \\(B\\) mutually exclusive corresponding sets disjoint.\nTABLE 2.1: language used set theory probability. examples based examples rolling six-sided die, possible outcomes \\(\\{1, 2, 3, 4, 5, 6\\}\\), \\(= \\{1, 2, 3\\}\\) \\(B = \\{1, 6\\}\\).\nExample 2.8  (Tossing coin twice) Consider simple random process tossing coin twice (Example 2.5), define events \\(M\\) \\(N\\) follows:two sets disjoint, sample points common.\nevents therefore mutually exclusive.Since events really just sets, set algebra Sect. 1.7 applies events also.Example 2.9  (Rolling die) Suppose roll single, six-sided die.\nrolling die, sample space \\(S = \\{1, 2, 3, 4, 5, 6\\}\\).\ncan define two events:\n\\[\\begin{align*}\n   E = \\text{even number thrown}         &= \\{2, 4, \\phantom{5, }6\\};\\\\\n   G = \\text{number larger 3 thrown} &= \\{\\phantom{2,\\ }4, 5, 6\\}.\n\\end{align*}\\]\n, following compound events defined:\n\\[\\begin{align*}\n   E \\cap G       &= \\{4, 6\\}      &    E \\cup G  &= \\{ 2, 4, 5, 6\\}\\\\\n   E^c            &= \\{ 1, 3, 5\\}  &    G^c       &= \\{ 1, 2, 3\\}.\n\\end{align*}\\]\ncan make observations :\n\\[\\begin{align*}\n   E \\cap G^c\n   &= \\{2, 4, 6\\} \\cap \\{ 1, 2, 3\\} = \\{ 2 \\};\\\\\n   E^c \\cap G^c\n   &= \\{1, 3, 5\\} \\cap \\{ 1, 2, 3\\} = \\{ 1, 3 \\}.\n\\end{align*}\\]\nSee Venn diagram Fig. 2.1.\nFIGURE 2.1: Venn diagram showing events \\(E\\) \\(G\\).\nExample 2.10  (Throwing cricket ball) Consider throwing cricket ball, distance throw (metres) interest (Example 1.19).\ndefine sample space \\(D\\) \\(D = \\{ d \\\\mathbb{R} \\mid d \\ge 0 \\}\\).\npractically, write\n\\[\n  D = \\{ d \\\\mathbb{R} \\mid 0 < d < 150 \\}\n\\]\ngiven throwing cricket ball greater \\(150\\,\\text{m}\\) effectively impossible (never recorded), throwing cricket ball exactly \\(0\\,\\text{m}\\) also impossible practice.can define two events:\n\\[\\begin{align*}\n   B_1 &= \\{b \\S \\mid b \\ge 40\\} &&\\quad \\text{(.e., throw cricket ball least $40\\,\\text{m}$)};\\\\\n   B_2 &= \\{b \\S \\mid b < 50\\}   &&\\quad \\text{(.e., throw cricket ball less $50\\,\\text{m}$)}.\n\\end{align*}\\]\n:\n\\[\\begin{align*}\n   B_1 \\cap B_2   &= \\{b\\S \\mid 40\\le b<50\\}\\quad \\text{(.e., throw ball least $40$m less $50\\,\\text{m}$)};\\\\\n   B_1 \\cup B_2   &= S;\\\\\n   B_1^c          &= \\{b\\S \\mid b < 40\\}\\quad \\text{(.e., throw ball less $40\\,\\text{m}$)}.\n\\end{align*}\\]\nFIGURE 2.2: two events \\(B_1\\)  \\(B_2\\) defined throwing cricket ball, three events defined  \\(B_1\\)  \\(B_2\\). open, indicated value included region.\n","code":""},{"path":"ChapterProbability.html","id":"Probability","chapter":"2 Probability","heading":"2.4 Probablility","text":"","code":""},{"path":"ChapterProbability.html","id":"ProbabilityDefinition","chapter":"2 Probability","heading":"2.4.1 Definitions","text":"Usually, interested likely various outcomes random experiment occur.\n, likely observe various events defined sample space.\nProbability mathematical term quantifying likelihood.\nprobability event \\(E\\) occurring denoted \\(\\text{Pr}(E)\\).Definition 2.8  (Probability) Probability function assigns number event.\n, event \\(E\\), value \\(\\Pr(E)\\) represents probability event \\(E\\) occurs.probability event \\(E\\) occurring can denoted \\(\\text{P}(E)\\), \\(\\text{Pr}(E)\\), \\(\\text{Pr}\\{E\\}\\), using similar notation.definition Def. 2.8 allows number assigned event, without rules restrictions (‘assigns number’).\nrestrictions must placed numbers can assigned make definition workable practical.","code":""},{"path":"ChapterProbability.html","id":"ProbabilityAxioms","chapter":"2 Probability","heading":"2.4.2 Three axioms of probability","text":"\ndefined probability function assigns number event, stated numbers can assigned ‘probability’.\nvalues probability take?\nnumerical likelihoods assigned?rigorous foundation probability found using three fundamental axioms, called Axioms Probability.\nUsing axioms, rules probability can derived.\naxioms formally define rules apply probabilities.axiom self-evident truth require proof, proven.\nform starting point building proofs.Definition 2.9  (Kolmogorov's three axioms probability) Consider sample space \\(S\\) random process, event \\(\\) \\(S\\) \\(\\subseteq S\\).\nevery event \\(\\) (subset  \\(S\\)), number \\(\\Pr()\\) can assigned called probability event \\(\\).Kolmogorov’s three axioms probability :Non-negativity: \\(\\Pr() \\ge 0\\).\nprobability event non-negative real number.Exhaustive: \\(\\Pr(S) = 1\\).\nevent something happens probability \\(1\\) (.e., certain), since sample space lists possible outcomes.Additivity: \\(A_1\\) \\(A_2\\) two mutually exclusive events \\(S\\) (.e., \\(A_1 \\cap A_2 = \\varnothing\\)), \n\\[\n   \\Pr(A_1 \\cup A_2) = \\Pr(A_1) + \\Pr(A_2).\n\\]","code":""},{"path":"ChapterProbability.html","id":"ProbabilityRules","chapter":"2 Probability","heading":"2.4.3 Rules of probability","text":"\npurpose axioms formally define probability rules apply probabilities.\naxioms probability can used develop probability formulae.\nexample, properties follow three axioms, Events \\(\\)  \\(B\\) defined sample space \\(S\\):Bounds:\\(0 \\le \\Pr()\\le 1\\); , probabilities numbers zero one inclusive event \\(\\).Empty sets:\\(\\Pr(\\varnothing) = 0\\); , probability impossible event zero.Monotonicity:\n\\(\\subseteq B\\), \\(\\Pr() \\le \\Pr(B)\\); , every outcome event \\(\\) also event \\(B\\), probability  \\(\\) exceed probability  \\(B\\).Complements:\\(\\Pr(^c) = 1 - Pr()\\); , probability event \\(\\) happens  \\(1\\) minus probability happen.Addition:\\(\\Pr(A_1 \\cup A_2) = \\Pr(A_1) + \\Pr(A_2) - \\Pr(A_1 \\cap A_2)\\), general result third axiom.can proven using three axioms definitions presented far.\ngiv etwo examoples using axiom prove results.empty set \\(\\varnothing\\),: \\(\\Pr(\\varnothing) = 0\\).Proof. may appear ‘obvious’, one three axioms.\ndefinition, empty set \\(\\varnothing\\) contain outcomes; hence \\(\\varnothing \\cup = \\) event \\(\\); two events mutually exclusive., third axiom, \\(\\varnothing\\cap = \\varnothing\\), \\(\\varnothing\\) \\(\\) mutually exclusive.\nHence, third axiom\n\\[\\begin{equation}\n   \\Pr(\\varnothing\\cup ) = \\Pr(\\varnothing) + \\Pr().\n   \\tag{2.1}\n\\end{equation}\\]\nsince \\(\\varnothing \\cup = \\), \\(\\Pr(\\varnothing \\cup ) = \\Pr()\\), \\(\\Pr() = \\Pr(\\varnothing) + \\Pr()\\) Eq. (2.1).\nHence \\(\\Pr(\\varnothing) = 0\\).result may seemed obvious, probability formulae can developed just assuming three axioms probability.Theorem 2.1  (Complementary rule probability) event \\(\\), probability ‘\\(\\)’ \n\\[\n   \\Pr(^c) = 1 - \\Pr().\n\\]Proof. definition complement event, \\(^c\\) \\(\\) mutually exclusive.\nHence, third axiom, \\(\\Pr(^c \\cup ) = \\Pr(^c) + \\Pr()\\).\\(^c\\cup = S\\) (definition complement) \\(\\Pr(S) = 1\\) (Axiom 2), \\(1 = \\Pr(^c) + \\Pr()\\), result follows.three axioms dictate probability real value  \\(0\\)  \\(1\\).\nways also exists quantify likelihood event occuring.\nexample, sometimes chance event occurring expressed odds, probabilities.\nOdds ratio often event likely occur, often event likely occur.Importantly: ‘odds’ ‘probability’ .\nthree axioms define rules probabilities must follow.seen axioms, rules follow , can now consider determine probability assigned certain events.\n","code":""},{"path":"ChapterProbability.html","id":"AssignProbDiscrete","chapter":"2 Probability","heading":"2.5 Assigning probabilities: discrete samples spaces","text":"Developing method assigning probability event difficult.\nHowever, discrete sample spaces, two options :finding probabilities using classical probability (Sect. 2.5.1).\napproach works simple events sample space equally likely (.e., reason suspect one outcome likely ).estimating probabilities using relative-frequency (Sect. 2.5.2), trials can repeated many times.","code":""},{"path":"ChapterProbability.html","id":"ClassicalProb","chapter":"2 Probability","heading":"2.5.1 Classical probability","text":"discrete sample space, outcomes sample space equally likely (.e., reason suspect one outcome likely ), probability event \\(E\\) defined \n\\[\n  \\Pr(E)\n  = \\frac{|E|}{|S|}\n  = \\frac{\\text{number elements $E$}}{\\text{number elements $S$}},\n\\]\\(|\\cdot|\\) refers cardinality notation (Sect. 1.5.4).probability  \\(0\\) assigned event never occurs (.e., \\(E\\) corresponds impossible event),  \\(1\\) event certain occur (.e., \\(E\\) corresponds universal set).\nNotice approach conforms restriction probabilities numbers  \\(0\\)  \\(1\\) inclusive, result follows three axioms probability.Using classical approach probability often requires careful counting number elements sample space, number elements event interest.\nMethods careful counting explored Sect. 2.6.","code":""},{"path":"ChapterProbability.html","id":"EmpiricalApproach","chapter":"2 Probability","heading":"2.5.2 Relative frequency (empirical) approach","text":"mathematical definition probability axioms describe properties probability measure. classical definition probability naturally satisfies axioms, requires equally-likely outcomes sample space \\(S\\).However, outcomes rarely equally likely; probability ‘receiving rain tomorrow’ always probability ‘receiving rain tomorrow’.\nrandom process can repeated many times, counting number times event interest occurs means can compute proportion times event occurs.\nMathematically, random process repeated \\(n\\) times, event \\(E\\) occurs  \\(m\\) (\\(m < n\\)), probability event occurring \n\\[\n   \\Pr(E) = \\lim_{n\\\\infty} \\frac{m}{n}.\n\\]\npractice, \\(n\\) needs large—repetitions random—compute probabilities accuracy.\npractice , approximate probabilities can found (since \\(n\\) finite practice).relative frequency (empirical) approach probability.method always used practice.\nConsider probability air bag car correctly deploys crash.\nCrashing thousands cars financially viable estimate probability air bag correctly deploying.\nFortunately, car manufacturers can crash small numbers cars get appoximate indications probabilities correct air bag deployment.\nSometimes, computer simulations can used approximate probabilities., probability  \\(0\\) assigned event never occurs (.e., \\(E\\) corresponds impossible event),  \\(1\\) event certain occur (.e., \\(E\\) corresponds universal set), \\(n\\\\infty\\).\nNotice approach conforms restriction probabilities numbers  \\(0\\)  \\(1\\) inclusive, result follows three axioms probability.Example 2.11  (Salk vaccine) 1954, Jonas Salk developed vaccine polio (Williams (1994), 1.1.3).\ntest effectiveness vaccine, data Table 2.2 collected.relative frequency approach can used estimate probabilities developing polio vaccine without vaccine (control group):\n\\[\\begin{align*}\n   \\Pr(\\text{develop polio control group})\n   &\\approx \\frac{115}{201\\,229} = 0.000571;\\\\[3pt]\n   \\Pr(\\text{develop polio vaccinated group})\n   &\\approx \\frac{33}{200\\,745} = 0.000164,\n\\end{align*}\\]\n‘\\(\\approx\\)’ means ‘approximately equal ’.\nestimated probability contracting polio control group 3.5 times greater control group.\nprecision sample estimates quantified producing confidence interval proportions.\nTABLE 2.2: number paralytic cases two groups children: one group controls another vaccinated Salk polio vaccine.\n","code":""},{"path":"ChapterProbability.html","id":"CombsAndPerms","chapter":"2 Probability","heading":"2.6 Counting elements: combinatorics","text":"","code":""},{"path":"ChapterProbability.html","id":"MultiplicationRule","chapter":"2 Probability","heading":"2.6.1 Basic ideas: multiplication rule","text":"Applying classical approach probability often requires counting number elements finite, discrete sample space, given event.Example 2.12  (Counting elements) many outcomes possible coin flipped \\(3\\) times?\nListing possible outcomes feasible:\n\\[\\begin{align*}\n  &(\\text{Head}, \\text{Head}, \\text{Head}),   & &(\\text{Head}, \\text{Head}, \\text{Tail}),\\\\\n  &(\\text{Head}, \\text{Tail}, \\text{Head}),   & &(\\text{Head}, \\text{Tail}, \\text{Tail}),\\\\\n  &(\\text{Tail}, \\text{Head}, \\text{Head}),   & &(\\text{Tail}, \\text{Head}, \\text{Tail}),\\\\\n  &(\\text{Tail}, \\text{Tail}, \\text{Head}),   & &(\\text{Tail}, \\text{Tail}, \\text{Tail}).\n\\end{align*}\\]\ncan also count four outcomes Tail tossed last.probability event ‘tail final toss three coin tosses’ (using classical probability) \\(4/8 = 0.5\\).considering \\(25\\) tosses coin, however, listing outcomes counting becomes tedious.\nHowever, don’t even need know outcomes ; need know many outcomes .\ncounting methods useful.basic counting principle multiplication rule.Definition 2.10  (Multiplication rule) Event 1 \\(m\\) possible outcomes, Event 2 \\(n\\) possible outcomes, total number combined outcomes \\(m\\times n\\).principle can extended number events.\nexample: three sets events  \\(m_1\\), \\(m_2\\)  \\(m_3\\) outcomes respectively, number distinct triplets containing one element set \\(m_1 m_2 m_3\\).Example 2.13  (Counting elements) Example 2.12, use multiplication rule.\nFlip 1, two possible outcomes.\nLikewise, Flips 2  3, two possible outcomes.\ntotal number possible outcomes \\(2\\times 2\\times 2 = 8\\), found example.\\(25\\) tosses, \\(2^{25} = 33\\, 554\\, 432\\) possible outcomes.Example 2.14  (Multiplication rule) Suppose restaurants offers five main courses three desserts.\n‘meal’ consists one main plus one dessert, \\(5\\times 3 = 15\\) meal combinations possible.Example 2.15  (Counting elements) Consider selecting random password exactly six characters length, using set lower-case letters (‘’, ‘b’, ‘z’).\\(26\\) choices first character, \\(26\\) choices second character, ,\ntotal number passwords \n\\[\n  26^6 = 308\\,915\\,776.\n\\]example , notice letters can reused; letter selected, effectively returned pool letters can chosen .\ncalled selection replacement; selected elements can reselected.multiplication rule demonstrates basic idea behind counting (enumerating) events sample spaces.\ngenerally, though, permutations combinations needed selection made without replacement: , elements selected, selected .Permutations combinations used count outcomes selections made fixed number elements, without replacement,\norder selection important, permutations appropriate.\norder selection important, combinations appropriate.Example 2.16  (Permutations combinations) Consider selecting random six-letter password set lower-case letters : ‘’, ‘b’, ‘z’, letter can repeated (unrealistic).\npasswords, order characters important: passwords listen silent different passwords, even though contain characters.\nSince order characters important, permutations used count many passwords possible.Consider dealing five cards two different players game.\norder cards dealt important; matters cards dealt player.\nSince order cards dealt important, combinations used count many ways deal cards.may help remember use combinations permutations:Permutations used passwords: order important.Combinations used dealing cards: order important.Permutations combinations studied following sections (Sect. 2.6).\nHowever, counting number outcomes can also achieved listing possible outcomes ways.Example 2.17  (Rolling two dice) Consider rolling two standard dice; sample space shown Table 2.3, \\(6\\times 6 = 36\\) elements.\n, example, \\(\\Pr(\\text{sum 5}) = 4/36\\) found counting equally-likely outcomes sum seven (Table 2.4).\nTABLE 2.3: sample space rolling two dice.\n\nTABLE 2.4: sum rolling two dice.\n","code":""},{"path":"ChapterProbability.html","id":"PermutationSelectWithoutReplacement","chapter":"2 Probability","heading":"2.6.2 Permutations","text":"Permutations concern selecting elements fixed number elements (without replacement).Definition 2.11  (Permutations) permutation ordered selection elements (without replacement).Consider finite, discrete sample space  \\(n\\) distinct elements.\nfirst element chosen can selected  \\(n\\) different ways.\nsecond element must chosen, can done \\((n - 1)\\) ways (first element, selection without replacement), using multiplication rule.\n\\((n - 2)\\) ways third, .\nNotice element selected, selected ; called selecting without replacement.\nContinuing , using multiplication rule (Def. 2.10), \n\\[\n  n(n - 1)(n - 2)\\ldots 2 \\times 1\n\\]\ndifferent ways order  \\(r\\) elements.\ndenoted  \\(n!\\) called ‘\\(n\\)-factorial’:\n\\[\n   n! = n(n - 1)(n - 2) \\ldots (2)(1)\n\\]\n\\(n\\geq 1\\), define \\(0! = 1\\).definition, \\(0! = 1\\).\ndefinition, many formulas (follow) remain valid valid choices  \\(n\\)  \\(r\\).Example 2.18  (Factorials) Factorials get large quickly: \\(4! = 4\\times 3\\times 2\\times 1 = 24\\), \\(10! = 3\\,628\\,800\\).Example 2.19  (Factorials) Consider expression\n\\[\n   \\frac{6!}{3!} =\n   \\frac{6\\times 5\\times 4\\times 3!}{3!} =\n   6\\times 5\\times 4 = 120.\n\\]\nNotice top line need evaluation.\ntrick often used working factorials.\ninstance, can compute:\n\\[\n   \\frac{57!}{53!} =\n   \\frac{57\\times 56\\times 55\\times 54\\times 53!}{53!} =\n   57\\times 56\\times 55\\times 54 = 9\\,480\\,240,\n\\]\nwithout needing compute value  \\(57!\\) (approximate value \\(4\\times 10^{76}\\)).Now, consider finite, discrete sample space \\(S\\)  \\(n\\) distinct elements .\nSuppose wish count number permutations size \\(r\\) can drawn  \\(S\\), selected items reselected (‘without replacement’).,  \\(n\\) options first item selected,  \\(n - 1\\) options second item selected, since element selected first re-selected.\nidea applies  \\(r\\) elements.\nTherefore, number permutations size \\(r\\), selection without replacement, (using multiplication rule Def. 2.10 idea Example 2.19):\n\\[\n   n \\times (n - 1)\\times (n - 2)\\times\\cdots\\times (n - r + 1)\n   = \\frac{n!}{(n - r)!}.\n\\]\nnumber denoted \\(^nP_r\\), write\n\\[\n   P^n_r = n(n - 1)(n - 2)\\ldots (n - r + 1) = \\frac{n!}{(n - r)!}.\n\\]\nexpression referred number permutations  \\(r\\) elements  \\(n\\) elements.Notation permutations varies.\nnotation permutations include \\(nPr\\), \\(^nP_r\\), \\(P_n^r\\) \\(P(n,r)\\).Example 2.20  (Permutations) Eight runners compete \\(100\\,\\text{m}\\) race.\nmany ways Gold, Silver Bronze medals awarded?situation like selecting \\(r = 3\\) \\(n = 8\\) runners award medals.\naddition, order definitely important (runner coming first happy given Bronze medal), permutations appropriate.\n\n\\[\n  P^8_3 = \\frac{8!}{(8 - 3)!} = 336\n\\]\nways three medals allocated.R, \\(n!\\) given factorial(n).function explicitly compute number permutations, two options computing \\(^nP_r\\) usefactorial(n) / factorial(n - r)\norprod( n : (n - r + 1) )\norprod( (n - r + 1) : n )\n(since prod() computes product given elements).example,\n\\[\n   P^{12}_3 =\\frac{12\\times 11\\times 10\\times 9!}{9!} = 12\\times 11\\times 10 = 1320\n\\]\ncomputed follows:common properties permutations given .\\(P^n_n = n!\\).\\(P^n_0 = 1\\).\\(\\displaystyle P^n_1 = n\\).\\(\\displaystyle P^n_{n - 1} = n!\\).\\(\\displaystyle\\frac{P^n_r}{P^n_{r - 1}} = n - r + 1\\).\\(\\displaystyle P^n_r = P^{n - 1}_r + r \\times P^{n - 1}_{r - 1}\\).\\(\\displaystyle P^n_r = n \\times P^{n - 1}_{r - 1} = n \\times (n - 1) \\times P^{n - 2}_{r - 2}\\), .","code":"\nfactorial(12) / factorial(12-3)\n#> [1] 1320\nprod( 10:12 )\n#> [1] 1320\nprod( 12:10 )\n#> [1] 1320"},{"path":"ChapterProbability.html","id":"Combinations","chapter":"2 Probability","heading":"2.6.3 Combinations","text":"selection order important (.e., dealing hand cards), combination appropriate counting number possibilities.Definition 2.12  (Combinations) combination unordered selection elements (without replacements).Consider finite, discrete sample space \\(S\\)  \\(n\\) distinct elements.\nnumber permutations size \\(r\\) can drawn sample space \\(S\\) \\(^n P_r\\).\neffectively outcomes; instance, two hands cards, shown order dealt, since selection order important:\n\\[\n  (3\\spadesuit, 5\\heartsuit)\\qquad\\text{}\\qquad (5\\heartsuit, 3\\spadesuit).\n\\]\nhands counted separately permutations, considered outcome combinations.\n(means number combinations \\(r\\) elements \\(n\\) elements never smaller number permutations \\(r\\) elements \\(n\\) elements.)selected \\(r\\) elements,  \\(r\\) ways rearrange elements (multiplication rule), order important.\ncan state number combinations \\(r\\) elements drawn form \\(n\\) elements, order important, \n\\[\n   C^n_r = \\frac{P_r}{r!} = \\frac{\\text{number permutations}}{\\text{number equivalent re-arrangements permutation}}.\n\\]\ndirectly, number combinations \\(r\\) elements \\(n\\) elments, selection order important, \n\\[\\begin{equation}\n   C^n_r = \\binom{n}{r} = \\frac{n(n - 1)\\ldots (n - r + 1)}{r!} = \\frac{n!}{(n - r)!\\,r!} = \\frac{P^n_r}{r!}\n       \\tag{2.2}\n\\end{equation}\\]\nelements ‘selected without replacement’.\ntwo common notations shown: \\(^nC_r\\) \\(\\binom{n}{r}\\).Notation combinations varies.\nnotation combinations include \\(nCr\\), \\({}^nC_r\\), \\(C_n^r\\), \\(\\binom{n}{r}\\) \\(C(n,r)\\).Example 2.21  (Combinations cards) Suppose hand five cards drawn well-shuffled pack \\(52\\) cards.\nSince order cards dealt important, combinations appropriate.\nnumber possible hands \n\\[\n  P^{52}_5 = \\frac{52!}{(52 - 5)!} =\n  \\frac{52!}{47!} = \\frac{52\\times 51\\times\\cdots \\times 48\\times 47!}{47!} =\n  52\\times 51\\times\\cdots \\times 48 = 311\\,875\\,200\n\\]\n\\(311\\) million hands possible.Example 2.22  (Oz Lotto combinations) Oz Lotto, players select seven numbers 47, try match seven randomly selected numbers.\norder seven numbers selected important, combinations (permutations) appropriate.\n, winning numbers drawn order, \\(\\{1, 2, 3, 4, 5, 6, 7\\}\\) effectively drawn order \\(\\{7, 2, 1, 3, 4, 6, 5\\}\\).number options players choose \n\\[\n   \\binom{47}{7} = \\frac{47!}{40!\\times 7!} = 62\\,891\\,499;\n\\]\n, almost \\(63\\) million combinations possible.probability picking one correct set seven numbers single guess therefore\n\\[\n   \\frac{1}{62\\,891\\,499} = 1.59\\times 10^{-8} = 0.000\\,000\\,015\\,9.\n\\]R:number combinations n elements k time found using choose(n, k).list combinations n elements, m time given combn(x, m).\\(n!\\) given factorial(n).binomial expansion\n\\[\n  (+ b)^n = \\sum^{n}_{r = 0} \\binom{n}{r} ^r b^{n - r}\n\\]\n\\(n\\) positive integer, often referred Binomial Theorem hence \\(\\binom{n}{r}\\) referred binomial coefficient.\nseries, associated properties, sometimes useful counting.\nproperties combinations stated .\\(\\binom{n}{r} = \\binom{n}{n - r}\\), \\(r = 0, 1, \\ldots, n\\).special case , \\(\\binom{n}{0} = 1 = \\binom{n}{n}\\).\\(\\sum_{r = 0}^n \\binom{n}{r} = 2^n\\).Example 2.23  () given values \\(n\\)  \\(r\\), number permutations never smaller number combinations (clear Eq. (2.2)).\nmany permutation corresponds single combination, since order important permutations.example, suppose two cards dealt, order:\n\\[\n  3\\spadesuit, 5\\heartsuit.\n\\]\nhand exactly dealt opposite order:\n\\[\n  5\\heartsuit, 3\\spadesuit.\n\\]\nhands count one combination, since order important.However, order important, two hands different, counted: two separate outcomes.larger example:Example 2.24  (Selecting digits) Consider set integers \\({1, 3, 5, 7}\\) (integers common factors).\nChoose two numbers, without replacement; call first \\(\\) second \\(b\\).\ncompute \\(\\times b\\), \\(C^4_2 = 6\\) answers possible since selection order important (e.g., \\(3 \\times 7\\) gives answer \\(7 \\times 3\\)).However, compute \\(\\div b\\), \\(P^4_2 = 12\\) answers possible, since selection order important (e.g., \\(3 \\div 7\\) gives different answer \\(7 \\div 3\\)).common properties combinations given .\\(\\displaystyle\\binom{n}{n} = \\binom{n}{0} = 1\\).\\(\\displaystyle\\binom{n}{1} = n\\).\\(\\displaystyle\\binom{n}{r} = \\binom{n}{n - r}\\).\\(\\displaystyle\\binom{n}{r} + \\binom{n}{r - 1} = \\binom{n + 1}{r}\\).","code":"\n# The number of *combinations* of r = 3 items from n = 20:\nchoose(10, 3)\n#> [1] 120\n\n# The number of *permutations* of r = 3 items from n = 20:\nchoose(10, 3) * factorial(3)\n#> [1] 720"},{"path":"ChapterProbability.html","id":"AssignProbContinuous","chapter":"2 Probability","heading":"2.7 Assigning probabilities: continuous sample spaces","text":"","code":""},{"path":"ChapterProbability.html","id":"AllocatingProbabilitiesContinuous","chapter":"2 Probability","heading":"2.7.1 Allocating probabilities","text":"Events defined continuous sample space elements can counted (Sect. 1.5.3), different means needed compute probabilities situations.\nfact, continuous sample space, probability observing specific, individual outcome probability \\(0\\).\n, continuous sample space \\(S = \\mathbb{R}\\), events\n\\[\\begin{align*}\n   &= \\{x \\S \\mid 10 < x < 20\\},\\\\\n   B &= \\{x \\S \\mid 10 \\le x < 20\\},\\\\\n   C &= \\{x \\S \\mid 10 < x \\le 20\\}\\quad\\text{}\\\\\n   D &= \\{x \\S \\mid 10 \\le x \\le 20\\}\n\\end{align*}\\]\nprobability.\ncontinuous sample space, probability observing single value (observing value exactly \\(10\\)) zero.\n, whether endpoints interval included excluded, probability .means probability () based counting elements continuous sample spaces.\nInstead, probability density function (PDF) used describe probability assigned across sample space.\nPDF defined sample space, quantifies concentration probability different regions sample space.example, consider heights adult females (Example 1.14).\nEvery female height, sample space \\(S\\) heights females can defined , instance,\n\\[\n   S = \\{ x \\\\mathbb{R} \\mid 50 < x < 300\\},\n\\]\n\\(x\\) height centimetres.\nassumed height less \\(50\\,\\text{cm}\\) (shortest-ever recorded height adult female greater ) greater \\(300\\,\\text{cm}\\) (tallest-ever recorded height adult female less ).\nSince sample space \\(S\\), probability event \\(S\\) one (third axiom probability; Sect. 2.4.2): certain height given woman  \\(S\\).\n, total probability  \\(S\\) one; every adult female represented somewhere within \\(S\\).Now consider total probability one distributed various ranges heights.\nprobability finding adult female height less \\(75\\,\\text{cm}\\) ( \\(29\\,\\text{inches}\\)) basically impossible.\nAlmost probability concentrated  \\(75\\,\\text{cm}\\).probability finding adult female height less \\(150\\,\\text{cm}\\) (\\(4\\,\\text{ft}\\) \\(11\\,\\text{inches}\\)) unlikely, possible.\nThus, little probability concentrated  \\(150\\,\\text{cm}\\).Likewise, probability finding adult female height greater \\(300\\,\\text{cm}\\) (\\(9\\,\\text{ft}\\) \\(10\\,\\text{inches}\\)) practically impossible.\nAlmost probability concentrated  \\(300\\,\\text{cm}\\).probability finding adult female height greater \\(200\\,\\text{cm}\\) (\\(6\\,\\text{ft}\\) \\(7\\,\\text{inches}\\)) unlikely impossible; little probability concentrated  \\(200\\,\\text{cm}\\).contrast, probability finding adult female height  \\(150\\,\\text{cm}\\)  \\(200\\,\\text{cm}\\) high; almost probability concentrated  \\(150\\,\\text{cm}\\)  \\(200\\,\\text{cm}\\).\ncan draw picture represents concentration probability (Fig. 2.3, top panel).\nfigure shows probability concentrated, allocated, distributed, sample space.total probability  \\(S\\) one; computed area five rectangles Fig. 2.3 (top panel) get one.\nHowever, values vertical axis really helpful (probabilities).\ncontext continuous sample space, means total area graph (.e., shaded areas Fig. 2.3) must equal one.Rather dividing sample space just five large intervals height, finer division heights used; instance, Fig. 2.3 (bottom panel) uses ideas \\(10\\,\\text{cm}\\) intervals heights.\nrepresentation shows probability concentrated \\(155\\,\\text{cm}\\) \\(165\\,\\text{cm}\\), suggesting finding adult female height range relatively high probability.\n, total probability  \\(S\\) one, total area graph (.e., shaded area) must equal one.\n, values vertical axis really helpful, probabilities.\nFIGURE 2.3: Allocating concentration probability regions sample space. shaded regions area  \\(1\\).\ncontinuous sample space, intervals sample space represent events, areas curve represent probabilities.","code":""},{"path":"ChapterProbability.html","id":"PDFs","chapter":"2 Probability","heading":"2.7.2 Probability density functions","text":"intervals become smaller, graph showing allocations probability become smoother (Fig. 2.4).\nsmooth curve, say \\(f(x)\\), called probability density function (PDF): shows density (concentration) probability various ranges sample space.\n, integral sample space \\(S\\) must equal one:\n\\[\n   \\int_S f(x)\\,dx = 1.\n\\]\nvertical axis, noted , informative, usually given.\nFIGURE 2.4: Smoothly allocating concentration probability regions sample space. shaded region area  \\(1\\).\ncontinuous sample spaces, probability observing event \\(E\\) assigned interval sample space \\(S\\).\nprobability event \\(E\\) \n\\[\n   \\Pr(E) = \\int_E f(x)\\,dx.\n\\]three axioms probability (Sect. 2.4.2) still apply continuous sample space, though equivalent statement involves integration density function \\(f(x)\\) sample space \\(S\\) rather summations (compare Sect. 2.4.2):Non-negativity:\nIntegration region sample space must never produce negative value, \\(f(x) \\ge 0\\) values \\(x\\).Exhaustive:\nwhole sample space, probability function must integrate one:\n\\[\n  \\int_S f(x)\\,dx = 1.\n\\]Additivity:\nprobability union non-overlapping regions sum individual regions:\n\\[\n  \\int_{A_1} f(x)\\, dx + \\int_{A_2} f(x)\\, dx  = \\int_{A_1 \\cup A_2} f(x)\\, dx.\n\\]Using axioms implies probability function event \\(\\) defined continuous sample space \\(S\\) \n\\[\n   \\Pr(X\\) = \\int_{(x)} f_X(x)\\, dx.\n\\]probability function \\(f_X(x)\\) give probability observing value \\(X = x\\).\nsample space infinite number elements, probability observing single point zero.\nInstead, probabilities computed intervals.implies \\(f_X(x) > 1\\) may true values \\(x\\), provided total area sample space one.","code":""},{"path":"ChapterProbability.html","id":"SubjectiveApproach","chapter":"2 Probability","heading":"2.8 Assigning probability: subjective approach","text":"‘Subjective’ probabilities estimated identifying information may influence probability, evaluating combining information.\nuse method someone asks team’s chance winning weekend.final (subjective) probability may, example, computed using mathematical models use relevant information.\ndifferent people systems identify different information relevant, combine differently, different subjective probabilities eventuate.examples include:chance investment return positive yield next year?likely Auckland average rainfall next year?Subjective probabilities can used discrete continuous sample space; always, probabilities can allocated regions continuous sample spaces.Example 2.25  (Subjective probability) likelihood rain Charleville (town western Queensland) April?\nMany farmers give subjective estimate probability based experience conditions farm.Using classical approach determine probability possible.\ntwo outcomes possible—rain, rain—almost certainly equally likely.relative frequency approach adopted.\nData Bureau Meterology, 1942 2022 (81 years), shows rain fell 71 years April.\napproximation probability therefore \\(71 / 81 = 0.877\\), \\(87.7\\)%.\napproach take account current climatic weather conditions, can change every year.","code":""},{"path":"ChapterProbability.html","id":"DiagramsForProbability","chapter":"2 Probability","heading":"2.9 Using diagrams to visualise outcomes","text":"","code":""},{"path":"ChapterProbability.html","id":"VennDiagrams","chapter":"2 Probability","heading":"2.9.1 Venn diagrams","text":"Venn diagrams can useful visualising probabilities, usig regions (often circles) represent events.\nVenn diagrams useful two events, sometimes three, become unworkable three.\nOften, tables can used better represent situations shown Venn diagrams (Sect. 2.9.2).Example 2.26  (Venn diagrams) Suppose Event probability \\(\\Pr() = 0.4\\) Event B \\(\\Pr(B) = 0.3\\).\naddition, \\(\\Pr(\\cap B) = 0.1\\).\nVenn diagram (Fig. 2.5) shows two events sample space.\nintersection (probability \\(0.1\\)) includes elements Event \\(\\) Event \\(B\\).can see, example, \\(\\Pr(\\setminus B) = 0.3\\), \\(\\Pr(\\cup B) = 0.6\\).\nFIGURE 2.5: Venn diagram simple situation two events. rectangle represents sample space \\(S\\); purple circle represents Event \\(\\) green circle represents Event \\(B\\). Left: two events. Right: probabilities section sample soace.\n","code":""},{"path":"ChapterProbability.html","id":"Tables","chapter":"2 Probability","heading":"2.9.2 Tables of probability","text":"two variables interest, probability tables may convenient way summarizing information.\nprobability represents whole sample spacem shows sample space divided two events.Example 2.27  (Probability tables) information Example 2.26 can compiled two-way table (Table 2.5): Events \\(\\) ‘ \\(\\)’ shown columns, Events \\(B\\) ’ \\(B\\) shown rows.\nTABLE 2.5: probabilities two-way table.\n","code":""},{"path":"ChapterProbability.html","id":"TreeDiagrams","chapter":"2 Probability","heading":"2.9.3 Tree diagrams","text":"Tree diagrams useful random process can seen, thought , occurring steps stages.\nprobabilities second step may depend happens forst step (called conditional probabilities, studied Sect. 2.10)\nideas extend multiple steps.Example 2.28  (Tree diagrams) Suppose probability customer makes purchase using online store \\(0.35\\); , probability customer requests refund \\(0.30\\).\nHowever, customer makes purchase using physical store, probability customer requests refund \\(0.05\\).\n, customers make purchases physical store, less likely request refund compared online customers.two events interest :\n\\[\\begin{align*}\n  O&: \\text{customer makes purchase online; }\\\\\n  R&: \\text{customer requests refund.}\n\\end{align*}\\]\nUsing notation, \\(\\Pr(O) = 0.35\\), \\(\\Pr(O^c) = 0.65\\) probability customer makes purchase physical store.\nvalue \\(\\Pr(R)\\) (hence \\(\\Pr(R)\\)) depends whether purchase made online -store.situation can considered two stages.\nStage 1 purchase made (online, physical store).\nStage 2 whether customer requests refund.\ntree diagram situation shown Fig. 2.6.\nprobabilities Stage 2 different, depending purchase made.understand use tree diagrams requires stuy conditional probability, next.\nFIGURE 2.6: Tree diagram customer-satisfaction example.\n","code":""},{"path":"ChapterProbability.html","id":"CondProbIndependence","chapter":"2 Probability","heading":"2.10 Conditional probability and independent events","text":"","code":""},{"path":"ChapterProbability.html","id":"CondProb","chapter":"2 Probability","heading":"2.10.1 Conditional probability","text":"tree diagram Example 2.28 example conditional probability: probability requesting refund conditional (depends ) whether customer made online -store purchase.Example 2.28, see \\(\\Pr(R) = 0.30\\) event \\(O\\) occurred; write \\(\\Pr(R \\mid O ) = 0.30\\), read ‘probability Event \\(R\\) occurs given Event \\(O\\) occurred’.\n\\(\\Pr(R \\mid O )\\) conditional probability.see also \\(\\Pr(R) = 0.05\\) event \\(O\\) occurred; write \\(\\Pr(R \\mid O^c ) = 0.05\\), read ‘probability Event \\(R\\) occurs given Event \\(O\\) occurred’.\n\\(\\Pr(R \\mid O^c )\\) also conditional probability.generally, assume sample space \\(S\\) random process constructed, event \\(\\) identified, probability, \\(\\Pr()\\), determined.\nreceive additional information event \\(B\\) occurred.\nPossibly, new information can change value  \\(\\Pr()\\).now need determine probability  \\(\\) occur, given know information provided event \\(B\\).\ncall probability conditional probability  \\(\\) given \\(B\\), denoted \\(\\Pr(\\mid B)\\).Example 2.29  (Conditional probability) Suppose roll die.\nDefine event \\(\\) ‘rolling 6’.\n, compute \\(\\Pr() = 1/6\\) (using classical approach; Sect. 2.5.1).However, suppose provide extra information: Event \\(B\\) already occurred, Event \\(B\\) event ‘number rolled even’.extra information, three numbers possibly rolled; reduced sample space \n\\[\n   S^* = \\{2, 4, 6 \\}.\n\\]\nthree outcomes equally likely.\nHowever, probability number six now \\(\\Pr(\\mid B) = 1/3\\).Knowing extra information Event \\(B\\) changed calculation  \\(\\Pr()\\).Example 2.30  (Planes) Consider two events:\n\\[\\begin{align*}\n   D:&\\quad \\text{person dies};\\\\\n   F:&\\quad \\text{person falls airborne plane parachute}.\n\\end{align*}\\]\nConsider probability \\(\\Pr(D \\mid F)\\).\ntold someone falls airborne plane parachute, probability die high., consider probability \\(\\Pr(F\\mid D)\\).\ntold died, cause unlikely fall airborne plane.Thus, first probability close one, second close zero.Two methods exist computing conditional probability: first principles, formal definition \\(\\Pr(\\mid B)\\).\nUsing first principles, consider original sample space \\(S\\): remove sample points inconsistent new information  \\(B\\) provided; form new sample space, say \\(S^*\\); recompute probability Event \\(\\) relative  \\(S^*\\).\n\\(S^*\\) called reduced sample space.method appropriate number outcomes relatively small.\nfollowing formal definition applies generally.Definition 2.13  (Conditional probability) Let \\(\\)  \\(B\\) events  \\(S\\) \\(\\Pr(B) > 0\\).\n\n\\[\n   \\Pr(\\mid B) = \\frac{\\Pr(\\cap B)}{\\Pr(B)}.\n\\]definition automatically takes care sample space reduction noted earlier.Example 2.31  (Rainfall) Consider rainfall Charleville April (Example 2.25).\nDefine \\(L\\) event ‘receiving \\(30\\) mm April’,  \\(R\\) event ‘receiving rainfall April’.\nEvent \\(L\\) occurs 24 times 81 years data, Event \\(R\\) occurs 71 times.Using relative frequency approach Bureau Meteorology data, probability obtaining \\(30\\) mm April :\n\\[\n   \\Pr(L) = \\frac{24}{81} = 0.296.\n\\]\nHowever, conditional probability receiving \\(30\\) mm, given rainfall recorded, :\n\\[\n   \\Pr(L \\mid  R) = \\frac{\\Pr(L \\cap R)}{\\Pr(R)} = \\frac{\\Pr(L)}{\\Pr(R)} = \\frac{0.2963}{0.8765} = 0.338.\n\\]\nknow rain fallen, probability amount greater \\(30\\) mm 0.338.\nWithout prior knowledge, probability 0.296.\n, probability rolling  \\(6\\), given rolled number even number, \\(1/3\\).Example 2.32  (Conditional probability) Soud et al. (2009) discusses response students mumps outbreaks Kansas 2006.\nStudents asked isolate; Table 2.6 shows behaviour male female student studied sample.females, probability complying isolation request :\n\\[\n   \\Pr(\\text{Compiled} \\mid  \\text{Females}) = 63/84 = 0.75.\n\\]\nmales, probability complying isolation request \n\\[\n   \\Pr(\\text{Compiled} \\mid  \\text{Males}) = 36/48 = 0.75.\n\\]Whether look females males, probability selecting student sample complied isolation request : \\(0.75\\).\nAlso, non-conditional probability student isolated (ignoring sex) :\n\\[\n   \\Pr(\\text{Student isolated}) = \\frac{99}{132} = 0.75.\n\\]\nTABLE 2.6: Students response isolation request Kansas university.\n","code":""},{"path":"ChapterProbability.html","id":"GeneralMultiplicationRule","chapter":"2 Probability","heading":"2.10.2 General multiplication rule","text":"consequence Def. 2.13 following theorem.Theorem 2.2  (Multiplication rule probabilities) events \\(\\) \\(B\\), probability \\(\\) \\(B\\) \n\\[\\begin{align*}\n     \\Pr(\\cap B)\n     &= \\Pr() \\Pr(B \\mid )\\\\\n     &= \\Pr(B) \\Pr(\\mid B).\n\\end{align*}\\]rule can generalised number events.\nexample, three events \\(\\), \\(B\\)  \\(C\\),\n\\[\n  \\Pr(\\cap B\\cap C) = \\Pr()\\Pr(B\\mid )\\Pr(C\\mid \\cap B).\n\\]Example 2.33  (General multiplication rule) Consider probabilities Example 2.28.\n\\(\\Pr(O) = 0.35\\); :\n\\[\\begin{align*}\n  \\Pr(R \\mid O)   &= 0.30\\quad\\text{}\\quad \\Pr(R^c \\mid O) = 0.70;\\\\\n  \\Pr(R \\mid O^c) &= 0.05\\quad\\text{}\\quad \\Pr(R^c \\mid O^c) = 0.95.\n\\end{align*}\\]\nUsing general multiplication rule,\n\\[\\begin{align*}\n  \\Pr(R \\cap O)       &=   \\Pr(R \\mid O)     \\times \\Pr(O)   = 0.30\\times 0.35 = 0.105;\\\\\n  \\Pr(R^c \\cap O)     &=   \\Pr(R^c \\mid O)   \\times \\Pr(O)   = 0.70\\times 0.35 = 0.245;\\\\\n  \\Pr(R \\cap O^c)     &=   \\Pr(R \\mid O^c)   \\times \\Pr(O^c) = 0.05\\times 0.65 = 0.0325;\\\\\n  \\Pr(R^c \\cap O^c)   &=   \\Pr(R^c \\mid O^c) \\times \\Pr(O^c) = 0.95\\times 0.65 = 0.6175.\n\\end{align*}\\]\nfour probabilities represent ‘final destinations’ tree diagram, can now add (Fig. 2.7).\nNotice probabilities right add one, represent entire sample space: every customer represented one four branches.can also determine probability customer requests refund:\n\\[\n  \\Pr(R) = \\Pr(R \\cap O) + \\Pr(R \\cap O^c) = 0.105 + 0.0325 = 0.1375.\n\\]\ncan determine probability customer online customer, given refund requested\n\\[\n  \\Pr(O\\mid R) = \\frac{\\Pr(O\\cap R)}{\\Pr(R)} = \\frac{0.105}{0.1375} = 0.7636\\dots.\n\\]\nrefund requested, probability customer online shopper  \\(0.76\\).\nFIGURE 2.7: Tree diagram customer-satisfaction example, adding probabilities four outcomes.\ntree diagram Example 2.28 example conditional probability: probability requesting refund conditional (depends ) whether customer made online -store purchase.","code":""},{"path":"ChapterProbability.html","id":"Independence","chapter":"2 Probability","heading":"2.10.3 Independent events","text":"important idea independent events can now defined.Definition 2.14  (Independence) Two events \\(\\)  \\(B\\) independent events \n\\[\n  \\Pr(\\cap B) = \\Pr()\\Pr(B).\n\\]\nOtherwise events independent (dependent).Proof. Exercise.Provided \\(\\Pr(B) > 0\\), Defs. 2.13  2.14 show  \\(\\)  \\(B\\) independent , , \\(\\Pr(\\mid B) = \\Pr()\\).\nstatement independence makes sense: \\(\\Pr(\\mid B)\\) probability  \\(\\) occurring  \\(B\\) already occurred, \\(\\Pr()\\) probability  \\(\\) occurs without knowledge whether \\(B\\) occurred .\nequal, \\(B\\) occurring made difference probability  \\(\\) occurs, independence means.Example 2.34  Example 2.32, probability males isolating probability females isolating.\nsex student independent whether isolate.\n, whether look females males, probability isolated .idea independence can generalised two events.\nthree events, following definition mutual independence applies, naturally extends number events.Definition 2.15  (Mutual independence) Three events \\(\\), \\(B\\)  \\(C\\) mutually independent , ,\n\\[\\begin{align*}\n     \\Pr(\\cap B) & = \\Pr()\\Pr(B).\\\\\n     \\Pr(\\cap C) & = \\Pr()\\Pr(C).\\\\\n     \\Pr(B\\cap C) & = \\Pr(B)\\Pr(C).\\\\\n     \\Pr(\\cap B\\cap C) & = \\Pr() \\Pr(B) \\Pr(C).\n     \\end{align*}\\]Three events can pairwise independent sense Def. 2.14, mutually independent.following theorem concerning independent events sometimes useful.Theorem 2.3  (Independent events)  \\(\\)  \\(B\\) independent events, \\(\\)  \\(B^c\\) independent.\\(^c\\)  \\(B\\) independent.\\(^c\\)  \\(B^c\\) independent.Proof. Exercise.","code":""},{"path":"ChapterProbability.html","id":"IndependentEvents","chapter":"2 Probability","heading":"2.10.4 Independent and mutually exclusive events","text":"Mutually exclusive evets (Def. 2.7) independent events (Def. 2.14) sometimes get confused.simple events defined outcomes sample space mutually exclusive, since one can occur realisation random process.\nMutually exclusive events common outcomes: example, passing failing course possible one semester.\nObtaining one excludes possibility … whether one occurs depends whether occurred.contrast, two events independent, whether one occurs affect chance happening.\nevent \\(\\) can occur, \\(B\\) happening influence chance \\(\\) happening independent, exclude possibility occurring.Confusion mutual exclusiveness independence arises sometimes sample space clearly identified.Consider random process involving tossing two coins time.\nsample space \n\\[\n   S_2 = \\{(HH), (HT), (TH), (TT)\\}\n\\]\noutcomes mutually exclusive, probability \\(1/4\\) (using classical approach).\nexample, \\(\\Pr\\big( (HH) \\big) = 1/4\\).alternative view random process think repeating process tossing coin .\none toss coin, sample space\n\\[\n   S_1 = \\{ H, T \\}\n\\]\n\\(\\Pr(H) = 1/2\\) probability getting head first toss.\nalso probability getting head second toss.events ‘getting head first toss’ ‘getting head second toss’ mutually exclusive, events can occur together: event \\((HH)\\) outcome  \\(S_2\\).\nWhether outcomes \\((HH)\\) occurred simultaneously, two coins tossed one time, sequentially, one coin tossed twice, irrelevant.interest joint outcomes two tosses.\nevent ‘getting head “first” toss’ :\n\\[\n   E_1 = \\{ (HH), (HT) \\}\n\\]\n‘getting head “second” toss’ \n\\[\n   E_2 = \\{ (HH), (TH) \\},\n\\]\n \\(E_1\\)  \\(E_2\\) events defined  \\(S_2\\).\nmakes clear Events \\(E_1\\)  \\(E_2\\) mutually exclusive \\(E_1\\cap E_2 \\ne \\varnothing\\).two events \\(E_1\\)  \\(E_2\\) independent , whether head occurs one tosses, probability head occurring still \\(1/2\\).\nSeeing events independent provides another way calculating probability two heads occurring ‘together’: \\(1/2\\times 1/2 = 1/4\\), since probabilities independent events can multipliedExample 2.35  (Mendell's peas) Mendel (1886) conducted famous experiments genetics.\none study, Mendel crossed pure line round yellow peas pure line wrinkled green peas.\nTable 2.7 shows happened second generation.\nexample, \\(\\Pr(\\text{round peas}) = 0.7608\\).\nBiologically, \\(75\\)% peas expected round; data appear reasonably sound respect.type pea (rounded wrinkled) independent colour?\n, pea rounded, impact colour pea?Independence can evaluated using formula \\(\\Pr(\\text{round} \\mid \\text{yellow}) = \\Pr(\\text{round})\\).\nwords, fact pea yellow affect probability pea rounded.\nTable 2.7:\n\\[\\begin{align*}\n   \\Pr(\\text{rounded})\n   &= 0.5665 + 0.1942 = 0.7608,\\\\\n   \\Pr(\\text{round} \\mid \\text{yellow})\n   &= 0.5665/(0.5665 + 0.1817) = 0.757.\n\\end{align*}\\]two probabilities close.\ndata table just sample (population peas), assuming colour shape peas independent reasonable.\nTABLE 2.7: second generation results Mendel’s experiment, crossing pure line round yellow peas pure line wrinkled green peas.\n","code":""},{"path":"ChapterProbability.html","id":"SSPartitions","chapter":"2 Probability","heading":"2.10.5 Partitioning the sample space","text":"concepts introduced section allow us determine probability event using event-partioning approach, now discuss.Definition 2.16  (Partitioning) events \\(B_1, B_2, \\ldots , B_k\\) said represent partition sample space \\(S\\) ifthe events mutually exclusive: \\(B_i \\cap B_j = \\varnothing\\) \\(\\neq j\\).events exhaustive: \\(B_1 \\cup B_2 \\cup \\ldots \\cup B_k = S\\).events non-zero probability occurring: \\(\\Pr(B_i) > 0\\) \\(\\).implication random process performed, exactly one one events \\(B_i\\) (\\(= 1, \\ldots, k)\\) occurs.\nuse concept following theorem.Theorem 2.4  (Law total probability) Let \\(\\) event  \\(S\\) \\(\\{B_1, B_2, \\ldots , B_k\\}\\) partition  \\(S\\).\n\n\\[\\begin{align*}\n   \\Pr()\n   &= \\Pr(\\mid B_1) \\Pr(B_1) + \\Pr(\\mid B_2)\\Pr(B_2) + \\ldots \\\\\n   & \\qquad {} + \\Pr(\\mid B_k)\\Pr(B_k).\n\\end{align*}\\]Proof. proof follows writing \\(= (\\cap B_1) \\cup (\\cap B_2) \\cup \\ldots \\cup (\\cap B_k)\\), events RHS mutually exclusive.\nthird axiom probability together multiplication rule yield result.Example 2.36  (Law total probability) Consider event \\(\\): ‘rolling even number die’, also define events\n\\[\n   B_i:\\quad\\text{number $$ rolled die}\n\\]\n\\(= 1, 2, \\dots 6\\).\nEvents \\(B_i\\) represent partition sample space, Events \\(B_i\\) mutually exclusive, exhaustive non-zero probability occurring., using Law total probability (Sect. 2.4):\n\\[\\begin{align*}\n   \\Pr()\n   &= \\Pr(\\mid B_1)\\times \\Pr(B_1)\\quad +  \\quad\\Pr(\\mid B_2)\\times \\Pr(B_2) \\quad+ {}\\\\\n   &\\quad \\Pr(\\mid B_3)\\times \\Pr(B_3)\\quad + \\quad\\Pr(\\mid B_4)\\times \\Pr(B_4) \\quad+ {} \\\\\n   &\\quad \\Pr(\\mid B_5)\\times \\Pr(B_5)\\quad +  \\quad\\Pr(\\mid B_6)\\times \\Pr(B_6)\\\\\n   &= \\left(0\\times \\frac{1}{6}\\right) + \\left(1\\times \\frac{1}{6}\\right) + {} \\\\\n   &\\quad \\left(0\\times \\frac{1}{6}\\right) + \\left(1\\times \\frac{1}{6}\\right) + {}\\\\\n   &\\quad \\left(0\\times \\frac{1}{6}\\right) + \\left(1\\times \\frac{1}{6}\\right)\n    = \\frac{1}{2}.\n\\end{align*}\\]\nanswer obtained using classical approach.","code":""},{"path":"ChapterProbability.html","id":"BayesTheorem","chapter":"2 Probability","heading":"2.10.6 Bayes’ theorem","text":"event known occurred (.e., non-zero probability), sample space partitioned, result known Bayes’ theorem enables us determine probabilities associated partitioned events.Theorem 2.5  (Bayes' theorem) Let \\(\\) event  \\(S\\) \\(\\Pr() > 0\\), \\(\\{ B_1, B_2, \\ldots , B_k\\}\\) partition  \\(S\\).\n\n\\[\n   \\Pr(B_i \\mid ) = \\frac{\\Pr(B_i) \\Pr(\\mid B_i)}\n                          {\\displaystyle \\sum_{j = 1}^k \\Pr(B_j)\\Pr(\\mid B_j)}\n\\]\n\\(= 1, 2, \\dots, k\\).Proof. direct application Def. 2.13, multiplication rule Theorem 2.4.Notice right-side includes conditional probabilities form \\(\\Pr(\\mid B_i)\\), left-side contains probability \\(\\Pr(B_i\\mid )\\).\neffect, theorem takes conditional probability can ‘reverse’ conditioning.Bayes’ theorem many uses, uses conditional probabilities easy find estimate compute conditional probability easy find estimate.\ntheorem basis branch statistics known Bayesian statistics involves using pre-existing evidence drawing conclusions data.Example 2.37  (Breast cancer) success mammograms detecting breast cancer well documented (White, Urban, Taylor 1993).\nMammograms generally conducted women \\(40\\), though breast cancer rarely occurs women \\(40\\) also.can define two events interest:\n\\[\\begin{align*}\n   C:&\\quad \\text{woman breast cancer; }\\\\\n   D:&\\quad \\text{mammogram returns positive test result.}\n\\end{align*}\\]\ndiagnostic tool, mammogram perfect.\nSensitivity specificity used describe accuracy test:Sensitivity probability true positive test result: probability positive test result people disease.\n\\(\\Pr(D \\mid C)\\).Specificity probability true negative test result: probability negative test people without disease.\n\\(\\Pr(D^c \\mid C^c)\\).Clearly, like probabilities high possible.\nmammograms (Houssami et al. 2003), sensitivity estimated  \\(0.75\\) specificity  \\(0.90\\).\ncan write:\\(\\Pr(D \\mid C ) = 0.75\\) (\\(\\Pr(D^c \\mid C) = 0.25\\));\\(\\Pr(D^c \\mid C^c) = 0.90\\) (\\(\\Pr(D \\mid C^c) = 0.10\\)).Furthermore,  \\(2\\)% women  \\(40\\) get breast cancer (Houssami et al. 2003); , \\(\\Pr(C) = 0.02\\) (hence \\(\\Pr(C^c) = 0.98\\)).study, probabilities \\(\\Pr(D\\mid C)\\) easy find: women known breast cancer mammogram, record whether mammogram result positive negative.consider woman  \\(40\\) gets mammogram.\nresults returned, interest whether breast cancer, given test results; example \\(\\Pr(C \\mid D)\\).\nwords: test returns positive result, probability actually breast cancer?, like take probabilities like \\(\\Pr(D\\mid C)\\), can found easily, determine \\(\\Pr(C \\mid  D)\\), interest practice.\nUsing Bayes’ Theorem:\n\\[\\begin{align*}\n   \\Pr(C \\mid D)\n   &= \\frac{\\Pr(C) \\times \\Pr(D \\mid  C)}\n            {\\Pr(C)\\times \\Pr(D \\mid  C) + \\Pr(C^c)\\times \\Pr(D \\mid  C^c) }\\\\\n   &= \\frac{0.02 \\times 0.75}\n            {(0.02\\times 0.75) + (0.98\\times 0.10)}\\\\\n   &= \\frac{0.015}{0.015 + 0.098} =  0.1327.\n\\end{align*}\\]\nConsider says: Given mammogram returns positive test (woman  \\(40\\)), probability woman really breast cancer  \\(6\\)%…\npartly explains mammograms women \\(40\\) commonplace: women return positive test result actually breast cancer.reason surprising result explained Example 2.38.Example 2.38  (Breast cancer) Consider using tree diagram describe breast cancer information Example 2.37 (Fig. 2.8).\nfollowing ‘branch’ tree, can compute, example:\n\\[\n   \\Pr(C \\cap D) = \\Pr(C)\\times \\Pr(D\\mid C) = 0.02 \\times 0.75 = 0.015;\n\\]\n, probability woman positive test breast cancer \\(0.015\\).\ncompare:\n\\[\n   \\Pr(C^c \\cap D) = \\Pr(C^c)\\times \\Pr(D\\mid C^c) = 0.98 \\times 0.10 = 0.098;\n\\]\n, probability woman positive test breast cancer (, false positive) \\(0.098\\).explains surprisingly result Example 2.37: breast cancer uncommon younger women, false positives (\\(0.098\\)) overwhelm true positives (\\(0.015\\)).positive mammogram, tests conducted conform cancer diagnosis.\nyounger women, almost every positive mammogram returns negative diagnosis tests.\nFIGURE 2.8: Tree diagram breast-cancer example.\nExample 2.39  (Breast cancer) Consider breast cancer data (Example 2.37, events \\(C\\) \\(D\\) defined earlier).\nVenn diagram constructed show sample space (Fig. 2.9).\nFIGURE 2.9: Venn diagram breast cancer example. rectangle panel represents sample space.\n","code":""},{"path":"ChapterProbability.html","id":"ProbStatisticalComputing","chapter":"2 Probability","heading":"2.11 Statistical computing and simulation","text":"","code":""},{"path":"ChapterProbability.html","id":"SimulationWhyUse","chapter":"2 Probability","heading":"2.11.1 Why use computing and simulation?","text":"Computing working probabilities can hard!\nmany cases, using computer, computer simulation, can helpful.Definition 2.17  (Simulation) simulation computer used imitate real situation many times, probabilities outcomes can estimated.Using computer can useful many reasons working probabilities statistics generally:Checking answers, intuition reasoning.\nSimulation can used check answers reasoning.\nAnalytic solutions can verified comparing simulation results.Demonstration theory.\nSimulation can used visually demonstrate theory (e.g., Sect. 2.11.3).\nSometimes theory can difficult understand, simulation can bridge gap formulas reality.Confirmation validation results.\nSimulation can confirm (prove) counter-intuitive results (Monty Hall problem (Exercise 2.39), birthday paradox (Exercise 2.23), non-transitive dice (Exercise 2.37).Answering tedious complex situations.\nSimulation can make tedious, difficult complex computations easier.\nSimulation gives quick approximations bypass heavy combinatorial formulas.\ncases, simulation can used problems closed-form solutions.Performing sensitivity analysis.\nSimulation allows easy tweaking changing parameters see impacts results.example given .\nfull benefit simulation may apparent learnt distribution theory progress book.Simulation, definition, imitates scenario many times.\nscenario uses computer replicate scenario, using random number (e.g., random die roll; random hand cards; etc.).\ngeneral, precise estimates found using larger number replications.many simulations necessary useful precision?\nsingle answer exists, since computers fast using large number replications usually imposition.\ninstance, \\(5\\,000\\) simulations reasonable number simple scenarios.\nimpact number simulations covered detail Chap. 12.Computers produces truly random numbers, rather pseudo-random numbers generated random number seed.\nrandom number seed produces sequence pseudo-random numbers.\nbook, examples reproducible, set random number seed using R (using set.seed()), examples reproducible.\n(Nonetheless, call ‘random numbers’ understanding really pseudo-random numbers…)\nR function replicate() allows R expression repeated (‘replicated’) given number times, often proves useful creating simulations.","code":""},{"path":"ChapterProbability.html","id":"SimulationChecking","chapter":"2 Probability","heading":"2.11.2 Simulation to check answers, intuition and reasoning","text":"Consider drawing five cards random fair pack; probability least cards \\(3\\) hearts?question difficult answer using counting methods Sect. 2.6; show answer approximately \\(0.0927\\).\nHowever, can easy miscount.\nuse R simulation check answer.First, set simulation:simulate numerous hands \\(5\\) cards using replicate().\nfirst input number replications (.e., num_Reps) second command replicated (case, taking sample() size five Deck cards, replace-ment):count number Hearts Hand (.e., column):determine many Hands least three hearts:fin dthe probability, print result:close computed using theory.\nprecise estimate found using larger number simulations.","code":"\n# 'Make' the deck of cards.\n# Note: the denomination is not important, just the suit\nDeck <- c(rep(\"Hearts\", 13),   # 13 cards of each suit\n          rep(\"Diamonds\", 13), \n          rep(\"Clubs\", 13), \n          rep(\"Spades\", 13))\n\nset.seed(12043)   # For reproducibility \nnum_Reps <- 5000  # Number of replications to use\n# Each replication goes into a column of  Hands\nHands <- replicate(num_Reps, sample(Deck,   # Sample from the  Deck  of cards\n                                    size = 5,  # A hand of *five* cards \n                                    replace = FALSE)  ) # Without replacement\nHands[, 1:4]  # Show the first four columns; each col is a hand of five cards\n#>      [,1]       [,2]       [,3]       [,4]    \n#> [1,] \"Clubs\"    \"Spades\"   \"Diamonds\" \"Clubs\" \n#> [2,] \"Diamonds\" \"Spades\"   \"Hearts\"   \"Hearts\"\n#> [3,] \"Spades\"   \"Diamonds\" \"Diamonds\" \"Spades\"\n#> [4,] \"Hearts\"   \"Diamonds\" \"Diamonds\" \"Hearts\"\n#> [5,] \"Diamonds\" \"Diamonds\" \"Clubs\"    \"Spades\"\n# For each Hand (i.e., column), count how many  Hearts\nnum_Hearts <- colSums(Hands == \"Hearts\")\n\n# Show the count of Hearts in those first four hands:\nnum_Hearts[1 : 4]\n#> [1] 1 0 1 2\n# Count how many have *at least 3* hearts\nat_Least_3_Hearts <- sum( num_Hearts >= 3 )  \n# Compute the *probability* of at least three Hearts\nprob_At_Least_3_Hearts <- at_Least_3_Hearts / num_Reps\n\n# Print the (rounded) results (where  \"\\n\"  means to start a new line)\ncat(\"The prob. estimate is\", \n    round( mean(prob_At_Least_3_Hearts), \n           4), \"\\n\") # Round to four decimal places\n#> The prob. estimate is 0.0978"},{"path":"ChapterProbability.html","id":"SimulationTheoryDemo","chapter":"2 Probability","heading":"2.11.3 Simulation to demonstrate theory","text":"Weak Law Large Numbers important statistical concept.Definition 2.18  (Weak Law Large Numbers) sample proportion random outcome converges (probability) true probability number trials increases.can shown running computer simulation, simulate large numbers coin tosses.\nFirst, set scenario:use R function sample() take ‘sample’ heads tails (replacement!) replication:Notice probability Head (Tail) set  \\(0.5\\) simulation, simulate tossing coin.\ntoss, probability obtaining head using available information toss computed:results can plotted (Fig. 2.10):\nFIGURE 2.10: simulation tossing fair coin \\(500\\) times. probability getting head computed data toss. grey horizontal line \\(0.5\\).\none simulation, running probabilities shown Fig. 2.10.\nresult single toss unpredictable, see Weak Law Large Numbers action: sample proportion heads approaches \\(0.5\\) number tosses increases.Using empirical approach shows probabilities  \\(0\\)  \\(1\\) (inclusive), since proportions \\(m/n\\) always  \\(0\\)  \\(1\\) (inclusive).\ncase, sample proportion heads converges (probability) true probability head \\(p = 0.5\\) number trials increases.","code":"\nset.seed(966141) # For repeatability\n\n# Set the number of replications to use\nNum_Tosses <- 1000\nTosses <- sample(x = c(\"H\", \"T\"),    # Choose \"H\"  or  \"T\"\n                 size = Num_Tosses,  # Do this for each replication\n                 replace = TRUE,     # H and T can be reselected \n                 prob = c(0.5, 0.5)) # Pr(Head) = Pr(Tail) = 0.5\nTosses[1 : 10]                       # Show the first 10 results\n#>  [1] \"H\" \"T\" \"T\" \"H\" \"H\" \"H\" \"T\" \"T\" \"H\" \"H\"\nToss_Number <- 1:Num_Tosses            # Sequence: from 1 to Num_Tosses\n\n# Compute P(Heads) after each toss (cumsum()  is the 'cumulative sum')\nProp_Heads <- cumsum(Tosses == \"H\") / Toss_Number\nProp_Heads[1 : 8]  # Show the first eight results\n#> [1] 1.0000000 0.5000000 0.3333333 0.5000000\n#> [5] 0.6000000 0.6666667 0.5714286 0.5000000\nplot(Prop_Heads,\n     main = \"The proportion of heads after a given number of tosses\",\n     xlab = \"Toss number\",          # Label on x-axis\n     ylab = \"Proportion of heads\",  # Label on y-axis\n     type = \"l\",       # Show a \"l\"ine rather than \"p\"oints\n     lwd = 2,          # Make line of width '2'\n     ylim = c(0, 1),   # y-axis limits\n     las = 1,          # Make axes labels horizontal\n     col = \"cyan4\")    # Line colour\nabline( h = 0.5,       # Draw horizontal line at y = 0.5\n        col = \"grey\")  # Make line grey in colour"},{"path":"ChapterProbability.html","id":"SimulationConfirmation","chapter":"2 Probability","heading":"2.11.4 Simulation to confirm and validate results","text":"Consider breast cancer example (Example 2.37), saw probability breast cancer positive test results quite low.\nresult surprising, use computer confirm correct.Step 1 set scenario, setting random number seed (reproducibility) defining parameters:Now apply useful ‘trick’, first allocating patient random number  \\(0\\)  \\(1\\):runif() command produces random numbers  \\(0\\)  \\(1\\) person population.\ntime runif() run, different random numbers allocated person population.Now, number less  \\(0.02\\) equivalent saying patient cancer (given probability  \\(2\\)%).\nsaying  \\(0.02\\) people cancer population.\nlike flipping biased coin patient, chance landing ‘cancer’  \\(2\\)%.Now can obtain list whether person cancer .\ntime runif() run, different people, different number people population, cancer can vary:, split population cancer without cancer:test sensitivity specificity now can applied (using trick ):, time runif() run, different people, different number people population, can return positive test results:Now print useful information:code three places random numbers used:randomly allocate people cancer ;randomly determine people cancer return positive result; andto randomly determine people without cancer return positive result.Thus, time code run produce different results (unless random-number seed remains ).simulation repeats scenario many times, many replications different random numbers used.\n, probability interest (, probability cancer, given positive test result) can averaged many replications.\n( \\(5\\,000\\) replications), code repeated result interest retained replication (cancer_Given_Pos):Example 2.37, probability given  \\(0.1327\\), compares favourably answer simulation (approximation).","code":"\npopulation_Size <- 10000\ncancer_Prob <- 0.02\n\n# Test sensitivity and specificity \ncorrect_Positive <- 0.75\ncorrect_Negative <- 0.90\n# Allocate a random number between 0 and 1 to each patient\npatient_Probs <- runif( population_Size)\npatient_Probs[1 : 8]\n#> [1] 0.8604976 0.8495931 0.8000998 0.6782192\n#> [5] 0.2884367 0.5052399 0.2357213 0.5244750\ncancer_Result <- patient_Probs < cancer_Prob\ncancer_Result[1 :10]\n#>  [1] FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n#>  [8] FALSE FALSE FALSE\n# Now find those *with* cancer who return a positive result\nwith_Cancer <- which( cancer_Result) # Indices of people *with* cancer\nno_Cancer   <- which(!cancer_Result) # Indices of people *without* cancer\n# Patients WITH cancer: 75% chance the test is positive\nwith_Cancer_and_Positive <- runif(length(with_Cancer)) < correct_Positive\n\n# Patients WITHOUT cancer: 10% chance the test is (wrongly) positive\nno_Cancer_and_Positive <- runif(length(no_Cancer)) < (1 - correct_Negative)\n\n# Combine results\npositive_Test <- c(\n  with_Cancer[with_Cancer_and_Positive],\n  no_Cancer[no_Cancer_and_Positive]\n)\n# True positives among those who tested positive\ntrue_positives  <- sum(cancer_Result[positive_Test])\n\n# Total positives\ntotal_positives <- length(positive_Test)\n\n# Estimated Prob(cancer | positive test)\ncancer_Given_Pos <- \n  if (total_positives > 0) {\n    true_positives / total_positives \n  } else {\n    NA\n  }\ncancer_Given_Pos\n#> [1] 0.1351119\ncat(\"Prob (cancer | positive test) :\", cancer_Given_Pos, \"\\n\")\n#> Prob (cancer | positive test) : 0.1351119\n# Set the parameters for the scenario\nnum_Reps <- 5000 # The number of replications\npopulation_Size <- 10000\ncancer_Prob <- 0.02\n\n# Set the test sensitivity and specificity \ncorrect_Positive <- 0.75\ncorrect_Negative <- 0.90\n\n# Create an array to hold the info we need from each replication\ncancer_Given_Pos <- array( dim = num_Reps)\n\n# Now replicate the scenario  num_Reps  times:\nfor (i in 1:num_Reps){\n  # Allocate a random number between 0 and 1 to each patient\n  patient_Probs <- runif( population_Size)\n  cancer_Result <- ( patient_Probs < cancer_Prob )\n\n  # Now find those *with* and *without* cancer who return a positive result\n  with_Cancer <- which( cancer_Result) # Indices of people *with* cancer\n  no_Cancer   <- which(!cancer_Result) # Indices of people *without* cancer\n  \n  # Patients WITH cancer: 75% chance the test is positive\n  with_Cancer_and_Positive <- runif(length(with_Cancer)) < correct_Positive\n  \n  # Patients WITHOUT cancer: 10% chance the test is (wrongly) positive\n  no_Cancer_and_Positive <- runif(length(no_Cancer)) < (1 - correct_Negative)\n  \n  # Combine results\n  positive_Test <- c(\n    with_Cancer[with_Cancer_and_Positive],\n    no_Cancer[no_Cancer_and_Positive]\n  )\n  \n  # True positives among those who tested positive\n  true_positives  <- sum(cancer_Result[positive_Test])\n  \n  # Total positives\n  total_positives <- length(positive_Test)\n  \n  # Estimated Prob(cancer | positive test)\n  cancer_Given_Pos[i] <- ifelse( total_positives > 0, \n                                 true_positives / total_positives, # If  TRUE\n                                 NA)                               # If  FALSE\n}\n\n# Average over the replications\nmean(cancer_Given_Pos)\n#> [1] 0.1328058"},{"path":"ChapterProbability.html","id":"SimulationComplex","chapter":"2 Probability","heading":"2.11.5 Simulation to answers tedious or complex situations","text":"Suppose \\(10\\) people invited party, sit round table; however, two people (say, Anh Barb) refuse sit next .\nprobability random seating arrangement satisfies constraint?difficult (impossible) compute exactly: number circular permutations need counted, excluding adjacent pairs.\nSimulation, however, used.\nstrategy :Set simulation.Repeat numerous times:\nCreate random permutation seating arrangements  \\(10\\) people.\nCheck Anh Barb ‘neighbours’ arrangement.\nCreate random permutation seating arrangements  \\(10\\) people.Check Anh Barb ‘neighbours’ arrangement.Print result.process repeated numerous randomly-generated seating arrangements, proportion acceptable seating arrangements computed.First, set scenario:, create R function determine Anh () Barb (B) next :replicate numerous times, R function replicate() used; repeatedly evaluates given R code specified number times.\ncase:command first creates random arrangement poeple (.e., sample(People)), checks given permutation people Anh Barb seated neighbours (using function check_If_Neighbours()).\nreplicated num_Reps times (using replicate()), adds (using sum()) many times seated together.\n, result can printed:","code":"\nset.seed(8723) # Set the random number seed, for reproducibility\n\nnum_Reps <- 5000  # Set the number of simulations\n\n# 'Name' the 10 people by using the 'initials' of the ten people\nPeople <- LETTERS[1:10] # The first 10 capital letters\nPeople\n#>  [1] \"A\" \"B\" \"C\" \"D\" \"E\" \"F\" \"G\" \"H\" \"I\" \"J\"\n# The function takes a seating arrangement as input.\n# It checks, for any given seating arrangement, if Anh and Barb are neighbours.\n\ncheck_If_Neighbours <- function(Arrangement) { \n  n <- length(Arrangement)  # n  is the length of the arrangement (so, n = 10)\n  \n  # We must allow for circular adjacency: first and last are also neighbours.\n  # So, we add the person in the first position to the end as well:\n  extended_Arrangement <- c(Arrangement, \n                            Arrangement[1])\n  are_Neighbours <- FALSE \n  # Assume not neighbours, unless we find them adjacent.\n  # TRUE means they *ARE* neighbours; FALSE means they *ARE NOT* neighbours.\n  \n  # Loop over the  n  people, and check their neighbour to the right.\n  for (i in 1:n) {\n    # Check if Anh and Barb  are neighbours\n    # First: check if A, then B to he right:\n    if (extended_Arrangement[i] == \"A\" &&  \n        extended_Arrangement[i + 1] == \"B\") are_Neighbours <- TRUE\n\n    # Then: check if B, then A to he right:\n    if (extended_Arrangement[i] == \"B\" &&\n        extended_Arrangement[i + 1] == \"A\") are_Neighbours <- TRUE\n  }\n  # Return the value of are_Neighbours, to state whether the given arrangement\n  # has  A  and  B  as nighbours (TRUE) or not (FALSE):\n  return(are_Neighbours)\n}\n# Run simulation\n# Summing works, because  TRUE  is treated as 1, FALSE  is treated as 0\nseated_Together <- sum( replicate(num_Reps, \n                                  check_If_Neighbours( sample(People))) )\n# sample(People) makes a random arrangement of the 10 people\n# Estimated probability\nprob <- seated_Together / num_Reps\ncat(\"Prob. of an UNacceptable seating arrangement: approx. \", \n    round(prob, 4), \"\\n\") # Round to four decimal places\n#> Prob. of an UNacceptable seating arrangement: approx.  0.216"},{"path":"ChapterProbability.html","id":"SimulationSensitivityAnalysis","chapter":"2 Probability","heading":"2.11.6 Simulation for sensitivity analysis","text":"Sensitivity analysis refers process checking (much) results problem model change inputs assumptions adjusted: sensitive results inputs.Suppose doctor schedules \\(20\\) patients per day, history shows patient chance showing (‘-show’).\nsimplicity, assume patient ‘-show’ probability, patients operate independently.\n \\(15\\) patients show one day, clinic runs late.\nprobability clinic runs late?\n(later chapter, theoretical approach introduced answering type question; Sect. 7.4).scenario, probability patients -show varied (e.g., \\(10\\)%; \\(15\\)%; \\(20\\)%), see impact probability clinic runs late.First, set scenario:create R function simulate happens day, given -show probability:run simulation various -show probabilities:results can printed (plotted; see Fig. 2.11):Clearly, greater -show probability, smaller chance running late (expected).\nFIGURE 2.11: simulations shows probability day medical clinic runs late.\nalso change number patients scheduled day see impact (e.g., \\(18\\) \\(22\\) patients).","code":"\nset.seed(8091) # For reproducibility\n\nnum_Reps <- 50000     # Number of replications (i.e., simulated days)\ndaily_Patients <- 20  # Number of patients per day\n# Create an R function to simulate what happens on a day\nsimulate_Day <- function(no_Show_Prob = 0.15,  # No-show probability\n                         num_Patients = 20,    # Number of patients per day\n                         patient_Limit = 15) { # More than this many: clinic late\n  \n  # Allocate a random number between 0 and 1 to each patient\n  patient_Probs <- runif( num_Patients)\n  \n  # sum()  works, because  TRUE = 1,  and  FALSE = 0\n  number_No_Shows <- sum( patient_Probs < no_Show_Prob )\n\n  running_Late <- ( (num_Patients - number_No_Shows) > patient_Limit) \n  # TRUE  means clinic will run late.  FALSE  means clinic will NOT run late\n  \n  return(running_Late)\n}\n# Declare the no-show probabilities to use\nno_Show_Probs <- seq(0.0, 0.15,\n                     by = 0.01)\n\n# Find the mean of the TRUE and FALSE values returned.\n# This works because  R  treats TRUE as 1, FALSE as 0\nprob_Day_Is_Late <- array(dim = length(no_Show_Probs))\n\nfor (i in (1:length(no_Show_Probs)) ){ # For each probability:\n  num_Days_Run_Late <- sum(replicate(num_Reps, \n                                     simulate_Day(no_Show_Probs[i]) ) )\n  prob_Day_Is_Late[i] <- num_Days_Run_Late / num_Reps\n}\nround(prob_Day_Is_Late, 4)\n#>  [1] 1.0000 1.0000 0.9999 0.9998 0.9989 0.9974\n#>  [7] 0.9945 0.9891 0.9821 0.9706 0.9567 0.9385\n#> [13] 0.9180 0.8916 0.8595 0.8307\n\nplot(prob_Day_Is_Late ~ no_Show_Probs,\n     type = \"b\",     # Plot \"both\" lines and points\n     las = 1,        # Make axis labels horizontal\n     lwd = 2,        # Line width of 2\n     col = \"cyan4\",  # Set colour\n     xlab = \"Probability that a patient is a no-show\",\n     ylab = \"Probability that clinic runs late\")"},{"path":"ChapterProbability.html","id":"ProbabilityExercises","chapter":"2 Probability","heading":"2.12 Exercises","text":"Selected answers appear Sect. E.2.Exercise 2.1  Suppose \\(\\Pr() = 0.53\\), \\(\\Pr(B) = 0.24\\) \\(\\Pr(\\cap B) = 0.11\\).Display situation using Venn diagram, tree diagram table.\neasier situation?Find \\(\\Pr(\\cup B)\\).Find \\(\\Pr(^c\\cap B)\\).Find \\(\\Pr(^c \\cup B^c)\\).Find \\(\\Pr(\\mid B)\\).events \\(\\) \\(B\\) independent?Exercise 2.2  Suppose box contains \\(100\\) tickets numbered \\(1\\) \\(100\\) inclusive.\nFour tickets drawn box one time (without replacement).\nFind probability :four numbers drawn odd.exactly two odd numbers drawn.least two odd numbers drawn drawing first even number.sum numbers drawn odd.Exercise 2.3  courier company interested length time certain set traffic lights green.\nlights set time green lights one direction \\(15\\) \\(150\\) seconds.\nemployee observes lights record length time consecutive green lights.random variable?sample space?Can classical approach probability used determine probability time green lights less \\(90\\) seconds?\n?Can relative frequency approach used determine probability?\n, ? , ?Exercise 2.4  Suppose touring cricket squad consist fifteen players, team eleven must chosen game.\nSuppose squad consists seven batters, five bowlers, two -rounders one wicketkeeper.Find number teams possible playing team consists five batters, four bowlers, one -rounder one wicketkeeper.game, member one playing team shakes hands member opposing playing team, member playing teams shakes hands two umpires.\nmany handshakes total conclusion game?Exercise 2.5  Researchers (Dexter et al. 2019) observed behaviour pedestrians Brisbane, Queensland, around midday summer.\nresearchers found probability wearing hat \\(0.025\\) males, \\(0.060\\) females.\nUsing information:Construct tree diagram sample space.Construct table sample space.Construct Venn diagram sample space.Exercise 2.6  family six non-driving children, two driving parents eight-seater vehicle.many ways can family seated car (legally go driving)?Suppose one children obtains driving licence.\nmany ways can family seated car (legally go driving) now?Two children needs car seats, two car seats fixed vehicle (.e., moved different seats).\ntwo parents drivers, many ways can family seated car (legally go driving) now?Exercise 2.7  group four people sit play Monopoly.\neight tokens distributed randomly.\nmany ways can done?Exercise 2.8  company password policy users must select eight-letter password comprising lower-case letters.\ncompany considering following changes separately:Suppose policy changes allow eight-, nine-, ten-letter passwords just lower-case letters.\nmany passwords possible now?Suppose policy changes allow eight-letter passwords comprising lower-case upper-case letters letters.\nmany passwords possible now?Suppose policy changes allow eight-letter passwords comprising lower-case, upper-case letters letters ten digits \\(0\\) \\(9\\).\nmany passwords possible now?Suppose policy changes allow eight-letter passwords comprising lower-case, upper-case letters letters ten digits \\(0\\) \\(9\\), password must one category.\nmany passwords possible now?Exercise 2.9  Many document processors help users match brackets.\nBracket matching interesting mathematical problem!\ninstance, string (()) syntactically valid, whereas ())( , even though contain two opening two closing brackets.List ways two opening two closing brackets can written way syntactically valid.many ways can three opening three closing brackets written way syntactically valid?\nList .general, number ways \\(n\\) opening \\(n\\) closing brackets can written syntactically valid given Catalan numbers \\(C_n\\), :\n\\[\nC_{n}\n=\n{\\frac {1}{n + 1}}\\binom{2n}{n}.\n\\]\nShow equivalent expression \\(C_n\\) \\(\\displaystyle C_n = {\\frac {(2n)!}{(n + 1)!\\,n!}}\\).Show another equivalent expression \\(C_n\\) \\(\\displaystyle \\binom{2n}{n} - \\binom{2n}{n + 1}\\) \\(n\\geq 0\\).Find first nine Catalan numbers, starting \\(C_0\\).\nExercise 2.10  Stirling’s approximation \n\\[\n   n!\\approx {\\sqrt {2\\pi n}}\\left({\\frac {n}{e}}\\right)^{n}.\n\\]Compare values actual factorials Stirling approximation values \\(n = 1, \\dots, 10\\).\n(Use technology!)Plot relative error Stirling’s approximation \\(n = 1, \\dots, 10\\).\n(, use technology!)Exercise 2.11  two-person game, fair die thrown turn player.\nfirst player roll  wins.Find probability first player throw die wins.Suppose player throw first selected toss fair coin.\nShow player equal chance winning.Exercise 2.12  get honest answers sensitive questions, sometimes randomised response technique used.\nexample, suppose aim discover proportion students used illegal drugs past twelve months.\\(N\\) cards prepared, \\(m\\) statement ‘used illegal drug past twelve months’.\nremaining \\(N - m\\) cards statement ‘used illegal drug past twelve months’.student sample selects one card random prepared pile \\(N\\) cards, answers ‘True’ ‘False’ asked question ‘statement selected card true false?’ without divulging statement card.\nSince interviewer know card presented, interviewer know person used drugs answer.Let \\(T\\) probability student answers ‘True’, \\(p\\) probability student chosen random used illegal drug.\nAssume student answers question chosen card truthfully.understanding problem, show \n\\[\n  T = (1 - p) + \\frac{m}{N}(2p - 1).\n\\]Find expression \\(p\\) terms \\(T\\), \\(m\\) \\(N\\) rearranging previous expression.Explain happens \\(m = 0\\), \\(m = N\\) \\(m = N/2\\), make sense context question.Suppose , sample 400 students, 175 answer ‘True’.\nEstimate \\(p\\) expression found , given \\(N = 100\\) \\(m = 25\\).Exercise 2.13  multiple choice question contains \\(m\\) possible choices.\nprobability \\(p\\) candidate chosen random know correct answer.\ncandidate know answer, candidate guesses equally likely select \\(m\\) choices.randomly selected candidate, probability question answered correctly?Exercise 2.14  2019/2020 English Premier League (EPL), full-time home team won \\(91\\) \\(208\\) games, away team won \\(67\\), \\(50\\) games drawn.\n(Data : https://sports-statistics.com/sports-data/soccer-datasets/)Define \\(W\\) win, \\(D\\) draw.Explain difference \\(\\Pr(W)\\) \\(\\Pr(W \\mid D^c)\\).Compute probabilities, comment.Exercise 2.15  Consider square size \\(1\\times 1\\) metre.\nrandom process consists selecting two points random square.sample space distance two points?Suppose grid (lines parallel sides) drawn square grid lines equally spaced \\(25\\) cm apart.\nTwo points chosen , must intersection grid lines.\nWrite R code generate sample space distance two points.Exercise 2.16  Suppose \\(30\\)% residents certain suburb subscribe local newsletter.\naddition, \\(8\\)% residents belong local online group.percentage residents belong ?\nGive range possible values.Suppose \\(6\\)% belong .\nCompute:\nprobability random chosen newsletter subscriber also member online group.\nprobability random chosen online member also subscriber newsletter.\nprobability random chosen newsletter subscriber also member online group.probability random chosen online member also subscriber newsletter.Exercise 2.17  data Table 2.8 tabulates information school children Queensland 2019 (Dunn 2023).probability randomly chosen student First Nations student?probability randomly chosen student government school?sex student approximately independent whether student First Nations student, students government schools?sex student approximately independent whether student First Nations student, students non-government schools?whether student First Nations student approximately independent type school, female students?whether student First Nations student approximately independent type school, male students?Based , can conclude data?\nTABLE 2.8: number First Nations non-First Nations students various Queensland schools 2019.\nExercise 2.18  Two cards randomly drawn (without replacement) \\(52\\)-card pack.probability second card Ace?probability first card lower rank (Ace low) second?probability card ranks consecutive order Ace low high order irrelevant (e.g., (Jack, Queen), (Queen, Jack), (Ace, Two) (kKng, Ace))?Exercise 2.19  octave contains \\(12\\) distinct notes: seven white keys 5 black keys piano.many different eight-note sequences within single octave can played using white keys ?many different eight-note sequences within single octave can played white black keys alternate (starting either colour)?many different eight-note sequences within single octave can played white black keys alternate key played ?Exercise 2.20  Find \\(\\Pr(\\cap B)\\) \\(\\Pr() = 0.2\\), \\(\\Pr(B) = 0.4\\), \\(\\Pr(\\mid B) + \\Pr(B \\mid ) = 0.375\\).Exercise 2.21  Solve \\(12\\times P^7_k = 7\\times P^6_{k + 1}\\) using:algebra; thenusing R search possible values \\(k\\).Exercise 2.22  Solve \\(P^7_{r + 1} = 10 {C^7_r}\\) \\(r\\).Exercise 2.23  Take guess: many randomly-selected individuals need probability least two birthday exceed \\(50\\)%?Show probability , group \\(N\\) randomly selected individuals, least two birthday (assuming \\(365\\) days year) can written \n\\[\n  1 - \\left(\\frac{365}{365}\\right) \\times \\left(\\frac{364}{365}\\right) \\times \\left(\\frac{363}{365}\\right) \\times \\dots\\times \\left(\\frac{365 - n + 1}{365}\\right).\n\\]Graph relationship various values \\(N\\), using form compute probability.assumptions necessary?\nreasonable?Use computer simulation confirm results.Exercise 2.24  Six numbers randomly selected without replacement numbers \\(1, 2, 3,\\dots, 45\\).\nModel process using R estimate probability consecutive numbers amongst numbers selected.\n(, sequence like 4, 5 33, 34 21, 22, 23 appears, numbers sorted smallest largest.)Exercise 2.25  Suppose events \\(\\) \\(B\\) probabilities \\(\\Pr() = 0.4\\) \\(\\Pr(B) = 0.3\\), \\(\\Pr(\\cup B) = 0.5\\).\nDetermine \\(\\Pr( ^c \\cap B^c)\\).\n\\(\\) \\(B\\) independent events?Exercise 2.26  sets \\(\\) \\(B\\), show :\\(\\cup (\\cap B) = \\);\\(\\cap (\\cup B) = \\).called absorption laws.Exercise 2.27  new cars can purchased options:Seven different paints colours available;Three different trim levels available;Cars can purchased without sunroof.many possible combinations possible?Exercise 2.28  Suppose number plates three numbers, followed two letters another number.\nmany number plates possible scheme?Exercise 2.29  forms poker, five cards dealt player, certain combinations beat combinations.probability initial five cards include exactly one pair.\n(implies getting three kind four kind.)\nExplain reasoning.probability initial five cards includes picture cards (Ace, King, Queen, Jack)?Exercise 2.30  Prove \\(P^n_n = P^n_{n-1}\\).Exercise 2.31  Without using technology, compute value \\(\\displaystyle \\frac{C^{25}_8}{C^{25}_6}\\).Exercise 2.32  Use set notation show relationship complex numbers \\(\\mathbb{C}\\) \\(\\mathbb{R}\\).Exercise 2.33  \\(A_1, A_2, \\dots, A_n\\) independent events, prove \n\\[\n\\Pr(A_1 \\cup A_2 \\cup \\dots \\cup A_n) = 1 - [1 - \\Pr(A_1)][1 - \\Pr(A_2)] \\dots [1 - \\Pr(A_n)].\n\\]Consider two events \\(\\) \\(B\\) \\(\\Pr() = r\\) \\(\\Pr(B) = s\\) \\(r, s > 0\\) \\(r + s > 1\\).\nProve \n\\[\n   \\Pr(\\mid B) \\ge 1 - \\left(\\frac{1 - r}{s} \\right).\n\\]Exercise 2.34  Consider diagram Fig. 2.12.\npoint \\(P\\) randomly placed within \\(1\\times 1\\) square \\(ABCD\\).\nprobability angle \\(APB\\) greater \\(90^\\circ\\)?\nFIGURE 2.12: Point \\(P\\) placed randomly \\(1\\times 1\\) square \\(ABCD\\).\nExercise 2.35  Consider diagram Fig. 2.13.\nprobability point randomly placed within circle (radius \\(r = 1\\)) also lands within square?\nFIGURE 2.13: random point placed within circle centre \\(C\\) radius \\(r = 1\\).\nExercise 2.36  combination lock work setting (example) four digits numbers specified order.\nSuggest accurate name ‘combination lock’.Exercise 2.37  Consider following dice:Die : six sides labelled: Die B: six sides labelled: Die C: six sides labelled: dice non-transitive (Miwin’s dice); , long run, Die beats Die B, Die B beats Die C… Die C also beats Die .Use computer simulation confirm results ‘rolling’ many times, find probabilities die winning two.Exercise 2.38  Adjust R code used Sect. 2.11.6 examine probability running late changes number booked patients increases \\(15\\) \\(25\\) people.Exercise 2.39  game show contestant told car behind one three doors, goat behind doors.\ncontestant asked select door.host show (knows car ) now opens one doors selected contestant, reveals goat.\nhost now gives contestant choice either () retaining door chosen first, (b) switching choosing (unopened) door.following think contestant’s best strategy?Always retain first choice.Always change select door.Choose either unopened door random.Use computer simulation estimate probabilities determine best strategy.\nHint: remember crucial information: host show knows car , opens one doors selected contestant, reveals goat.","code":""},{"path":"DistributionRandomVariables.html","id":"DistributionRandomVariables","chapter":"3 Random variables and their distributions","heading":"3 Random variables and their distributions","text":"Upon completion chapter, able :distinguish discrete, continuous mixed random variables.determine probability function random variables defined random process.determine distribution function random variable probability function.apply probability functions distribution functions compute probabilities defined events.plot probability function distribution function random variable.","code":""},{"path":"DistributionRandomVariables.html","id":"RandomVariables","chapter":"3 Random variables and their distributions","heading":"3.1 Random variables","text":"Chapter 2 introduced language tools probability describe uncertainty.\nconcept sample space introduced, describes possible outcomes random process.\nOften, however, individual elements sample space directly interest, especially sample space large infinite.\nSubsets sample space elements usually greater interest convenient work .example, sample space observing rolls two dice (Example 2.17) contains \\(36\\) elements.\nmay interested sum two rolls, rather elements sample space produce given sum.\n, may interested whether roll sum  \\(5\\) sum  \\(5\\) obtained, elements sample space give rise sum  \\(5\\).\nvarious elements sample space interest can collected together, treated collective.generally, collecting elements sample space together useful, can assign real number collection.\nleads idea random variable.\nRandom variable may abbreviated ‘rv’ ‘RV’.Definition 3.1  (Random variable) random variable function assigns real number outcome \\(s\\) sample space \\(S\\).\nrandom variable \\(X\\) maps \\(s\\\\mathbb{R}\\), value assigned outcome \\(s\\S\\) written \\(X(s)\\).Many random variables take integer values, number heads three coin tosses; called discrete random variables.\nMany random variables take values interval, height randomly chosen person; continuous random variables.\nMixed random variables partially discrete partially continuous.\nMixed random variables combine types behaviour single variable.\ncases, \\(X(s)\\) real number, set possible values \\(X\\) may finite set, countable set (like integers), interval \\(\\mathbb{R}\\).Random variables different variables used algebra.\nalgebra, variable typically represents unknown fixed quantity.\ncontrast, random variable represents quantity whose value depends outcome random process.Definition 3.2  (Domain range space) domain random variable sample space \\(S\\), range (range space, value set) set real numbers taken function.range space random variable \\(X\\) often denoted \\(\\mathcal{R}_X\\), \\(\\mathcal{R}_X\\\\mathbb{R}\\).\ndomain  \\(X\\) set \\(S\\), range space set \\(\\{X(s)\\mid s\\S\\}\\).Since \\(X\\) function, \\(s\\S\\) assigned exactly one value \\(X(s)\\); however, multiple values \\(s\\S\\) may assigned value \\(X(s)\\).\nvariable random since value depends upon outcome random process.capital letter (\\(X\\) \\(Y\\)) usually used denote description random variable, lower-case letters (\\(x\\) \\(y\\)) used represent values random variable can take.\nexample, consider rolling two dice observing sum two rolls.\nWriting \\(X = 3\\) means:‘random variable \\(X\\)’ (e.g., description ‘sum roll two dice’)…‘… takes value \\(3\\) outcome random process’.Example 3.1  (Rolling die twice) Consider rolling fair die twice.\nsample space \\(S\\) contains \\(36\\) elements shown Table 2.3:\n\\[\n  S = \\{ (1, 1), (1, 2), (1, 3), \\dots (6, 6)\\}.\n\\]\nHowever, may interested random variable \\(X\\), product two numbers rolled.\nelement \\(s_i\\)  \\(S\\) can assigned exactly one real number (case, exactly one integer):\n\\[\\begin{align*}\n  s_1    &= (1, 1) \\mapsto X = 1;\\\\\n  s_2    &= (1, 2) \\mapsto X = 2;\\\\\n  s_3    &= (1, 3) \\mapsto X = 3;\\\\\n  \\vdots &\\qquad   \\vdots\\\\\n  s_{36} &= (6, 6) \\mapsto X = 36.\n\\end{align*}\\]\nelement sample space mapped exactly one value  \\(X\\).\nHowever, multiple elements sample space can mapped value  \\(X\\):\n\\[\\begin{align*}\n(3, 4) &\\mapsto X = 12; \\quad\\text{}\\\\\n(6, 2) &\\mapsto X = 12.\n\\end{align*}\\]\nWriting \\(X = 12\\) means ‘product numbers two rolls  \\(12\\)’.random variable defined, events can defined terms values random variable.\nRandom variables provide convenient way express events numerically; rather listing specific outcomes, events can described using inequalities equations involving random variable.Example 3.2  (Rolling die twice: range) Example 3.1, range random variable \\(X\\) \n\\[\n   \\mathcal{R}_X = \\{1,  2,  3,  4,  5,  6,  8,  9, 10, 12, 15, 16, 18, 20, 24, 25, 30, 36\\}.\n\\]\ndomain sample space, set ordered pairs\n\\[\n  S = \\{ (1, 1), (1, 2), (1, 3), \\dots (3, 2), (3, 3), (3, 4),\\dots (6, 5), (6, 6)\\}.\n\\]Example 3.3  (Rolling die twice: events) random variable \\(X\\) Example 3.1 can used define different events.\nexample, define Event \\(A_1\\) \n\\[\n  A_1 = \\{s \\S \\mid  X(s) > 10\\} = \\{ 12, 15, 16, 18, 20, 24, 25, 30, 36\\},\n\\]\nusually written succinctly \\(X > 10\\).\nevents can defined also:\n\\[\\begin{align*}\n   A_2 &= \\{\\text{$4 \\le X < 10$}\\} = \\{ 4,  5,  6, 8,  9 \\};\\\\\n   A_3 &= \\{\\text{$X < 0$ }\\} = \\varnothing;\\\\\n   A_4 &= \\{\\text{$X$ prime}\\} = \\{2, 3, 5 \\};\\\\\n   A_5 &= \\{\\text{$X$ evenly divisible \\ $8$}\\} = \\{ 8, 16, 24\\}.\n\\end{align*}\\]Example 3.4  (Sum two die rolls) Consider observing rolls two dice.\nsample space contains \\(36\\) elements (Example 2.17), can denoted using ordered pairs \\((r_1, r_2)\\), \\(r_1\\) \\(r_2\\) results roll \\(1\\)  \\(2\\) respectively.\nsample space listed Table 2.3.\nexample, define \\(s_1\\) sample point \\((1, 1)\\).random variable \\(Y\\) can defined sample space :\n\\[\n   Y(s)  = \\text{sum two rolls $s$} = r_1 + r_2.\n\\]\ndefinition assigns real number outcome sample space:example, elements  \\(S\\) assigned \\(Y = 4\\) \n\\[\n   (1, 3), (2, 2), (3, 1).\n\\]\nNotice many elements sample space can assigned value random variable (typical random variables).domain  \\(Y\\) sample space \\(S\\); range space \n\\[\n   \\mathcal{R}_Y = \\{ Y(s) \\mid s\\S\\} = \\{2, 3, \\dots, 12\\}.\n\\]Example 3.5  (Tossing coin till head appears) Consider random process ‘tossing coin Head observed’.\nsample space \n\\[\n   \\Omega = \\{(H), (TH), (TTH), (TTTH), \\dots \\}.\n\\]\ndefine random variable \\(N\\) ‘number tosses first head observed’.\nelement sample space can assigned real number:\n\\[\\begin{align*}\n   (H)\\quad    &\\text{assigned $N = 1$};\\\\\n   (TH)\\quad   &\\text{assigned $N = 2$};\\\\\n   (TTH)\\quad  &\\text{assigned $N = 3$};\\\\\n   \\vdots\\quad &\\qquad \\vdots\n\\end{align*}\\]\n.\nWriting \\(N = 2\\) means ‘number tosses observe first head two’.Example 3.6  (Drawing two cards) Consider drawing two cards standard, well-shuffled pack cards, observing colour card (B: Black; R: Red).\nsample space \\(\\Omega\\) :\n\\[\n   \\Omega = \\{ (BB), (BR), (RR), (RB)\\}.\n\\]\nMany random variables defined sample space; example:\n\\[\\begin{align*}\n   T&: \\text{number black cards drawn};\\\\\n   M&: \\text{number red cards drawn first draw};\\\\\n   D&: \\text{number black cards drawn,}\\\\\n    &\\quad \\text{minus number red cards drawn}.\n\\end{align*}\\]\nassign real number element \\(\\Omega\\).\nrandom variable \\(D\\), instance, defined :domain \\(\\Omega\\), range space \n\\[\n   \\mathcal{R}_D = \\{ D(s) \\mid s\\\\Omega\\} = \\{-2, 0, 2\\}.\n\\]","code":""},{"path":"DistributionRandomVariables.html","id":"DiscreteContMixed","chapter":"3 Random variables and their distributions","heading":"3.2 Discrete, continuous and mixed random variables","text":"observed earlier, random variables can discrete, continuous, mixture .","code":""},{"path":"DistributionRandomVariables.html","id":"RVsDiscrete","chapter":"3 Random variables and their distributions","heading":"3.2.1 Discrete random variables","text":"Definition 3.3  (Discrete random variable) discrete random variable contains finite, countably infinite, number values within given interval given domain.Example 3.7  Example 3.1, exactly \\(36\\) values random variable \\(X\\) possible: \\(1, 2, \\dots 36\\).Example 3.4, exactly \\(11\\) values random variable \\(Y\\) possible: \\(2, 3, \\dots 12\\).Example 3.5, random variable \\(N\\) takes countably infinite number possible values: \\(1, 2, 3, \\dots\\)Example 3.6, random variable \\(D\\) can take one three possible values: \\(-2\\), \\(0\\) \\(2\\).definition refers values random variable, sample space (.e., inputs function).Examples discrete random variables include:number children aged  \\(18\\) living household.number errors per month.number incidents lung cancer hospital.number cyclones per season.number wins football team.number kangaroos observed five-hectare transect.","code":""},{"path":"DistributionRandomVariables.html","id":"RVsContinuous","chapter":"3 Random variables and their distributions","heading":"3.2.2 Continuous random variables","text":"continuous sample space, random variable usually identity function \\(Y(s) = s\\).\nexample, Example 1.19 sample space describes far cricket ball can thrown already defined positive reals.\nHence, can define random variable \\(T(s) = s\\),  \\(s\\) distance specified sample space.Definition 3.4  (Continuous random variable) continuous random variable can take value within given interval given domain (least theory).value continuous random variable can never, principle, measured exactly, practice needs rounded.Example 3.8  (Heights) Height \\(H\\) often recorded nearest centimetre (e.g., \\(179\\,\\text{cm}\\)) convenience practicality.\nBetter measuring instruments may able record height one decimal places centimetre.\nrange space \\(\\mathcal{R}_H = \\{ H(s) \\mid s \\(0, \\infty)\\}\\), \\(\\mathcal{R}_H = \\{ H(s) \\mid s \\\\mathbb{R}_{+}\\}\\).Even though height may change, notion random variable means height varies one realisation random process another; , one person next.Examples continuous random variables include:volume waste water treated sewage plant per day.weight hearts normal rats.lengths wings butterflies.yield barley large paddock.amount rainfall recorded year.time taken perform psychological test.percentage cloud cover.","code":""},{"path":"DistributionRandomVariables.html","id":"RVsMixed","chapter":"3 Random variables and their distributions","heading":"3.2.3 Mixed random variables","text":"random variables completely discrete continuous; called mixed random variables.\ncommonly-occuring mixed random variable continuous positive real numbers, plus discrete component zero.Definition 3.5  (Mixed random variable) mixed random variable subsets random variable discrete subsets random variable continuous.Example 3.9  (Vehicle wait times) Consider time spent vehicles waiting set traffic lights proceeding intersection.light green arrival, wait time exactly zero (.e., discrete): vehicle can drive straight intersection.\nwait time zero seconds can measured exactly.\nHowever, light red arrival, vehicle needs wait continuous amount time turns green.time spent waiting mixed random variable.Examples mixed random variables include:amount rainfall falls month (exactly zero, continuous amount).weight fruit produced per tree (exactly zero fruit produced, continuous amount).mass fish-catch per trawl (exactly zero fish caught, continuous amount).lifetime computer components (exactly zero component fails immediately, continuous time).","code":""},{"path":"DistributionRandomVariables.html","id":"UnivariateProbabilityFunctions","chapter":"3 Random variables and their distributions","heading":"3.3 Probability functions","text":"previous section introduced random variables: real values assigned outcomes sample space.\nOften, many elements sample space assigned value random variable.\nTherefore, probabilities can assigned various values random variable, develop probability model random variable.model describes theoretical patterns infinite trials.\nsingle roll die,  may may occur, theoretically (infinite rolls) expect  appear \\(1/6\\) time.\nprobability model describes probability various values random variable might appear one realisation theory.probability model called probability function.Example 3.10  (Tossing coin outcomes) Consider tossing coin twice observing outcome two tosses.\nSince random variable real-valued function, simply observing outcome \\((H, T)\\), example, define random variable.define random variable interest, say \\(H\\), number heads two tosses coin.\nsample space experiment \n\\[\n   S = \\{ (TT), (TH), (HT), (HH)\\}.\n\\]\nconnection sample space  \\(H\\) shown table .\ncase, range space \\(H\\) \\(\\mathcal{R}_H = \\{0, 1, 2\\}\\).\nprobability observing value  \\(H\\) can computed using classical probability:probability function defined \n\\[\\begin{align*}\n      \\Pr(H = 0):&\\quad 1/4\\\\\n      \\Pr(H = 1):&\\quad 1/2\\\\\n      \\Pr(H = 2):&\\quad 1/4\\\\\n      \\Pr(H = h):&\\quad 0\\quad \\text{values $h$}.\n\\end{align*}\\]Probability functions written interpreted differently, depending whether random variable discrete, continuous mixed random variable.","code":""},{"path":"DistributionRandomVariables.html","id":"ProbabilityFunctionsDiscrete","chapter":"3 Random variables and their distributions","heading":"3.3.1 Discrete random variables: probability mass functions","text":"discrete random variable, probability function indicates probabilities assigned values discrete random variable.\ndiscrete random variable, probability function often called probability mass function (PMF).Definition 3.6  (Probability function) Let range space discrete random variable \\(X\\)  \\(\\mathcal{R}_X\\).\n\\(x\\\\mathcal{R}_X\\), associate number\n\\[\n   p_X(x) = \\Pr(X = x).\n\\]\nfunction \\(p_X(x)\\) called probability mass function  \\(X\\).following properties probability function implied definition rules probability.\\(p_X(t) \\ge 0\\) values  \\(t\\); , probabilities never negative.\\(\\displaystyle \\sum_{t \\\\mathcal{R}_X}  p_X(t) = 1\\)  \\(\\mathcal{R}_X\\) range space  \\(X\\); , probability function covers probability possible sample points sample space.\\(p_X(t) = 0\\)  \\(t \\notin \\mathcal{R}_X\\).event \\(\\) defined sample space \\(S\\), probability event \\(\\) \n\\[\n   \\Pr() = \\sum_{t\\} p_X(t).\n\\]Definition 3.7  (Probability distribution) \\(\\mathcal{R}_X =\\{ x_1, x_2, \\dots \\}\\),\npair\n\\[\n   \\{ \\big(x_i, p_X(x_i)\\big); \\quad = 1, 2,\\dots\\}\n\\]\ncalled probability distribution discrete random variable \\(X\\).probability distribution discrete random variable \\(X\\) can represented : listing outcome probability; giving formula; using table; using graph displays probabilities \\(p(x)\\) corresponding \\(x\\\\mathcal{R}_X\\).Sometimes probability function denoted \\(p(x)\\) rather  \\(p_X(x)\\).\nUsing subscript avoids confusion situations many random variables considered .\nsubscript used throughout book.probability distribution random variable description range variable associated assignment probabilities.Example 3.11  (Independence) Five balls numbered \\(1\\), \\(2\\), \\(3\\), \\(4\\)  \\(5\\) urn.\nTwo balls selected random.\nConsider finding probability distribution larger two numbers.sample space :\n\\[\n    S =\\{ (1, 2), (1, 3), (1, 4), (1, 5), (2, 3), (2, 4), (2, 5), (3, 4), (3, 5), (4, 5)\\},\n\\]\n\\(10\\) elements equally likely.\n, let \\(X\\) random variable ‘larger two numbers chosen’, \\(\\mathcal{R}_X = \\{2, 3, 4, 5\\}\\).\nListing probabilities:\n\\[\\begin{alignat*}{3}\n    \\Pr(X = 2) &= \\Pr\\big((1, 2)\\big)                                     &\\quad &= 1/10;\\\\\n    \\Pr(X = 3) &= \\Pr\\big((1, 3) \\text{ } (2, 3)\\big)                 &\\quad &= 2/10;\\\\\n    \\Pr(X = 4) &= \\Pr\\big((1, 4) \\text{ } (2, 4) \\text{ } (3, 4)\\big) &\\quad &= 3/10;\\\\\n    \\Pr(X = 5) &= \\Pr\\big((1, 5) \\text{ } (2, 5) \\text{ } (3, 5)\\text{ } (4, 5)\\big) &\\quad &= 4/10;\\\\\n    \\Pr(\\text{values } X) & \\text{ }                             &\\quad &= 0.\n\\end{alignat*}\\]\nprobability distribution  \\(X\\), also given formula:\n\\[\n   \\Pr(X = x) =\n   \\begin{cases}\n      (x - 1)/10 & \\text{$x = 2, 3, 4, 5$}\\\\\n      0 & \\text{elsewhere}.\n  \\end{cases}\n\\]\nprobability function also given table (Table 3.1) graph (Fig. 3.1), probability assumed zero values shown.\nTABLE 3.1: table showing distribution \\(X\\), largest number two draws urn\n\nFIGURE 3.1: probability function larger two numbers drawn.\nExample 3.12  (Tossing heads) Suppose fair coin tossed twice, uppermost face noted.\nsample space \n\\[\n   S = \\{ (HH), (HT), (TH), (TT) \\}.\n\\]\nLet \\(H\\) number heads observed.\n\\(H\\) (discrete) random variable, range  \\(H\\) \\(\\mathcal{R}_H = \\{0, 1, 2\\}\\), representing values  \\(H\\) can take.probability function maps values associated probability.\nUsing techniques Chap. 2.4, probabilities :\n\\[\\begin{align*}\n   \\Pr(H = 0) &= \\Pr(\\text{heads}) = 0.25;\\\\\n   \\Pr(H = 1) &= \\Pr(\\text{one head}) = 0.5;\\\\\n   \\Pr(H = 2) &= \\Pr(\\text{two heads}) = 0.25.\n\\end{align*}\\]\nfunction, probability function \n\\[\n   p_H(h) = \\Pr(H = h)\n          = \\begin{cases}\n               0.25 & \\text{$h = 0$};\\\\\n               0.5 & \\text{$h = 1$};\\\\\n               0.25 & \\text{$h = 2$};\\\\\n               0 & \\text{otherwise}.\n              \\end{cases}\n\\]\nsuccinctly,\n\\[\n   p_H(h) = \\Pr(H = h)\n          = \\begin{cases}\n                  (0.5)0.5^{|h - 1|} & \\text{$h = 0$, $1$ $2$};\\\\\n                  0 & \\text{otherwise}.\n              \\end{cases}\n\\]\ninformation can also presented table (Table 3.2) graph (Fig. 3.2).\nNote \\(\\sum_{t \\\\{0, 1, 2\\}} p_H(t) = 1\\) \\(p_H(h)\\ge0\\) \\(h\\), required pf.\nTABLE 3.2: table showing distribution \\(H\\), number heads two tosses coin\n\nFIGURE 3.2: probability function  \\(H\\), number heads two tosses coin.\n","code":"\n\n\n\nplot( x = 1:6,  ### The values for which PMF > 0\n      y = c(0, 0.1, 0.2, 0.3, 0.4, 0),\n      xlim = c(0.5, 6.6), ylim = c(0, 0.45),\n      type = \"h\",  ### type = \"h\": vertical lines\n      las = 1, lty = 3,  ### lty = 3: Dotted lines \n      axes = FALSE,      ### Supress drawing labelled axes\n      col = \"grey\",\n      main = expression( \n        paste( \"The probability distribution of \", italic(X)) \n        ),\n      xlab = expression( \n        paste(\"Values of the random variable \", italic(X)) \n        ),\n      ylab = expression( \n        paste( \"Probability function \", italic(p)[italic(X)](italic(x)) )\n      )\n)\npoints( x = 1:6,  ### Adds the points on top of the vertical lines\n        y = c(0, 0.1, 0.2, 0.3, 0.4, 0),\n        pch = 19)\n\naxis(side = 1, at = 1:6) ### Add axis on bottom (side = 1)\naxis(side = 2,           ### Add axis at left (side = 2)\n     at = seq(0, 0.4, by = 0.1),\n     las = 1)\nbox() ### Surround plot with a box\n\n\n"},{"path":"DistributionRandomVariables.html","id":"ProbabilityFunctionsContinuous","chapter":"3 Random variables and their distributions","heading":"3.3.2 Continuous random variables: probability density functions","text":"discrete case, probability can distributed distinct points (possibly countably infinite number) point non-zero mass.\nHowever, continuous case, mass thought attribute point rather region surrounding point.\nideas Sect. 2.7 relevant .Definition 3.8  (Probability density function) probability density function (PDF) continuous random variable \\(X\\) function \\(f_X(\\cdot)\\) \n\\[\n   f_X(x) = \\Pr(< X \\le b) = \\int_a^b f_X(x)\\,dx\n\\]\ninterval \\((, b]\\) (\\(< b\\)) real line.usually concerned \\((, b)\\\\mathcal{R}_X\\), makes sense think PDF defined  \\(x\\), insisting \\(f_X(x) = 0\\) \\(x\\notin \\mathcal{R}_X\\).\ndefinition states areas graph PDF represent probabilities leads following properties probability density function.\\(f_X(x) \\ge 0\\) \\(-\\infty < x < \\infty\\):\ndensity function never negative.\\(\\displaystyle \\int_{-\\infty}^\\infty f_X(x)\\,dx = 1\\):\ntotal probability one.event \\(\\) defined sample space \\(S\\), probability event \\(\\) \n\\[\n   \\Pr() = \\int_{} f_X(x)\\, dx.\n\\]Since exact values possible:\n\\[\\begin{align*}\n       & \\Pr(< X \\le b) = \\Pr(< X < b)\\\\\n   {} =& \\Pr(\\le X < b) = \\Pr(\\le X \\le b) = \\int_a^b f_X(x)\\,dx\n\\end{align*}\\]\nProperties 1  2 sufficient prove function PDF.\n, show function \\(g(x)\\) PDF, showing \\(g(x) \\ge 0\\) \\(-\\infty < x < \\infty\\) \\(\\int_{-\\infty}^\\infty g(x)\\,dx = 1\\) sufficient.Property 4 results noting  \\(X\\) continuous random variable, \\(\\Pr(X = ) = 0\\) every value \\(\\) reason point mass zero.value PDF point \\(x\\) represent probability, rather probability density.\nHence, PDF can non-negative value arbitrary size specific value  \\(X\\).last statement easy show.\nSee Fig. 3.3, shows probability density function \\(f(x)\\).\nprobability  \\(X\\) occurring can expressed \n\\[\n  \\Pr(X = x^*)\n  \\approx \\Pr\\left(x^* - \\frac{\\Delta x}{2} < X < x^* + \\frac{\\Delta x}{2} \\right)\n\\]\n\\(\\Delta x \\0\\).\nshown shaded area, can approximated dotted rectangle shown.\n,\n\\[\\begin{align*}\n  \\Pr(X = x^*)\n  &\\approx \\Pr\\left(x^* - \\frac{\\Delta x}{2} < X < x^* + \\frac{\\Delta x}{2} \\right) \\\\\n  &= \\Delta x \\times f(x).\n\\end{align*}\\]\nRearranging,\n\\[\n  f(x) = \\lim_{\\Delta x \\0} \\frac{\\Pr(X = x^*)}{\\Delta x}.\n\\]\n, density function \\(f(x)\\) probability \\(\\Pr(X = x^*)\\).\nFIGURE 3.3: Finding probabilities continuous random variable \\(X\\).\nExample 3.13  (Probability density function) Consider continuous random variable \\(W\\) PDF\n\\[\n   f_W(w) =\n   \\begin{cases}\n      2w & \\text{$0 < w < 1$};\\\\\n      0  & \\text{elsewhere}.\n   \\end{cases}\n\\]\nprobability \\(\\Pr(0 < W < 0.5)\\) can computed two ways.\nOne use calculus:\n\\[\n   \\Pr(0 < W < 0.5) = w^2\\Big|_0^{0.5} = 0.25.\n\\]\nAlternately, probability can computed geometrically graph PDF (Fig. 3.4).\nregion corresponding \\(\\Pr(0 < W < 0.5)\\) triangular; integration simply finds area triangle.\nThus, area can found using area triangle directly: length base triangle, times height rectangle, divided two:\n\\[\n   0.5 \\times 1 /2 = 0.25,\n\\]\nanswer .Note \\(f_W(w)\\) greater one values  \\(w\\).\nSince \\(f_W(w)\\) represent probabilities point ( \\(W\\) continuous), problem.\nHowever, \\(\\int_{\\mathbb{R}} f_W(w) \\, dw = 1\\) required probability density.\nFIGURE 3.4: probability function  \\(W\\). shaded area represents \\(0 < W < 0.5\\).\n","code":""},{"path":"DistributionRandomVariables.html","id":"ProbabilityFunctionsMixed","chapter":"3 Random variables and their distributions","heading":"3.3.3 Mixed random variables","text":"random variables entirely continuous entirely discrete, components .\nrandom variables called mixed random variables.Example 3.14  (Mixed random variable) factory producing diodes, proportion diodes \\(p\\) fail immediately.\ndistribution lifetime (hundreds days), say \\(Y\\), diodes given discrete component \\(y = 0\\) \\(\\Pr(Y = 0) = p\\), continuous part \\(y > 0\\) described \n\\[\n   f_Y(y) = (1 - p) \\exp(-y) \\quad \\text{$y > 0$.}\n\\]\n, \\(f_Y(y)\\) PDF doesn’t integrate one; however total probability \n\\[\n   p + \\int_0^\\infty (1 - p)\\exp(-y) \\, dy = p + (1 - p) = 1\n\\]\nrequired.Consider diode \\(p = 0.4\\).\nprobability distribution displayed Fig. 3.5 solid dot included show discrete part.\nRepresenting probability distribution mixed case difficult, need combine probability distribution PDF.\ndifficulties overcome using distribution function (Sect. 3.4).\nFIGURE 3.5: probability function diodes example.\n","code":""},{"path":"DistributionRandomVariables.html","id":"DistributionFunction","chapter":"3 Random variables and their distributions","heading":"3.4 Distribution functions","text":"Another way describing random variables using distribution function (DF), also called cumulative distribution function (CDF).\nDF gives probability random variable \\(X\\) less equal given value  \\(x\\).Definition 3.9  (Distribution function) random variable \\(X\\) distribution function, \\(F_X(x)\\), given \n\\[\n     F_X(x) = \\Pr(X \\leq x) \\quad \\text{$x\\\\mathbb{R}$}.\n\\]distribution function applies discrete, continuous mixed random variables.\nImportantly, definition includes less equal sign; distribution function defined real numbers. \\(X\\) discrete random variable range space \\(\\mathcal{R}_X\\), DF \n\\[\\begin{align*}\n     F_X(x)\n     &= \\Pr(X \\leq x)\\\\\n     &= \\sum_{x_i \\leq x} \\Pr(X = x_i)\\text{ }x_i\\\\mathcal{R}_X,\\text{ }-\\infty < x < \\infty.\n\\end{align*}\\]\n \\(X\\) continuous random variable, DF \n\\[\\begin{align*}\n     F_(x)\n     &= \\Pr(X \\leq x)\\\\\n     &= \\int^x_{-\\infty} f(t)\\,dt \\text{ } -\\infty < x < \\infty.\n\\end{align*}\\]Example 3.15  (Tossing heads) Consider simple example Example 3.12 coin tossed .\nprobability function \\(H\\) given example numerous forms.\ndetermine DF, first note \\(h < 0\\), accumulated probability zero; hence, \\(F_H(h) = 0\\) \\(h < 0\\).\n\\(h = 0\\), probability \\(0.25\\) accumulated , probability accumulated \\(h = 1\\).\nThus, \\(F_H(h) = 0.25\\) \\(0 \\le h < 1\\).\nContinuing, DF \n\\[\n   F_H(h) = \\begin{cases}\n               0 & \\text{$h < 0$};\\\\\n               0.25 & \\text{$0\\le h < 1$};\\\\\n               0.75 & \\text{$1\\le h < 2$};\\\\\n               1 & \\text{$h\\ge 2$}.\n            \\end{cases}\n\\]\nDF can displayed graphically, careful clarify happens \\(H = 1\\), \\(H = 2\\) \\(H = 3\\) using open filled circles (Fig. 3.6).\nFIGURE 3.6: graphical representation distribution function tossing-heads example. filled circles contain given point, empty circles omit given point.\nExample 3.16  (Distribution function) Consider continuous random variable \\(V\\) PDF\n\\[\n   f_V(v) = \\begin{cases}\n               v/2 & \\text{$0 < v < 2$};\\\\\n               0 & \\text{otherwise.}\n            \\end{cases}\n\\]\nDF zero whenever \\(v\\le 0\\).\n\\(0 < v < 2\\),\n\\[\n   F_V(v) = \\int_0^v t/2\\,dt = v^2/4.\n\\]\nWhenever \\(v\\ge 2\\), DF one.\nDF \n\\[\n   F_V(v) = \\begin{cases}\n               0 & \\text{$v\\le 0$};\\\\\n               v^2/4 & \\text{$0 < v < 2$};\\\\\n               1 & \\text{$v\\ge 2$.}\n             \\end{cases}\n\\]\npicture distribution function shown Fig. 3.7.\nFIGURE 3.7: probability function  \\(V\\).\nintegral, write\n\\[\n   \\int_0^v v/2\\,dv.\n\\]\nmakes sense variable integration limit integral also function integrated.\nEither write integral given example, write \\(\\int_0^t v/2\\,dv = t^2/4\\) change variable  \\(t\\)  \\(v\\).Example 3.17  (Mixed random variable) Example 3.14 discussed mixed random variable \\(Y\\), lifetimes diodes (hundreds days).\nproportion diodes \\(p = 0.4\\) fail immediately.\ndiodes, distribution function  \\(Y\\) described \n\\[\\begin{align*}\n  F_Y(y)\n  &= \\Pr(Y\\le y)\\\\\n  &= p + \\int_0^y f_Y(t)\\, dt \\\\\n  &= p + (1 - p)  \\int_0^y\\exp(-t)\\, dt \\\\\n  &= p + (1 - p) [1 - \\exp(-y)]\\\\\n  &= 0.4 + 0.6 [1 - \\exp(-y)]\n\\end{align*}\\]\n\\(y > 0\\).\nprobability distribution displayed Fig. 3.8 solid dot included show discrete part.complete distribution function :\n\\[\n   F_Y(y) = \\begin{cases}\n               0 & \\text{$y < 0$}\\\\\n               0.4 & \\text{$y = 0$}\\\\\n               0.4 + 0.6(1 - \\exp(-y)) & \\text{$y > 0$}.\n            \\end{cases}\n\\]\nNotice total probability \n\\[\n   0.4 + 0.6\\int_0^\\infty \\exp(-y) \\, dy = 1\n\\]\nrequired.\nFIGURE 3.8: distribution function diodes example.\nProperties DF stated .\\(0\\leq F_X(x)\\leq 1\\) \\(F_X(x)\\) probability.\\(F_X(x)\\) non-decreasing function  \\(x\\).\n, \\(x_1 < x_2\\) \\(F_X(x_1) \\leq F_X(x_2)\\).\\(\\displaystyle{\\lim_{x\\\\infty} F_X(x)} = 1\\) \\(\\displaystyle{\\lim_{x\\-\\infty} F_X(x)} = 0\\).\\(\\Pr(< X \\leq b) = F_X(b) - F_X()\\). \\(X\\) discrete, \\(F_X(x)\\) step-function.\n \\(X\\) continuous, \\(F_X\\) continuous function  \\(x\\).seen find \\(F_X(x)\\) given \\(\\Pr(X = x)\\), find \\(F_X(x)\\) given \\(f_X(x)\\).\ncan proceed direction ?\n, given \\(F_X(x)\\), find \\(\\Pr(X = x)\\)  \\(X\\) discrete, \\(f_X(x)\\)  \\(X\\) continuous?seen graph distribution Example 3.15, values  \\(x\\) ‘jump’ \\(F_X(x)\\) occurs points range space, probability associated particular point  \\(\\mathcal{R}_X\\) ‘height’ jump .\n,\n\\[\\begin{equation}\n      p_X(x_j) = \\Pr(X = x_j) = F_X(x_j) - F_X(x_{j - 1}).\n   \\tag{3.1}\n\\end{equation}\\]Example 3.18  (Mass function distribution function) Consider DF discrete random variable \\(X\\):\n\\[\n   F_X(x) =\n   \\begin{cases}\n      0     & \\text{$x < 10$;}\\\\\n      0.1   & \\text{$10 \\le x < 11$;}\\\\\n      0.4   & \\text{$11 \\le x < 15$;}\\\\\n      0.9   & \\text{$15 \\le x < 17$;}\\\\\n      1     & \\text{$x \\ge 17$.}\n   \\end{cases}\n\\]\nfind PDF, use Eq. (3.1):\n\\[\n   f_X(x) =\n   \\begin{cases}\n      0.1   & \\text{$x = 10$}\\\\\n      0.3   & \\text{$x = 11$}\\\\\n      0.5   & \\text{$x = 15$}\\\\\n      0.1   & \\text{$x = 17$}\\\\\n      0     & \\text{elsewhere}\\\\\n   \\end{cases}.\n\\]\nSee Fig. 3.9.\nFIGURE 3.9: Left: distribution function \\(X\\). Right: probability mass function \\(X\\).\n \\(X\\) continuous, Fundamental Theorem Calculus,\n\\[\\begin{equation}\n   f_X(x) = \\frac{dF_X(x)}{dx} \\quad \\text{derivative exists.}\n   \\tag{3.2}\n\\end{equation}\\]Example 3.19  (Density function distribution function) Consider DF continuous random variable \\(X\\):\n\\[\n   F_X(x) =\n   \\begin{cases}\n      0        & \\text{$x < 0 $;}\\\\\n      x(2 - x) & \\text{$0 \\le x \\le 1$;}\\\\\n      1        & \\text{$x > 1$.}\n   \\end{cases}\n\\]\nfind PDF, use Eq. (3.2):\n\\[\\begin{align*}\n   f_X(x) &=\n   \\begin{cases}\n      \\frac{d}{dx} 0        & \\text{$x < 0 $;}\\\\\n      \\frac{d}{dx} x(2 - x) & \\text{$0 \\le x \\le 1$;}\\\\\n      \\frac{d}{dx} 1        & \\text{$x > 1$}\n   \\end{cases} \\\\\n   &=\n      \\begin{cases}\n      0        & \\text{$x < 0 $;}\\\\\n      2(1 - x) & \\text{$0 \\le x \\le 1$;}\\\\\n      0        & \\text{$x > 1$}\n   \\end{cases}\n\\end{align*}\\]\nusually written \\(f_X(x) = 2(1 - x)\\) \\(0 < x < 1\\),  \\(0\\) elsewhere.","code":""},{"path":"DistributionRandomVariables.html","id":"QuantileFunction","chapter":"3 Random variables and their distributions","heading":"3.5 Quantile functions","text":"random variable \\(X\\), distribution function \\(F_X(x)\\) computes probability \\(X < x\\) given value  \\(x\\).\nvalue  \\(F_X(x)\\) probability, value  \\(0\\)  \\(1\\).quantile function \\(Q_X(p)\\) inverse distribution function; takes value  \\(0\\)  \\(1\\) (called \\(p\\)), returns smallest value \\(x\\) \\(F_X(x) \\ge p\\) given values  \\(p\\).\nformally,\n\\[\\begin{equation}\n  Q_X(p) = \\inf\\{x \\\\mathbb{R} \\mid F_X(x) \\ge p\\}\\quad\\text{$0 < p < 1$}\n  \\tag{3.3}\n\\end{equation}\\]\n‘inf’ ‘infimum’, greatest lower bound set.\npractice refers leftmost value \\(x\\) distribution function \\(F_X(x)\\) reaches exceeds target probability \\(p\\).endpoints (.e., \\(p = 0\\) \\(p = 1\\)), ambiguity exists.authors set \\(Q_X(0) = \\min\\{x \\mid F_X(x) > 0\\}\\), smallest support point positive mass.\nauthors define \\(Q_X(0) = \\lim_{p\\downarrow} Q_X(p)\\), may give \\(Q_X(0)\\-\\infty\\).Similarly, authors leave \\(Q_X(1)\\) undefined, others set \\(Q_X(1) = \\sup\\{x \\mid F_X(x) < 1\\}\\), largest support point (possibly \\(+\\infty\\)).\nauthors define \\(Q_X(1) = \\lim_{p\\uparrow} Q_X(p)\\), may \\(+\\infty\\). \\(X\\) continuous random variable \\(F_X(x)\\) strictly increasing function, quantile function inverse distribution function, can write\n\\[\n   Q_X(p) = F_X^{-1}(p).\n\\]built-distributions (see Chaps. 7  8), R allows values p = 0 p = 1.\ndistribution unbounded, R returns \\(-\\infty\\) \\(+\\infty\\) appropriate.\ndistribution bounded, R returns actual minimum maximum support.Example 3.20  (Quantile function: continuous rv) Consider continuous random variable \\(X\\) probability density function\n\\[\n  f_X(x)\n  =\n  \\begin{cases}\n    x/2 & \\text{$0 \\le x \\le 2$}\\\\\n    0   & \\text{elsewhere}.\n  \\end{cases}\n\\]\ndistribution function \n\\[\n  F_X(x)\n  =\n  \\begin{cases}\n    0     & \\text{$x < 0$}\\\\\n    x^2/4 & \\text{$0 \\le x \\le 2$}\\\\\n    1     & \\text{$x > 2$}.\n  \\end{cases}\n\\]\n\\(0 < p < 1\\), quantile function \\(Q_X(p)\\) given solving \\(F_X(x) = p\\) (.e., solving \\(p = x^2 /4\\)), gives \\(Q_X(p) = 2\\sqrt{p}\\).\nmeans quantile function \n\\[\n  Q_X(p)\n  = 2\\sqrt{p} \\quad\\text{$0 < p < 1$.}\\\\\n\\]\nSee Fig. 3.10.\nFIGURE 3.10: Left: distribution function \\(X\\). Right: quantile function \\(X\\).\nExample 3.21  (Quantile function: continuous rv) Consider probability density function continuous random variable \\(X\\) \n\\[\n   f_X(x) =\n   \\begin{cases}\n      2\\exp(-2x)   & \\text{$x > 0$}\\\\\n      0            & \\text{elsewhere}\n   \\end{cases}\n\\]\ndistribution function :\n\\[\n   F_X(x) =\n   \\begin{cases}\n      0                    & \\text{$x < 0$}\\\\\n      1 - \\exp(-2x)        & \\text{$x \\ge 0 $}.\n   \\end{cases}\n\\]\n\\(0 < p < 1\\), quantile function \\(Q(p)\\) found solving \\(F_X(x) = p\\); .e., solution \\(p = 1 - \\exp(-2x)\\).\nyields\n\\[\n   Q_X(p) =\n   F_X^{-1}(p) =\n      -\\frac{1}{2}\\log(1 - p)\n\\]\n\\(0 \\le p \\le 1\\).\nexample, three quarters probability occurs \n\\[\n   Q(3/4) = -\\frac{1}{2}\\log\\big(1 - (3/4)\\big) = \\frac{\\log 4}{2} = 0.6931\\dots\n\\]\nwords, \\(75\\)% probability occurs \\(x = 0.6931...\\).\nSee Fig. 3.11.\nFIGURE 3.11: Left: distribution function \\(X\\). Right: quantile function \\(X\\).\n \\(X\\) discrete random variable, distribution function discontinuous, great care needed applying Eq. (3.3).Example 3.22  (Quantile function: discrete rv) Consider discrete random variable Example 3.18, \n\\[\n   F_X(x) =\n   \\begin{cases}\n      0     & \\text{$x < 10$;}\\\\\n      0.1   & \\text{$10 \\le x < 11$;}\\\\\n      0.4   & \\text{$11 \\le x < 15$;}\\\\\n      0.9   & \\text{$15 \\le x < 17$;}\\\\\n      1     & \\text{$x \\ge 17$,}\n   \\end{cases}\n\\]\nshown Fig. 3.12 (left panel).quantile function found solving \\(F_X(x) \\ge p\\) \\(0 < p \\le 1\\).\ngives:\n\\[\n  Q_X(p) =\n  \\begin{cases}\n    10 & \\text{$0 < p \\le 0.1$;}\\\\\n    11 & \\text{$0.1 < p \\le 0.4$;}\\\\\n    15 & \\text{$0.4 < p \\le 0.9$;}\\\\\n    17 & \\text{$0.9 < p \\le 1$.}\n  \\end{cases}\n\\]\nshown Fig. 3.12.\nexample:\\(Q_X(0.02) = 10\\).\\(Q_X(0.3) = 11\\).\\(Q_X(0.4) = 11\\), since \\(F_X(11) = 0.4\\) value  \\(x\\) value \\(F_X(x)\\) greater equal  \\(0.4\\).\\(Q_X(1) = 17\\).\nFIGURE 3.12: Left: distribution function \\(X\\). Right: quantile function \\(X\\).\n","code":""},{"path":"DistributionRandomVariables.html","id":"RandomNumbers","chapter":"3 Random variables and their distributions","heading":"3.6 Generating random numbers","text":"Many computers facilities generating pseudo-random numbers  \\(0\\)  \\(1\\), though random numbers distributions often useful computer modelling simulation (example, Sects. 7.11  8.10).\ninstance, generating heights people random requires heights average value (say) \\(180\\,\\text{cm}\\), heights \\(175\\,\\text{cm}\\) \\(185\\,\\text{cm}\\), smaller \\(175\\,\\text{cm}\\) larger \\(185\\,\\text{cm}\\).Generate random number \\((0, 1)\\), say \\(p^*\\).Use quantile function evaluate \\(Q(p^*)\\); becomes random number distribution specified quantile function.R, random numbers  \\(0\\)  \\(1\\) found using function runif():Example 3.23  (Random numbers continuous distribution) Suppose want generate random numbers distribution\n\\[\n  f_X(x) = \\exp(-2x)\n\\]\nExample 3.21.\ndistribution, quantile function shown Example 3.21.\nrandom numbers distribution can generated using R:histogram (Fig. 3.13) indeed similar shape density function.\nFIGURE 3.13: Left: density function. Right: histogram random numbers distribution.\n","code":"\nrunif(10) # '10' here means to generate 10 random numbers between 0 and 1\n#>  [1] 0.2516103 0.5757754 0.0141046 0.8898474\n#>  [5] 0.5825408 0.4986948 0.8753130 0.3948804\n#>  [9] 0.3206488 0.2908654\n# Create an R function for the quantile function\nquantileFn <- function(p){ -log(1 - p) / 2}\n\n# Create 2000 random numbers between 0 and 1\nrnos <- runif(2000)\n\n# Random numbers from the specified distribution\nrSpecified <- quantileFn(rnos)\n\n# A histogram of these random numbers should have a shape like the density fn\nhist(rSpecified)"},{"path":"DistributionRandomVariables.html","id":"RVsStatisticalComputing","chapter":"3 Random variables and their distributions","heading":"3.7 Statistical computing","text":"Many statistical distributions generated existing distributions, many relationships explored later.\nrelationships can also shown using computer simulation.example, chi-squared distribution related normal distribution, discussed later (Sect. 12.5.2).\nrandom variable \\(Z\\) standard normal distribution \\(Z\\sim N(0, 1)\\),  \\(Z^2\\) chi-squared distribution (one degree freedom).\ncan shown using simulation (Fig. 3.14).\nFIGURE 3.14: Left: histogram \\(1000\\) simulated normal variates. Right: histogram squared normal variates. solid lines right panel theoretical distribution chi-squared distribution \\(1\\) degree freedom.\nFurthermore, sum  \\(k\\) independent standard normal distributions chi-squared distribution \\(k\\) degrees freedom (Fig. 3.15):\nFIGURE 3.15: Left: histogram \\(5000\\) simulated normal variates. Right: histogram sum five squared normal variates. solid lines right panel theoretical distribution chi-squared distribution \\(5\\) degrees freedom.\n","code":"\n### A standard normal variate: 1000 random values \nZ <- rnorm(n = 1000,\n           mean = 0, \n           sd = 1)\n\n### Set up for two plots, side-by-side\npar(mfrow = c(1, 2) )\n\n### Plot two histograms: Z and Z^2\n###  truehist  is part of the MASS package,which must be loaded first, using: \n###  library(MASS)\ntruehist(Z,\n         las = 1,\n         xlab = expression(italic(Z) ),\n         ylab = \"Density\")\ntruehist(Z^2,\n         las = 1,\n         xlab = expression(sum(italic(Z^2)) ),\n         ylab = \"Density\")\n\n### Now plot the theoretical chi-squared distribution (with one df)\n# Use these values of X:\nx_Plot <- seq(0, 10,\n              length = 100)\n# Add lines to the histogram of chi-square random values\nlines( dchisq(x_Plot, df = 1) ~ x_Plot,\n       type = \"l\",\n       lwd = 3)\n# dchisq()  is the density function for a chi-squared distribution\nset.seed(7701)\n\n### Standard normal variate: in 100 rows of 5 columns each \nZ <- matrix( data = rnorm(n = 5000,\n                          mean = 0, \n                          sd = 1),\n             nrow = 1000,\n             ncol = 5)\n\n### Add the squared values, across rows (so each value is the\n### sum of *five* standard normal variates)\nZ_SumsSq <- rowSums(Z^2)\n\n### Set up for two plots, side-by-side\npar(mfrow = c(1, 2) )\n\n### Plot two histograms: Z and Z^2\n###  truehist  is part of the MASS package,which must be loaded first, using: \n###  library(MASS)\ntruehist(Z,\n         las = 1,\n         xlab = expression(italic(Z)),\n         ylab = \"Density\")\ntruehist(Z_SumsSq,\n         las = 1,\n         ylim = c(0, 0.16),\n         xlab = expression(sum(italic(Z^2))~over~five~values ),\n         ylab = \"Density\")\n\n### Now plot the theoretical chi-squared distribution (with *five* df)\n# Use these values of X:\nx_Plot <- seq(0, 20,\n              length = 100)\nlines( dchisq(x_Plot, df = 5) ~ x_Plot,\n       type = \"l\",\n       lwd = 3)"},{"path":"DistributionRandomVariables.html","id":"RVExercises","chapter":"3 Random variables and their distributions","heading":"3.8 Exercises","text":"Selected answers appear Sect. E.3.Exercise 3.1  following random processes, determine range space \\(\\mathcal{R}_X\\) define random variable interest.\nDetermine whether random variable discrete, continuous mixed.\nJustify answer.number heads two throws fair coin.number throws fair coin head observed.time taken download webpage.time takes walk work.Exercise 3.2  following random processes, determine range space \\(\\mathcal{R}_X\\) define random variable interest.\nDetermine whether random variable discrete, continuous mixed.\nJustify answer.number cars pass intersection day.number X-rays taken hospital per day.barometric pressure given city \\(5\\)pm afternoon.length phone call connection.Exercise 3.3  random variable \\(X\\) probability function\n\\[\n   p_X(x) = \\begin{cases}\n               0.3 & \\text{$x = 10$};\\\\\n               0.2 & \\text{$x = 15$};\\\\\n               0.5 & \\text{$x = 20$};\\\\\n               0 & \\text{elsewhere}.\n            \\end{cases}\n\\]Show \\(p_X(x)\\) valid probability distribution.Plot probability function  \\(X\\).Find plot distribution function  \\(X\\).Compute \\(\\Pr(X > 13)\\).Compute \\(\\Pr(X \\le 10 \\mid  X\\le 15)\\).Exercise 3.4  random variable \\(X\\) probability mass function\n\\[\n   p_X(x) = \\begin{cases}\n               2^{-x} & \\text{$x = 1, 2, 3, \\dots$};\\\\\n               0      & \\text{elsewhere}.\n            \\end{cases}\n\\]Show \\(p_X(x)\\) valid probability distribution.Plot probability function  \\(X\\).Find plot distribution function  \\(X\\).Compute \\(\\Pr(X > 13)\\).Compute \\(\\Pr(X \\le 10 \\mid  X\\le 15)\\).Exercise 3.5  Consider continuous random variable \\(Z\\) probability function\n\\[\n   f_Z(z) = \\begin{cases}\n               \\alpha (3 - z) & \\text{$-1 < z < 2$};\\\\\n               0 & \\text{elsewhere}.\n            \\end{cases}\n\\]Find value  \\(\\alpha\\).Plot probability function  \\(Z\\).Find plot distribution function  \\(Z\\).Find \\(\\Pr(Z < 0)\\).Exercise 3.6  Consider continuous random variable \\(X\\) probability density function\n\\[\n   f_X(x) = \\begin{cases}\n               3(4 - x^2)/16 & \\text{$0 < x < 2$};\\\\\n               0 & \\text{elsewhere}.\n            \\end{cases}\n\\]Plot probability function  \\(X\\).Find plot distribution function  \\(X\\).Find \\(\\Pr(X < 1)\\).Exercise 3.7  Consider mixed random variable \\(Y\\) probability function\n\\[\n   f_Y(y) = \\begin{cases}\n               p & \\text{$y = 0$};\\\\\n               1 - y & \\text{$0 < y < 1$}.\\\\\n               0 & \\text{elsewhere}.\n            \\end{cases}\n\\]Find value  \\(p\\).Carefully plot probability function  \\(Y\\).Find carefully plot distribution function  \\(Y\\).Find \\(\\Pr(Y < 0.5)\\).Exercise 3.8  Consider mixed random variable \\(X\\) probability function\n\\[\n   f_X(x) = \\begin{cases}\n               c   & \\text{$x = 0$};\\\\\n               x/2 & \\text{$0 < x < 1$};\\\\\n               (1 - x)/4 & \\text{$1 < x < 3$};\\\\\n               0 & \\text{elsewhere}.\n            \\end{cases}\n\\]Find value  \\(c\\).Carefully plot probability function  \\(X\\).Find plot distribution function  \\(X\\).Find \\(\\Pr(X > 1)\\).Exercise 3.9  Consider random variable \\(Y\\) probability mass function\n\\[\n  f_Y(y)\n  =\n  \\begin{cases}\n    y^\\alpha - 2 & \\text{$y = 1, 2$}\\\\\n    0            & \\text{elsewhere.}\n  \\end{cases}\n\\]\nFind value(s) \\(\\alpha\\) \\(f_Y(y)\\) valid probability function.Exercise 3.10  Consider random variable \\(X\\) probability mass function\n\\[\n  f_X(x)\n  =\n  \\begin{cases}\n    x + 1 & \\text{$x = b, b + 1, b + 2$}\\\\\n    0     & \\text{elsewhere.}\n  \\end{cases}\n\\]\nFind value(s) \\(b\\) \\(f_X(x)\\) valid probability function.Exercise 3.11  Consider random variable \\(T\\) probability density function shown Fig. 3.16 (left panel).\nFind value(s) \\(\\) represents valid probability function.Exercise 3.12  Consider random variable \\(W\\) probability density function shown Fig. 3.16 (right panel).\nFind value(s) \\(\\) represents valid probability function.\nFIGURE 3.16: Two probability density functions\nExercise 3.13  Consider discrete random variable \\(W\\) DF\n\\[\n   F_W(w) = \\begin{cases}\n               0   & \\text{$w < 10$};\\\\\n               0.3 & \\text{$10 \\le w < 20$};\\\\\n               0.7 & \\text{$20 \\le w < 30$};\\\\\n               0.9 & \\text{$30 \\le w < 40$};\\\\\n               1 & \\text{$w \\ge 40$}.\n            \\end{cases}\n\\]Find plot PMF  \\(W\\).Compute \\(\\Pr(W < 25)\\) using PMF, using DF.Exercise 3.14  Consider continuous random variable \\(Y\\) DF\n\\[\n   F_Y(y) = \\begin{cases}\n               0 & \\text{$y \\le 0$};\\\\\n               y(4 - y^2)/3 & \\text{$0 < y < 1$};\\\\\n               1 & \\text{$y \\ge 1$}.\n            \\end{cases}\n\\]Find plot PDF  \\(Y\\).Compute \\(\\Pr(Y < 0.5)\\) using PDF using DF.Exercise 3.15  study service life concrete various conditions (Liu Shi 2012) used following distribution chlorine threshold concrete1:\n\\[\n   f(x; , b, c) =\n   \\begin{cases}\n      \\displaystyle \\frac{2(x - )}{(b - )(c - )} & \\text{$\\le x\\le c$};\\\\[6pt]\n      \\displaystyle \\frac{2(b - x)}{(b - )(b - c)} & \\text{$c\\le x\\le b$};\\\\[3pt]\n      0 & \\text{otherwise},\n   \\end{cases}\n\\]\n \\(c\\) mode,  \\(\\)  \\(b\\) lower upper limits.Show distribution valid PDF.\n(may easier geometrically using integration.)Previous studies, cited article, suggest values \\(= 0.6\\) \\(c = 5\\), distribution symmetric.\nWrite PDF case.Determine DF using values .Plot PDF DF.Determine \\(\\Pr(X > 3)\\).Determine \\(\\Pr(X > 3 \\mid X > 1)\\).Explain difference meaning last two answers.Exercise 3.16  study modelling waiting times hospital (Khadem et al. 2008), patients classified one three categories:Red: Critically ill injured patients.Yellow: Moderately ill \ninjured patients.Green: Minimally injured \nuninjured patients.‘Yellow’ patients, service time doctors modelled using triangular distribution, minimum \\(3.5\\,\\text{mins}\\), maximum \\(30.5\\,\\text{mins}\\) mode \\(5\\,\\text{mins}\\).Plot PDF DF. \\(S\\) service time, compute \\(\\Pr(S > 20 \\mid S > 10)\\).Exercise 3.17  meteorological studies, rainfall often graphed using rainfall exceedance charts (Dunn 2001): plotted function \\(1 - F_X(x)\\) (also called survivor function contexts),  \\(X\\) represents rainfall.Explain measured \\(1 - F_X(x)\\) context, explain may useful just \\(F_X(x)\\) studying rainfall.data Table 3.3 shows total monthly rainfall Charleville, Queensland, 1942 2022 months June December.\n(Data supplied Bureau Meteorology.)\nPlot \\(1 - F_X(x)\\) rainfall data month graph.Suppose producer requires least \\(50\\,\\text{mm}\\) rain June plant crop.\nFind probability occurring plot .Determine median monthly rainfall Charleville June December.Decide mean median appropriate measure central tendency, giving reasons answer.Compare probabilities obtaining \\(30\\,\\text{mm}\\), \\(50\\,\\text{mm}\\), \\(100\\,\\text{mm}\\) \\(150\\,\\text{mm}\\) rain months June December.Write one-two-paragraph explanation use diagrams like presented .\nexplanation aimed producers (, experts farming, statistics) able demonstrate usefulness graphs decision making.\nexplanation clear without jargon.\nUse diagrams necessary.\nTABLE 3.3: monthly rainfall Charleville, mm, 1942 2022: number months indicated rainfall\nExercise 3.18  Five people, including friend, line random.\nrandom variable \\(X\\) denotes number people friend.\nDetermine probability function  \\(X\\).Exercise 3.19  Let \\(Y\\) continuous random variable PDF\n\\[\n   f_Y(y) = (1 - y)^2,\\quad \\text{$0\\le y\\le 1$}.\n\\]Show \\(= 3\\).Find \\(\\Pr\\left(| Y - \\frac{1}{2}| > \\frac{1}{4} \\right)\\).Use R draw graph \\(f_Y(y)\\) show area described .Exercise 3.20  Suppose random variable \\(Y\\) PDF\n\\[\n  f_Y()y) =\n  \\begin{cases}\n    \\frac{2}{3}(y - 1) & \\text{$1 < y < 2$};\\\\\n    \\frac{2}{3} & \\text{$2 < y < 3$}.\n  \\end{cases}\n\\]Plot PDF.Determine DF.Confirms DF valid DF.Exercise 3.21  Quartic polynomials sometimes used model mortality (e.g., Fitzpatrick Moore (2018)).\nSuppose model\n\\[\n   f_Y(y) = k y^2 (100 - y)^2\\quad \\text{$0 \\le y \\le 100$}\n\\]\n(value  \\(k\\)) used describing mortality  \\(Y\\) denotes age death years.\nUsing model, person likely die age  \\(60\\)  \\(70\\), live past \\(70\\)?Exercise 3.22  detect disease population blood test, usually every individual tested.\ndisease uncommon, however, alternative method often efficient.alternative method (called pooled test), blood  \\(n\\) individuals combined, one test conducted.\ntest returns negative result, none  \\(n\\) people disease; test returns positive result,  \\(n\\) individuals tested individually identify individual(s) disease.Suppose disease occurs unknown proportion people \\(p\\) people.\nLet \\(X\\) number tests performed group  \\(n\\) individuals using pooled test approach.Determine sample space  \\(X\\).Deduce probability distribution random variable \\(X\\).Exercise 3.23  deck cards, consider Ace high, picture cards (.e., Jacks, Queens, Kings, Aces) worth ten points.\ncards worth face value (.e.,  8 worth eight points.)Deduce probability distribution absolute difference value two cards drawn random well-shuffled deck \\(52\\) cards.Exercise 3.24  leading digits natural data span many orders magnitude (e.g., lengths rivers; populations countries) often follow Benford’s law.\nNumbers said satisfy Benford’s law leading digit, say \\(d\\) (\\(d\\\\{1, \\dots, 9\\}\\)) probability mass function\n\\[\n   p_D(d) = \\log_{10}\\left( \\frac{d + 1}{d} \\right).\n\\]Plot probability mass function  \\(D\\).Compute plot distribution function  \\(D\\).Exercise 3.25  Consider random variable \\(X\\) PDF\n\\[\n   f(x) =\n   \\begin{cases}\n      \\displaystyle \\frac{k}{\\sqrt{x(1 - x)}} & \\text{$0 < x < 1$};\\\\[6pt]\n      0                                       & \\text{elsewhere},\n   \\end{cases}\n\\]\nconstant \\(k\\).Plot density function.Determine, plot, distribution function.Compute \\(\\Pr(X > 0.25)\\).Exercise 3.26  Consider distribution function\n\\[\n   F_X(x) =\n   \\begin{cases}\n        0          & \\text{$x < 0$};\\\\\n        \\exp(-1/x) & \\text{$x \\ge 0$}.\n   \\end{cases}\n\\]Show valid DF.Compute PDF.Plot DF PDF.Exercise 3.27  Suppose dealer deals four cards standard \\(52\\)-card pack.\nDefine \\(Y\\) number suits four cards.\nDeduce distribution  \\(Y\\).Exercise 3.28  Consider random variable \\(T\\) probability function\n\\[\n  f_T(t) =\n  \\begin{cases}\n    & \\text{$t = 0$}; \\\\\n    \\frac{2}{3} - (t - 1)^2 & \\text{$0 < t < 2$}.\n  \\end{cases}\n\\]Determine value  \\(\\).Plot probability function.Determine distribution function.Plot distribution function.Exercise 3.29  Consider rolling fair, six-sided die.Find probability mass function number rolls required roll total least four.Find probability mass function number rolls required roll total exactly four.Exercise 3.30  Consider random variable \\(Z\\) standard normal distribution \\(N(0, 1)\\), random variable \\(V\\) chi-squared distribution  \\(v\\) degrees freedom.Simulate distribution \n\\[\n  T = \\frac{Z}{\\sqrt{V/\\nu}},\n\\]\nshow \\(t\\)-distribution  \\(\\nu\\) degrees freedom.\nHint: use R functions \\(t\\)-distribution (e.g., dt()) chi-squared distribution (e.g, dchisq()).Exercise 3.31  Consider two random variables \\(X\\)  \\(Y\\) uniform distributions, probability density functions\n\\[\\begin{align*}\n  f_X(x) = 1\\quad\\text{$0< x < 1$; }\\\\\n  f_Y(y) = 1\\quad\\text{$0< y < 1$}\n\\end{align*}\\]Using simulation, show distribution \\(X + Y\\) triangular distribution.mean variance resulting distribution, based simulation?Explain answers change distribution  \\(Y\\) changes \n\\[\n  f_Y(y) = \\frac{1}{2}\\quad\\text{$-1 < y < 1$}.\n\\]Exercise 3.32  Consider random variable \\(Z\\) standard normal distribution \\(N(0, 1)\\), random variable \\(V_\\nu\\) chi-squared distribution  \\(\\nu\\) degrees freedom.Simulate distribution \n\\[\n  T = \\frac{Z}{\\sqrt{V_9/9}},\n\\]\nshow \\(t\\)-distribution  \\(\\nu = 9\\) degrees freedom.\nHint: use R functions \\(t\\)-distribution (e.g., dt()) chi-squared distribution (e.g, dchisq()).","code":""},{"path":"ChapBivariate.html","id":"ChapBivariate","chapter":"4 Bivariate distributions","heading":"4 Bivariate distributions","text":"completion chapter able :apply concept bivariate random variables.compute joint probability functions distribution function two random variables.find marginal conditional probability functions random variables discrete continuous cases.apply concept independence two random variables.compute expectation variance linear combinations random variables.interpret compute covariance coefficient correlation two random variables.compute conditional mean conditional variance random variable given value another random variable.use multinomial bivariate normal distributions.","code":""},{"path":"ChapBivariate.html","id":"BivariateIntroduction","chapter":"4 Bivariate distributions","heading":"4.1 Introduction","text":"random processes sufficiently simple outcome denoted single variable \\(X\\).\nMany situations require observing two numerical characteristics simultaneously.\nchapter mainly discusses two-variable (bivariate) case, also discusses multivariable (two variables) case using matrix notation (Sect. 10.2).","code":""},{"path":"ChapBivariate.html","id":"BivariateRVs","chapter":"4 Bivariate distributions","heading":"4.2 Bivariate random variables and their distributions","text":"joint probability function function simultaneously describes ho two random variables vary.\nHence, range space \\((X, Y)\\), written \\(\\mathcal{R}_{X \\times Y}\\), subset Euclidean plane.\noutcome \\(X(s)\\), \\(Y(s)\\) may represented point \\((x, y)\\) plane.\none-dimensional case, distinguishing discrete continuous random variables necessary.Definition 4.1  (Random vector) Let \\(X = X(s)\\) \\(Y = Y(s)\\) two functions, assigning real number sample point \\(s \\S\\).\n\\((X, Y)\\) called two-dimensional random variable, random vector.Example 4.1  (Bivariate discrete) Consider random process , simultaneously, two coins tossed, one die rolled.\nLet \\(X\\) number heads show two coins, \\(Y\\) number rolls needed roll .\\(X\\) discrete \\(\\mathcal{R}_X = \\{0, 1, 2\\}\\).\n\\(Y\\) discrete countably infinite range space \\(\\mathcal{R}_Y = \\{ 1, 2, 3, \\dots\\}\\).range space \\(\\mathcal{R}_{X\\times Y} = \\{ (x, y): 0 \\le x \\le 2, y = 1, 2, 3, \\dots\\}\\).univariate case, description language probability function different, depending whether random variables \\(X\\)  \\(Y\\) discrete continuous case (though ideas remain similar).\ncommon, case one variable, say \\(X\\), continuous , say \\(Y\\), discrete also occurs.\ndefer case Sect. 4.7.Definition 4.2  (Discrete bivariate distribution function) Let \\((X, Y)\\) \\(2\\)-dimensional random variable  \\(X\\)  \\(Y\\) discrete random variables.\n\\((x_i, y_j)\\) associate number \\(p_{X, Y}(x_i, y_j)\\) representing \\(\\Pr(X = x_i, Y = y_j)\\) satisfying\n\\[\\begin{align}\n   p_{X, Y}(x_i, y_j) &\\geq 0, \\text{ } (x_i, y_j)  \\\\\n   \\sum_{j = 1}^{\\infty} \\sum_{= 1}^{\\infty} p_{X, Y}(x_i, y_j)\n   &= 1.\n   \\tag{4.1}\n\\end{align}\\]\nfunction \\(p_{X, Y}(x, y)\\), defined \\((x_i, y_j) \\R\\) called probability function \\((X, Y)\\).\nAlso,\n\\[\n   \\{x_i, y_j, p_{X,Y}(x_i, y_j); , j = 1, 2, \\ldots\\}\n\\]\ncalled probability distribution \\((X, Y)\\).Definition 4.3  (Continuous bivariate distribution function) Let \\((X, Y)\\) \\(2\\)-dimensional random variable  \\(X\\)  \\(Y\\) continuous random variables.\njoint probability density function, \\(f_{X, Y}\\), function satisfying\n\\[\\begin{align}\n   f_{X, Y}(x, y) &\\geq 0, \\text{ } (x, y) \\R, \\\\\n   \\int \\!\\! \\int_{R} f_{X, Y}(x, y) \\, dx \\, dy &= 1.\n\\end{align}\\]second indicates volume surface \\(f_{X, Y}(x, y)\\) one.\nAlso, \\(\\Delta x, \\Delta y\\) sufficiently small,\n\\[\\begin{equation}\n   f_{X, Y}(x, y) \\, \\Delta x \\Delta y \\approx \\Pr(x \\leq X \\leq x + \\Delta x, y \\leq Y \\leq y + \\Delta y).\n\\end{equation}\\]\nProbabilities events can determined probability function probability density function follows.Definition 4.4  (Bivariate distribution probabilities) event \\(\\), probability  \\(\\) given \n\\[\\begin{align*}\n   \\Pr()\n   &= \\sum_{(x, y) \\} p(x, y),                       &&\\text{$(X, Y)$ discrete;}\\\\\n   \\Pr()\n   &= \\int \\!\\! \\int_{(x, y) \\}f(x, y) \\, dx \\, dy   &&\\text{$(X, Y)$ continuous.}\n\\end{align*}\\]univariate case, bivariate distribution can given various ways:enumerating range space corresponding probabilities;formula; ora graph; orby table.Example 4.2  (Bivariate discrete) Consider following discrete distribution probabilities \\(\\Pr(X = x, Y = y)\\) shown graph\n(Fig. 4.1)\ntable (Table 4.1).find \\(\\Pr(X + Y = 2)\\):\n\\[\\begin{align*}\n   \\Pr(X + Y = 2)\n   &=\\Pr\\big(\\{X = 2, Y = 0\\} \\text{ }\n             \\{X = 1, Y = 1\\} \\text{ }\n             \\{X = 0, Y = 2\\}\\big)\\\\\n   &= \\Pr(X = 2, Y = 0) \\, +\n      \\, \\Pr(X = 1, Y = 1) \\, +\n      \\, \\Pr(X = 0, Y = 2)\\\\\n   &= \\frac{9}{42} \\ + \\ \\frac{12}{42} \\ + \\ \\frac{12}{42}\n   = \\frac{33}{42}.\n\\end{align*}\\]\nFIGURE 4.1: bivariate discrete probability function.\n\nTABLE 4.1: bivariate discrete probability function\nExample 4.3  (Bivariate uniform distribution) Consider following continuous bivariate distribution joint PDF\n\\[\n   f_{X, Y}(x, y) = 1, \\quad \\text{$0 \\leq x \\leq 1$ $0 \\leq y \\leq 1$}.\n\\]\nsometimes called bivariate continuous uniform distribution\n(see ).\nvolume surface one.find \\(\\Pr(0 \\leq x \\leq \\frac{1}{2}, 0 \\leq y \\leq \\frac{1}{2})\\), find volume square vertices \\((0, 0), (0, 1/2), (1/2, 0), (1/2, 1/2)\\).\nHence probability  \\(1/4\\).\nFIGURE 4.2: bivariate continuous uniform distribution.\nExample 4.4  (Bivariate discrete) Consider random process two coins tossed, one die rolled simultaneously (Example 4.1).\nLet \\(X\\) number heads show two coins,  \\(Y\\) number die.Since toss coin roll die independent, probabilities computed follows:\n\\[\\begin{align*}\n   \\Pr(X = 0, Y = 1)\n   &= \\Pr(X = 0) \\times \\Pr(Y = 1) = \\frac{1}{4}\\times\\frac{1}{6} = \\frac{1}{24};\\\\\n   \\Pr(X = 1, Y = 2)\n   &= \\Pr(X = 1) \\times \\Pr(Y = 2) = \\frac{1}{2}\\times\\frac{1}{6} = \\frac{1}{12};\n\\end{align*}\\]\n.\ncomplete joint probability function can displayed graph (often tricky), function, table (Table 4.2).\n, joint pf given function\n\\[\n   p_{X, Y}(x, y) =\n      \\begin{cases}\n         \\left(\\frac{1}{12}\\right) 0.5^{|x - 1|} & \\text{$(x, y)\\S$ defined earlier};\\\\\n         0                                         & \\text{elsewhere.}\n      \\end{cases}\n\\]\nTABLE 4.2: joint pf Example 4.4\nExample 4.5  (Two dice) Consider bivariate discrete distribution results two dice thrown.Let \\(X\\) number times  appears,  \\(Y\\) number times  appears.\nrange spaces  \\(X\\)  \\(Y\\) \\(\\mathcal{R}_X = \\{0, 1 ,2 \\}\\), \\(\\mathcal{R}_Y = \\{0, 1, 2\\}\\) range space random process Cartesian product  \\(\\mathcal{R}_X\\)  \\(\\mathcal{R}_Y\\), understanding resulting points may probability zero.\nprobabilities Table 4.3 \\(\\Pr(X = x, Y = y)\\) \\((x, y)\\) pairs range space.probabilities found realising really two repetitions simple random process three possible outcomes, \\(\\{5, 6, (\\text{$5$ $6$})^c \\}\\), probabilities \\(\\frac{1}{6}, \\frac{1}{6}, \\frac{2}{3}\\), repetition.\n(Recall: \\(\\overline{\\text{$5$ $6$}}\\) means ‘5 6’; see Def. 1.7.)\ncourse event \\(X = 2, Y = 1\\) occur two trials, probability zero.\nTABLE 4.3: Joint probability distribution Example 4.5.\nExample 4.5 special case multinomial distribution (generalisation binomial distribution), described later (Sect. 7.9).Example 4.6  (Banks) bank operates ATM teller.\nrandomly selected day, let \\(X_1\\) proportion time ATM use (least one customer served waiting served),  \\(X_2\\) proportion time teller busy.set possible values  \\(X_1\\)  \\(X_2\\) rectangle \\(R = \\{(x_1, x_2)\\mid 0 \\le x_1 \\le 1, 0 \\le x_2 \\le 1\\}\\).\nexperience, joint PDF \\((X_1, X_2)\\) \n\\[\n   f_{X_1, X_2}(x_1, x_2) =\n      \\begin{cases}\n        c(x_1 + x_2^2) & \\text{$0\\le x_1\\le 1$; $0\\le x_2\\le 1$};\\\\\n        0 & \\text{elsewhere.}\n    \\end{cases}\n\\]determine value  \\(c\\), first see \\(f_{X_1, X_2}(x_1, x_2) \\ge 0\\)  \\(x_1\\)  \\(x_2\\), \\(c > 0\\); \n\\[\n   \\int_{-\\infty}^{\\infty}\\!\\int_{-\\infty}^{\\infty} f_{X_1, X_2}(x_1, x_2)\\, dx_1\\,dx_2 = 1.\n\\]\nHence,\n\\[\\begin{align*}\n    \\int_{-\\infty}^{\\infty}\\!\\int_{-\\infty}^{\\infty} f_{X_1, X_2}(x_1, x_2)\\, dx_1\\,dx_2\n    &= \\int_{0}^{1}\\!\\!\\!\\int_{0}^{1} f_{X_1, X_2}(x_1, x_2)\\, dx_1\\,dx_2 \\\\\n    &= c \\int_{x_2 = 0}^{1}\\left\\{\\int_{x_1=0}^{1} (x_1 + x_2^2)\\, dx_1\\right\\} dx_2\\\\\n    &= c (1/2 + 1/3) = 5c/6,\n\\end{align*}\\]\n\\(c = 6/5\\).Consider probability neither facility busy half time.\nMathematically, question asking find \\(\\Pr( 0\\le X_1\\le 0.5, 0\\le X_2\\le 0.5)\\); call event \\(\\).\n,\n\\[\\begin{align*}\n   \\Pr()\n   &= \\int_{0}^{0.5}\\,\\,\\, \\int_{0}^{0.5} f_{X_1, X_2}(x_1, x_2)\\, dx_1\\, dx_2 \\\\\n   &= \\frac{6}{5} \\int_{0}^{0.5}\\left\\{\\int_{0}^{0.5} x_1 + x_2^2\\, dx_1\\right\\} dx_2 \\\\\n   &= \\frac{6}{5} \\int_{0}^{0.5} (1/8 + x_2^2/2) \\, dx_2 = 1/10.\n\\end{align*}\\]","code":""},{"path":"ChapBivariate.html","id":"BivariateDistributionFunction","chapter":"4 Bivariate distributions","heading":"4.3 Joint distribution function","text":"(cumulative) distribution function represents sum probabilities, volume surface, denoted \\(F_{X, Y}(x, y)\\), defined follows.Example 4.7  (Bivariate distribution function) bivariate distribution function \n\\[\\begin{align}\n   F(x, y)\n   &= \\Pr(X \\leq x, \\, Y \\leq y),                           & \\text{$(X,Y)$ discrete;}\n   \\tag{4.2}\\\\\n   F(x, y)\n   &= \\int_{-\\infty}^y \\int_{-\\infty}^x f(u,v) \\, du \\, dv, & \\text{$(X,Y)$ continuous.}\n   \\tag{4.3}\n\\end{align}\\]Example 4.8  (Bivariate discrete) Consider random process Example 4.4, two coins tossed, one die rolled (simultaneously).\nprobability function given Table 4.2.complete joint distribution function given Table 4.4, complicated even simple case.\nexample, joint df \\((1, 2)\\) computed follows:\n\\[\\begin{align*}\n   F_{X, Y}(1, 2)\n   &= \\displaystyle \\sum_{x\\le1} \\,  \\sum_{y\\le 2} p_{X, Y}(x, y)\\\\\n   &= p_{X, Y}(0, 1) + p_{X, Y}(0, 2) +  p_{X, Y}(1, 1) + p_{X, Y}(1, 2) \\\\\n   &= 1/24 + 1/24 + 1/12 + 1/12 = 6/24.\n\\end{align*}\\]\nTABLE 4.4: bivariate probability function \\(X\\) \\(Y\\).\nExample 4.9  (Bivariate continuous) Example 4.6,\n\\[\\begin{align*}\n   F_{X, Y}(x, y)\n   &= \\frac{6}{5} \\int_0^{x} \\int_0^{y} (t_1 + t_2^2)\\, dt_2 dt_1 \\\\\n   &= \\frac{6}{5} \\int_0^{x} (t_1 t_2 + t_2^3/3)\\Big|_{t_2 = 0}^{t_2 = y} \\, dt_1 \\\\\n   &= \\frac{6}{5} \\int_0^{x} (t_1 y + x_2^3/3)\\, dt_1 \\\\\n   &= \\frac{6}{5} \\left( \\frac{x y}{2} + \\frac{x y^3}{3}\\right)\n\\end{align*}\\]\n\\(0 < x < 1\\) \\(0 < y < 1\\).\n\n\\[\n   F_{X, Y}(x, y)\n   = \\begin{cases}\n      0 & \\text{$x < 0$ $y < 0$};\\\\\n      \\frac{6}{5} \\left( x y/2 + x x_2^3/3\\right) & \\text{$0 \\le x \\le 1$ $0 \\le y \\le 1$};\\\\\n      1 & \\text{$x > 1$ $y > 1$}.\n     \\end{cases}\n\\]","code":""},{"path":"ChapBivariate.html","id":"MarginalDistributions","chapter":"4 Bivariate distributions","heading":"4.4 Marginal distributions","text":"two-dimensional random variable \\((X, Y)\\) two one-dimensional random variables, namely \\(X\\)  \\(Y\\), can described.\ncan find probability distributions  \\(X\\)  \\(Y\\) separately.case discrete random vector \\((X, Y)\\), event \\(X = x_i\\) union mutually exclusive events\n\\[\n   \\{X = x_i, Y = y_1\\}, \\{\\ X = x_i, Y = y_2\\}, \\{X = x_i, Y = y_3\\}, \\dots\n\\]\nThus,\n\\[\\begin{align*}\n   \\Pr(X = x_i)\n   &= \\Pr(X = x_i, Y = y_1) + \\Pr(X = x_i, Y = y_2) + \\dots \\\\\n   &= \\sum_jp_{X, Y}(x_i, y_j),\n\\end{align*}\\]\nnotation means sum values given summation sign.\nHence, marginal distributions can defined \\((X, Y)\\) discrete random vector.Definition 4.5  (Bivariate discrete marginal distributions) Given \\((X, Y)\\) joint discrete probability function \\(p(x, y)\\), marginal probability functions  \\(X\\)  \\(Y\\) , respectively\n\\[\\begin{equation}\n   \\Pr(X = x) = \\sum_{y}p_{X, Y}(x, y)\n   \\quad\\text{}\\quad\n   \\Pr(Y = y) = \\sum_{x}p_{X, Y}(x, y).\n   \\tag{4.4}\n\\end{equation}\\]analogous definition exists random vector \\((X,Y)\\) continuous.Definition 4.6  (Bivariate continuous marginal distributions) \\((X, Y)\\) joint continuous PDF \\(f(x, y)\\), marginal PDFs  \\(X\\)  \\(Y\\), denoted \\(f_X(x)\\), \\(f_Y(y)\\) respectively, \n\\[\n   f_X(x) = \\int_{-\\infty}^{\\infty}f(x,y) \\, dy\n   \\quad\\text{}\\quad\n   f_Y(y) = \\int_{-\\infty}^{\\infty}f(x,y) \\, dx.\n\\]Example 4.10  (Bivariate continuous marginal distributions) joint probability density functions  \\(X\\)  \\(Y\\) \n\\[\n   f(x, y) =\n   \\left\\{\n   \\begin{array}{ll}\n       \\frac{1}{3} (3x^2 + xy), & 0 \\leq x \\leq 1, \\, 0 \\leq y \\leq 2;\\\\\n       0 & \\text{ elsewhere.}\n   \\end{array}\n   \\right.\n\\]\nmarginal probability density functions  \\(X\\) \n\\[\\begin{align*}\n   f_X(x)\n   = \\int_0^2\\left(x^2 + \\frac{xy}{3}\\right) dy\n   &= \\left.x^2y + \\frac{xy^2}{6}\\right|_{y = 0}^2\\\\\n   &= 2x^2 + \\frac{2x}{3}\\quad\\text{$0 \\leq x \\leq 1$}.\n\\end{align*}\\]\nAlso,\n\\[\n   f_Y(y)\n   = \\int_0^1\\left(x^2 + \\frac{xy}{3}\\right)dx\n   = \\left.\\frac{1}{3}x^3 + \\frac{1}{6}x^2y\\right|_{x = 0}^1.\n\\]\n\\(\\displaystyle f_Y(y) = \\frac{1}{6}(2 + y)\\), \\(0 \\leq y \\leq 2\\).Consider computing \\(\\Pr(Y < X)\\); see Fig. 4.3; \n\\[\\begin{align*}\n   \\Pr(Y < X)\n   &= \\int \\!\\!\\int_{\\substack{(x, y) \\\\\\ y < x}} f(x,y) \\, dx \\, dy \\\\\n   &= \\frac{1}{3}\\int_0^1 \\int_y^1(3x^2 + xy) \\, dx \\, dy\\\\\n   &= \\frac{1}{3} \\int_0^1\\left. x^3 + \\frac{1}{2}x^2y\\right|_y^1 dy\\\\\n   &= \\frac{1}{3} \\int_0^1(1 + \\frac{1}{2}y - \\frac{3}{2}y^3) \\, dy = \\frac{7}{24}.\n\\end{align*}\\]\nFIGURE 4.3: region \\(Y < X\\).\nExample 4.11  (Bivariate discrete marginal distributions) Recall Example 4.5, two dice rolled.\ncan find marginal distributions  \\(X\\)  \\(Y\\) (Table 4.5).\nprobabilities first row (\\(Y = 0\\)), instance, summed appear first term final column; marginal distribution \\(Y = 0\\).\nSimilarly rows.Recalling  \\(X\\) number times  rolled two dice thrown, distribution  \\(X\\) \\(\\text{Bin}(2,  1/6\\)); probabilities given last row table agree .\n,\n\\[\n  \\Pr(X = x) = \\binom{2}{x}\\left(\\frac{1}{6}\\right)^x \\left(\\frac{5}{6}\\right)^{2 - x}\n\\]\n\\(x = 0, 1, 2\\).\ncourse, distribution  \\(Y\\) .\nTABLE 4.5: marginal distributions.\nExample 4.12  (Bivariate discrete marginal distributions) Consider random process Example 4.8.\nTable 4.2, marginal distribution  \\(X\\) found simply summing values  \\(Y\\) table.\n\\(x = 0\\),\n\\[\n   p_{X}(0) = \\sum_{y} p_{X, Y}(0, y) = 1/24 + 1/24 + 1/24 +\\dots = 6/24.\n\\]\nLikewise,\n\\[\\begin{align*}\n   p_{X}(1) &= \\sum_{y} p_{X, Y}(1, y) = 6/12;\\\\\n   p_{X}(2) &= \\sum_{y} p_{X, Y}(2, y) = 6/24.\n\\end{align*}\\]\nmarginal distribution  \\(X\\) \n\\[\n   p_{X}(x) =\n   \\begin{cases}\n     1/4 & \\text{$x = 0$};\\\\\n     1/2 & \\text{$x = 1$};\\\\\n     1/4 & \\text{$x = 2$};\\\\\n     0 & \\text{otherwise}.\\\\\n  \\end{cases}\n\\]\nequivalent adding row probabilities Table 4.2.\nexample, marginal distribution easily found total column Table 4.2.","code":""},{"path":"ChapBivariate.html","id":"ConditionalDistributions","chapter":"4 Bivariate distributions","heading":"4.5 Conditional distributions","text":"Consider \\((X, Y)\\) joint probability function Example 4.2, marginal distributions  \\(X\\)  \\(Y\\) shown Table 4.6.\nTABLE 4.6: joint distribution marginal distributions.\nSuppose want evaluate conditional probability \\(\\Pr(X = 1 \\mid Y = 1)\\).\nuse \\(\\Pr(\\mid B) = \\Pr(\\cap B)/\\Pr(B)\\).\n\n\\[\n   \\Pr(X = 1 \\mid Y = 1)\n   = \\frac{\\Pr(X = 1, Y = 1)}{\\Pr(Y = 1)}\n   = \\frac{1/3}{4/9}\n   = \\frac{3}{4}.\n\\]\n, \\(x\\\\mathcal{R}_X\\) find \\(\\Pr(X = x, Y = 1)\\) conditional distribution  \\(X\\) given \\(Y = 1\\).Definition 4.7  (Bivariate discrete conditional distributions) discrete random vector \\((X, Y)\\) probability function \\(p_{X, Y}(x, y)\\) conditional probability distribution  \\(X\\) given \\(Y = y\\) defined \n\\[\\begin{align}\n   p_{X \\mid Y}(x \\mid Y = y)\n   &= \\Pr(X = x \\mid Y = y)\\\\\n   &= \\frac{\\Pr(X = x, Y = y)}{\\Pr(Y = y)}\\\\\n   &= \\frac{p_{X, Y}(x, y)}{p_Y(y)}\n\\end{align}\\]\n\\(x \\\\mathcal{R}_X\\) provided \\(p_Y(y) > 0\\).continuous case analogous.Definition 4.8  (Bivariate continuous marginal distributions) \\((X, Y)\\) continuous \\(2\\)-dimensional random variable joint PDF \\(f_{X, Y}(x, y)\\) respective marginal probability density functions \\(f_X(x)\\), \\(f_Y(y)\\), conditional probability distribution  \\(X\\) given \\(Y = y\\) defined \n\\[\\begin{equation}\n   f_{X \\mid Y}(x \\mid Y = y)\n   = \\frac{f_{X, Y}(x, y)}{f_Y(y)}\n\\end{equation}\\]\n\\(x \\\\mathcal{R}_X\\) provided \\(f_Y(y) > 0\\).conditional probability density functions satisfy requirements univariate PDF; , \\(f_{X \\mid Y}(x \\mid y) \\ge 0\\)  \\(x\\) \\(\\int_0^\\infty f_{X\\mid Y}(x\\mid y)\\,dx = 1\\).Example 4.13  (Bivariate continuous marginal distributions) Example 4.10, joint PDF  \\(X\\)  \\(Y\\) considered \n\\[\n   f_{X,Y}(x,y) =\n   \\begin{cases}\n      \\frac{1}{3}(3x^2 + xy)  & \\text{$0 \\leq x \\leq 1$ $0 \\leq y \\leq 2$};\\\\\n      0                       & \\text{elsewhere}.\n   \\end{cases}\n\\]\nmarginal probability density functions  \\(X\\)  \\(Y\\) \n\\[\\begin{align*}\n   f_X(x) &= 2x^2 + \\frac{2}{3}x \\quad\\text{$0 \\leq x \\leq 1$}; \\\\\n   f_Y(y) &= \\frac{1}{6}(2 + y) \\quad \\text{$0 \\leq y \\leq 2$}.\n\\end{align*}\\]\nHence, conditional distribution \\(X \\mid Y = y\\) \n\\[\n   f_{X\\mid Y}(x \\mid Y = y)\n   = \\frac{(3x^2 + xy)/3}{(2 + y)/6}\n   = \\frac{2x(3x + y)}{2 + y} \\quad\\text{$0 \\leq x \\leq 1$},\n\\]\nconditional distribution \\(Y \\mid X = x\\) \n\\[\n   f_{Y \\mid X}(y \\mid X = x)\n   = \\frac{3x + y}{2(3x + 1)}\\quad\\text{$0 \\leq y \\leq 2$}.\n\\]\nconditional density functions valid density functions (verify!).marginal distribution  \\(Y\\), two conditional distributions  \\(Y\\) (given \\(X = 0.1\\) \\(X = 0.9\\)) shown Fig. 4.4.\nFIGURE 4.4: marginal distribution \\(Y\\) (left panel), conditional distribution \\(Y\\) \\(X = 0\\) (centre panel) \\(X = 1\\) (right panel).\ninterpret conditional distribution, example \\(f_{X \\mid Y = y}(x \\mid y)\\), consider slicing surface \\(f_{X, Y}(x, y)\\) plane \\(y = c\\) say,  \\(c\\) constant\n(see ).\nintersection plane surface, proportional \\(1\\)-dimensional PDF.\n\\(f_{X, Y}(x, c)\\), , general, density function since area curve \\(f_Y(c)\\).\nDividing constant \\(f_Y(c)\\) ensures area \\(\\displaystyle\\frac{f_{X,Y}(x,c)}{f_Y(c)}\\) one.\none-dimensional PDF,  \\(X\\) given \\(Y = c\\); \\(f_{X \\mid Y = c}(x\\mid c)\\).\nFIGURE 4.5: bivariate distribution.\n\nFIGURE 4.6: bivariate distribution, sliced \\(Y =  1\\), showing conditional distribution \\(X\\) \\(Y = 0.5\\).\nExample 4.14  (Bivariate discrete conditional distributions) Consider random process Example 4.4.\nconditional distribution  \\(Y\\) given \\(X = 0\\) can found Table 4.6.\nFirst, \\(p_{X}(x)\\), needed, found Example 4.12.\n,\n\\[\\begin{align*}\n   p_{Y\\mid X}(y\\mid X = 0)\n   &= \\frac{p_{X, Y}(0, y)}{p_{X}(0)} \\\\\n   &= \\frac{p_{X, Y}(0, y)}{1/4},\n\\end{align*}\\]\ncan deduce\n\\[\n   p_{Y \\mid X = 0}(y \\mid X = 0) =\n   \\begin{cases}\n      \\frac{1/24}{1/4} = 1/6 & \\text{$y = 1$};\\\\\n      \\frac{1/24}{1/4} = 1/6 & \\text{$y = 2$};\\\\\n      \\frac{1/24}{1/4} = 1/6 & \\text{$y = 3$};\\\\\n      \\frac{1/24}{1/4} = 1/6 & \\text{$y = 4$};\\\\\n      \\frac{1/24}{1/4} = 1/6 & \\text{$y = 5$};\\\\\n      \\frac{1/24}{1/4} = 1/6 & \\text{$y = 6$}.\\\\\n   \\end{cases}\n\\]\nconditional distribution \\(p_{Y\\mid X = x}(y\\mid x)\\) probability function  \\(Y\\) (verify!).\nSince \\(Y\\) number top face die, exactly expect.","code":""},{"path":"ChapBivariate.html","id":"BivariateIndependentRVs","chapter":"4 Bivariate distributions","heading":"4.6 Independent random variables","text":"Recall events \\(\\)  \\(B\\) independent , ,\n\\[\n   \\Pr(\\cap B) = \\Pr()\\Pr(B).\n\\]\nanalogous definition applies random variables.Definition 4.9  (Independent random variables) random variables \\(X\\)  \\(Y\\) joint distribution function \\(F_{X, Y}\\) marginal distribution functions \\(F_X\\)  \\(F_Y\\) independent , ,\n\\[\\begin{equation}\n   F_{X, Y}(x, y) = F_X(x) \\times F_Y(y)\n\\end{equation}\\]\n \\(x\\)  \\(y\\). \\(X\\)  \\(Y\\) independent dependent, independent.following theorem often used establish independence dependence random variables.\nproof omitted.Theorem 4.1  discrete random variables \\(X\\)  \\(Y\\) joint probability function \\(p_{X, Y}(x, y)\\) marginal distributions \\(p_X(x)\\) \\(p_Y(y)\\) independent , ,\n\\[\\begin{equation}\n   p_{X, Y}(x, y) = p_X(x) \\times p_Y(y) \\text{ every }(x, y) \\\\mathcal{R}_{X \\times Y}.\n      \\tag{4.5}\n\\end{equation}\\]\ncontinuous random variables \\((X, Y)\\) joint PDF \\(f_{X, Y}\\) marginal PDFs \\(f_X\\) \\(f_Y\\) independent , ,\n\\[\\begin{equation}\n   f_{X, Y}(x, y) = f_X(x)\\times f_Y(y)\n\\end{equation}\\]\n \\(x\\)  \\(y\\).show independence continuous random variables (analogously discrete random variables) must show \\(f_{X, Y}(x, y) = f_X(x)\\times f_Y(y)\\) pairs \\((x, y)\\).\n\\(f_{X, Y}(x, y)\\neq f_X(x)\\times f_Y(y)\\), even one particular pair \\((x, y)\\),  \\(X\\)  \\(Y\\) dependent.Example 4.15  (Bivariate discrete: Independence) random variables \\(X\\)  \\(Y\\) joint probability distribution shown Table 4.7.\nSumming across rows, marginal probability function  \\(Y\\) :\n\\[\n   p_Y(y) =\n   \\begin{cases}\n      1/6  & \\text{$y = 1$};\\\\\n      1/3  & \\text{$y = 2$};\\\\\n      1/2  & \\text{$y = 3$}.\n   \\end{cases}\n\\]\ndetermine  \\(X\\)  \\(Y\\) independent, marginal probability function  \\(X\\) also needed:\n\\[\n   p_X(x) =\n   \\begin{cases}\n      1/5  & \\text{$x = 1$};\\\\\n      1/5  & \\text{$x = 2$};\\\\\n      2/5  & \\text{$x = 3$};\\\\\n      1/5  & \\text{$x = 4$}.\n   \\end{cases}\n\\]\nClearly, Eq. (4.5) satisfied pairs \\((x, y)\\),  \\(X\\)  \\(Y\\) independent.\nTABLE 4.7: joint probability function.\nExample 4.16  (Bivariate continuous: independence) Consider random variables \\(X\\)  \\(Y\\) joint PDF\n\\[\n   f_{X, Y}(x, y)\n   = \\begin{cases}\n      4xy & \\text{$0 < x < 1$ $0 < y < 1 $}\\\\\n      0   & \\text{elsewhere}.\\\\\n   \\end{cases}\n\\]\nshow  \\(X\\)  \\(Y\\) independent, marginal distributions  \\(X\\)  \\(Y\\) needed.\nNow\n\\[\n   f_X(x)\n   = \\int_0^1 4xy \\, dy = 2x\\quad\\text{$0 < x < 1$}.\n\\]\nSimilarly \\(f_Y(y) = 2y\\) \\(0 < y < 1\\).\nThus \\(f_X(x) \\cdot f_Y(y) = f(x,y)\\),  \\(X\\)  \\(Y\\) independent.Example 4.17  (Bivariate discrete: independence) Consider random process Example 4.4.\nmarginal distribution  \\(X\\) found Example 4.12.\nmarginal distribution  \\(Y\\) (check!)\n\\[\n   p_{Y}(y) =\n   \\begin{cases}\n      \\frac{1/24}{1/4} = 1/6 & \\text{$y = 1$};\\\\\n      \\frac{1/24}{1/4} = 1/6 & \\text{$y = 2$};\\\\\n      \\frac{1/24}{1/4} = 1/6 & \\text{$y = 3$};\\\\\n      \\frac{1/24}{1/4} = 1/6 & \\text{$y = 4$};\\\\\n      \\frac{1/24}{1/4} = 1/6 & \\text{$y = 5$};\\\\\n      \\frac{1/24}{1/4} = 1/6 & \\text{$y = 6$}.\\\\\n   \\end{cases}\n\\]\ndetermine  \\(X\\)  \\(Y\\) independent,  \\(x\\)  \\(y\\) pair must considered.\nexample, see\n\\[\\begin{align*}\n   p_{X}(0) \\times p_{Y}(1) = 1/4 \\times 1/6 = 1/24 &= p_{X, Y}(0, 1);\\\\\n   p_{X}(0) \\times p_{Y}(2) = 1/4 \\times 1/6 = 1/24 &= p_{X, Y}(0, 2);\\\\\n   p_{X}(1) \\times p_{Y}(1) = 1/2 \\times 1/6 = 1/12 &= p_{X, Y}(1, 1);\\\\\n   p_{X}(2) \\times p_{Y}(1) = 1/4 \\times 1/6 = 1/24 &= p_{X, Y}(2, 1).\n\\end{align*}\\]\ntrue pairs,  \\(X\\)  \\(Y\\) independent random variables.\nIndependence , however, obvious description random process (Example 4.1), easily seen Table 4.2.Example 4.18  (Bivariate continuous: independence) Consider continuous random variables \\(X\\)  \\(Y\\) joint PDF\n\\[\n   f_{X, Y}(x, y)  =\n   \\begin{cases}\n      \\frac{2}{7}(x + 2y) & \\text{$0 < x < 1$ $1 < y < 2$};\\\\\n      0 & \\text{elsewhere.}\n   \\end{cases}\n\\]\nmarginal distribution  \\(X\\) \n\\[\n   f_{X}(x)\n   = \\int_1^2 \\frac{2}{7}(x + 2y)\\,dy\\\\\n   =   \\frac{2}{7}(x + 3)\n\\]\n\\(0 < x < 1\\) (zero elsewhere).\nLikewise, marginal distribution  \\(Y\\) \n\\[\n   f_{Y}(y)\n   = \\frac{2}{7}(x^2/2 + 2 x y)\\Big|_{x = 0}^1\n   = \\frac{1}{7}(1 + 4y)\n\\]\n\\(1 < y < 2\\) (zero elsewhere).\n(marginal distributions must valid density functions; verify!)\nSince\n\\[\n   f_{X}(x) \\times f_{Y}(y) = \\frac{2}{49}(x + 3)(1 + 4y) \\ne f_{X, Y}(x, y),\n\\]\nrandom variables \\(X\\)  \\(Y\\) independent.conditional distribution  \\(X\\) given \\(Y = y\\) \n\\[\\begin{align*}\n   f_{X \\mid Y = y}(x \\mid y)\n   &= \\frac{ f_{X, Y}(x, y)}{ f_{Y}(y)} \\\\\n   &= \\frac{ (2/7) (x + 2y)}{ (1/7)(1 + 4y)}\\\\\n   &= \\frac{ 2 (x + 2y)}{ 1 + 4y}\n\\end{align*}\\]\n\\(0 < x < 1\\) given value \\(1 < y < 2\\).\n(, conditional density must valid probability density function.)\n, example,\n\\[\n   f_{X \\mid Y}(x\\mid Y = 1.5)\n   = \\frac{ 2 (x + 2\\times 1.5)}{ 1 + (4\\times 1.5)}\n   = \\frac{2}{7}(x + 3)\n\\]\n\\(0 < x < 1\\) zero elsewhere.\n,\n\\[\n   f_{X\\mid Y}(x \\mid Y = 1)\n   = \\frac{ 2 (x + 2\\times 1)}{ 1 + (4\\times 1)}\n   = \\frac{2}{5}(x + 2)\n\\]\n\\(0 < x < 1\\) zero elsewhere.\nSince distribution  \\(X\\) depends given value  \\(Y\\), \\(X\\)  \\(Y\\) independent.Example 4.19  (Bivariate continuous: independence) Consider two continuous random variables \\(X\\)  \\(Y\\) joint probability function\n\\[\n   f_{X, Y}(x, y)=\n   \\begin{cases}\n      2(x + y) & \\text{$0 < x < y < 1$};\\\\\n      0        & \\text{elsewhere}.\n   \\end{cases}\n\\]\ndiagram region  \\(X\\)  \\(Y\\) defined shown Fig. 4.7.\ndetermine  \\(X\\)  \\(Y\\) independent, two marginal distributions needed.\nexample:\n\\[\n   f_{X}(x) = 1 + 2x - 3x^2\\quad\\text{$0 < x < 1$}.\n\\]\nSince distribution  \\(X\\) depends value  \\(Y\\), means \\(X\\)  \\(Y\\) independent.\nFIGURE 4.7: region \\(f_{X, Y}(x, y)\\) defined.\n","code":""},{"path":"ChapBivariate.html","id":"MixedJointPF","chapter":"4 Bivariate distributions","heading":"4.7 Mixed joint probability functions","text":"far, bivariate distributions included cases two discrete two continuous random variables.\nHowever, also possible one variable, say \\(X\\), continuous , say \\(Y\\), discrete.Definition 4.10  (Mixed bivariate probability function) Let \\((X, Y)\\) random vector  \\(X\\) continuous range space \\(S_X \\subseteq \\mathbb{R}\\),  \\(Y\\) discrete range space \\(S_Y = \\{y_1, y_2, \\dots\\}\\).\njoint density–mass function \\((X, Y)\\) defined \n\\[\n  \\mathcal{R}_{X, Y} = S_X \\times S_Y\n\\]\n\n\\[\n  f_{X, Y}(x, y_j)\n  = f_{X \\mid Y}(x \\mid y_j) \\cdot p_Y(y_j),\n\\]\n:\\(p_Y(y_j) = \\Pr(Y = y_j)\\) probability mass function  \\(Y\\), \\(f_{X \\mid Y}(x \\mid y_j)\\) conditional density function  \\(X\\) given \\(Y = y_j\\).Like probability functions, joint density–mass function non-negative:\n\\[\n  f_{X, Y}(x, y_j) \\geq 0 \\quad \\text{$(x, y_j)\\\\mathcal{R}_{X, Y}$}.\n\\]\naddition, total probability one:\n\\[\n  \\sum_{j\\S_Y} \\int_{S_X} f_{X,Y}(x, y_j)\\, dx = 1.\n\\]probability event \\(\\subseteq S_X\\) \\(y_j \\S_Y\\) \n\\[\n  \\Pr(X \\, Y = y_j) = \\int_A f_{X,Y}(x, y_j)\\, dx.\n\\]Example 4.20  (Mixed random variable) Suppose random vector \\((X, Y)\\) defined \\(Y \\\\{1, 2\\}\\) \n\\[\n  p_Y(1) = 0.4, \\quad p_Y(2) = 0.6.\n\\]\nConditional \\(Y = 1\\), probability density function  \\(X\\) \n\\[\n  f_{X \\mid Y}(x \\mid y = 1) =\n  \\begin{cases}\n    1 & 0 \\le x \\le 1, \\\\\n    0 & \\text{otherwise,}\n  \\end{cases}\n\\]\nconditional \\(Y = 2\\), probability density function  \\(X\\) \n\\[\n  f_{X \\mid Y}(x \\mid y = 2)\n  =\n  \\begin{cases}\n    1/2   & 0 \\le x \\le 2, \\\\\n    0     & \\text{otherwise.}\n  \\end{cases}\n\\]\njoint density–mass function \n\\[\n  f_{X, Y}(x, y)\n  = f_{X \\mid Y}(x \\mid y) \\cdot p_Y(y)\n\\]\n, explicitly:\n\\[\n  f_{X, Y}(x, 1) =\n  \\begin{cases}\n    0.4  & 0 \\le x \\le 1, \\\\\n    0,   & \\text{otherwise;}\n\\end{cases}\n\\quad\nf_{X, Y}(x, 2)\n=\n\\begin{cases}\n  0.3 & 0 \\le x \\le 2, \\\\\n  0   & \\text{otherwise.}\n\\end{cases}\n\\]\nNotice \n\\[\n  \\sum_{y \\\\{1,2\\}} \\int_{-\\infty}^{\\infty} f_{X,Y}(x,y)\\, dx\n  = \\int_0^1 0.4 \\, dx + \\int_0^2 0.3 \\, dx\n  = 0.4 + 0.6 = 1\n\\]\nrequired., instance, can find \\(\\Pr(X \\le 0.5, Y = 2)\\):\n\\[\n  \\Pr(X \\le 0.5, Y = 2)\n  = \\int_0^{0.5} f_{X, Y}(x, 2)\\, dx\n  = \\int_0^{0.5} 0.3\\, dx\n  = 0.15.\n\\]","code":""},{"path":"ChapBivariate.html","id":"BivariateNumerical","chapter":"4 Bivariate distributions","heading":"4.8 Numerical approaches","text":"","code":""},{"path":"ChapBivariate.html","id":"ChapBivariateExercises","chapter":"4 Bivariate distributions","heading":"4.9 Exercises","text":"Selected answers appear Sect. E.4.Exercise 4.1  discrete random variables \\(X\\)  \\(Y\\) joint probability function shown Table 4.8.Determine \\(\\Pr(X = 1, Y = 2)\\)Determine \\(\\Pr(X + Y \\le 1)\\).Compute \\(\\Pr(X > Y)\\).Find marginal probability function  \\(X\\).Find probability function \\(Y \\mid X = 1\\).\nTABLE 4.8: joint probability mass function \\(X\\) \\(Y\\).\nExercise 4.2  discrete random variables \\(X\\)  \\(Y\\) joint probability function shown Table 4.9.Determine \\(\\Pr(X < 3, Y = 0)\\)Determine \\(\\Pr(X + Y > 3)\\).Compute \\(\\Pr(X > (Y/2) )\\).Find marginal probability function  \\(Y\\).Find marginal probability function  \\(X\\).Find probability function \\(Y \\mid X = 1\\).\nTABLE 4.9: joint probability mass function \\(X\\) \\(Y\\).\nExercise 4.3  continuous random variables \\(X\\)  \\(Y\\) joint probability function\n\\[\n  f_{X, Y}(x, y)\n  = k\\,(x + y^2)\n\\]\n\\(0 < x < 1\\) \\(0 < y < 2\\).Determine value  \\(k\\).Compute \\(\\Pr(X > 1/2, Y > 1)\\).Compute \\(\\Pr(X + Y > 1)\\).Find marginal probability function  \\(X\\).Find marginal probability function  \\(Y\\).Find probability function \\(Y \\mid X\\).Find probability function \\(Y \\mid X = 1\\).Find probability function \\(X \\mid Y\\).Find probability function \\(X \\mid Y = 1\\). \\(X\\)  \\(Y\\) independent random variables?\nExplain.Exercise 4.4  continuous random variables \\(X\\)  \\(Y\\) joint probability function\n\\[\n  f_{X, Y}(x, y)\n  = k\\,(2 + x - y)\n\\]\n\\(1 < x < 2\\) \\(-1 < y < 1\\).Determine value  \\(k\\).Compute \\(\\Pr(X > 1, Y > 0)\\).Compute \\(\\Pr(X + Y \\ge 1)\\).Find marginal probability function  \\(X\\).Find marginal probability function  \\(Y\\).Find probability function \\(Y \\mid X\\).Find probability function \\(Y \\mid X = 1\\).Find probability function \\(X \\mid Y\\).Find probability function \\(X \\mid Y = 1\\). \\(X\\)  \\(Y\\) independent random variables?\nExplain.Exercise 4.5  ??? MIXEDExercise 4.6  ??? MIXEDExercise 4.7  pair random variables \\((X, Y)\\) joint probability function given \n\\[\n  \\Pr(X = x, Y = y) = k\\,|x - y|\n\\]\n\\(x = 0, 1, 2\\) \\(y = 1, 2, 3\\).Find value \\(k\\).Construct table probabilities distribution.Find \\(\\Pr(X \\le 1, Y = 3)\\).Find \\(\\Pr(X + Y \\ge 3)\\).Exercise 4.8  value  \\(k\\) \\(f(x,y) = kxy\\) (\\(0 \\le x \\le 1\\); \\(0 \\le y \\le 1\\), valid joint PDF?, find \\(\\Pr(X \\le x_0, Y\\le y_0)\\).Hence evaluate \\(\\Pr\\left(X \\le (3/8), Y \\le (5/8) \\right)\\).Exercise 4.9  random vector \\((X, Y)\\), conditional PDF  \\(Y\\) given \\(X = x\\) \n\\[\n  f_{Y \\mid X = x}(y\\mid x) = \\frac{2(x + y)}{2x + 1},\n\\]\n\\(0 < y <1\\).\nmarginal PDF  \\(X\\) given \n\\[\n  g_X(x) = x + \\frac{1}{2}\n\\]\n\\(0 <x < 1\\).Find \\(F_Y(y \\mid x)\\) hence evaluate \\(\\Pr(Y < 3/4 \\mid  X = 1/3)\\).Find joint PDF, \\(f_{X, Y}(x, y)\\),  \\(X\\)  \\(Y\\).Find \\(\\Pr(Y < X)\\).Exercise 4.10  Consider random process fair coin tossed twice.\nLet \\(X\\) number heads observed two tosses,  \\(Y\\) number heads first toss coin.Construct table joint probability function  \\(X\\)  \\(Y\\).Determine marginal probability function  \\(X\\).Determine conditional distribution  \\(X\\) given one head appeared first toss.Determine variables \\(X\\)  \\(Y\\) independent , justifying answer necessary calculation argument.Exercise 4.11  Two fair, six-sided dice rolled, numbers top faces observed.\nEvent \\(\\) maximum two numbers, Event \\(B\\) minimum two numbers.\n, define \\(C\\)  \\(0\\) maximum odd,  \\(1\\) otherwise; define \\(D\\)  \\(0\\) minimum divisible three,  \\(1\\) otherwise.Construct joint probability function  \\(C\\)  \\(D\\).Exercise 4.12  Consider joint PDF\n\\[\n   f_{X, Y}(x, y) =\n   \\begin{cases}\n      c x(y + 1)  & \\text{$x + y < 2$ $x > 0$ $y > 0$};\\\\\n      0           & \\text{elsewhere}.\n   \\end{cases}\n\\]Draw region joint PDF defined.Compute value  \\(c\\).Compute \\(P(Y < 1 \\mid X > 1)\\).Compute \\(P(Y < 1 \\mid X > 0.25)\\).Compute \\(\\Pr(Y < 1)\\)Exercise 4.13  Consider joint PDF\n\\[\n   f_{X, Y}(x, y) =\n   \\begin{cases}\n      k ( 1 - x) y & \\text{region $R$ };\\\\\n      0            & \\text{elsewhere},\n   \\end{cases}\n\\]\nregion \\(R\\) shown Fig. 4.8 (left panel).Determine value  \\(k\\).Compute \\(\\Pr(X > Y)\\).Compute \\(\\Pr(X > 0.5)\\).Exercise 4.14  Consider joint PDF\n\\[\n   f_{X, Y}(x, y) =\n   \\begin{cases}\n      k ( x + 2y) y & \\text{region $$ };\\\\\n      0             & \\text{elsewhere},\n   \\end{cases}\n\\]\nregion \\(\\) shown Fig. 4.8 (right panel).Determine value  \\(k\\).Compute \\(\\Pr(X > Y)\\).Compute \\(\\Pr(X > 0.5)\\).\nFIGURE 4.8: region \\(R\\) (left) region \\(\\) (right).\n","code":""},{"path":"ChapExpectation.html","id":"ChapExpectation","chapter":"5 Mathematical expectation","heading":"5 Mathematical expectation","text":"Upon completion chapter, able :understand concept definition mathematical expectation.compute expectations random variable, functions random variable linear functions random variable.compute variance higher moments random variable.derive moment-generating function random variable linear functions random variable.find moments random variable moment-generating function.state use Tchebysheff’s inequality.","code":""},{"path":"ChapExpectation.html","id":"ExpectedValue","chapter":"5 Mathematical expectation","heading":"5.1 Expected values","text":"random variables random, knowing outcome one realisation random process possible.\nInstead, can talk might expect happen, might happen average.idea mathematical expectation.\nusual terms, mathematical expression probability distribution random variable mean random variable.\nMathematical expectation goes far beyond just computing means, begin idea mean easily understood.definition looks different detail discrete, continuous mixed random variables, intention .Definition 5.1  (Expectation) expectation expected value (mean) random variable \\(X\\) written \\(\\operatorname{E}[X]\\) (\\(\\mu\\), \\(\\mu_X\\) distinguish random variables).discrete random variable \\(X\\) PMF \\(p_X(x)\\), expected value \n\\[\n   \\operatorname{E}[X] =\n        \\sum_{x\\\\mathcal{R}_X} x\\, p_X(x).\n\\]\ncontinuous random variable \\(X\\) PDF \\(f_X(x)\\), expected value \n\\[\n   \\operatorname{E}[X] =\n        \\int_{-\\infty}^\\infty x\\, f_X(x).\n\\]mixed random variable \\(X\\), expected value combination two results, discrete continuous components  \\(\\mathcal{R}_X\\); ,\n\\[\n   \\operatorname{E}[X] =\n     \\sum_{x_i} x_i \\, p_X(x_i) + \\int_{-\\infty}^\\infty x \\, f_X(x) \\, dx,\n\\]\n\\(p_X(x)\\) probability mass function discrete points \\(x_i\\\\mathcal{R}_X\\), \\(f_X(x)\\) probability density function (PDF) continuous regions \\(x\\\\mathcal{R}_X\\) PDF \\(f_X(x)\\).rest chapter, case mixed random variable \\(X\\) explicitly discussed; however, results remain combination discrete case discrete points  \\(\\mathcal{R}_X\\) continuous case continuous component  \\(\\mathcal{R}_X\\).Effectively \\(\\operatorname{E}[X]\\) weighted average points  \\(\\mathcal{R}_X\\), weights probabilities value \\(x\\\\mathcal{R}_X\\) discrete case probability densities continuous case.Example 5.1  (Expectation discrete variables) Consider discrete random variable \\(U\\) probability function\n\\[\n   p_U(u) = \\begin{cases}\n               (u^2 + 1)/5 & \\text{$u = -1, 0, 1$};\\\\\n               0 & \\text{elsewhere},\n           \\end{cases}\n\\]\n\\(\\mathcal{R}_U = \\{-1, 0, 1\\}\\).\nexpected value  \\(U\\) \n\\[\\begin{align*}\n   \\operatorname{E}[U]\n   &= \\sum_{\\mathcal{R}_U} u\\, p_U(u) \\\\\n   &= \\sum_{\\mathcal{R}_U} u \\times\\left( \\frac{u^2 + 1}{5} \\right) \\\\\n   &= \\left( -1 \\times \\frac{(-1)^2 + 1}{5} \\right ) +\n       \\left( 0 \\times \\frac{(0)^2  + 1}{5} \\right ) +\n       \\left( 1 \\times \\frac{(1)^2  + 1}{5} \\right ) \\\\\n   &= -2/5 + 0 + 2/5 = 0.\n\\end{align*}\\]\nexpected value  \\(U\\)  \\(\\operatorname{E}[U] = 0\\).Example 5.2  (Expectation continuous variables) Consider continuous random variable \\(X\\) PDF\n\\[\n   f_X(x) = \\begin{cases}\n               x/4 & \\text{$1 < x < 3$};\\\\\n               0 & \\text{elsewhere}.\n            \\end{cases}\n\\]\nexpected value  \\(X\\) \n\\[\\begin{align*}\n   \\operatorname{E}[X]\n   &= \\int_{-\\infty}^\\infty x\\, f_X(x) \\, dx\n    = \\int_1^3 x(x/4)\\, dx\\\\\n   &= \\left.\\frac{1}{12} x^3\\right|_1^3 = 13/6.\n\\end{align*}\\]\nexpected value  \\(X\\) \\(\\operatorname{E}[X] = 13/6\\).Example 5.3  (Expectation mixed variables) Consider continuous random variable \\(W\\) probability function\n\\[\n  f_W(w) =\n  \\begin{cases}\n     1/2       & \\text{$w = 0$};\\\\\n     \\exp(-2w) & \\text{$w > 0$};\\\\\n     0         & \\text{elsewhere},\n  \\end{cases}\n\\]\n\\(p_W(w) = 1/2\\) \\(w = 0\\), \\(f_W(w) = \\exp(-2w)\\) \\(w > 0\\).\nexpected value  \\(W\\) \n\\[\\begin{align*}\n   \\operatorname{E}[W]\n   &= \\overbrace{\\sum_{w = 0} w\\, p_W(w)}^{\\text{Discrete component}} \\quad + \\quad \\overbrace{\\int_{-\\infty}^\\infty w\\, f_W(w)\\, dw}^{\\text{Continuous component}}\\\\\n   &= \\sum_{w = 0} 0\\times (1/2)\\quad + \\quad \\int_{0}^\\infty w\\times \\exp(-2w)\\, dw\\\\\n   &= 0 \\quad + \\quad 1/4\\\\\n   &= 1/4.\n\\end{align*}\\]\nexpected value  \\(W\\) \\(\\operatorname{E}[W] = 1/4\\).Example 5.4  (Expectation coin toss) Consider tossing coin counting number tails.\nLet random variable  \\(T\\).\nprobability function \n\\[\n   p_T(t) = \\begin{cases}\n               0.5 & \\text{$t = 0$ $t = 1$};\\\\\n               0   & \\text{otherwise.}\n            \\end{cases}\n\\]\nexpected value  \\(T\\) \n\\[\\begin{align*}\n   \\operatorname{E}[T]\n   &= \\sum_{= 1}^2 t\\, p_T(t)\\\\\n   &= \\Pr(T = 0) \\times 0 \\quad + \\quad \\Pr(T = 1) \\times 1\\\\\n   &= (0.5 \\times 0) \\qquad + \\qquad (0.5 \\times 1) = 0.5.\n\\end{align*}\\]\ncourse, \\(0.5\\) tails can never actually observed practice one toss.\nsilly round () say expected number tails one toss coin one (zero).\nexpected value  \\(0.5\\) simply means large number repetitions random process, tail expected occur half repetitions.Example 5.5  (Mean defined) Consider distribution  \\(Z\\), probability density function\n\\[\n   f_Z(z) =\n   \\begin{cases}\n      z^{-2} & \\text{$z \\ge 1$};\\\\\n      0      & \\text{elsewhere}\n   \\end{cases}\n\\]\nFig. ??.\nexpected value  \\(Z\\) \n\\[\n   \\operatorname{E}[Z] = \\int_1^{-\\infty} z \\frac{1}{z^2}\\, dz = \\int_1^\\infty \\frac{1}{z} = -\\log z \\Big|_1^\\infty.\n\\]\nHowever, \\(\\displaystyle\\lim_{z\\\\infty}\\, -\\log z \\\\infty\\).\nexpected value \\(\\operatorname{E}[Z]\\) undefined.\nFIGURE 5.1: probability function random variable \\(Z\\). mean defined.\n","code":"\n\n\n# Define values of z > 1 to plot over\nz <- seq(1, 6, length.out = 100)\n\n# Plot for z > 1\nplot(x = z, y = z^(-2),\n     type = \"l\", lwd = 2, las = 1,\n     xlim = c(0, 6), ylim = c(-0.025, 1),\n     xlab = expression( italic(z)),\n     ylab = \"Density\",\n     main = expression(paste(The~probability~\"function for\"~italic(Z))))\n\n# Line for x = 0 to x = 1\nlines( x = c(-1, 1),\n       y = c(0, 0),\n       lwd = 2) ### lwd = 2: Thicker line width\n\n# Dotted vertical line\nabline(v = 1, lty = 2, ### lty = 2: means 'dotted lines' \n       col = \"grey\") \n\n# Show open point\npoints(x = 1, y = 0,\n       pch = 1) ### pch = 1: open circle\n\n\n"},{"path":"ChapExpectation.html","id":"ExpectationFunction","chapter":"5 Mathematical expectation","heading":"5.2 Expectation of a function of a random variable","text":"\nmean can expressed terms mathematical expectation, mathematical expectation general concept.Let \\(X\\) discrete random variable probability function \\(p_X(x)\\), continuous random variable PDF \\(f_X(x)\\).\nAlso assume \\(g(X)\\) real-valued function  \\(X\\).\ncan define expected value \\(g(X)\\).Definition 5.2  (Expectation function random variable) expected value function \\(g(\\cdot)\\) random variable \\(X\\) written \\(\\operatorname{E}[ g(X)]\\).discrete random variable \\(X\\) wth PMF \\(p_X(x)\\), expected value \\(g(X)\\) \n\\[\n   \\operatorname{E}\\big[g(X)\\big)] = \\sum_{x\\\\mathcal{R}_X} g(x)\\, p_X(x).\n\\]\ncontinuous random variable \\(X\\) wth PDF \\(f_X(x)\\), expected value \\(g(X)\\) \n\\[\n   \\operatorname{E}\\big[g(X)\\big] = \\int_{-\\infty}^\\infty g(x)\\, f_X(x)\\,dx.\n\\]Example 5.6  (Expectation function discrete variable) Consider discrete random variable \\(U\\) probability function shown Example 5.1:\n\\[\n   p_U(u) = \\begin{cases}\n               (u^2 + 1)/5 & \\text{$u = -1, 0, 1$};\\\\\n               0 & \\text{elsewhere}.\n           \\end{cases}\n\\]\nSince \\(\\mathcal{R}_U = \\{-1, 0, 1\\}\\), \\(\\mathcal{R}_V = \\{ (-1)^2, 0^2, 1^2\\} = \\{0, 1\\}\\).\nexpected value \\(V = U^2\\), \\(g(U) = U^2\\), \n\\[\\begin{align*}\n   \\operatorname{E}[V] = \\operatorname{E}[g(U)]\n   &= \\sum_{\\mathcal{R}_U} g(u)\\, p_U(u) \\\\\n   &= \\left( (-1)^2 \\times \\frac{(-1)^2 + 1}{5} \\right ) +\n       \\left( 0^2 \\times \\frac{(0)^2  + 1}{5} \\right ) +\n       \\left( 1^2 \\times \\frac{(1)^2  + 1}{5} \\right ) \\\\\n   &= 2/5 + 0 + 2/5 = 4/5.\n\\end{align*}\\]\nexpected value \\(V = U^2\\)  \\(\\operatorname{E}[V] = 4/5\\).Example 5.7  (Expectation function continuous variable) Consider continuous random variable \\(X\\) probability density function shown Example 5.2:\n\\[\n   f_X(x) = \\begin{cases}\n               x/4 & \\text{$1 < x < 3$};\\\\\n               0 & \\text{elsewhere}.\n            \\end{cases}\n\\]\nexpected value \\(Y = \\sqrt{X}\\), \\(g(X) = \\sqrt{X}\\), \n\\[\\begin{align*}\n   \\operatorname{E}[Y] = \\operatorname{E}[ g(X) ]\n   &= \\int_{-\\infty}^\\infty g(x)\\, f_X(x) \\, dx\\\\\n   &= \\int_1^3 \\sqrt{x}\\times \\frac{x}{4}\\, dx\\\\\n   &= \\frac{9\\sqrt{3} - 1}{10}\\approx 1.458...\n\\end{align*}\\]\nexpected value \\(Y = \\sqrt{X}\\) \\(\\operatorname{E}[Y] = (9\\sqrt{3} - 1)/10\\).Importantly, expectation operator linear operator, stated .Theorem 5.1  (Expectation properties) random variable \\(X\\) constants \\(\\)  \\(b\\),\n\\[\n   \\operatorname{E}[aX + b] = \\operatorname{E}[X] + b.\n\\]Proof. Assume \\(X\\) discrete random variable probability function \\(p_X(x)\\).\nDef. 5.2 \\(g(X) = aX + b\\),\n\\[\n   \\operatorname{E}[aX + b] = \\sum_x (ax + b)\\, p_X(x) = \\sum_x p_X(x) + \\sum_x b\\, p_X(x) = \\operatorname{E}[X] + b,\n\\]\nusing \\(\\sum_x p_X(x) = 1\\).\n(proof continuous case similar, probability function PDF integrals replace summations.)Example 5.8  (Expectation function random variable) Consider random variable \\(Z = 2X\\)  \\(X\\) defined Example 5.2.\nUsing Theorem 5.1 \\(= 2\\) \\(b = 0\\), value \\(\\operatorname{E}[Z]\\) \n\\[\n      \\operatorname{E}[Z] = \\operatorname{E}[2X] = 2\\operatorname{E}[X] = 2 \\times 13/6 = 13/3.\n\\]","code":""},{"path":"ChapExpectation.html","id":"VarianceStdDev","chapter":"5 Mathematical expectation","heading":"5.3 The variance and standard deviation","text":"\nApart mean, important description random variable variability: quantifying values random variable dispersed.\nimportant measure variability variance.variance random variable measure variability random variable.\n(correct say ‘variance distribution random variable’ rather ‘variance random variable’, language commonly used.)\nsmall variance means observations nearly (.e., small variation); large variance means quite different.\nvariance can expressed function random variable.Definition 5.3  (Variance) variance random variable \\(X\\) (, distribution  \\(X\\)) \n\\[\n   \\operatorname{var}[X]  = \\operatorname{E}\\big[(X - \\mu)^2\\big]\n\\]\n\\(\\mu = \\operatorname{E}[X]\\).\nvariance  \\(X\\) commonly denoted  \\(\\sigma^2\\),  \\(\\sigma^2_X\\) distinguishing among variables needed.variance expected value squared distance values random variable mean, weighted probability function.\nunit measurement variance original unit measurement squared.\n,  \\(X\\) measured metres, variance  \\(X\\) \\(\\text{metres}^2\\).Describing variability terms original units natural, taking square root variance.Definition 5.4  (Standard deviation) standard deviation random variable \\(X\\) defined positive square root variance (denoted  \\(\\sigma\\)); .e.,\n\\[\n   \\text{sd}[X] = \\sigma = +\\sqrt{\\operatorname{var}[X]}\n\\]variance less popular standard deviation practice describe variability.\ntheoretical work, however, variance easier work standard deviation (due square root), variance, rather standard deviation, features many results theoretical statistics.Example 5.9  (Variance die toss) Suppose fair die tossed,  \\(X\\) denotes number points showing.\n\\(\\Pr(X = x) =  1/6\\) \\(x = 1, 2, 3, 4, 5, 6\\) \n\\[\n   \\mu = \\operatorname{E}[X] = \\sum_S x\\,\\Pr(X = x) = (1 + 2 + 3 + 4 + 5 + 6 )/6 = 7/2.\n\\]\nvariance  \\(X\\) \n\\[\\begin{align*}\n   \\sigma^2\n   &= \\operatorname{var}[X] = \\sum (X - \\mu)^2 \\Pr(X = x)\\\\\n   &= \\frac{1}{6}\\left[ \\left(1 - \\frac{7}{2}\\right)^2 + \\left(2 - \\frac{7}{2}\\right)^2 + \\dots + \\left(6 - \\frac{7}{2}\\right)^2 \\right] = \\frac{70}{24}.\n\\end{align*}\\]\nstandard deviation \\(\\sigma = \\sqrt{70/24} = 1.71\\).important result computational formula variance, usually easier use practice formula given Definition] 5.3.Theorem 5.2  (Computational formula variance) random variable \\(X\\),\n\\[\n   \\operatorname{var}[X] = \\operatorname{E}[X^2] - \\operatorname{E}[X]^2.\n\\]Proof. Let \\(\\operatorname{E}[X] = \\mu\\), (using properties expectation Theorem 5.1):\n\\[\\begin{align*}\n\\operatorname{var}[X]\n   = \\operatorname{E}\\left[(X - \\mu)^2\\right]\n   &= \\operatorname{E}[X^2 - 2X\\mu + \\mu^2] \\\\\n   &= \\operatorname{E}[X^2] - \\operatorname{E}[2X\\mu] + \\operatorname{E}[\\mu^2]\\quad\\text{(since $\\operatorname{E}[\\cdot]$ linear operator)}\\\\\n   &= \\operatorname{E}[X^2] - 2\\mu\\operatorname{E}[X] + \\mu^2\\\\\n   &= \\operatorname{E}[X^2] - 2\\mu^2 + \\mu^2 \\\\\n   &= \\operatorname{E}[X^2] - \\mu^2 \\\\\n   &= \\operatorname{E}[X^2] - \\operatorname{E}[X]^2.\n\\end{align*}\\]formula often easier use compute \\(\\operatorname{var}[X]\\) using definition directly.Example 5.10  (Variance die toss) Consider Example 5.9 .\n\n\\[\\begin{align*}\n  \\operatorname{E}[X^2] = \\sum_S x^2 \\Pr(X = x)\n  &= \\frac{1}{6}[1^2 + 2^2 + 3^2 + 4^2 = 5^2 + 6^2]\\\\\n  &= 91/6,\n\\end{align*}\\]\n\\(\\operatorname{var}[X] = 91/6 - (7/2)^2 = 70/24\\), .Example 5.11  (Variance using computational formula) Consider continuous random variable \\(X\\) PDF\n\\[\n   f_X(x) = \\begin{cases}\n            3x(2 - x)/4  & \\text{$0 < x < 2$};\\\\\n            0 & \\text{elsewhere}.\n            \\end{cases}\n\\]\nvariance  \\(X\\) can computed two ways: using \\(\\operatorname{var}[X] = \\operatorname{E}[(X - \\mu)^2]\\) using computational formula.\nexpected value \\(X\\) \n\\[\n   \\operatorname{E}[X] = \\int_0^2 x\\times 3x(2 - x)4\\, dx = 1.\n\\]\nuse computational formula, also find\n\\[\n   \\operatorname{E}[X^2] = \\frac{6}{5},\n\\]\n\\(\\operatorname{var}[X] = \\operatorname{E}[X^2] - \\operatorname{E}[X]^2 = 1/5\\).Using definition,\n\\[\\begin{align*}\n   \\operatorname{var}[X]\n   = \\operatorname{E}\\big[(X - \\operatorname{E}[X])^2\\big]\n   &= \\operatorname{E}\\big[(X - 1)^2\\big]\\\\\n   &= \\int_0^2 (x - 1)^2 \\times 3x(2 - x)/4\\,dx = 1/5.\n\\end{align*}\\]\nmethods give answer course, methods require initial computation \\(\\operatorname{E}[X]\\).variance represents expected value squared distance values random variable mean.\nvariance never negative, zero values random variable identical (, variation).probability lies near mean, dispersion small; probability spread considerable range dispersion large.Example 5.12  (Variance exist) \nExample 5.5, \\(\\operatorname{E}[X]\\) defined.\nreason, variance also undefined, since computing variance relies finite value \\(\\operatorname{E}[X]\\).Theorem 5.3  (Variance properties) random variable \\(X\\) constants \\(\\)  \\(b\\),\n\\[\n   \\operatorname{var}[aX + b] = ^2\\operatorname{var}[X].\n\\]Proof. Using computational formula variance:\n\\[\\begin{align*}\n   \\operatorname{var}[aX + b]\n   &= \\operatorname{E}[ (aX + b)^2 ] - \\left[\\operatorname{E}[aX + b] \\right] ^2\\\\\n   &= \\operatorname{E}[^2 X + 2abX + b^2] + (\\mu+b)^2\\\\\n   &= ^2 \\operatorname{E}[X^2] - ^2\\mu^2\\\\\n   &= ^2 \\operatorname{var}[X].\n\\end{align*}\\]special case \\(= 0\\) instructive: \\(\\operatorname{var}[b] = 0\\) \\(b\\) constant; , constant zero variation, expected.Example 5.13  (Variance function random variable) Consider random variable \\(Y = 4 - 2X\\) \\(\\operatorname{E}[X] = 1\\) \\(\\operatorname{var}[X] = 3\\).\n:\n\\[\\begin{align*}\n  \\operatorname{E}[Y]\n  &= \\operatorname{E}[4 - 2X] = 4 - 2\\times\\operatorname{E}[X] = 2;\\\\\n  \\operatorname{var}[Y]\n  &= \\operatorname{var}[4 - 2X] = (-2)^2\\operatorname{var}[X] = 12.\n\\end{align*}\\]","code":""},{"path":"ChapExpectation.html","id":"HigherMoments","chapter":"5 Mathematical expectation","heading":"5.4 Higher moments","text":"","code":""},{"path":"ChapExpectation.html","id":"RawCentralMoments","chapter":"5 Mathematical expectation","heading":"5.4.1 Raw and central moments","text":"ideas mean variance can generalised.\nmean special case ‘raw moment’, variance special case ‘central moment’.Definition 5.5  (Raw moments) \\(r\\)th raw moment, \\(r\\)th moment origin, random variable \\(X\\) ( \\(r\\) positive integer) denoted \\(\\mu'_r\\) defined \\(\\mu'_r = \\operatorname{E}[X^r]\\).discrete random variable \\(X\\), \\(r\\)th moment origin \n\\[\n  \\mu'_r = \\operatorname{E}[X^r] = \\sum_X x^r\\, p_X(x).\n\\]\ncontinuous random variable \\(X\\), \\(r\\)th moment origin \n\\[\n   \\mu'_r = \\operatorname{E}[X^r] =  \\int_{-\\infty}^\\infty x^r\\, f_X(x)\n\\]Definition 5.6  (Central moments) \\(r\\)th central moment, \\(r\\)th moment mean ( \\(r\\) positive integer), denoted \\(\\mu_r\\) defined \\(\\mu_r = \\operatorname{E}[(X - \\mu)^r]\\).discrete random variable \\(X\\), \\(r\\)th central moment \n\\[\n   \\mu_r = \\operatorname{E}[(X - \\mu)^r] = \\sum_x (x - \\mu)^r\\, p_X(x).\n\\]\ncontinuous random variable \\(X\\), \\(r\\)th central moment \n\\[\n   \\mu_r = \\operatorname{E}\\big[(X - \\mu)^r\\big] = \\int_{-\\infty}^{\\infty} (x - \\mu)^r\\, f_X(x).\n\\]definitions:mean \\(\\mu'_1 = \\mu\\) first raw moment;\\(\\mu'_2 = \\operatorname{E}[X^2]\\) second raw moment; andthe variance \\(\\mu_2 = \\sigma^2\\) second central moment.","code":""},{"path":"ChapExpectation.html","id":"Skewness","chapter":"5 Mathematical expectation","heading":"5.4.2 Skewness","text":"\nHigher moments also exist describe features random variable.\nthird central moment related skewness, measure asymmetry distribution.Definition 5.7  (Symmetry) distribution  \\(X\\) said symmetric , \\(x\\\\mathcal{R}_X\\),\\(p_X(\\mu + x) = p_X(\\mu - x)\\) discrete random variable \\(X\\) PMF \\(p_X(x)\\), \\(f_X(\\mu + x) = f_X(\\mu - x)\\) continuous random variable \\(X\\) PDF \\(f_X(x)\\), \\(\\mu = \\operatorname{E}[X]\\) mean  \\(X\\).symmetric distribution, odd central moments zero (Exercise 5.19).\nsuggests odd central moments (third central moment) can used measure asymmetry distribution.However, rather using third central moment explicitly, applying Def. 5.6, finding appropriate expected value normalised version random variable (.e., mean zero variance one) preferred.\n, definition skewness finds appropriate expected value \\((X - \\mu)/\\sigma\\) rather  \\(X\\) directly.\nmeans value skewness random variable \\(X\\) unaffected linear transformation type \\(Y = aX + b\\) (constants \\(\\)  \\(b\\)).Definition 5.8  (Skewness) skewness distribution random variable \\(X\\) mean \\(\\operatorname{E}[X] = \\mu\\) variance \\(\\operatorname{var}[X] = \\sigma^2\\) defined \n\\[\\begin{align}\n  \\text{skewness} = \\gamma_1\n  &= \\operatorname{E}\\left[\\left(\\frac{X-\\mu}{\\sigma}\\right)^3\\right]\\notag\\\\\n  &= \\frac{\\mu_3}{(\\sigma^2)^{3/2}}\n   = \\frac{\\mu_3}{\\mu_2^{3/2}}.\n  \\tag{5.1}\n\\end{align}\\]\\(\\gamma_1 > 0\\) say distribution positively (right) skewed, ‘stretched’ positive (negative) direction.\nSimilarly, \\(\\gamma_1 < 0\\) say distribution negatively (left) skewed.\ndistribution symmetric \\(\\gamma_1 = 0\\).\nsymmetric distribution, mean also median distribution, results show.Example 5.14  (Skewness) Figure 5.2 shows examples right-skewed (left panels), symmetric (centre panels) left-skewed (right panels) distributions, continuous random variable (top panels) discrete random variable (bottom panels).(top distributions beta distributions; bottom distributions binomial distributions.)\nFIGURE 5.2: Examples right-skewed (left panels), symmetric (centre panels) left-skewed (right panels) distributions. Top: continuous random variable. Bottom: discrete random variable.\nExample 5.15  (Skewness) Consider random variable \\(X\\) Example 5.11, \\(f_X(x) = x(2 - x)\\) \\(0 < x < 2\\).\nexample, \\(\\operatorname{E}[X] = \\mu'_1 = 1\\) \\(\\operatorname{E}[X^2] = \\mu_2 = 6/5\\).\n,\n\\[\n   \\mu_3 = \\int_0^2 (x - 1)^3\\times 3x(2 - x)/4 \\,dx = 0,\n\\]\nskewness Eq. (5.1) zero.\nexpected, since distribution symmetric (Fig. 5.3).\nFIGURE 5.3: probability density function  \\(X\\).\nExample 5.16  (Skewness) Consider random variable \\(Y\\) PMF\n\\[\n   p_Y(y) =\n   \\begin{cases}\n      0.2 & \\text{$y = 5$};\\\\\n      0.3 & \\text{$y = 6$};\\\\\n      0.5 & \\text{$y = 7$};\\\\\n      0   & \\text{elsewhere}.\n    \\end{cases}\n\\]\n\n\\[\n   \\mu'_1 = \\operatorname{E}[Y] = (5\\times 0.2) + (6\\times 0.3) + (7\\times 0.5) = 6.3.\n\\]\nLikewise,\n\\[\\begin{align*}\n   \\mu_2\n   &= \\operatorname{E}\\big[(y - 6.3)^2 \\big]\n    = (5 - 6.3)^2\\times 0.2 + (6 - 6.3)^2\\times 0.3 + (7 - 6.3)^2\\times 0.5\n    = 0.61;\\quad{\\text{}}\\\\\n   \\mu_3\n   &= \\operatorname{E}\\big[(y - 6.3)^3  \\big]\n    = (5 - 6.3)^3\\times 0.2 + (6 - 6.3)^3\\times 0.3 + (7 - 6.3)^3\\times 0.5\n    = -0.276.\n\\end{align*}\\]\nHence, skewness \n\\[\n   \\gamma_1 = \\frac{\\mu_3}{\\mu_2^{3/2}} = \\frac{-0.276}{0.61^{3/2}} =  -0.579\\dots,\n\\]\ndistribution slight negative skewness.","code":""},{"path":"ChapExpectation.html","id":"Kurtosis","chapter":"5 Mathematical expectation","heading":"5.4.3 Kurtosis","text":"\nAnother description distribution kurtosis, measures heaviness tails distribution; , much probability random variable \\(X\\) concentrated extremes values  \\(X\\).\nrelated fourth central moment.\n, finding appropriate expected value normalised version random variable (.e., mean zero variance one) preferred.\n, definition kurtosis finds appropriate expected value \\((X - \\mu)/\\sigma\\) rather  \\(X\\) directly.Definition 5.9  (Kurtosis) kurtosis random variable \\(X\\) mean \\(\\mu = \\operatorname{E}[X]\\) variance \\(\\sigma^2 = \\operatorname{var}[X]\\) defined \n\\[\n  \\text{kurtosis}\n  = \\operatorname{E}\\left[\\left(\\frac{X-\\mu}{\\sigma}\\right)^4\\right]\n  = \\frac{\\mu_4}{\\mu^2_2}.\n\\]\nexcess kurtosis distribution random variable defined \n\\[\\begin{equation*}\n     \\gamma_2 = \\frac{\\mu_4}{\\mu^2_2} - 3.\n\\end{equation*}\\]\nexcess kurtosis definition defines excess kurtosis compared bell-shaped (normal distribution), excess kurtosis zero.Excess kurtosis commonly used often just called ‘kurtosis’.One way understand kurtosis (Moors (1986)) first define \\(Z = (X - \\mu)/\\sigma\\); kurtosis , Def. 5.9, \\(\\operatorname{E}[Z^4]\\), \\(\\operatorname{E}[Z] = 0\\) \\(\\operatorname{var}[X] = 1\\).\nAlso observe since \\(\\operatorname{var}[X] =  \\operatorname{E}[X^2] - \\operatorname{E}[X]^2\\) (definition variance), can write\n\\[\\begin{equation}\n   \\operatorname{E}[X^2] = \\operatorname{var}[X] + \\operatorname{E}[X]^2.\n   \\tag{5.2}\n\\end{equation}\\]\n, kurtosis \n\\[\\begin{align*}\n  \\operatorname{E}[Z^4]\n  &= \\operatorname{var}[Z^2] + \\operatorname{E}[Z^2]^2\\quad\\text{(using Eq.~(\\ref{eq:VarianceRearranged}))}\\\\\n  &= \\operatorname{var}[Z^2] +\n     \\left\\{ \\operatorname{var}[Z] + \\operatorname{E}[Z]^2\\right\\}^2\\quad\\text{(using Eq.~(\\ref{eq:VarianceRearranged}) )}\\\\\n  &= \\operatorname{var}[Z^2] + (1 + 0)^2 \\\\\n  &= \\operatorname{var}[Z^2] + 1.\n\\end{align*}\\]\nThus, kurtosis related variance  \\(Z^2\\) ( \\(Z\\)) mean.\n, kurtosis emphasises focuses probability function extremes random variable.Large values kurtosis corresponds greater proportion distribution tails.\n(see Fig. 5.4):distributions negative excess kurtosis called platykurtic.\ndistribution fewer, less extreme, observations tail compared normal distribution (‘thinner tails’).\nExamples include Bernoulli distribution (Sect. 7.3).distributions positive excess kurtosis called leptokurtic.\ndistribution , extreme, observations tail compared normal distribution (‘fatter tails’).\nExamples include exponential distribution (Sect. 8.4) Poisson distributions (Sect. 7.7).distributions zero excess kurtosis called mesokurtic.\nnormal distribution (Sect. 8.3) obvious example.\nFIGURE 5.4: Kurtosis three distributions plotted \\(x = -3\\) \\(x = +3\\); plots mean  \\(0\\), variance  \\(1\\) symmetric. grey line shows middle distribution reference, \\(\\gamma_1 = 0\\) (zero excess kurtosis).\nExample 5.17  (Uses skewness kurtosis) Monypenny Middleton (1998b) Monypenny Middleton (1998a) use skewness kurtosis analyse wind gusts Sydney airport.Example 5.18  (Uses skewness kurtosis) Galagedera, Henry, Silvapulle (2002) used higher moments capital analysis pricing model Australian stock returns.Example 5.19  (Skewness kurtosis) Consider discrete random variable \\(U\\) Example 5.1.\nraw moments \n\\[\\begin{align*}\n   \\mu'_r = \\operatorname{E}[U^r]\n   &= \\sum_{u = -1, 0, 1} u^r \\frac{u^2 + 1}{5} \\\\\n   &= (-1)^r \\frac{ (-1)^2 + 1}{5} +\n       (0)^r \\frac{ (0)^2 + 1}{5} +\n       (1)^r \\frac{ (1)^2 + 1}{5} \\\\\n   &= \\frac{2(-1)^r}{5} + 0 + \\frac{2}{5} \\\\\n   &= \\frac{2}{5}[ (-1)^r + 1]\n\\end{align*}\\]\n\\(r\\)th raw moment.\n,\n\\[\\begin{align*}\n   \\operatorname{E}[X]   &= \\mu'_1 = \\frac{2}{5}[ (-1)^1 + 1 ] = 0;\\\\\n   \\operatorname{E}[X^2] &= \\mu'_2 = \\frac{2}{5}[ (-1)^2 + 1 ] = 4/5;\\\\\n   \\operatorname{E}[X^3] &= \\mu'_1 = \\frac{2}{5}[ (-1)^3 + 1 ] = 0;\\\\\n   \\operatorname{E}[X^4] &= \\mu'_2 = \\frac{2}{5}[ (-1)^4 + 1 ] = 4/5.\n\\end{align*}\\]\nSince \\(\\operatorname{E}[U] = 0\\), \\(r\\)th central raw moments : \\(\\mu'_r = \\mu_r\\).\nNotice initial computations find \\(\\mu'_r\\) complete, evaluation raw moment simple.skewness \n\\[\n   \\gamma_1 = \\frac{\\mu_3}{\\mu_2^{3/2}} = \\frac{0}{(4/5)^{3/2}} = 0,\n\\]\ndistribution symmetric.\nexcess kurtosis \n\\[\n   \\gamma_2 = \\frac{\\mu_4}{\\mu_2^2} -3 = \\frac{4/5}{(4/5)^2} -3 = -7/4,\n\\]\ndistribution platykurtic.","code":""},{"path":"ChapExpectation.html","id":"MGF","chapter":"5 Mathematical expectation","heading":"5.5 Moment-generating functions","text":"","code":""},{"path":"ChapExpectation.html","id":"MGFIntroduction","chapter":"5 Mathematical expectation","heading":"5.5.1 Introduction","text":", mean, variance, skewness kurtosis completely describe distribution; many different distributions can found given mean, variance, skewness kurtosis.\nHowever, general, moments distribution together define distribution.\nleads idea moment-generating function.Suppose asked draw probability density function random variable \\(X\\), \\(\\operatorname{E}[X] = 2\\).\nsix distributions Fig. 5.5 meet (first moment) criterion, information sufficient uniquely define distribution., suppose second criterion added: addition, require \\(\\operatorname{var}[X] = 1\\).\nfirst five distributions Fig. 5.5 meet two criteria (based first two moments), information sufficient uniquely define distribution.Suppose third criterion added: distribution must symmetric.\ntop four distributions Fig. 5.5 meet three criteria (based first three moments); , information sufficient uniquely define distribution.Suppose fourth criterion added: distribution must zero excess kurtosis.\nEither top two distributions Fig. 5.5 meet four criteria (based first four moments); , information sufficient uniquely define distribution.general, moments distribution needed uniquely define distribution.\nHowever, computing (even many) moments distribution usually tedious.\nreason, moment generating function (MGF) now introduced, function encapsulates moments distribution.\nFIGURE 5.5: Six distributions, mean 1 variance 1. top four also symmetric (.e., \\(\\gamma_1 = 0\\)); top two also zero excess kurtosis (.e., \\(\\gamma_2=0\\)).\n","code":""},{"path":"ChapExpectation.html","id":"MGFDefinition","chapter":"5 Mathematical expectation","heading":"5.5.2 Definition","text":"far, distribution random variable described using probability function distribution function.\nSometimes, however, working different representation useful (example, see Sect. 6.4).section, moment-generating function used represent distribution probabilities random variable.\nname suggests, function can used generate moment distribution.\nuses moment-generating function seen later (see Sect. 6.4).Definition 5.10  (Moment-generating function (MGF)) moment-generating function (MGF)\n\\(M_X(t)\\) random variable \\(X\\) defined range \\(\\mathcal{R}_X\\) denoted \\(M_X(t)\\), defined \n\\[\n   \\operatorname{E}\\big[\\exp(tX)\\big],\n\\]\nprovided expectation exists values  \\(t\\) interval includes \\(t = 0\\). \\(X\\) discrete random variable,\n\\[\n  M_X(t)  = \\operatorname{E}\\big[\\exp(tX)\\big] = \\sum_{x\\\\mathcal{R}_X} \\exp(tx)\\, p_X(x).\n\\]\n \\(X\\) continuous random variable,\n\\[\n  M_X(t)  = \\operatorname{E}\\big[\\exp(tX)\\big] = \\int_{-\\infty}^\\infty \\exp(tx)\\, f_X(x).\n\\]MGF may always exist (, converge finite value) values  \\(t\\), MGF may defined values  \\(t\\).\nNote MGF always exists \\(t = 0\\); fact \\(M_X(0) = 1\\).Provided MGF defined values \\(t\\) zero, uniquely defines probability distribution, can use easily generate moments distribution, described Theorem 5.4.Moment-generating functions related Laplace transformations.Example 5.20  (Moment-generating function) Consider random variable \\(Y\\) PDF\n\\[\n   f_Y(y) =\n   \\begin{cases}\n      \\exp(-y) & \\text{$y > 0$};\\\\\n      0        & \\text{elsewhere.}\n   \\end{cases}\n\\]\nMGF \n\\[\\begin{align*}\n   M_Y(t)\n    = \\operatorname{E}[\\exp(tY)]\n   &= \\int_0^\\infty \\exp(ty)\\,\\exp(-y)\\, dy \\\\\n   &= \\int_0^\\infty \\exp\\{ y(t-1) \\}\\, dy \\\\\n   &= (1 - t)^{-1}\n\\end{align*}\\]\nprovided \\(t - 1 < 0\\); , \\(t < 1\\) (includes \\(t = 0\\)).\n\\(t > 1\\), integral converge.\nexample, \\(t = 2\\),\n\\[\n   \\left. \\frac{1}{2 - 1} \\exp(y)\\right|_{y = 0}^{y = \\infty} = \\exp(0) - \\lim_{y\\\\infty} \\exp(y)\n\\]\nconverge.Example 5.21  (MGF die rolls) Consider PMF  \\(X\\), outcome tossing fair die (Example 5.9).\nMGF  \\(X\\) \n\\[\\begin{align*}\n   M_X(t)\n   &= \\operatorname{E}[\\exp(tX)] = \\sum_{x = 1}^6 \\exp(tx)\\, p_X(x)\\\\\n   &= \\frac{1}{6}\\left(e^t + e^{2t} + e^{3t} + e^{4t} + e^{5t} + e^{6t}\\right),\n\\end{align*}\\]\nexists values  \\(t\\).Example 5.22  (MGF exist) Consider Cauchy distribution PDF\n\\[\n   f_X(x) = \\frac{1}{\\pi(1 + x^2)},\n\\]\ndefined \\(x\\\\mathbb{R}\\).\nmoment generating function \n\\[\n  \\operatorname{E}[\\exp(tX)]\n  = \\int_{-\\infty}^{\\infty} e^{tx}\\frac{1}{\\pi(1 + x^2)}\\,dx.\n\\]\nConsider integrand \\(\\exp(tx)/\\big(\\pi(1 + x^2)\\big)\\).\nintegrand converge unless \\(t = 0\\).example, consider \\(t > 0\\): \\(x\\\\infty\\), see \\(\\exp(tx)\\\\infty\\), \\(1/(1 + x^2)\\0\\) quite slowly; integrand diverges (see Fig. 5.6 (left panel) example \\(t = 1\\)).\nNow consider \\(t < 0\\): \\(x\\-\\infty\\), see \\(\\exp(tx)\\\\infty\\), \\(1/(1 + x^2)\\0\\) quite slowly; integrand diverges (see Fig. 5.6 (right panel) example \\(t = -1\\)).integral converges \\(t = 0\\).\ndefinition MGF states MGF exists ‘provided expectation exists values  \\(t\\) interval includes \\(t = 0\\)’.\ncase: integral exists \\(t = 0\\).\nMGF exist Cauchy distribution.\nFIGURE 5.6: MGF computed, integrand diverges \\(t > 0\\) (left panel) \\(t < 0\\) (right panel).\n","code":""},{"path":"ChapExpectation.html","id":"MGFMoments","chapter":"5 Mathematical expectation","heading":"5.5.3 Using the MGF to generate moments","text":"Replacing \\(\\exp(xt)\\) series expansion (App. B) definition MGF discrete random variable \\(X\\) gives\n\\[\\begin{align*}\n     M_X(t)\n     & = {\\sum_x} \\left(1 + xt + \\frac{x^2t^2}{2!} + \\dots\\right) \\Pr(X = x)\\\\\n     & = 1 + \\mu'_1t + \\mu'_2 \\frac{t^2}{2!} +\\mu'_3 \\frac{t^3}{3!} + \\dots\n\\end{align*}\\]\n, \\(r\\)th moment distribution origin seen coefficient \\(t^r/r!\\) series expansion \\(M_X(t)\\):\n\\[\\begin{align*}\n     \\frac{d M_X(t)}{dt}\n     & = \\sum_x x\\,e^{xt}\\Pr(X = x)\\\\\n     \\frac{d^2 M_X(t)}{dt^2}\n     & = \\sum_x x^2\\,e^{xt} \\Pr(X = x),\n\\end{align*}\\]\n, general, positive integer \\(r\\):\n\\[\n   \\frac{d^r M_X(t)}{dt^r} = \\sum_x x^re^{xt}\\Pr(X = x).\n\\]\nsetting \\(t = 0\\),\n\\[\\begin{align*}\n\\left.\\frac{d M_X(t)}{dt}\\right|_{t = 0} &= \\operatorname{E}[X]\\\\\n\\left.\\frac{d^2M_X(t)}{dt^2}\\right|_{t = 0} &= \\operatorname{E}[X^2].\n\\end{align*}\\]\n(notation left means evaluate expression \\(t = 0\\).)\ngeneral, positive integer \\(r\\),\n\\[\\begin{equation}\n  \\left.\\frac{d^r M_X(t)}{dt^r}\\right|_{t = 0} = \\operatorname{E}[X^r].\n\\end{equation}\\]\n(Sometimes, \\(d^r M_X(t)/dt^r\\) evaluated \\(t = 0\\) written \\(M^{(r)}(0)\\) brevity.)\nresult summarised following theorem.Theorem 5.4  (Moments) \\(r\\)th moment \\(\\mu'_r\\) distribution random variable \\(X\\) origin given eitherthe coefficient \\(t^r/r!,  r = 1, 2, 3,\\dots\\) power series expansion \\(M_X(t)\\); \\(\\displaystyle \\mu'_r = \\left.\\frac{d^rM(t)}{dt^r}\\right|_{t = 0}\\) \\(M_X(t)\\) MGF \\(X\\).Example 5.23  (Mean variance MGF) Continuing Example 5.20, mean variance  \\(Y\\) can found MGF.\nfind mean, first find\n\\[\n   \\frac{d}{dt}M_Y(t) = (1 - t)^{-2}.\n\\]\nSetting \\(t = 0\\) gives mean \\(\\operatorname{E}[Y] = 1\\).\nLikewise,\n\\[\n   \\frac{d^2}{dt^2}M_Y(t) = 2(1 - t)^{-3}.\n\\]\nSetting \\(t = 0\\) gives \\(\\operatorname{E}[Y^2] = 2\\).\nvariance therefore \\(\\operatorname{var}[Y] = 2 - 1^2 = 1\\).moment-generating function computed, raw moments can computed using\n\\[\n   \\operatorname{E}[Y^r] = \\mu'_r = \\left.\\frac{d^r}{dt^r} M_Y(t)\\right|_{t = 0}.\n\\]","code":""},{"path":"ChapExpectation.html","id":"some-useful-results","chapter":"5 Mathematical expectation","heading":"5.5.4 Some useful results","text":"moment-generating function can used derive distribution function random variable (see Sect. 6.4).\nfollowing theorems valuable task.Theorem 5.5  (MGF linear combinations) random variable \\(X\\) MGF \\(M_X(t)\\) \\(Y = aX + b\\)  \\(\\)  \\(b\\) constants, MGF  \\(Y\\) \n\\[\n   M_Y(t) = \\operatorname{E}\\big[\\exp\\{t(aX + b)\\}\\big] = \\exp(bt) M_X().\n\\]Theorem 5.6  (MGF independent rvs) \\(X_1\\), \\(X_2\\), \\(\\dots\\), \\(X_n\\) \\(n\\) independent random variables,  \\(X_i\\) MGF \\(M_{X_i}(t)\\), MGF \\(Y = X_1 + X_2 + \\cdots X_n\\) \n\\[\n   M_Y(t) = \\prod_{= 1}^n M_{X_i}(t).\n\\]Proof. proofs left exercise.Note special case random variables independently identically distributed Theorem 5.5, \n\\[\n   M_Y(t) = [M_{X_i}(t)]^n.\n\\]Example 5.24  (MGF linear combinations) Consider random variable \\(X\\) pf\n\\[\n   p_X(x) = 2(1/3)^x \\qquad \\text{$x = 1, 2, 3, \\dots$}\n\\]\nzero elsewhere.\nMGF  \\(X\\) \n\\[\\begin{align*}\n   M_X(t)\n   &= \\sum_{x: p(x) > 0} \\exp(tx)\\, p_X(x) \\\\\n   &= \\sum_{x = 1}^\\infty \\exp(tx)\\, 2(1/3)^x \\\\\n   &= 2\\sum_{x = 1}^\\infty (\\exp(t)/3)^x \\\\\n   &= 2\\left\\{ \\frac{\\exp(t)}{3} + \\left(\\frac{\\exp(t)}{3}\\right)^2\n   + \\left(\\frac{\\exp(t)}{3}\\right)^3 + \\dots\\right\\} \\\\\n   &= \\frac{2\\exp(t)}{3 - \\exp(t)}\n\\end{align*}\\]\n\\(\\sum_{y = 1}^\\infty ^y = /(1 - )\\) \\(< 1\\) used (App. B); \\(= \\exp(t)/3\\).Next consider finding MGF \\(Y = (X - 2)/3\\).\nTheorem 5.5 \\(= 1/3\\) \\(b = -2/3\\),\n\\[\n   M_Y(t)\n   = \\exp(-2t/3) M_X(t/3)\n   = \\frac{2\\exp\\{(-t)/3\\}}{3 - \\exp(t/3)}.\n\\]\npractice, rather identify \\(\\)  \\(b\\) remember Theorem 5.5, problems like best solved directly definition MGF:\n\\[\\begin{align*}\n   M_Y(t)\n    = \\operatorname{E}[\\exp(tY)]\n   &= \\operatorname{E}[\\exp\\{t(X - 2)/3\\}]\\\\\n   &= \\operatorname{E}[\\exp\\{tX/3 - 2t/3\\}]\\\\\n   &= \\exp(-2t/3) M_X(t/3) \\\\\n   &= \\frac{2\\exp\\{(-t)/3\\}}{3 - \\exp(t/3)}.\n\\end{align*}\\]","code":""},{"path":"ChapExpectation.html","id":"DistributionFromMGF","chapter":"5 Mathematical expectation","heading":"5.5.5 Determining the distribution from the MGF","text":"MGF (exists) completely determines distribution random variable hence, given MGF, deducing probability function possible.\ndistributions, PDF written closed form (PDF can evaluated numerically; example, see Sect. 9.4), MGF relatively simple write .discrete random variable \\(X\\), MGF defined \n\\[\\begin{equation}\n   M_X(t)\n   = \\operatorname{E}[\\exp(tX)]\n   = \\sum_X e^{tx} p_X(x)\n   \\tag{5.3}\n\\end{equation}\\]\n \\(X\\) discrete PMF \\(p_X(x)\\).\ncan expressed \n\\[\\begin{align*}\n   M_X(t)\n   &= \\exp(t x_1) p_X(x_1) + \\exp(t x_2)p_X(x_2) + \\dots\\\\\n   &= \\exp(t x_1) \\Pr(X = x_1) + \\exp(t x_2)\\Pr(X = x_2) + \\dots\\\\\n\\end{align*}\\]\nprobability function  \\(Y\\) can deduced MGF.Example 5.25  (Distribution MGF) Suppose discrete random variable \\(D\\) MGF\n\\[\n   M_D(t) = \\frac{1}{3} \\exp(2t) + \\frac{1}{6}\\exp(3t) + \\frac{1}{12}\\exp(6t)\n   + \\frac{5}{12}\\exp(7t).\n\\]\n, definition MGF discrete case given , coefficient  \\(t\\) exponential indicates values  \\(D\\), coefficient indicates probability value  \\(Y\\):\n\\[\\begin{align*}\n   M_D(t)\n   &= \\overbrace{\\frac{1}{3} \\exp(2t)}^{D = 2} + \\overbrace{\\frac{1}{6}\\exp(3t)}^{D = 3} +\n       \\overbrace{\\frac{1}{12}\\exp(6t)}^{D = 6} + \\overbrace{\\frac{5}{12}\\exp(7t)}^{D = 7}\\\\\n   &= \\Pr(D = 2)\\exp(2t) + \\Pr(D = 3)\\exp(3t) + \\\\\n   & \\quad \\Pr(D = 6)\\exp(6t) + \\Pr(D = 7)\\exp(7t).\n\\end{align*}\\]\nPMF \n\\[\n   p_D(d) =\n   \\begin{cases}\n      1/3 & \\text{$d=2$}\\\\\n      1/6 & \\text{$d=3$}\\\\\n      1/12 & \\text{$d=6$}\\\\\n      5/12 & \\text{$d=7$}\\\\\n      0 & \\text{otherwise}\n   \\end{cases}\n\\]\n(course, easy check computing MGF  \\(D\\) pf found ; get original MGF.)Sometimes, using results App. B can helpful.Example 5.26  (Distribution MGF) Consider MGF\n\\[\n   M_X(t) = \\frac{\\exp(t)}{3 - 2\\exp(t)}.\n\\]\nfind corresponding probability function, one approach write MGF \n\\[\n   M_X(t) = \\frac{\\exp(t)/3}{1 - 2\\exp(t)/3}.\n\\]\nsum geometric series (Eq. (B.5)):\n\\[\n   + ar + ar^2 + \\ldots + ar^{n - 1}\n   \\rightarrow \\frac{}{1 - r} \\text{ $n  \\rightarrow  \\infty$},\n\\]\n\\(= \\exp(t)/3\\) \\(r = 2\\exp(t)/3\\).\nHence MGF can expressed \n\\[\n   \\frac{1}{3}\\exp(t) +\n   \\frac{1}{3}\\left(\\frac{2}{3}\\right) \\exp(2t) +\n   \\frac{1}{3}\\left(\\frac{2}{3}\\right)^2 \\exp(3t) + \\dots\n\\]\nprobability function can deduced \n\\[\\begin{align*}\n   \\Pr(X = 1) &= \\frac{1}{3};\\\\\n   \\Pr(X = 2) &= \\frac{1}{3}\\left(\\frac{2}{3}\\right);\\\\\n   \\Pr(X = 3) &= \\frac{1}{3}\\left(\\frac{2}{3}\\right)^2,\n\\end{align*}\\]\n, general,\n\\[\n   p_x(x) = \\frac{1}{3}\\left( \\frac{2}{3}\\right)^{x - 1}\\quad\\text{$x = 1, 2, 3, \\dots$}.\n\\]\n(Later, identified geometric distribution.)continuous random variable \\(X\\), approach involved.\nSuppose continuous random variable \\(X\\) MGF \\(M_X(t)\\).\nprobability density function (see Abramowitz Stegun (1964), 26.1.10)\n\\[\\begin{equation}\n   f_X(x) =\n   \\frac{1}{2\\pi} \\int_{-\\infty}^{\\infty} M_X() \\exp(-itx)\\, dt,\n   \\tag{5.4}\n\\end{equation}\\]\n\\(= \\sqrt{-1}\\).Example 5.27  (Distribution MGF) Consider MGF continuous random variable \\(X\\) \\(M_X(t) = \\exp(t^2/2)\\), \\(y\\\\mathbb{R}\\) \\(t\\\\mathbb{R}\\).\n, \\(M_X() = \\exp\\left( ()^2/2 \\right) = \\exp(-t^2/2)\\).\nUsing Eq. (5.4), PDF :\n\\[\\begin{align*}\nf_X(x)\n&= \\frac{1}{2\\pi} \\int_{-\\infty}^{\\infty} \\exp(-t^2/2) \\exp(-itx)\\, dt,\\\\\n&= \\frac{1}{2\\pi} \\int_{-\\infty}^{\\infty} \\exp(-t^2/2)\\left[ \\cos(-tx) + \\sin(-itx)\\right]\\, dt,\n\\end{align*}\\]\nsince \\(x\\\\mathbb{R}\\) \\(t\\\\mathbb{R}\\).\nExtracting just real components:\n\\[\\begin{align*}\nf_X(x)\n&= \\frac{1}{2\\pi} \\int_{-\\infty}^{\\infty} \\exp(-t^2/2) \\cos(-tx) \\, dt\\\\\n&= \\frac{1}{2\\pi} \\left( \\sqrt{2\\pi} \\exp( -x^2/2 ) \\right)\n  = \\frac{1}{ \\sqrt{2\\pi} } \\exp( -x^2/2 ),\n\\end{align*}\\]\nlater identified normal distribution mean zero, standard deviation one.practice, using Eq. (5.4) can become tedious intractable producing closed form expression PDF.\nHowever, Eq. (5.4) used compute numerical values probability density function.\nexample, Dunn Smyth (2008) used Eq. (5.4) evaluate Tweedie distributions (Dunn Smyth 2001), (general) PDF closed form, simple MGF.\n","code":""},{"path":"ChapExpectation.html","id":"Tchebysheff","chapter":"5 Mathematical expectation","heading":"5.6 Tchebysheff’s inequality","text":"Tchebysheff’s inequality applies probability distribution, sometimes useful theoretical work provide bounds probabilities.Theorem 5.7  (Tchebysheff's theorem) Let \\(X\\) random variable finite mean \\(\\mu\\) variance \\(\\sigma^2\\).\npositive \\(k\\),\n\\[\\begin{equation}\n   \\Pr\\big(|X - \\mu| \\geq k\\sigma \\big)\\leq \\frac{1}{k^2}\n   \\tag{5.5}\n\\end{equation}\\]\n, equivalently\n\\[\\begin{equation}\n   \\Pr\\big(|X - \\mu| < k\\sigma \\big)\\geq 1 - \\frac{1}{k^2}.\n\\end{equation}\\]Proof. proof continuous case given.\nLet \\(X\\) continuous PDF \\(f(x)\\).\n\\(c > 0\\), \n\\[\\begin{align*}\n     \\sigma^2\n     & = \\int^\\infty_{-\\infty} (x - \\mu )^2f(x)\\,dx\\\\\n     & = \\int^{\\mu -\\sqrt{c}}_{-\\infty} (x - \\mu )^2f(x)\\, dx +\n         \\int^{\\mu + \\sqrt{c}}_{\\mu-\\sqrt{c}}(x - \\mu )^2f(x)\\,dx +\n         \\int^\\infty_{\\mu + \\sqrt{c}}(x - \\mu)^2f(x)\\,dx\\\\\n     & \\geq \\int^{\\mu -\\sqrt{c}}_{-\\infty} (x - \\mu )^2f(x)\\,dx +\n       \\int^\\infty_{\\mu + \\sqrt{c}}(x - \\mu )^2f(x)\\,dx,\n\\end{align*}\\]\nsince second integral non-negative.\nNow \\((x - \\mu )^2 \\geq c\\) \\(x \\leq \\mu -\\sqrt{c}\\) \\(x\\geq \\mu + \\sqrt{c}\\).\nremaining integrals , replace \\((x - \\mu )^2\\) \\(c\\) without altering direction inequality:\n\\[\\begin{align*}\n  \\sigma^2\n  &\\geq  c \\int^{\\mu -\\sqrt{c}}_{-\\infty} f(x)\\,dx + c\\int^\\infty_{\\mu + \\sqrt{c}}f(x)\\,dx\\\\\n  &=  c\\,\\Pr(X \\leq \\mu - \\sqrt{c}\\,) + c\\,\\Pr(X \\geq \\mu + \\sqrt{c}\\,)\\\\\n  &=  c\\,\\Pr(|X - \\mu| \\geq \\sqrt{c}\\,).\n\\end{align*}\\]\nPutting \\(\\sqrt{c} = k\\sigma\\), Eq. (5.5) obtained.probability function PDF random variable \\(X\\), \\(\\operatorname{E}[X]\\) \\(\\operatorname{var}[X]\\) can found, converse true.\n, knowledge \\(\\operatorname{E}[X]\\) \\(\\operatorname{var}[X]\\) reconstruct probability distribution  \\(X\\) hence compute probabilities \\(\\Pr(|X - \\mu| \\geq k\\sigma)\\).\nNonetheless, using Tchebysheff’s inequality can find useful bound either probability outside inside \\(\\mu \\pm k\\sigma\\).","code":""},{"path":"ChapExpectation.html","id":"ExpectationsBivariate","chapter":"5 Mathematical expectation","heading":"5.7 Mathematical expectation for bivariate distributions","text":"","code":""},{"path":"ChapExpectation.html","id":"ExpectationsBivariateFunctions","chapter":"5 Mathematical expectation","heading":"5.7.1 Expected values of a bivariate function","text":"manner analogous univariate case, expectation functions two random variables can given.Definition 5.11  (Expectation bivariate distributions) Let \\((X, Y)\\) \\(2\\)-dimensional random variable let \\(u(X, Y)\\) function  \\(X\\)  \\(Y\\).discrete bivariate distribution probability mass function \\(p_{X, Y}(x, y)\\) defined \\((x, y) \\R\\), expectation expected value \\(\\operatorname{E}[u(X, Y)]\\) \n\\[\n   \\operatorname{E}[u(X, Y)]\n   = \\mathop{\\sum\\sum}_{(x, y)\\R} u(x, y)\\, p_{X, Y}(x, y).\n\\]\ncontinuous bivariate distribution probability density function \\(f_{X, Y}(x, y)\\) defined \\((x, y) \\R\\), expectation expected value \\(\\operatorname{E}[u(X, Y)]\\) \n\\[\n   \\operatorname{E}[u(X, Y)]\n   = \\mathop{\\sum\\sum}_{(x, y)\\R} u(x, y)\\, p_{X, Y}(x, y).\n\\]definition can extended expectation function number random variables.Example 5.28  (Expectation function two rvs (discrete)) Consider joint distribution  \\(X\\)  \\(Y\\) Example 4.4.\nDetermine \\(\\operatorname{E}[X + Y]\\); .e., mean number heads plus number showing die.Def. 5.11, write \\(u(X, Y) = X + Y\\) \n\\[\\begin{align*}\n   \\operatorname{E}[X + Y]\n   &= \\sum_{x = 0}^2 \\sum_{y = 1}^6 (x + y)\\, p_{X, Y}(x, y)\\\\\n   &= 1\\times(1/24) + 2\\times(1/24) + \\dots + 6\\times(1/24)\\\\\n   & \\qquad + 2\\times(1/12) + 3\\times(1/12) + \\dots + 7\\times(1/12)\\\\\n   & \\qquad + 3\\times(1/24) + 4\\times(1/24) + \\dots + 8\\times(1/24)\\\\\n   &= 21/24 + 27/12 + 33/24\\\\\n   &= 4.5.\n\\end{align*}\\]\nanswer just \\(\\operatorname{E}[X] + \\operatorname{E}[Y] = 1 + 3.5 = 4.5\\).\ncoincidence, see Theorem 5.8.Example 5.29  (Expectation function two rvs (continuous)) Consider Example 4.6.\ndetermine \\(\\operatorname{E}[XY]\\), write \\(u(X, Y) = XY\\) proceed:\n\\[\n  \\operatorname{E}[XY]\n   = \\frac{6}{5} \\int_0^1\\int_0^1 xy(x + y^2)\\,dx\\,dy\n   = \\frac7{20}.\n\\]\nUnlike previous example, alternative simple calculation based \\(\\operatorname{E}[X]\\) \\(\\operatorname{E}[Y]\\) possible, since \\(\\operatorname{E}[XY]\\neq\\operatorname{E}[X] \\operatorname{E}[Y]\\) general.Theorem 5.8  (Expectations two rvs)  \\(X\\)  \\(Y\\) random variables,  \\(\\)  \\(b\\) constants, \n\\[\n   \\operatorname{E}[aX + ] = \\operatorname{E}[X] + b\\operatorname{E}[Y].\n\\]theorem surprise seeing Theorem 5.1, powerful useful.\nproof given discrete case; continuous case analogous.Proof. \\[\\begin{align*}\n  \\operatorname{E}[aX + ]\n  &= \\mathop{\\sum\\sum}_{(x, y) \\R}(ax + ) \\, p_{X, Y}(x, y), \\text{ definition}\\\\\n  &= \\sum_x \\sum_y ax\\, p_{X, Y}(x, y) + \\sum_x \\sum_y \\, p_{X, Y}(x, y)\\\\\n  &= \\sum_x x\\sum_y p_{X, Y}(x, y) + b\\sum_y y\\sum_x p_{X, Y}(x, y)\\\\\n  &= \\sum_x x \\Pr(X = x) + b\\sum_y y \\Pr(Y = y)\\\\\n  &= \\operatorname{E}[X] + b\\operatorname{E}[Y].\n\\end{align*}\\]result true whether  \\(X\\)  \\(Y\\) independent.\nTheorem 5.8 naturally generalises expected value linear combination random variables (see Theorem 11.1).","code":""},{"path":"ChapExpectation.html","id":"moments-of-a-bivariate-distribution-covariance","chapter":"5 Mathematical expectation","heading":"5.7.2 Moments of a bivariate distribution: covariance","text":"idea moment univariate case naturally extends bivariate case.\nHence, define \\(\\mu'_{rs} = \\operatorname{E}[X^r Y^s]\\) \\(\\mu_{rs} = \\operatorname{E}\\big[(X - \\mu_X)^r (Y - \\mu_Y)^s\\big]\\) raw central moments bivariate distribution.important moments covariance.Definition 5.12  (Covariance) covariance  \\(X\\)  \\(Y\\) defined \n\\[\n   \\operatorname{Cov}(X, Y) = \\operatorname{E}[(X - \\mu_X)(Y - \\mu_Y)].\n\\]\n \\(X\\)  \\(Y\\) discrete,\n\\[\n   \\operatorname{Cov}(X, Y) =\n   \\sum_{x} \\sum_{y} (x - \\mu_X)(y - \\mu_Y)\\, p_{X, Y}(x, y).\n\\]\n \\(X\\)  \\(Y\\) continuous,\n\\[\n   \\operatorname{Cov}(X, Y) =\n   \\int_{-\\infty}^\\infty\\!\\int_{-\\infty}^\\infty (x - \\mu_X)(y - \\mu_Y)\\, f_{X, Y}(x, y)\\, dx\\, dy.\n\\]covariance measure  \\(X\\)  \\(Y\\) vary jointly, sense positive covariance indicates ‘average’ \\(X\\)  \\(Y\\) increase (decrease) together whereas negative covariance indicates `average’  \\(X\\) increases  \\(Y\\) decreases (vice versa).\nsay covariance measure linear dependence.Covariance best evaluated computational formula.Theorem 5.9  (Covariance) random variables \\(X\\)  \\(Y\\),\n\\[\n   \\operatorname{Cov}(X, Y)\n   =\n   \\operatorname{E}[XY] - \\operatorname{E}[X]\\operatorname{E}[Y].\n\\]Proof. proof uses Theorems 5.1  5.8.\n\\[\\begin{align*}\n   \\operatorname{Cov}(X, Y)\n   &= \\operatorname{E}\\big[ (X - \\mu_X)(Y-\\mu_Y)\\big] \\\\\n   &= \\operatorname{E}[ XY - \\mu_X Y - \\mu_Y X + \\mu_X\\mu_Y] \\\\\n   &= \\operatorname{E}[ XY ] - \\mu_X\\operatorname{E}[Y] - \\mu_Y\\operatorname{E}[X] +  \\mu_X \\mu_Y \\\\\n   &= \\operatorname{E}[ XY ] - \\mu_X\\mu_Y - \\mu_Y\\mu_X +  \\mu_X \\mu_Y \\\\\n   &= \\operatorname{E}[ XY ] - \\mu_X \\mu_Y.\n\\end{align*}\\]Computing covariance tedious: \\(\\operatorname{E}[X]\\), \\(\\operatorname{E}[Y]\\), \\(\\operatorname{E}[XY]\\) need computed, joint marginal distributions  \\(X\\)  \\(Y\\) needed.Covariance units given product units  \\(X\\)  \\(Y\\).\nexample,  \\(X\\) measured metres  \\(Y\\) measured seconds \\(\\operatorname{Cov}(XY)\\) units metre–seconds.\ncompare strength covariation amongst pairs random variables, unitless measure useful.\nCorrelation scaling covariance terms standard deviations individual variables.Definition 5.13  (Correlation) correlation coefficient random variables \\(X\\)  \\(Y\\) denoted \\(\\text{Corr}(X, Y)\\) \\(\\rho_{X, Y}\\) defined \n\\[\n   \\rho_{X, Y}\n   = \\frac{\\operatorname{Cov}(X, Y)}{\\sqrt{ \\operatorname{var}[X]\\operatorname{var}[Y]}}\n   = \\frac{\\sigma_{X, Y}}{\\sigma_X \\sigma_Y}.\n\\]confusion random variables involved, write \\(\\rho\\) rather  \\(\\rho_{XY}\\).\ncan shown \\(-1 \\leq \\rho \\leq 1\\).Example 5.30  (Correlation coefficient (discrete rvs)) Consider two discrete random variables \\(X\\)  \\(Y\\) joint pf given Table 5.1.\ncompute correlation coefficient, following steps required.\\(\\text{Corr}(X, Y) = \\operatorname{Cov}(X, Y)/\\sqrt{ \\operatorname{var}[X]\\operatorname{var}[Y]}\\), \\(\\operatorname{var}[X]\\), \\(\\operatorname{var}[Y]\\) must computed;find \\(\\operatorname{var}[X]\\) \\(\\operatorname{var}[Y]\\), \\(\\operatorname{E}[X]\\) \\(\\operatorname{E}[X^2]\\), \\(\\operatorname{E}[Y]\\) \\(\\operatorname{E}[Y^2]\\) needed, marginal probability functions  \\(X\\)  \\(Y\\) needed.first, marginal pfs \n\\[\n   p_X(x) = \\sum_{y = -1, 1} p_{X, Y}(x, y) =\n      \\begin{cases}\n          7/24 & \\text{$x = 0$};\\\\\n          8/24 & \\text{$x = 1$};\\\\\n          9/24 & \\text{$x = 2$};\\\\\n          0 & \\text{otherwise}\n      \\end{cases}\n\\]\n\n\\[\n   p_Y(y) = \\sum_{x = 0}^2 p_{X, Y}(x, y) =\n      \\begin{cases}\n          1/2 & \\text{$y = -1$};\\\\\n          1/2 & \\text{$y = 1$};\\\\\n          0 & \\text{otherwise.}\n      \\end{cases}\n\\]\n,\n\\[\\begin{align*}\n   \\operatorname{E}[X]   &= (7/24 \\times 0) + (8/24 \\times 1) + (9/24\\times 2) = 26/24;\\\\\n   \\operatorname{E}[X^2] &= (7/24 \\times 0^2) + (8/24 \\times 1^2) + (9/24\\times 2^2) = 44/24;\\\\\n   \\operatorname{E}[Y]   &= (1/2 \\times -1) + (1/2 \\times 1) = 0;\\\\\n   \\operatorname{E}[Y^2] &= (1/2 \\times (-1)^2) + (1/2 \\times 1^2) = 1,\n\\end{align*}\\]\ngiving \\(\\operatorname{var}[X] = 44/24 - (26/24)^2 = 0.6597222\\) \\(\\operatorname{var}[Y] = 1 - 0^2 = 1\\).\n,\n\\[\\begin{align*}\n   \\operatorname{E}[XY] &= \\sum_x\\sum_y xy\\,p_{X,Y}(x,y) \\\\\n    &= (0\\times -1 \\times 1/8)  + (0\\times 1 \\times 1/6) + \\cdots + (2\\times 1 \\times 1/4) \\\\\n    &= 1/12.\n\\end{align*}\\]\nHence,\n\\[\n   \\operatorname{Cov}(X,Y)\n   = \\operatorname{E}[XY] - \\operatorname{E}[X] \\operatorname{E}[Y]\n   = \\frac{1}{12} - \\left(\\frac{26}{24}\\times 0\\right) = 1/12,\n\\]\n\n\\[\n   \\text{Corr}(X,Y)\n   = \\frac{ \\operatorname{Cov}(X,Y)}{\\sqrt{ \\operatorname{var}[X]\\operatorname{var}[Y] } }\n   = \\frac{1/12}{\\sqrt{0.6597222 \\times 1}}\n   = 0.1025978,\n\\]\ncorrelation coefficient  \\(0.10\\), small positive linear association exists  \\(X\\)  \\(Y\\).\nTABLE 5.1: bivariate discrete probability function.\n","code":""},{"path":"ChapExpectation.html","id":"properties-of-covariance-and-correlation","chapter":"5 Mathematical expectation","heading":"5.7.3 Properties of covariance and correlation","text":"correlation units.covariance units;  \\(X\\) measured kilograms  \\(Y\\) centimetres, units covariance kg-cm.units measurements change, numerical value covariance changes, numerical value correlation stays .\n(example,  \\(X\\) changed kilograms grams, numerical value correlation change value, numerical values covariance change.)correlation number  \\(-1\\)  \\(1\\) (inclusive).\ncorrelation coefficient (covariance) negative, negative linear relationship said exist two variables.\nLikewise, correlation coefficient (covariance) positive, positive linear relationship said exist two variables.correlation coefficient (covariance) zero, linear dependence said exist.Theorem 5.10  (Properties covariance) random variables \\(X\\), \\(Y\\)  \\(Z\\), constants \\(\\)  \\(b\\):\\(\\operatorname{Cov}(X, Y) = \\operatorname{Cov}(Y, X)\\).\\(\\operatorname{Cov}(aX,) = ab\\,\\operatorname{Cov}(X, Y)\\).\\(\\operatorname{var}[aX + ] = ^2\\operatorname{var}[X] + b^2\\operatorname{var}[Y] + ab\\,\\operatorname{Cov}(X, Y)\\).\\(X\\) \\(Y\\) independent, \\(\\operatorname{E}[XY] = \\operatorname{E}[X]\\operatorname{E}[Y]\\) hence \\(\\operatorname{Cov}(X,Y) = 0\\).\\(\\operatorname{Cov}(X, Y) = 0\\) imply \\(X\\)  \\(Y\\) independent, except special case bivariate normal distribution.zero correlation coefficient indication linear dependence .\nrelationship may still exist  \\(X\\)  \\(Y\\) even correlation zero.Example 5.31  (Linear dependence correlation) Consider \\(X\\) pf:, define \\(Y\\) explicitly related  \\(X\\): \\(Y = X^2\\).\n, know relationship exists  \\(X\\)  \\(Y\\) (relationship non-linear).\njoint probability function \\((X, Y)\\) shown Table 5.2.\n\n\\[\\begin{equation*}\n   \\operatorname{Cov}(X, Y)\n   = \\operatorname{E}[X, Y] - \\operatorname{E}[X]\\operatorname{E}[Y]\n   = 0 - 0\\times 2/3 = 0\n\\end{equation*}\\]\n\\(\\text{Corr}(X, Y) = 0\\).\n \\(X\\)  \\(Y\\) certainly related,  \\(Y\\) explicitly defined function  \\(X\\).Since correlation measure strength linear relationship two random variables, correlation zero simply indication linear relationship  \\(X\\)  \\(Y\\).\n(case example, may different relationship variables, linear relationship.)\nTABLE 5.2: bivariate discrete probability function.\n","code":""},{"path":"ChapExpectation.html","id":"ConditionalExpectation","chapter":"5 Mathematical expectation","heading":"5.7.4 Conditional expectations","text":"Conditional expectations simply expectations computed conditional distribution.conditional mean expected value computed conditional distribution.Definition 5.14  (Conditional expectation) conditional expected value conditional mean random variable \\(X\\) given \\(Y = y\\) denoted \\(\\operatorname{E}[X \\mid Y = y]\\).conditional distribution discrete probability mass function \\(p_{X\\mid Y}(x\\mid y)\\), \n\\[\n  \\operatorname{E}[X \\mid Y = y] =\n  \\displaystyle \\sum_{x} x \\cdot p_{X\\mid Y}(x\\mid y).\n\\]\nconditional distribution continuous probability density function \\(f_{X\\mid Y}(x\\mid y)\\), \n\\[\n  \\operatorname{E}[X \\mid Y = y] =\n  \\int_{-\\infty}^\\infty x \\cdot f_{X\\mid Y}(x\\mid y)\\, dx.\n\\]\\(\\operatorname{E}[X \\mid Y = y]\\) typically denoted \\(\\mu_{X \\mid Y = y}\\).Example 5.32  (Conditional mean (continuous)) Consider two random variables \\(X\\)  \\(Y\\) joint PDF\n\\[\n   f_{X, Y}(x, y) =\n      \\begin{cases}\n         \\frac{3}{5}(x + xy + y^2) & \\text{$0 < x < 1$ $-1 < y < 1$};\\\\\n         0 & \\text{otherwise.}\n      \\end{cases}\n\\]\nfind \\(f_{Y \\mid X = x}(y\\mid x)\\), first \\(f_X(x)\\) needed:\n\\[\n   f_X(x) = \\int_{-1}^1 f_{X,Y}(x,y) dy = \\frac{3}{15}(6x + 2)\n\\]\n\\(0 < x < 1\\).\n,\n\\[\n   f_{Y \\mid X = x}(y \\mid x)\n   = \\frac{ f_{X, Y}(x, y)}{ f_X(x) }\n   = \\frac{3(x + xy + y^2)}{6x + 2}\n\\]\n\\(-1 < y < 1\\) given \\(0 < x < 1\\).\nexpected value  \\(Y\\) given \\(X = x\\) \n\\[\n  \\operatorname{E}[Y\\mid X = x]\n   = \\frac{x}{3x + 1}.\n\\]\nexpression indicates conditional expected value  \\(Y\\) depends given value  \\(X\\); example,\n\\[\\begin{align*}\n   \\operatorname{E}[Y\\mid X = 0]   &= 0;\\\\\n   \\operatorname{E}[Y\\mid X = 0.5] &= 0.2;\\\\\n   \\operatorname{E}[Y\\mid X = 1]   &= 1/4.\n\\end{align*}\\]\nSince \\(\\operatorname{E}[Y\\mid X = x]\\) depends value  \\(X\\), means \\(X\\)  \\(Y\\) independent.conditional variance variance computed conditional distribution.Definition 5.15  (Conditional variance) conditional variance random variable \\(X\\) given \\(Y = y\\) denoted \\(\\operatorname{var}[X \\mid Y = y]\\).conditional distribution discrete probability mass function \\(p_{X\\mid Y}(x\\mid y)\\), \n\\[\n  \\operatorname{var}[X \\mid Y = y] =\n  \\displaystyle \\sum_{x} (x - \\mu_{X\\mid y})^2\\, p_{X\\mid Y}(x\\mid y),\n\\]\n\\(\\mu_{X \\mid y}\\) conditional mean  \\(X\\) given \\(Y = y\\).conditional distribution continuous probability density function \\(f_{X\\mid Y}(x\\mid y)\\), \n\\[\n  \\operatorname{var}[X \\mid Y = y] =\n  \\int_{-\\infty}^\\infty (x - \\mu_{X\\mid y})^2\\, f_{X\\mid Y}(x\\mid y)\\, dx.\n\\]\n\\(\\mu_{X \\mid y}\\) conditional mean  \\(X\\) given \\(Y = y\\).brevity, \\(\\operatorname{var}[X \\mid Y = y]\\) often denoted \\(\\sigma^2_{X \\mid Y = y}\\).Example 5.33  (Conditional variance (continuous)) Refer Example 5.32.\nconditional variance  \\(Y\\) given \\(X = x\\) can found first computing \\(\\operatorname{E}[Y^2\\mid X = x]\\):\n\\[\\begin{align*}\n   \\operatorname{E}[Y^2\\mid X = x]\n   &= \\int_{-1}^1 y^2 f_{Y\\mid X = x}(y\\mid x)\\,dy \\\\\n   &= \\frac{3}{6x + 2} \\int_{-1}^1 y^2 (x + xy + y^2)\\, dy \\\\\n   &= \\frac{5x + 3}{5(3x + 1)}.\n\\end{align*}\\]\nconditional variance \n\\[\\begin{align*}\n   \\operatorname{var}[Y\\mid X = x]\n   &= \\operatorname{E}[Y^2\\mid X = x] - \\left( \\operatorname{E}[Y\\mid X = x] \\right)^2 \\\\\n   &= \\frac{5x+3}{5(3x + 1)} - \\left( \\frac{x}{3x + 1}\\right)^2 \\\\\n   &= \\frac{10x^2 + 14x + 3}{5(3x + 1)^2}\n\\end{align*}\\]\ngiven \\(0 < x < 1\\).\nHence variance  \\(Y\\) depends value  \\(X\\) given; example,\n\\[\\begin{align*}\n   \\operatorname{var}[Y\\mid X = 0]   &= 3/5 = 0.6\\\\\n   \\operatorname{var}[Y\\mid X = 0.5] &= \\frac{10\\times (0.5)^2 + (14\\times0.5) + 3}{5(3\\times0.5 + 1)^2} = 0.4\\\\\n   \\operatorname{var}[Y\\mid X = 1]   &= 27/80 = 0.3375.\n\\end{align*}\\]general, compute conditional variance \\(X\\mid Y = y\\) given joint probability function, following steps required.Find marginal distribution  \\(Y\\).Use compute conditional probability function \\(p_{X \\mid Y = y}(x \\mid y) = p_{X, Y}(x, y)/p_{X}(x)\\).Find conditional mean \\(\\operatorname{E}[X \\mid Y = y]\\).Find conditional second raw moment \\(\\operatorname{E}[X^2 \\mid Y = y]\\).Finally, compute \\(\\operatorname{var}[X\\mid Y = y] = \\operatorname{E}[X^2\\mid Y=y] - (\\operatorname{E}[X\\mid Y=y])^2\\).Example 5.34  (Conditional variance (discrete)) Two discrete random variables \\(U\\)  \\(V\\) joint probability function given Table 5.3.\nfind conditional variance  \\(V\\) given \\(U = 11\\), use steps .First, find marginal distribution  \\(U\\):\n\\[\n      p_U(u) = \\begin{cases}\n         4/9 & \\text{$u = 10$};\\\\\n         7/18 & \\text{$u = 11$};\\\\\n         1/6 & \\text{$u = 12$};\\\\\n         0 & \\text{otherwise.}\\\\\n         \\end{cases}\n\\]\nSecondly, compute conditional probability function:\n\\[\\begin{align*}\n      p_{V\\mid U = 11}(v \\mid u = 11)\n      &= p_{U, V}(u,v)/p_{U}(u = 11) \\\\\n      &= \\begin{cases}\n             \\frac{1/18}{7/18} = 1/7 & \\text{$v = 0$};\\\\\n             \\frac{1/3}{7/18}  = 6/7 & \\text{$v = 1$}\n          \\end{cases}\n   \\end{align*}\\]\nusing \\(p_U(u = 11) = 7/18\\) Step 1.Thirdly, find conditional mean:\n\\[\n  \\operatorname{E}[V\\mid U = 11]\n  = \\sum_v v p_{V\\mid U}(v\\mid u = 11)\n  = \\left(\\frac{1}{7}\\times 0\\right) + \\left(\\frac{6}{7}\\times 1\\right)  \n  = 6/7.\n\\]\nFourthly, find conditional second raw moment:\n\\[\n  \\operatorname{E}[V^2\\mid U]\n  = \\sum_v v^2 p_{V\\mid U}(v\\mid u = 11)\n  = \\left(\\frac{1}{7}\\times 0^2\\right) + \\left(\\frac{6}{7}\\times 1^2\\right)  \n  = 6/7.\n\\]\nFinally, compute:\n\\[\\begin{align*}\n  \\operatorname{var}[V\\mid U = 11]\n  &= \\operatorname{E}[V\\mid U = 11] - (\\operatorname{E}[V\\mid U = 11])^2\\\\\n  &= (6/7) - (6/7)^2\\\\\n  &\\approx  0.1224.\n\\end{align*}\\]\nTABLE 5.3: bivariate discrete probability function.\n","code":""},{"path":"ChapExpectation.html","id":"numerical-approaches","chapter":"5 Mathematical expectation","heading":"5.8 Numerical approaches","text":"distributions written closed form (.e., neat function standard mathematical functions), makes (example) evaluation probability computation means difficult.\ncases, numerical methods, numerical integration, may needed.Consider random variable \\(X\\) density function\n\\[\n  f_X(x) = \\frac{c}{\\sqrt{x}}\\exp( -x - \\sqrt{2x})\\quad \\text{$x > 0$}.\n\\]\nnormalising constant \\(c\\).\ndensity function closed form, value  \\(c\\) found via integration.\nHowever, value  \\(c\\) found using numerical integration R using integrate():, \\(c = 1.0784\\dots\\).\ndistribution function can even found:now density distribution function can plotted (Fig. 5.7):\nFIGURE 5.7: density function (left) distribution function (right) distribution expressed closed form.\nProbabilities can computed; example, can find \\(\\Pr(X > 0.5)\\):expected value  \\(X\\), \\(\\operatorname{E}[X]\\), can found:","code":"\n### Define the function, without the constant term\ng <- function(x){\n  ifelse(x > 0, \n         x^(-0.5) * exp(-x - sqrt(2 * x)), # When x > 0\n         0)                                # When x <= 0\n}\n\n### Integrate between 0 and infinity:\n#\\n adds a \"new line\"\nConst <- 1 / integrate(g, \n                       lower = 0, \n                       upper = Inf)$value\ncat(\"The value of  c  is about\", round(Const, 4), \"\\n\")\n#> The value of  c  is about 1.0784\n\n# So now define f(x):\nf <- function(x){\n  ifelse(x > 0, \n         x^(-0.5) * exp(-x - sqrt(2 * x)) * Const,  # When x > 0\n         0)                                         # When x <= 0\n}\nF <- function(x){\n  F <- array(dim = length(x) )\n  for (i in 1:length(x)){\n    F[i] <- integrate( f, \n                       lower = 0,\n                       upper = x[i])$value\n  }\n  return(F)\n}\n### Make room for two plots side-by-side\npar( mfrow = c(1, 2))\n\n### Evaluate over these values of x\nx <- seq(0.01, 2, \n         length = 1000)\n\n### Now plot\nplot( f(x) ~ x,\n      type = \"l\",\n      las = 1,\n      lwd = 3,\n      main = \"Density function\",\n      xlab = expression(italic(x)),\n      ylab = \"Density fn.\")\nplot( F(x) ~ x,\n      type = \"l\",\n      las = 1,\n      lwd = 3,\n      main = \"Distribution function\",\n      xlab = expression(italic(x)),\n      ylab = \"Distribution fn.\")\nintegrate(f,\n          lower = 0.5,\n          upper = Inf)\n#> 0.1433935 with absolute error < 4.4e-06\nf_Expected <- function(x){\n  x * f(x)\n}\nintegrate(f_Expected, \n          lower = 0,\n          upper = Inf)\n#> 0.2374324 with absolute error < 2.1e-07"},{"path":"ChapExpectation.html","id":"ExercisesChapExpectation","chapter":"5 Mathematical expectation","heading":"5.9 Exercises","text":"Selected answers appear Sect. E.5.Exercise 5.1  PDF random variable \\(Y\\) defined \n\\[\n   f_Y(y) = \\begin{cases}\n               2y + k & \\text{$1\\le y \\le 2$};\\\\\n               0      & \\text{elsewhere}.\n            \\end{cases}\n\\]Find value  \\(k\\).Plot PDF  \\(Y\\).Compute \\(\\operatorname{E}[Y]\\).Compute \\(\\operatorname{var}[Y]\\).Compute \\(\\Pr(Y > 1.5)\\).Exercise 5.2  PDF random variable \\(X\\) defined \n\\[\n   f_Y(y) = \\begin{cases}\n               2(x + 1)/ 3 & \\text{$-1\\le x \\le 0$};\\\\\n               (2 - x)/ 3  & \\text{$0\\le x \\le 2$};\\\\\n               0           & \\text{elsewhere}.\n            \\end{cases}\n\\]Plot PDF  \\(X\\).Compute \\(\\operatorname{E}[X]\\).Compute \\(\\operatorname{var}[X]\\).Compute \\(\\Pr(X > 0)\\).Exercise 5.3  random variable \\(T\\) density function\n\\[\n  f_T(t) =\n  \\begin{cases}\n    k         & \\text{$0 < t < 1$};\\\\\n    2k(2 - t) & \\text{$1 < t < 2$}.\n  \\end{cases}\n\\]Plot PDF  \\(T\\).Compute \\(\\operatorname{E}[T]\\).Compute \\(\\operatorname{var}[T]\\).Find plot distribution function  \\(T\\).Compute \\(\\Pr(T \\le 1)\\).Exercise 5.4  random variable \\(Z\\) density function\n\\[\n  f_Z(z) =\n  \\begin{cases}\n    1 - z  & \\text{$0 < z < 1$};\\\\\n    2 + z  & \\text{$2 < z < 3$};\\\\\n    0      & \\text{elsewhere}.\n  \\end{cases}\n\\]Plot PDF  \\(Z\\).Compute \\(\\operatorname{E}[Z]\\).Compute \\(\\operatorname{var}[Z]\\).Find plot distribution function  \\(Z\\).Compute \\(\\Pr(Z > 2 \\mid Z > 1)\\).Exercise 5.5  PMF random variable \\(D\\) defined \n\\[\n   p_D(d) =\n   \\begin{cases}\n      1/2 & \\text{$d = 1$};\\\\\n      1/4 & \\text{$d = 2$};\\\\\n      k   & \\text{$d = 3$};\\\\\n      0 & \\text{otherwise},\n   \\end{cases}\n\\]\nconstant \\(k\\).Plot probability mass function.Compute mean variance  \\(D\\).Find MGF  \\(D\\).Compute mean variance  \\(D\\) MGF.Compute \\(\\Pr(D < 3)\\).Exercise 5.6  PMF random variable \\(Z\\) defined \n\\[\n   p_Z(z) =\n   \\begin{cases}\n      c/z^2 & \\text{$z = 1, 2, 3, 4$};\\\\\n      0 & \\text{otherwise},\n   \\end{cases}\n\\]\nconstant \\(c\\).Plot probability mass function.Compute mean variance  \\(Z\\).Find MGF  \\(Z\\).Compute mean variance  \\(Z\\) MGF.Compute \\(\\Pr(Z \\ge 2)\\).Exercise 5.7  MGF discrete random variable \\(Z\\) \n\\[\n   M_Z(t) = [0.3\\exp(t) + 0.7]^2.\n\\]Compute mean variance  \\(Z\\).Find probability function  \\(Z\\).Exercise 5.8  MGF discrete random variable \\(W\\) \n\\[\n   M_W(t) = \\frac{p}{1 - (1 - p)\\exp(w)}\\quad\\text{$t < -\\log(1 - p)$}.\n\\]Compute mean variance  \\(W\\).Find probability mass function  \\(W\\).Exercise 5.9  random variable \\(\\) mean \\(13\\) variance \\(5\\).\nrandom variable \\(B\\) mean \\(4\\) variance \\(2\\).\nAssuming \\(\\)  \\(B\\) independent, find:\\(\\operatorname{E}[+ B]\\).\\(\\operatorname{var}[+ B]\\).\\(\\operatorname{E}[2A - 3B]\\).\\(\\operatorname{var}[2A - 3B]\\).Exercise 5.10  Repeat Exercise 5.9, \\(\\operatorname{Cov}(, B) = 0.2\\).Exercise 5.11  MGF  \\(G\\) \\(M_G(t) = (1 - \\beta t)^{-\\alpha}\\) ( \\(\\alpha\\)  \\(\\beta\\) constants).\nFind mean variance  \\(G\\).Exercise 5.12  Suppose PDF  \\(X\\) \n\\[\n   f_X(x) = \\begin{cases}\n               2(1 - x) & \\text{$0 < x < 1$};\\\\\n               0 & \\text{otherwise}.\n            \\end{cases}\n\\]Find \\(r\\)th raw moment  \\(X\\).Find \\(r\\)th central moment  \\(X\\).Find \\(\\operatorname{E}\\big[(X + 3)^2\\big]\\) using previous answer.Find value skewness \\(\\gamma_1\\) using previous results.Find value excess kurtosis \\(\\gamma_2\\) using previous results.Find variance  \\(X\\).Exercise 5.13  Suppose PDF  \\(X\\) \n\\[\n   f_X(x) = \\begin{cases}\n               1/x^2 & \\text{$1 < x < \\infty$};\\\\\n               0     & \\text{otherwise}.\n            \\end{cases}\n\\]Find \\(r\\)th raw moment  \\(X\\).Find \\(r\\)th central moment  \\(X\\).Find \\(\\operatorname{E}\\big[(X -1)^2\\big]\\) using previous results.Find value skewness \\(\\gamma_1\\) using previous results.Find value excess kurtosis \\(\\gamma_2\\) using previous results.Find variance  \\(X\\).Exercise 5.14  Find MGF continuous random variable \\(Y\\) probability density function\n\\[\n  f_X(x) = 1/2\\quad\\text{$3 < x < 5$}.\n\\]Exercise 5.15  Find MGF continuous random variable \\(R\\) probability density function\n\\[\n  f_R(r) = 6 r (1 - r) \\quad\\text{$0 < r < 1$}.\n\\]Exercise 5.16  Consider PDF\n\\[\n  f_Y(y) = \\frac{2}{y^2}\\qquad y\\ge 2.\n\\]Show mean distribution defined.Show variance exist.Plot probability density function suitable range.Plot distribution function suitable range.Determine median distribution.Determine interquartile range distribution.\n(interquartile range measure spread, calculated difference third quartile first quartile.\nfirst quartile value \\(25\\)% data lie; third quartile value \\(75\\)% data lie.)Find \\(\\Pr(Y > 4 \\mid Y > 3)\\).Exercise 5.17  Cauchy distribution PDF\n\\[\\begin{equation}\n   f = \\frac{1}{\\pi(1 + x^2)}\\quad\\text{$x\\\\mathbb{R}$}.\n   \\tag{5.6}\n\\end{equation}\\]Use R draw probability density function.Compute distribution function  \\(X\\).\n, use R draw function.Show mean Cauchy distribution defined.Find variance mode Cauchy distribution.Exercise 5.18  exponential distribution PDF\n\\[\n   f_Y(y) = \\frac{1}{\\lambda}\\exp( -y/\\lambda)\n\\]\n(\\(\\lambda > 0\\)) \\(y > 0\\) zero elsewhere.Determine moment-generating function  \\(Y\\).Use moment-generating function compute mean variance exponential distribution.Exercise 5.19  Prove continuous random variable \\(X\\) distribution symmetric  \\(0\\) \\(M_X(t) = M_{-X}(t)\\).\nHence prove random variable, odd moments origin zero.Exercise 5.20  continuous random variable \\(X\\) defined probability density function\n\\[\n  f_X(x) = \\frac{x + + 1}{2(2 + )}\\quad \\text{$0 \\le x \\le 2$}\n\\]\nreal value \\(\\).Find possible values  \\(\\) \\(f_X(x)\\) valid probability function.\\(\\operatorname{E}[X] = 4/3\\), find values  \\(\\) \\(f_X(x)\\) valid probability function.Exercise 5.21  continuous random variable \\(X\\) defined probability density function\n\\[\n  f_X(x) = x^{2a} - x^+ 7/6\\quad \\text{$0 \\le x \\le 1$}\n\\]\nreal value \\(\\).Find possible values  \\(\\) \\(f_X(x)\\) valid probability function.\\(\\operatorname{E}[X] > 1/2\\), find values  \\(\\) \\(f_X(x)\\) valid probability function.Exercise 5.22  (exercise follows Exercise 3.16.)\nstudy modelling waiting times hospital (Khadem et al. 2008), patients classified one three categories:Red: Critically ill injured patients.Yellow: Moderately ill injured patients.Green: Minimally injured uninjured patients.‘Yellow’ patients, service time doctors modelled using triangular distribution, minimum \\(3.5\\,\\text{mins}\\), maximum \\(30.5\\,\\text{mins}\\) mode \\(5\\,\\text{mins}\\).Compute mean service times.Compute variance service times.Exercise 5.23  (Exercise follows Ex. 3.18.)\nFive people, including friend, line random.\nrandom variable \\(X\\) denotes number people friend.Use probability function  \\(X\\) found Ex. 3.18, find mean number people friend.\nSimulate R confirm answer.Exercise 5.24  characteristic function random variable \\(X\\), denoted \\(\\varphi(t)\\), defined \\(\\varphi_X(t) = \\operatorname{E}[\\exp(t X)]\\), \\(= \\sqrt{-1}\\).\nUnlike MGF, characteristic function always defined, sometimes preferred MGF.Show \\(M_X(t) = \\varphi_X(-)\\).Show mean random variable \\(X\\) given \\(-\\varphi'(0)\\) (, , notation means compute derivative \\(\\varphi(t)\\) respect  \\(t\\), evaluate \\(t = 0\\)).Exercise 5.25  App. B.2 prove useful.Write first three terms general term expansion \\((1 - )^{-1}\\).Write first three terms general term expansion \\(\\operatorname{E}\\left[(1 - tX)^{-1}\\right]\\).Suppose \\(\\mathcal{R}_X(t) = \\operatorname{E}\\left [(1 - tX)^{-1} \\right]\\), called geometric generating function  \\(X\\).\nSuppose random variable \\(Y\\) uniform distribution \\((0, 1)\\); .e., \\(f_Y(y) = 1\\) \\(0 < y < 1\\).\nDetermine geometric generating function  \\(Y\\) definition expected value.\nanswer involve term \\(\\log(1 - t)\\).Using answer Part 3, expand term \\(\\log(1 - t)\\) writing terms infinite series.Equate two series expansions Part 2 Part 4 determine expression \\(\\operatorname{E}[Y^n]\\), \\(n = 1, 2, 3,\\dots\\).Exercise 5.26  Suppose random variable \\(X\\) defined \n\\[\n   f_X(x) = k (3x^2 + 4)\\quad\\text{$-c < x < c$},\n\\]\nzero elsewhere.\nSolve  \\(c\\)  \\(k\\) \\(\\operatorname{var}[X] = 28/15\\).\n(Hint: Make sure use properties given probability distribution embarking complicated expressions!)Exercise 5.27  Suppose random variable \\(Y\\) defined \n\\[\n   f_Y(y) =\n   \\begin{cases}\n     c          & \\text{$0 < y < 1$}\\\\\n     k(y - 4)/3 & \\text{$1 < y < 4$};\\\\\n     0          & \\text{elsewhere}.\n  \\end{cases}\n\\]values  \\(c\\)  \\(k\\) possible?\\(c = k\\), values  \\(c\\)  \\(k\\)?\\(k = 2\\), value  \\(c\\)?Exercise 5.28  Suppose random variable \\(Y\\) defined \n\\[\n   f_Y(y) =\n   \\begin{cases}\n     \\exp(-y^2)    & \\text{$0 < y < k$}\\\\\n     0             & \\text{elsewhere},\n  \\end{cases}\n\\]\n\\(\\operatorname{E}[Y] = 1/2\\).\nvalue  \\(k\\)?Exercise 5.29  Suppose random variable \\(X\\) defined \n\\[\n   f_X(x) =\n   \\begin{cases}\n     x^r     & \\text{$0 < x < 5$}\\\\\n     0       & \\text{elsewhere},\n  \\end{cases}\n\\]\n\\(\\operatorname{E}[X] = 625\\).\nvalue  \\(r\\)?Exercise 5.30  Benford’s law (also see Exercise 3.24) describes distribution leading digits numbers span many orders magnitudes (e.g., lengths rivers) \n\\[\n   p_D(d) = \\log_{10}\\left( \\frac{d + 1}{d} \\right) \\quad\\text{$d\\\\{1, 2, \\dots 9\\}$},\n\\]\nFind mean  \\(D\\) (.e., mean leading digit).Exercise 5.31  von Mises distribution used model angular data.\nprobability function \n\\[\n  p_Y(y) = k \\exp\\{ \\lambda\\cos(y - \\mu) \\}\n\\]\n\\(0\\le y < 2\\pi\\), \\(0 \\le \\mu < 2\\pi\\)  \\(\\mu\\) mean, \\(\\lambda > 0\\).Show constant \\(k\\) function  \\(\\lambda\\) .Find median distribution.Using R, numerically integrate find value \\(k\\) \\(\\mu = \\pi/2\\) \\(\\lambda = 1\\).distribution function closed form.\nUse R plot distribution function \\(\\mu = \\pi/2\\) \\(\\lambda = 1\\).Exercise 5.32  inverse Gaussian distribution PDF\n\\[\n   P_Y(y) = \\frac{1}{\\sqrt{2\\pi y^3\\phi}} \\exp\\left\\{ -\\frac{1}{2\\phi} \\frac{(y - \\mu)^2}{y\\mu^2}\\right\\}\n\\]\n\\(y > 0\\), \\(\\mu > 0\\) \\(\\phi > 0\\).Plot distribution \\(\\mu = 1\\) various values  \\(\\phi\\); comment.MGF \n\\[\nM_Y(t) = \\exp\\left\\{ \\frac{\\lambda}{\\mu} \\left( 1 - \\sqrt{1 - \\frac{2\\mu^2 t}{\\lambda}} \\right) \\right\\}.\n\\]\nUse MGF deduce mean variance inverse Gaussian distribution.Exercise 5.33  Consider random variable \\(W\\) \n\\[\n   f_W(w) = \\frac{c}{w^3}\\quad\\text{$w > c$.}\n\\]Find value  \\(c\\).Find \\(\\operatorname{E}[W]\\).Find \\(\\operatorname{var}[W]\\).Exercise 5.34  Pareto distribution distribution function\n\\[\n   F_X(x) =\n   \\begin{cases}\n      1 - \\left(\\frac{k}{x}\\right)^\\alpha & \\text{$x > k$}\\\\\n      0                                  & \\text{elesewhere},\n   \\end{cases}\n\\]\n\\(\\alpha > 0\\) parameter \\(k\\),values  \\(k\\) possible?Find density function Pareto distribution.Compute mean variance Pareto distribution.Find mode Pareto distribution.Plot distribution \\(\\alpha = 3\\) \\(k = 3\\).\\(\\alpha = 3\\) \\(k = 3\\), compute \\(\\Pr(X > 4 \\mid X < 5)\\).Pareto distribution often used model incomes.\nexample, “\\(80\\)-–\\(20\\) rule” states  \\(20\\)% people receive \\(80\\)% income (, ,  \\(20\\)% highest-earning \\(20\\)% receive \\(80\\)%  \\(80\\)%).\nFind value  \\(\\alpha\\) rule holds.Exercise 5.35  mixture distribution mixture two univariate distributions.\nexample, heights adults may follow mixture distribution: one normal distribution adult females, another adult males.\nset probability functions \\(p^{()}_X(x)\\) \\(= 1, 2, \\dots n\\) set weights \\(w_i\\) \\(\\sum w_i = 1\\) \\(w_i \\ge 0\\) \\(\\), mixture distribution \\(f_X(x)\\) \n\\[\n   f_X(x) = \\sum_{= 1}^n w_i p^{()}_X(x).\n\\]Compute distribution function \\(p_X(x)\\).Compute mean variance \\(f_X(x)\\).Consider case \\(p^{()}_X(x)\\) normal distribution \\(= 1, 2, 3\\), means \\(-1\\), \\(2\\), \\(4\\) respectively, variances \\(1\\), \\(1\\) \\(4\\) respectively.\nPlot probability density function \\(f_X(x)\\) various instructive values weights.Suppose heights female adults normal distribution mean \\(163\\,\\text{cm}\\) standard deviation \\(5\\,\\text{cm}\\), adult males heights mean \\(175\\,\\text{cm}\\) standard deviation \\(7\\,\\text{cm}\\), constitute \\(48\\)% population (Australian Bureau Statistics 1995).\nDeduce plot probability density heights adult Australians.Exercise 5.36  Consider distribution \n\\[\n   p_X(x) =\n   \\begin{cases}\n      1/K                     & \\text{$x = 1$};\\\\\n      1/\\left(x(x - 1)\\right) & \\text{$x = 2, 3, \\dots, K$};\\\\\n      0                       & \\text{elsewhere}\n   \\end{cases}\n\\]\n\\(K > 2\\).Find mean variance  \\(X\\) (well possible).Plot distribution various values  \\(K\\).\\(K = 6\\), determine MGF.Exercise 5.37  random variable \\(V\\) PMF\n\\[\n   f_V(v) = (1 - p)^{v - 1} p\\quad\\text{$v = 1, 2, \\dots$},\n\\]\nzero elsewhere.Show valid PMF.Find \\(\\operatorname{E}[V]\\).Find \\(\\operatorname{var}[V]\\).Exercise 5.38  Two dice rolled.\nDeduce PMF absolute difference two numbers appear uppermost.Exercise 5.39  random variable \\(Y\\) PMF\n\\[\n   p_Y(y) = \\frac{e^{-\\lambda}\\lambda^y}{y!}\\quad\\text{$y = 0, 1, 2, \\dots$},\n\\]\n\\(\\lambda > 0\\).\nFind MGF  \\(Y\\), hence show \\(\\operatorname{E}[Y] = \\operatorname{var}[Y]\\).Exercise 5.40  practice, distributions written closed form, can given writing moment-generating function.\nevaluate density requires infinite summation infinite integral.\nGiven moment-generating function \\(M(t)\\), probability density function can reconstructed numerically \nintegral using inversion formula Eq. (5.4).evaluation integral generally requires advanced numerical techniques.\nquestion, just consider exponential distribution simple example demonstrate use inversion formula.Write expression Eq. (5.4) case exponential distribution, \\(M_X(t) = \\lambda/(\\lambda - t)\\) \\(t < \\lambda\\).real part integral needed.\nExtract real parts expression, simplify integrand.\n(integrand expression integrated.)Plot integrand last part \\(t = -50\\) \\(t = 2\\) case \\(\\lambda = 2\\) \\(x = 1\\).Exercise 5.41  density function random variable \\(X\\) given \n\\[\n   f(x) =  x e^{-x} \\quad\\text{$x > 0$}.\n\\]Determine moment-generating function (MGF)  \\(X\\).Use MGF verify \\(\\operatorname{E}[X] = \\operatorname{var}[X]\\).Suppose \\(Y = 1 - X\\).\nDetermine \\(\\operatorname{E}[Y]\\).Exercise 5.42  Gumbel distribution cumulative distribution function\n\\[\\begin{equation}\n   F(x; \\mu, \\beta) = \\exp\\left[ -\\exp\\left( -\\frac{x - \\mu}{\\sigma}\\right)\\right]\n   \\tag{5.7}\n\\end{equation}\\]\n(\\(\\mu > 0\\) \\(\\sigma > 0\\)) often used model extremes values (distribution maximum river height).Deduce probability function Gumbel distribution.Plot Gumbel distribution variety parameters.maximum daily precipitation (mm) Oslo, Norway, well-modelled using Gumbel distribution \\(\\mu = 2.6\\,\\text{mm}\\) \\(\\sigma = 1.86\\,\\text{mm}\\).\nDraw distribution, explain means.Exercise 5.43  density function random variable \\(X\\) given \n\\[\n   f(x) =  x e^{-x} \\quad\\text{$x > 0$}.\n\\]Determine \\(\\operatorname{E}[X]\\).Verify \\(\\operatorname{E}[X] = \\operatorname{var}[X]\\).Exercise 5.44  (exercise follows Ex. 3.22.)\ndetect disease population blood test, usually every individual tested.\ndisease uncommon, however, alternative method often efficient.alternative method (called pooled test), blood  \\(n\\) individuals combined, one test conducted.\ntest returns negative result, none  \\(n\\) people disease; test returns positive result,  \\(n\\) individuals tested individually identify individual(s) disease.Suppose disease occurs unknown proportion people \\(p\\) people.\nLet \\(X\\) number tests performed group  \\(n\\) individuals.expected number tests needed group  \\(n\\) people using pooled method?variance number tests needed group  \\(n\\) people using pooled method?Explain happens mean variance \\(p \\1\\) \\(p \\0\\), results make sense context questionIf pooling used group \\(n\\) people, number tests  \\(n\\): one person.\nDeduce expression value  \\(p\\) expected number tests using pooled approach exceeds non-pooled approach.Produce well-labelled plot showing expected number tests saved using pooled method \\(p = 0.1\\) values \\(n\\) \\(2\\) \\(10\\), comment shows practically.Suppose test costs $\\(15\\).\nexpected cost-saving using pooled-testing method \\(n = 4\\) \\(p = 0.1\\), \\(200\\) people must tested?Exercise 5.45  Consider rolling fair, six-sided die.\n‘running total’ total numbers rolled die.Find probability mass function  \\(R\\), number rolls needed obtain running total \\(3\\) .Find expected number rolls running total reaches \\(3\\) .Exercise 5.46  Besides variance, alternative measure variation mean absolute deviation (MAD), defined \\(\\operatorname{E}[\\,|X - \\mu|\\,]\\).Consider fair die described Example 5.9.Find \\(\\operatorname{E}[X]\\).Find \\(\\operatorname{MAD}[X]\\) using definition.Exercise 5.47  Suppose random variable \\(W\\) probability density function\n\\[\n  f_W(w) = K \\, \\exp(-w^4)\\quad\\text{$w\\\\mathbb{R}$,}\n\\]\nnormalising constant \\(K\\).Using computer, determine value  \\(K\\).Plot density function distribution function  \\(W\\).Using computer, find mean distribution.Using computer, find variance distribution.Using computer, find \\(\\Pr(W < 1 \\mid W > -1)\\).Exercise 5.48  Suppose random variable \\(Y\\) probability density function\n\\[\n  f_Y(y) = k\\, y^{\\alpha - 1} (1 + y)^{-\\alpha-\\beta}\\quad\\text{$y > 0$,}\n\\]\nnormalising constant \\(k\\), \\(\\alpha > 0\\) \\(\\beta > 0\\).Using computer, determine value  \\(k\\) \\(\\alpha = 0.5\\) \\(\\beta = 2.5\\).Plot density function distribution function  \\(Y\\).Using computer, find mean distribution.\n(Compare theoretical mean \\(\\operatorname{E}[Y] = \\alpha/(\\beta - 1)\\) provided \\(\\beta > 1\\).)Using computer, find variance distribution.Using computer, find \\(\\Pr(Y < 1)\\).mean defined \\(\\beta < 1\\).happens use computer produce density function \\(\\alpha = 0.5\\) \\(\\beta = 0.5\\)?simulation suggest value \\(\\operatorname{E}[Y]\\) \\(\\alpha = 0.5\\) \\(\\beta = 0.5\\)?Exercise 5.49  Consider random variable \\(X\\) probability density function\n\\[\n  f_X(x) = 3 x^2/2\\quad\\text{$-1 < x < 1$}\n\\]\nzero elsewhere.Plot PDF.Compute mean variance.Without computation, determine skewness.Compute kurtosis, show excess kurtosis negative value.text states distributions negative excess kurtosis (platykurtic distributions)\nfewer, less extreme, observations tail compared normal distribution.\nExplain distribution negative kurtosis.Exercise 5.50  Consider random variable \\(X\\) probability density function\n\\[\n  f_X(x) = x/2 \\quad\\text{$0 < x < 2$}\n\\]\nzero elsewhere.Plot PDF.Find expression \\(r\\)th raw moment.Compute mean variance.Compute skewness, explain value means.Compute kurtosis, explain value means.","code":""},{"path":"ChapterTransformations.html","id":"ChapterTransformations","chapter":"6 Transformations of random variables","heading":"6 Transformations of random variables","text":"completion chapter, able :derive distribution transformed variable, given distribution original variable, using distribution function method, change variable method, moment-generating function method appropriate.find joint distribution two transformed variables bivariate situation.","code":""},{"path":"ChapterTransformations.html","id":"introduction","chapter":"6 Transformations of random variables","heading":"6.1 Introduction","text":"\nchapter, consider distribution random variable \\(Y = u(X)\\), given random variable \\(X\\) known distribution, function \\(u(\\cdot)\\).\nAmong several available techniques, three considered:change variable method (Sect. 6.2);distribution function method continuous random variable (Sect. 6.3);moment-generating function method (Sect. 6.4).important concept context one--one transformation.Definition 6.1  (One--one transformation) Given random variables \\(X\\)  \\(Y\\) range spaces \\(\\mathcal{R}_X\\)  \\(\\mathcal{R}_Y\\) respectively, function \\(u(\\cdot)\\) one--one transformation (mapping) , \\(y\\\\mathcal{R}_Y\\), corresponds exactly one \\(x\\\\mathcal{R}_X\\).\\(Y = u(X)\\) one--one transformation, inverse function uniquely defined; , \\(X\\) can written uniquely terms  \\(Y\\).\nimportant considering distribution  \\(Y\\) distribution  \\(X\\) known.Example 6.1  transformation \\(Y = (X - 1)^2\\) one--one transformation \\(\\mathcal{R}_X = \\mathbb{R}\\); , \\(X\\) defined \\((-\\infty, +\\infty)\\).\nexample, inverse transformation \\(X = 1 \\pm \\sqrt{Y}\\), two values  \\(X\\) exists given value \\(Y > 0\\) (Fig. 6.1, left panel).However, random variable \\(X\\) defined \\(X > 2\\), transformation one--one function (Fig. 6.1, right panel).\nFIGURE 6.1: Two transformations: non-one--one transformation (left panel), one--one transformation (right panel).\n","code":""},{"path":"ChapterTransformations.html","id":"ChangeOfVariable","chapter":"6 Transformations of random variables","heading":"6.2 The change of variable method","text":"\nmethod relatively straightforward one--one transformations (\\(Y = 1 - X\\) \\(Y = \\exp(X)\\)).\nConsiderable care needs exercised transformation one--one; examples given .\ndiscrete continuous cases considered separately.","code":""},{"path":"ChapterTransformations.html","id":"ChangeOfVarDiscrete","chapter":"6 Transformations of random variables","heading":"6.2.1 Discrete random variables","text":"Let \\(X\\) discrete random variable probability function \\(p_X(x)\\), let \\(\\mathcal{R}_X\\) denote set discrete points \\(p_X(x) > 0\\).\nLet \\(Y = u(X)\\) define one--one transformation maps \\(\\mathcal{R}_X\\) onto \\(\\mathcal{R}_Y\\), set discrete points transformed variable \\(Y\\) non-zero probability.\nsolve \\(Y = u(X)\\)  \\(X\\) terms \\(Y\\), say \\(X = w(Y) = u^{-1}(Y)\\), \\(y \\\\mathcal{R}_Y\\), \\(x = w(y)\\\\mathcal{R}_X\\).Example 6.2  (One--one transformation) Suppose\n\\[\n   p_X(x) =\n   \\begin{cases}\n      x/15 & \\text{$x = 1, 2, 3, 4, 5$};\\\\\n      0    & \\text{elsewhere}.\n   \\end{cases}\n\\]\nfind probability function  \\(Y\\) \\(Y = 2X + 1\\) (.e., \\(u(x) = 2x + 1\\)), first see \\(\\mathcal{R}_X = \\{1, 2, 3, 4, 5\\}\\).\nHence \\(\\mathcal{R}_Y = \\{3, 5, 7, 9, 11\\}\\) mapping \\(y = 2x + 1 = u(x)\\) one--one.\nAlso, \\(w(y) = u^{-1}(y) = (y - 1)/2\\).\nHence,\n\\[\n   \\Pr(Y = y)\n   = \\Pr(2X + 1 = y)\n   = \\Pr\\left(X = \\frac{y - 1}{2}\\right)\n   = \\left(\\frac{y - 1}{2}\\right) \\times\\frac{1}{15}\n   = \\frac{y - 1}{30}.\n\\]\nprobability function  \\(Y\\) \n\\[\n   \\Pr(Y = y)\n   = \\begin{cases}\n      (y - 1)/30 & \\text{$y = 3, 5, 7, 9, 11$};\\\\\n      0          & \\text{elsewhere}.\n    \\end{cases}\n\\]\n(Note: probabilities probability function add  \\(1\\).)procedure \\(Y = u(X)\\) one--one mapping can stated generally \n\\[\\begin{align*}\n   \\Pr(Y = y)\n   &= \\Pr\\big(u(X) = y\\big)\\\\\n   &= \\Pr\\big(X = u^{-1} (y)\\big)\\\\\n   &= p_X\\big(u^{-1}(y)\\big), \\quad\\text{$y\\\\mathcal{R}_Y$}.\n\\end{align*}\\]Example 6.3  (Transformation (1:1)) Let \\(X\\) binomial distribution probability function\n\\[\n   p_X(x) = \\begin{cases}\n               \\binom{3}{x}0.2^x (0.8)^{3 - x} & \\text{$x = 0, 1, 2, 3$};\\\\\n               0 & \\text{otherwise}.\n            \\end{cases}\n\\]\nfind probability function \\(Y = X^2\\), first note \\(Y = X^2\\) one--one transformation general, range space  \\(X\\) (.e., \\(x = 0, 1, 2, 3\\)).transformation \\(y = u(x) = x^2\\), \\(\\mathcal{R}_X = \\{ x \\mid x = 0, 1, 2, 3 \\}\\) maps onto \\(\\mathcal{R}_Y = \\{y \\mid y = 0, 1, 4, 9\\}\\).\ninverse function \\(x = w(y) = \\sqrt{y}\\), hence probability function \\(Y\\) \n\\[\n   p_Y(y) = p_X(\\sqrt{y})\n   = \\begin{cases}\n               \\binom{3}{\\sqrt{y}}0.2^{\\sqrt{y}} (0.8)^{3 - \\sqrt{y}} & \\text{$y = 0, 1, 4, 9$};\\\\\n               0 & \\text{otherwise}.\n     \\end{cases}\n\\]transformation  \\(u(\\cdot)\\) 1:1, care needed.Example 6.4  (Transformation 1:1) Suppose \\(\\Pr(X = x)\\) defined Example 6.2, define \\(Y = |X - 3|\\).\nSince \\(\\mathcal{R}_Y = \\{0, 1, 2\\}\\) mapping one--one: event \\(Y = 0\\) occurs \\(X = 3\\), event \\(Y = 1\\) occurs \\(X = 2\\) \\(X = 4\\), event \\(Y = 2\\) occurs \\(X = 1\\) \\(X = 5\\).\nHence, \\(\\mathcal{R}_Y  \\{ 0, 1, 2\\}\\).find probability distribution  \\(Y\\):\n\\[\\begin{align*}\n   \\Pr(Y = 0)\n   &= \\Pr(X = 3) = 3/15 = \\frac{1}{5};\\\\\n   \\Pr(Y = 1)\n   &= \\Pr(X = 2 \\text{ } 4) = \\frac{2}{15} + \\frac{4}{15} = \\frac{2}{5};\\\\\n   \\Pr(Y = 2)\n   &= \\Pr(X = 1 \\text{ } 5) = \\frac{1}{15} + \\frac{5}{15} = \\frac{2}{5}.\n\\end{align*}\\]\nprobability function \\(Y\\) \n\\[\n   p_Y(y) =\n   \\begin{cases}\n       1/5 & \\text{$y = 0$};\\\\\n       2/5 & \\text{$y = 1$};\\\\\n       2/5 & \\text{$y = 2$};\\\\\n       0   & \\text{elsewhere}.\n   \\end{cases}\n\\]","code":""},{"path":"ChapterTransformations.html","id":"ChangeOfVarContinuous","chapter":"6 Transformations of random variables","heading":"6.2.2 Continuous random variables","text":"Theorem 6.1  (Change variable (continuous rv))  \\(X\\) PDF \\(f_X(x)\\) \\(x\\\\mathcal{R}_X\\)  \\(u(\\cdot)\\) one--one function \\(x\\\\mathcal{R}_X\\), random variable \\(Y = u(X)\\) PDF\n\\[\n   f_Y(y) = f_X(x) \\left|\\frac{dx}{dy}\\right|\n\\]\nright-hand side expressed function  \\(y\\).\nterm \\(\\left|dx/dy\\right|\\) called Jacobian transformation.Proof. Let inverse function \\(X = w(Y)\\) \\(w(y) = u^{-1}(x)\\).Case 1: \\(y = u(x)\\) strictly increasing function (Fig. 6.2, left panel).\n\\(< y < b\\) \\(w() < x < w(b)\\) \\(\\Pr(< Y < b) = \\Pr\\big(w() < X <w(b)\\big)\\), \n\\[\n   {\\int^b_a f_Y(y)\\,dy\n   =\\int^{w(b)}_{w()}f_X(x)\\,dx\n   =\\int^b_af\\big( w(y)\\big)\\frac{dx}{dy}\\,\\,dy}.\n\\]\nTherefore, \\(\\displaystyle {f_Y(y) = f_X\\big( w(y) \\big)\\frac{dx}{dy}}\\), \\(w(y) = u^{-1}(x)\\).\nFIGURE 6.2: strictly increasing transformation function (left panel) strictly decreasing function (right panel).\nCase 2: \\(y = u(x)\\) strictly decreasing function \\(x\\) (Fig. 6.2, right panel).\n\\(< y < b\\) \\(w(b) < x < w()\\) \\(\\Pr(< Y < b) = \\Pr\\big(w(b) < X < w()\\big)\\), \n\\[\\begin{align*}\n     \\int^b_a f_Y(y)\\,dy & = \\int^{w()}_{w(b)}f_X(x)\\,dx\\\\\n     & = \\int^a_bf_X(x)\\frac{dx}{dy}\\,\\,dy\\\\\n     & = - \\int ^b_a f_X(x)\\frac{dx}{dy}\\,dy.\n\\end{align*}\\]\nTherefore \\(f_Y(y) = -f_X\\left( w(y) \\right)\\displaystyle{\\frac{dx}{dy}}\\).\n\\(dx/dy\\) negative case decreasing function, general\n\\[\n   f_Y(y) = f_X(x)\\left|\\frac{dx}{dy} \\right|.\n\\]Example 6.5  (Transformation) Let PDF  \\(X\\) given \n\\[\n   f_X(x) =\n   \\begin{cases}\n      1 & \\text{$0 < x < 1$};\\\\\n      0 & \\text{elsewhere}.\n   \\end{cases}\n\\]\nConsider transformation \\(Y = u(X) = -2\\log X\\) (\\(\\log\\) refers logarithms base \\(e\\), natural logarithms).\ntransformation one--one, inverse transformation \n\\[\n   X = \\exp( -Y/2) = u^{-1}(Y) = w(Y).\n\\]\nspace \\(\\mathcal{R}_X = \\{x \\mid 0 < x < 1\\}\\) mapped \\(\\mathcal{R}_Y = \\{y \\mid 0 < y < \\infty\\}\\).\n,\n\\[\n   w'(y) = \\frac{d}{dy} \\exp(-y/2) = -\\frac{1}{2}\\exp(-y/2),\n\\]\nJacobian transformation \\(|w'(y)| = \\exp(-y/2)/2\\).\nPDF \\(Y = -2\\log X\\) \n\\[\\begin{align*}\n   f_Y(y)\n   &= f_X\\big(w(y)\\big) |w'(y)| \\\\\n   &= f_X\\big(\\exp(-y/2)\\big) \\exp(-y/2)/2 \\\\\n   &= \\frac{1}{2}\\exp(-y/2)\\quad\\text{$y > 0$}.\n\\end{align*}\\]\n, \\(Y\\) exponential distribution \\(\\beta = 2\\): \\(Y \\sim \\text{Exp}(2)\\) (Def. 8.8).Example 6.6  (Square root transformation) Consider random variable \\(X\\) PDF \\(f_X(x) = e^{-x}\\) \\(x \\geq 0\\).\nfind PDF \\(Y = \\sqrt{X}\\), first see \\(y = \\sqrt{x}\\) strictly increasing function \\(x \\geq 0\\) (Fig. 6.3).inverse relation \\(X = Y^2\\), \\(dx/dy = |2y| = 2y\\) \\(x \\ge 0\\).\nPDF  \\(Y\\) \n\\[\n    f_Y(y)\n    = f_X(x)\\left|\\frac{dx}{dy}\\right|\n    = 2y e^{-y^2}\\quad \\text{$y\\geq0$}.\n\\]\nFIGURE 6.3: square-root transformation (left panel); PDF \\(X\\) (centre panel) PDF \\(Y\\) (right panel).\nExample 6.7  (Tan transformation) Let random variable \\(X\\) uniformly distributed \\([-\\pi/2, \\pi/2]\\).\nSuppose seek distribution \\(Y = \\tan X\\) (Fig. 6.4).mapping \\(Y = \\tan X\\), see \\(\\mathcal{R}_Y = \\{ y\\mid -\\infty <y<\\infty\\}\\).\nmapping one--one, \\(X = \\tan^{-1}Y\\), \\(dx/dy = 1/(1 + y^2)\\).\nHence\n\\[\n   f_Y(y)\n   = f_X(x)\\left|\\frac{dx}{dy}\\right|\n   = \\frac{1}{\\pi(1 + y^2)}.\n\\]\nCauchy distribution.\nFIGURE 6.4: tan transformation (left panel); PDF \\(X\\) (centre panel) PDF \\(Y\\) (right panel).\ncase function \\(u(\\cdot)\\) one--one transformation considered using example, using modification Theorem 6.1.Example 6.8  (Transformation (1:1)) Given random variable \\(Z\\) follows \\(N(0, 1)\\) distribution, suppose seek probability distribution \\(Y = \\frac{1}{2} Z^2\\).relationship \\(Y = u(Z) = \\frac{1}{2}z^2\\) strictly increasing decreasing \\((-\\infty, \\infty )\\) Theorem 6.1 applied directly.\nInstead, subdivide range \\(Z\\) \\(Y\\) portion relationship monotonic.\n:\n\\[\n   f_Z(z) =\n   \\frac{1}{\\sqrt{2\\pi}}\\,e^{-\\frac{1}{2} z^2}\\quad\\text{$-\\infty < z < \\infty$}.\n\\]\ninverse relation, \\(Z = u^{-1}(Y)\\) \\(Z = \\pm \\sqrt{2Y}\\).\ngiven value \\(Y\\), two values \\(Z\\) possible.\nHowever, range \\(-\\infty < z < 0\\), \\(Y\\) \\(Z\\) monotonic relationship.\nSimilarly, \\(0 < z <\\infty\\), \\(Y\\) \\(Z\\) monotonic relationship.\nThus (see Fig. 6.5),\n\\[\n   \\Pr(< Y <b) = \\Pr(-\\sqrt{2b} < Z < -\\sqrt{2a}\\,) + \\Pr(\\sqrt{2a} < Z < \\sqrt{2b}\\,).\n\\]\ntwo terms right equal distribution  \\(Z\\) symmetrical  \\(z = 0\\).\nThus \\(\\Pr(< Y < b) = 2\\Pr(\\sqrt{2a} < Z < \\sqrt{2b}\\,)\\), \n\\[\\begin{align*}\n     f_Y(y)\n     &= 2f_Z(z)\\left| \\frac{dz}{dy}\\right|\\\\\n     &= 2\\frac{1}{\\sqrt{2\\pi}}e^{-y}\\frac{1}{\\sqrt{2y}};\n\\end{align*}\\]\n,\n\\[\n   f_Y(y)\n   = e^{-y}y^{-\\frac{1}{2}} / \\sqrt{\\pi}\\quad\\text{$0 < y < \\infty$}.\n\\]\nPDF gamma distribution parameters \\(\\alpha = 1/2\\) \\(\\beta = 1\\).\nfollows \\(X\\sim N(\\mu,\\sigma^2)\\), PDF \\(Y = \\frac{1}{2} (X - \\mu )^2 / \\sigma^2\\) \\(\\text{Gamma}(\\alpha = 1/2,\\beta = 1)\\) since \\((X - \\mu)/\\sigma\\) distributed \\(N(0, 1)\\).\nFIGURE 6.5: transformation 1:1.\nNote probability can doubled Example 6.8 \\(Y = u(Z)\\) PDF  \\(Z\\) symmetrical point.\n","code":""},{"path":"ChapterTransformations.html","id":"ChangeOfVarDiscreteBivariate","chapter":"6 Transformations of random variables","heading":"6.2.3 Discrete bivariate case????","text":"bivariate case similar univariate case.\nConsider joint probability function \\(p_{X_1, X_2}(x_1, x_2)\\) two discrete random variables \\(X_1\\)  \\(X_2\\) defined two-dimensional set points \\(R^2_X\\) \\(p(x_1, x_2) > 0\\).\nnow two one--one transformations:\n\\[\n   y_1 = u_1( x_1, x_2)\\qquad\\text{}\\qquad y_2 = u_2( x_1, x_2)\n\\]\nmap \\(R^2_X\\) onto \\(R^2_Y\\) (two-dimensional set points \\(p(y_1, y_2) > 0\\)).\ntwo inverse functions \n\\[\n   x_1 = w_1( y_1, y_2)\\qquad\\text{}\\qquad x_2 = w_2( y_1, y_2).\n\\]\njoint probability function new (transformed) random variables \n\\[\n   p_{Y_1, Y_2}(y_1, y_2) =\n   \\begin{cases}\n      p_{X_1, X_2}\\big( w_1(y_1, y_2), w_2(y_1, y_2)\\big) & \\text{$(y_1, y_2)\\R^2_Y$};\\\\\n      0 & \\text{elsewhere}.\n   \\end{cases}\n\\]Example 6.9  (Transformation (bivariate)) Let two discrete random variables \\(X_1\\)  \\(X_2\\) joint probability function shown Table 6.1.\nConsider two one--one transformations\n\\[\n   Y_1 = X_1 + X_2 \\qquad\\text{}\\qquad Y_2 = 2 X_1.\n\\]\njoint probability function  \\(Y_1\\)  \\(Y_2\\) can found noting \\((x_1, x_2)\\) pairs mapped \\(y_1, y_2\\) space:joint probability function can constructed shown Table 6.2.\nTABLE 6.1: bivariate probability function\n\nTABLE 6.2: joint probability function \\(Y_1\\) \\(Y_2\\)\nSometimes, joint probability function two random variables given, one new random variable required.\ncase, second (dummy) transformation used, usually simple transformation.Example 6.10  (Transformation (bivariate)) Let \\(X_1\\)  \\(X_2\\) two independent random variables joint probability function\n\\[\n   p_{X_1, X_2}(x_1, x_2) =\n         \\frac{\\mu_1^{x_1} \\mu_x^{x_2} \\exp( -\\mu_1 - \\mu_2 )}{x_1!\\, x_2!}\n         \\quad\\text{$x_1$ $x_2 = 0, 1, 2, \\dots$}\n\\]\njoint probability function two independent Poisson random variables.\nSuppose wish find probability function \\(Y_1 = X_1 + X_2\\).Consider two one--one transformations, \\(Y_2 = X_2\\) just dummy transformation:\n\\[\\begin{align}\n   y_1 &= x_1 + x_2             = u_1(x_1, x_2)\\\\\n   y_2 &= x_2\\phantom{{} + x_2} = u_2(x_1, x_2)\n\\end{align}\\]\nmap points  \\(R^2_X\\) onto\n\\[\n   R^2_Y = \\left\\{ (y_1, y_2)\\mid y_1 = 0, 1, 2, \\dots; y_2 = 0, 1, 2, \\dots, y_1\\right\\}.\n\\]\n\\(Y_2\\) dummy transform, chosen simple.\nsecond transform chosen (direct interest), choose one simple.\ninverse functions \n\\[\\begin{align*}\n   x_1 &= y_1 - y_2              = w_1(y_1, y_2)\\\\\n   x_2 &= y_2 \\phantom{{} - y_2} = w_2(y_2)\n\\end{align*}\\]\nrearranging original transformations.\njoint probability function  \\(Y_1\\)  \\(Y_2\\) \n\\[\\begin{align*}\n   p_{Y_1, Y_2}(y_1, y_2)\n   &= p_{X_1, X_2}\\big( w_1(y_1, y_2), w_2(y_1, y_2)\\big) \\\\\n   &= \\frac{\\mu_1^{y_1 - y_2}\\mu_2^{y_2} \\exp(-\\mu_1 - \\mu_2)}{(y_1 - y_2)! y_2!}\\quad\n   \\text{$(y_1, y_2)\\R^2_Y$}.\n\\end{align*}\\]\nRecall seek probability function just \\(Y_1\\), need find marginal probability function \\(p_{Y_1, Y_2}(y_1, y_2)\\).\nmarginal probability function  \\(Y_1\\) \n\\[\n   p_{Y_1}(y_1) = \\sum_{y_2 = 0}^{y_1} p_{Y_1, Y_2}(y_1, y_2)\n   = \\sum_{y_2 = 0}^{y_1} \\frac{\\mu_1^{y_1 - y_2}\\mu_2^{y_2} \\exp(-\\mu_1 - \\mu_2)}{(y_1 - y_2)!\\, y_2!},\n\\]\nequivalent \n\\[\n   p_{Y_1}(y_1) =\n   \\begin{cases}\n      \\displaystyle{\\frac{(\\mu_1 + \\mu_2)^{y_1}\\exp\\big[-(\\mu_1 + \\mu_2)\\big]}{y_1!}} & \\text{$y_1 = 0, 1, 2, \\dots$}\\\\\n      0 & \\text{otherwise}.\n   \\end{cases}\n\\]\nrecognise probability function Poisson random variable (Def. 7.12) mean \\(\\mu_1 + \\mu_2\\).\nThus \\(Y_1 \\sim \\text{Pois}(\\lambda = \\mu_1 + \\mu_2)\\).","code":""},{"path":"ChapterTransformations.html","id":"DistributonFunctionMethod","chapter":"6 Transformations of random variables","heading":"6.3 The distribution function method","text":"method works continuous random variables.distribution function method involves two steps:Find distribution function transformed variable.Differentiate distribution function find probability density function.procedure best demonstrated using example.Example 6.11  (Distribution function method) Consider random variable \\(X\\) PDF\n\\[\n   f_X(x) = \\begin{cases}\n               x/4 & \\text{$1 < x < 3$};\\\\\n               0 & \\text{elsewhere}.\n            \\end{cases}\n\\]\nfind PDF random variable \\(Y\\) \\(Y = X^2\\), first see \\(1 < y < 9\\) transformation monotonic region.\ndistribution function  \\(Y\\) \n\\[\\begin{align*}\n   F_Y(y)\n   &= \\Pr(Y\\le y) \\qquad\\text{(definition)}\\\\\n   &= \\Pr(X^2 \\le y) \\qquad\\text{(since $Y = X^2$)}\\\\\n   &= \\Pr(X\\le \\sqrt{y}\\,).\n\\end{align*}\\]\nlast step trivial, critical.\nSometimes, care needed (see Example 6.12).\ncase, one--one relationship  \\(X\\)  \\(Y\\) region  \\(X\\) defined (.e., positive probability); see Fig. 6.6.continue follows:\n\\[\\begin{align*}\n   F_Y(y)\n    =\\Pr( X\\le \\sqrt{y}\\,)\n   &= F_X\\big(\\sqrt{y}\\,\\big) \\qquad\\text{(definition $F_X(x)$)} \\\\\n   &= \\int_1^{\\sqrt{y}} (x/4) \\,dx\n    = (y - 1)/8\n\\end{align*}\\]\n\\(1 < y < 9\\), zero elsewhere.\ndistribution function  \\(Y\\); find PDF:\n\\[\n   f_Y(y)\n   = \\frac{d}{dy} (y - 1)/8\n   = \\begin{cases}\n        1/8 & \\text{$1 < y < 9$};\\\\\n        0 & \\text{elsewhere}.\n     \\end{cases}\n\\]\nNote range  \\(Y\\) defined; since \\(1 < x < 3\\), \\(1 < y < 9\\).\nFIGURE 6.6: transformation \\(Y = X^2\\) \\(X\\) defined \\(1\\) \\(3\\). thicker line corresponds region transformation applies. Note \\(Y < y\\), \\(2 - \\sqrt{y - 1} < X < 2 + \\sqrt{y - 1}\\).\nExample 6.12  (Transformation) Consider random variable \\(X\\) previous example, transformation \\(Y = (X - 2)^2 + 1\\) (Fig. 6.7).case, transformation one--one transform.\nProceed find distribution function  \\(Y\\):\n\\[\\begin{align*}\n   F_Y(y)\n   &= \\Pr(Y\\le y) \\qquad\\text{(definition)}\\\\\n   &= \\Pr\\big( (X - 2)^2 + 1  \\le y\\big)\n\\end{align*}\\]\nsince \\(Y = (X - 2)^2 + 1\\).\nFig. 6.7, whenever \\((X - 2)^2 + 1 < y\\) value \\(y\\),  \\(X\\) must range \\(2 - \\sqrt{y - 1}\\) \\(2 + \\sqrt{y - 1}\\).\n:\n\\[\\begin{align*}\n   F_Y(y)\n   &= \\Pr\\big( (X - 2)^2 + 1 \\le y\\big) \\\\\n   &= \\Pr\\left( 2 - \\sqrt{y - 1} < X < 2 + \\sqrt{y - 1} \\right)\\\\\n   &= \\int_{2-\\sqrt{y - 1}}^{2 + \\sqrt{y - 1}} x/4\\,dx \\\\\n   &= \\left.\\frac{1}{8} x^2\\right|_{2 - \\sqrt{y - 1}}^{2 + \\sqrt{y - 1}} \\\\\n   &= \\frac{1}{8} \\left[ \\left(2 + \\sqrt{y - 1}\\right)^2 - \\left(2 - \\sqrt{y - 1}\\right)^2\\right] \\\\\n   &=  \\sqrt{y - 1}.\n\\end{align*}\\]\n, distribution function; differentiating:\n\\[\n   f_Y(y) = \\begin{cases}\n               \\frac{1}{2\\sqrt{y - 1}} & \\text{$1 < y < 2$};\\\\\n               0 & \\text{elsewhere}.\n            \\end{cases}\n\\]\nFIGURE 6.7: transformation \\(Y = (X - 2)^2 + 1\\) \\(X\\) defined \\(1\\) \\(3\\). thicker line corresponds region transformation applies. Note \\(Y < y\\), \\(2 - \\sqrt{y - 1} < X < 2 + \\sqrt{y - 1}\\).\nExample 6.13  (Transformation) Example 6.8 repeated using distribution function method.\nGiven \\(Z\\) distributed \\(N(0, 1)\\) seek probability distribution \\(Y = \\frac{1}{2} Z^2\\).\nFirst,\n\\[\n   f_Z(z)\n   = (2\\pi )^{-\\frac 12}\\,e^{-z^2/2}\\quad\\text{$z\\(-\\infty ,\\,\\infty )$}.\n\\]\nLet \\(Y\\) PDF \\(f_Y(y)\\) df \\(F_Y(y)\\).\n\n\\[\\begin{align*}\n     F_Y(y)\n      = \\Pr(Y\\leq y)\n     &= \\Pr\\left(\\frac{1}{2}Z^2\\leq y\\right)\\\\\n     &= \\Pr(Z^2\\leq 2y)\\\\\n     & = \\Pr(-\\sqrt{2y}\\leq Z\\leq \\sqrt{2y}\\,)\\\\\n     & = F_Z(\\sqrt{2y}\\,) - F_Z(-\\sqrt{2y}\\,)\n\\end{align*}\\]\n \\(F_Z\\) distribution function  \\(Z\\).\nHence\n\\[\\begin{align*}\n     f_Y(y)\n       = F_Y'(y)\n     &= F_Z'(\\sqrt{2y}\\,)-F_Z'(-\\sqrt{2y}\\,)\\\\\n     &= \\frac{\\sqrt{2}}{2\\sqrt{y}}f_Z(\\sqrt{2y}\\,) - \\frac{\\sqrt{2}}{-\n2\\sqrt{y}}f_Z(-\\sqrt{2y}\\,)\\\\[2mm]\n     &= \\frac{1}{\\sqrt{2y}}[f_Z(\\sqrt{2y}\\,) + f_Z(-\\sqrt{2y}\\,)]\\\\\n     &= \\frac{1}{\\sqrt{2y}} \\left[ \\frac{1}{\\sqrt{2\\pi}}\\,e^{-y}+\\frac{1}{\\sqrt{2\\pi}}\\,e^{-y}\\right]\\\\\n     &= \\frac{e^{-y}y^{-\\frac{1}{2}}}{\\sqrt{\\pi}}\n\\end{align*}\\]\n.Care needed ensure steps followed logically.\nDiagrams like Fig. 6.6  6.7 encouraged.functions produced PDFs; check case.method can also used one variable interest, cover .\n","code":""},{"path":"ChapterTransformations.html","id":"TransformationMoments","chapter":"6 Transformations of random variables","heading":"6.4 The moment-generating function method","text":"\nmoment-generating function (MGF) method useful finding distribution linear combination \\(n\\) independent random variables.\nmethod essentially involves computation MGF transformed variable \\(Y = u(X_1, X_2, \\dots, X_n)\\) joint distribution independent \\(X_1, X_2, \\dots, X_n\\) given.MGF method relies observation: since MGF random variable (exists) completely specifies distribution random variable, two random variables MGF must identical distributions.\n, transformation \\(Y = X_1 + X_2 + \\cdots X_n\\) demonstrated, principles can applied linear combinations also.Consider \\(n\\) independent random variables \\(X_1, X_2, \\dots, X_n\\) MGFs \\(M_{X_1}(t)\\), \\(M_{X_2}(t)\\), \\(\\dots\\), \\(M_{X_n}(t)\\), consider transformation \\(Y = X_1 + X_2 + \\cdots X_n\\).\nSince  \\(X_i\\) independent, \\(f_{X_1,X_2\\dots X_n}(x_1, x_2, \\dots, x_n) = f_{X_1}(x_1).f_{X_2}(x_2)\\dots f_{X_n}(x_n)\\).\n, definition MGF,\n\\[\\begin{align*}\n   M_Y(t)\n   &= \\operatorname{E}(\\exp(tY)) \\\\\n   &= \\operatorname{E}(\\exp[t(X_1 + X_2 + \\cdots X_n)]) \\\\\n   &= \\int\\!\\!\\!\\int\\!\\!\\!\\cdots\\!\\!\\!\\int \\exp[t(x_1 + x_2 + \\cdots x_n)] f(x_1, x_2, \\dots x_n)\\,dx_n\\dots dx_2\\, dx_1 \\\\\n   &= \\int\\!\\!\\!\\int\\!\\!\\!\\cdots\\!\\!\\!\\int \\exp(tx_1) f(x_1) \\exp(t{x_2}) f(x_2)\\dots \\exp(t{x_n})f(x_n) \\,dx_n\\dots dx_2\\, dx_1 \\\\\n   &= \\int \\exp(t x_1) f(x_1)\\,dx_1 \\int \\exp(t{x_2}) f(x_2)\\,dx_2 \\dots \\int \\exp(t{x_n})f(x_n)\\,dx_n \\\\\n   &= M_{X_1}(t) M_{X_2}(t)\\dots M_{X_n}(t) \\\\\n   &= \\prod_{= 1}^n M_{X_i}(t).\n\\end{align*}\\]\n(\\(\\prod\\) symbol product terms, way \\(\\sum\\) symbol summation terms.)\nresult also holds discrete variables, summations replace integrations.result follows: \\(X_1, X_2, \\dots, X_n\\) independent random variables \\(Y  =  X_1 + X_2 + \\dots + X_n\\), MGF  \\(Y\\) \n\\[\n   M_Y(t)  =  \\prod_{= 1}^n M_{X_i}(t)\n\\]\n\\(M_{X_i}(t)\\) MGF  \\(X_i\\) \\(t\\) \\(= 1, 2, \\dots, n\\).Example 6.14  (MGF method transformations) Suppose \\(X_i \\sim \\text{Pois}(\\lambda_i)\\) \\( =  1, 2, \\dots, n\\), wish find distribution \\(Y  =  X_1  +  X_2  + \\dots  +  X_n\\).Since \\(X_i\\) Poisson distribution parameter \\(\\lambda_i\\) \\(, 2, \\dots n\\), MGF  \\(X_i\\) \n\\[\n   M_{X_i}(t) = \\exp[ \\lambda_i(e^t - 1)].\n\\]\nMGF \\(Y  = X_1 + X_2 + \\cdots X_n\\) \n\\[\\begin{align*}\n   M_Y(t)\n   &= \\prod_{= 1}^n \\exp[ \\lambda_i(e^t - 1)] \\\\\n   &= \\exp[ \\lambda_1(e^t - 1)] \\exp[ \\lambda_2(e^t - 1)] \\dots \\exp[ \\lambda_n(e^t - 1)] \\\\\n   &= \\exp\\left[ (e^t - 1)\\sum_{= 1}^n \\lambda_i\\right].\n\\end{align*}\\]\nUsing \\(\\Lambda = \\sum_{= 1}^n \\lambda_i\\), MGF  \\(Y\\) \n\\[\n   M_Y(t) = \\exp\\left[ (e^t - 1)\\Lambda \\right],\n\\]\nMGF Poisson distribution mean \\(\\Lambda = \\sum_{= 1}^n \\lambda_i\\).\nmeans sum  \\(n\\) independent Poisson distribution also Poisson distribution, whose mean sum individual Poisson means.","code":""},{"path":"ChapterTransformations.html","id":"exercises","chapter":"6 Transformations of random variables","heading":"6.5 Exercises","text":"Selected answers appear Sect. E.6.Exercise 6.1  Suppose PDF  \\(X\\) given \n\\[\n   f_X(x) = \\begin{cases}\n                x/2 & \\text{$0 < x < 2$};\\\\\n                0 & \\text{otherwise}.\n             \\end{cases}\n\\]Find PDF \\(Y = X^3\\) using change variable method.Find PDF \\(Y = X^3\\) using distribution function method.Exercise 6.2  discrete bivariate random vector \\((X_1, X_2)\\) joint probability function\n\\[\n   f_{X_1, X_2}(x_1, x_2) =\n   \\begin{cases}\n      (2x_1+ x _2)/6 & \\text{$x_1 = 0, 1$ $x_2 = 0, 1$};\\\\\n      0               & \\text{elsewhere}.\n   \\end{cases}\n\\]\nConsider transformations\n\\[\\begin{align*}\n   Y_1 &= X_1 + X_2 \\\\\n   Y_2 &= \\phantom{X_1+{}} X_2\n\\end{align*}\\]Determine joint probability function \\((Y_1, Y_2)\\).Deduce distribution  \\(Y_1\\).Exercise 6.3  Consider \\(n\\) random variables \\(X_i\\) \\(X_i \\sim \\text{Gam}(\\alpha_i, \\beta)\\).\nDetermine distribution \\(Y = \\sum_{= 1}^n X_i\\) using MGFs.Exercise 6.4  random variable \\(X\\) PDF\n\\[\n   f_X(x) = \\frac{1}{\\pi(1 + x^2)}\n\\]\n\\(-\\infty < x < \\infty\\).\nFind PDF  \\(Y\\) \\(Y = X^2\\).Exercise 6.5  random variable \\(X\\) distribution function\n\\[\n   F_X(x) =\n   \\begin{cases}\n      0                & \\text{$x \\le -0.5$};\\\\\n      \\frac{2x + 1}{2} & \\text{$-0.5 < x < 0.5$};\\\\\n      1                & \\text{$x \\ge 0.5$}.\n   \\end{cases}\n\\]Find, plot, PDF  \\(X\\).Find distribution function, \\(F_Y(y)\\), random variable \\(Y = 4 - X^2\\).Hence find, plot, PDF \\(Y\\), \\(f_Y(y)\\).Exercise 6.6  Suppose projectile fired angle \\(\\theta\\) horizontal velocity \\(v\\).\nhorizontal distance projectile travels \\(D\\) \n\\[\n   D = \\frac{v^2}{g} \\sin 2\\theta,\n\\]\n \\(g\\) acceleration due gravity (\\(g\\approx 9.8\\) m.s\\(-2\\)). \\(\\theta\\) uniformly distributed range \\((0, \\pi/4)\\), find probability density function  \\(D\\).Sketch PDF  \\(D\\) suitable range \\(v = 12\\) using \\(g\\approx 9.8\\)m.s\\(-2\\).Exercise 6.7  computers facilities generate continuous uniform (pseudo-)random numbers zero one, say \\(X\\).\nneeded, exponential random numbers obtained  \\(X\\) using transformation \\(Y = -\\alpha\\ln X\\).Show  \\(Y\\) exponential distribution determine parameters.Deduce mean variance  \\(Y\\).Exercise 6.8  Consider random variable \\(W\\) \\(\\Pr(W = 2) = 1/6\\), \\(\\Pr(W = -2) = 1/3\\) \\(\\Pr(W = 0) = 1/2\\).Plot probability function  \\(W\\).Find mean variance  \\(W\\).Determine distribution \\(V = W^2\\).Find distribution function  \\(W\\).Exercise 6.9  study model load bridges (Lu, Ma, Liu 2019), researchers modelled Gross Vehicle Weight (GVM, kilonewtons) weight smaller trucks \\(S\\) using \\(S\\sim N(390, 740\\), weight bigger trucks \\(B\\) using \\(L\\sim N(865, 142)\\).\ntotal load distribution \\(L\\) modelled \\(L = 0.24S + 0.76B\\), reflecting expected proportion smaller bigger trucks using bridge.Plot distribution  \\(L\\).Compute mean standard deviation  \\(L\\).Exercise 6.10  Suppose random variable \\(X\\) normal distribution mean \\(\\mu\\) variance \\(\\sigma^2\\).\nrandom variable \\(Y = \\exp X\\) said log-normal distribution.Determine distribution function  \\(Y\\) terms function \\(\\Phi(\\cdot)\\) (see Def. 8.7).Differentiate find PDF \\(Y\\).Plot log-normal distribution various parameter values.Determine \\(\\Pr(Y > 2 | Y < 1)\\) \\(\\mu = 2\\) \\(\\sigma^2 = 2\\).(Hint: Use dlnorm() plnorm() functions R, \\(\\mu = {}\\)meanlog \\(\\sigma = {}\\)sdlog.)Exercise 6.11   \\(X\\) random variable probability function\n\\[\n   \\Pr(X = x) = \\binom{4}{x} (0.2)^x (0.8)^{4 - x} \\quad \\text{$x = 0, 1, 2, 3, 4$},\n\\]\nfind probability function random variable defined \\(Y = \\sqrt{X}\\).Exercise 6.12  Given random variable \\(X\\) probability function\n\\[\n   \\Pr(X = x) = \\frac{x^2}{30} \\quad \\text{$x = 1, 2, 3, 4$},\n\\]\nfind probability function \\(Y= (X - 3)^2\\).Exercise 6.13  random variable \\(X\\) distribution function\n\\[\n   F_X(x) =\n   \\begin{cases}\n      0 & \\text{$x < -0.5$};\\\\\n      \\frac{2x + 1}{2}, & \\text{$-0.5 < x < 0.5$}; \\\\\n      1 & \\text{$x > 0.5$}.\n   \\end{cases}\n\\]Find distribution function, \\(F_Y(y)\\), random variable \\(Y = 4 - X^2\\).Hence find PDF  \\(Y\\).Exercise 6.14  random variable \\(X\\) exponential distributed mean \\(1\\), show distribution \\(-\\log(X)\\) Gumbel distribution (Eq. (5.7)) \\(\\mu = 0\\) \\(\\sigma = 1\\).Exercise 6.15  Let \\(X\\) gamma distribution parameters \\(\\alpha > 2\\) \\(\\beta > 0\\).Prove mean  \\(1/X\\) \\(\\beta/(\\alpha - 1)\\).Prove variance \\(1/X\\) \\(\\beta^2/[(\\alpha - 1)^2(\\alpha - 2)]\\).Exercise 6.16  study modelling waiting times hospital (Khadem et al. 2008), patients classified one three categories:Red: Critically ill injured patients.Yellow: Moderately ill \ninjured patients.Green: Minimally injured \nuninjured patients.‘Green’ patients, service time \\(S\\) modelled \\(S = 4.5 + 11V\\), \\(V \\sim \\text{Beta}(0.287, 0.926)\\).Produce well-labelled plots PDF df  \\(S\\), showing important features.proportion patients service time exceeding \\(15\\,\\text{mins}\\)?quickest \\(20\\)% patients serviced within time?Exercise 6.17  study modelling waiting times hospital (Khadem et al. 2008), patients classified one three categories:Red: Critically ill injured patients.Yellow: Moderately ill \ninjured patients.Green: Minimally injured \nuninjured patients.time (minutes) spent reception ‘Yellow’ patients, say \\(T\\), modelled \\(T = 0.5 + W\\), \\(W\\sim \\text{Exp}(16.5)\\).Plot PDF df  \\(T\\).proportion patients waits \\(20 mins\\), already waiting \\(10\\,\\text{mins}\\)?long slowest \\(10\\)% patients need wait?Exercise 6.18  Suppose random variable \\(Z\\) PDF\n\\[\n  f_Z(z) =\n  \\begin{cases}\n    \\frac{1}{3} & \\text{$-1 < z < 2$};\\\\\n    0 & \\text{elsewhere}.\n  \\end{cases}\n\\]Find probability density function  \\(Y\\), \\(Y = Z^2\\), using distribution function method.Confirm final PDF  \\(Y\\) valid PDF.Produce well-labelled plot PDF  \\(Y\\).\nEnsure important features points clearly labelled.Exercise 6.19  Show chi-squared distribution special case gamma distribution, \\(\\alpha = \\nu/2\\) \\(\\beta = 2\\).Exercise 6.20  Suppose random variable \\(X\\) defined shown Fig. 6.8.Determine distribution function  \\(X\\).Find probability density function random variable \\(Y\\), \\(Y = 6 - 2X\\).Confirm probability density function  \\(Y\\) valid pdf.Plot probability density function \\(Y\\).\nFIGURE 6.8: probability density function random variable \\(X\\).\nExercise 6.21  Suppose random variable \\(X\\) defined shown Fig. 6.8.Determine distribution function  \\(X\\) (done Exercise 6.20).Find probability density function random variable \\(Z\\), \\(Z = (X - 2)^2\\).Confirm probability density function  \\(Z\\) valid pdf.Plot probability density function \\(Z\\).Exercise 6.22  time taken run distance \\(D\\) (metres) professional athlete, say \\(T\\) (seconds), varies distribution shown Fig. 6.9 (left panel).\naverage velocity runner, say \\(V\\), related time \\(V = D/T\\).Determine probability density function runner’s velocity.Suppose \\(D = 100\\), \\(\\mu = 12\\) \\(\\Delta = 0.25\\).\nPlot probability density function  \\(V\\).\nFIGURE 6.9: probability density function random variable \\(T\\), time run.\nExercise 6.23  Suppose instantaneous voltage \\(V\\) (volts) circuit varies time \n\\[\n  f_V(v) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\\left\\{ -\\frac{x^2}{2\\sigma^2}\\right\\}.\n\\]\nshown Fig. 6.9 (right panel).\n(Later, identify normal distribution.)Determine probability density function instantaneous power circuit \\(P\\), \\(P = V^2/R\\) circuit resistance \\(R\\) (ohms).Suppose \\(\\sigma = 1\\), \\(R = 10\\).\nPlot probability density function  \\(P\\).","code":""},{"path":"DiscreteDistributions.html","id":"DiscreteDistributions","chapter":"7 Standard discrete distributions","heading":"7 Standard discrete distributions","text":"Upon completion chapter, able :recognise probability functions underlying parameters uniform, Bernoulli, binomial, geometric, negative binomial, Poisson, hypergeometric random variables.know basic properties discrete distributions.apply discrete distributions appropriate problem solving.","code":""},{"path":"DiscreteDistributions.html","id":"introduction-1","chapter":"7 Standard discrete distributions","heading":"7.1 Introduction","text":"chapter, popular discrete distributions discussed.\nProperties definitions applications considered.","code":""},{"path":"DiscreteDistributions.html","id":"DiscreteUniform","chapter":"7 Standard discrete distributions","heading":"7.2 Discrete uniform distribution","text":"discrete random variable \\(X\\) can assume \\(k\\) different distinct values equal probability,  \\(X\\) said discrete uniform distribution.\none simplest discrete distributions.Definition 7.1  (Discrete uniform distribution) random variable \\(X\\) range space \\(\\{, + 1, + 2, \\dots, b\\}\\),  \\(\\)  \\(b\\) (\\(< b\\)) integers, probability function\n\\[\\begin{equation}\n   p_X(x; , b) = \\frac{1}{b - + 1}\\text{ $x = , + 1, \\dots, b$}\n   \\tag{7.1}\n\\end{equation}\\]\n \\(X\\) discrete uniform distribution.\nwrite \\(X\\sim U(, b)\\) \\(X\\sim \\text{Unif}(, b)\\).notation \\(p_X(x; , b)\\) means probability function  \\(X\\) depends parameters \\(\\)  \\(b\\).symbol \\(\\sim\\) means ‘distributed ’; hence, ‘\\(X\\sim U(, b)\\)’ means ‘\\(X\\) distributed discrete uniform distribution parameters \\(\\)  \\(b\\)’.plot probability function discrete uniform distribution shown Fig. 7.1.\nFIGURE 7.1: probability function discrete uniform distribution \\(\\text{Unif}(, b)\\).\nDefinition 7.2  (Discrete uniform distribution: distribution function) random variable \\(X\\) uniform distribution given probability function (7.1), distribution function \n\\[\n   F_X(x; , b) =\n   \\begin{cases}\n      0                                                        & \\text{$x < $}\\\\\n      \\displaystyle \\frac{\\lfloor x\\rfloor - + 1}{b - + 1} & \\text{$x = , + 1, \\dots, b$}\\\\\n      1                                                        & \\text{$x > b$}\n   \\end{cases}\n\\]\n\\(\\lfloor z \\rfloor\\) floor function (.e., round \\(z\\) nearest integer direction  \\(-\\infty\\)).Example 7.1  (Discrete uniform) Let \\(X\\) number showing single throw fair die.\n\\(X \\sim \\text{Unif}(1, 6)\\).select single-digit number table random digits, number chosen, \\(X\\), probability distribution \\(\\text{Unif}(0, 9)\\).following basic properties discrete uniform distribution.Theorem 7.1  (Discrete uniform properties) \\(X\\sim \\text{Unif}(, b)\\) \\(\\displaystyle \\operatorname{E}[X] = (+ b)/2\\).\\(\\displaystyle \\operatorname{var}[X] = \\frac{(b - )(b - + 2)}{12}\\).\\(\\displaystyle M_X(t) = \\frac {e^{} - e^{(b + 1)t}}{(b - + 1)(1 - e^{t})}\\).Proof. mean variance easier find working \\(Y = X - \\) rather  \\(X\\) ( \\(Y\\) defined \\(0 < y < (b - )\\)), using \\(\\operatorname{E}[X] = \\operatorname{E}[Y] + \\) \\(\\operatorname{var}[Y] = \\operatorname{var}[X]\\).\nSince \\(Y\\sim\\text{Unif}(0, b - )\\):\n\\[\\begin{align*}\n  \\operatorname{E}[Y]\n  &= \\sum_{y = 0}^{b - } \\frac{1}{b - + 1}\\\\\n  &= \\frac{1}{b - + 1}\\big(0 + 1 + 2 + \\dots + (b - )\\big)\\\\\n  &= \\frac{(b - )(b - + 1)}{2(b - + 1)} = (b - )/2\n\\end{align*}\\]\nusing (B.1).\nTherefore,\n\\[\n   \\operatorname{E}[X]= \\operatorname{E}[Y] + = \\frac{b - }{2} + = \\frac{+ b}{2}.\n\\]\nvariance  \\(Y\\) can found similarly (Exercise 7.20).find MGF:\n\\[\\begin{align*}\n  M_X(t)\n  &= \\sum_{x = }^{b} \\exp(xt) \\frac{1}{b - + 1}\\\\\n  &= \\frac{1}{b - + 1} \\sum_{x = }^{b} \\exp(xt)\\\\\n  &= \\frac{1}{b - + 1} \\left( \\exp\\{\\} + \\exp\\{(+ 1)t\\} + \\exp\\{(+ 2)t\\} + \\dots + \\exp\\{bt\\} \\right) \\\\\n  &= \\frac{\\exp()}{b - + 1} \\left( 1 + \\exp\\{t\\} + \\exp\\{2t\\} + \\dots + \\exp\\{(b - 1)t\\} \\right) \\\\\n  &= \\frac{\\exp()}{b - + 1} \\left( \\frac{1 - \\exp\\{(b - + 1)t\\}}{1 - \\exp(t)} \\right)\n\\end{align*}\\]\nusing (B.3).","code":""},{"path":"DiscreteDistributions.html","id":"BernoulliDistribution","chapter":"7 Standard discrete distributions","heading":"7.3 Bernoulli distribution","text":"\nBernoulli distribution used situation single trial random process two possible outcomes.\nsimple example tossing coin observing head falls.probability function simple:\n\\[\n   p_X(x) =\n   \\begin{cases}\n      1 - p & \\text{$x = 0$};\\\\\n      p     & \\text{$x = 1$},\n    \\end{cases}\n\\]\n \\(p\\) represents probability \\(x = 1\\), called ‘success’ (\\(x = 0\\) called ‘failure’).\nsuccinctly:\n\\[\\begin{equation}\n   p_X(x; p) = p^x (1 - p)^{1 - x} \\quad\\text{$x = 0, 1$}.\n   \\tag{7.2}\n\\end{equation}\\]Definition 7.3  (Bernoulli distribution) Let \\(X\\) number successes single trial \\(\\Pr(\\text{Success}) = p\\) (\\(0\\le p\\le 1\\)).\n \\(X\\) Bernoulli probability distribution parameter \\(p\\) probability function given (7.2).\nwrite \\(X\\sim\\text{Bern}(p)\\).Definition 7.4  (Bernoulli distribution: distribution function) random variable \\(X\\) Bernoulli distribution given (7.2), distribution function \n\\[\n  F_X(x; p) =\n  \\begin{cases}\n    0     & \\text{$x < 0$}\\\\\n    1 - p & \\text{$0\\leq x < 1$}\\\\\n    1     & \\text{$x\\geq 1$}.\n  \\end{cases}\n\\]terms ‘success’ ‘failure’ literal.\n‘Success’ simply refers event interest.\nevent interest whether cyclone causes damage, still called ‘success’.ideas also introduces common idea Bernoulli trial.Definition 7.5  (Bernoulli trials) Bernoulli trial random process two possible outcomes, usually labelled ‘success’ \\(s\\) ‘failure’ \\(f\\).\nsample space can denoted \\(S = \\{ s, f\\}\\).following basic properties Bernoulli distribution.Theorem 7.2  (Bernoulli distribution properties) \\(X\\sim\\text{Bern}(p)\\) \\(\\operatorname{E}[X] = p\\).\\(\\operatorname{var}[X] = p(1 - p) = pq\\) \\(q = 1 - p\\).\\(M_X(t) = pe^t + q\\).Proof. definition:\n\\[\n     \\operatorname{E}[X] = \\sum^1_{x = 0} x\\, p_X(x) = 0\\times (1 - p) + 1\\times p = p.\n\\]\nfind variance, use computational formula \\(\\operatorname{var}[X] = \\operatorname{E}[X^2] - \\operatorname{E}[X]^2\\).\n,\n\\[\n     \\operatorname{E}[X^2] = \\sum^1_{x = 0} x^2\\, p_X(x) = 0^2\\times (1 - p) + 1^2\\times p = p,\n\\]\n\n\\[\n   \\operatorname{var}[X]\n   = \\operatorname{E}[X^2] - \\operatorname{E}[X]^2\n   = p - p^2\n   = p (1- p).\n\\]MGF  \\(X\\) \n\\[\\begin{align*}\n   M_X(t)\n   &= \\operatorname{E}\\big[\\exp(tX)\\big]\\\\\n   &= \\sum^1_{x = 0} e^{tx} p^x q^{1 - x}\\\\\n   &= \\sum^1_{x = 0}(pe^t)^x q^{1 - x}\n    = pe^t + q.\n\\end{align*}\\]\nProving third result first, using prove others, easier (using methods Sect. 5.5.3—try exercise.)","code":""},{"path":"DiscreteDistributions.html","id":"BinomialDistribution","chapter":"7 Standard discrete distributions","heading":"7.4 Binomial distribution","text":"\nbinomial distribution used model number successes  \\(n\\) independent Bernoulli trials (Def. 7.5).\nsimple example tossing coin ten times observing often head falls.\nrandom process repeated (tossing coin), two outcomes possible trial (head tail), probability head remains constant trial (.e., tosses independent).","code":""},{"path":"DiscreteDistributions.html","id":"BinomialDerivation","chapter":"7 Standard discrete distributions","heading":"7.4.1 Derivation of a binomial distribution","text":"Consider tossing die five times observing number times  rolled.\nprobability observing  three times can found follows:\nfive tosses,  must appear three times; \\(\\binom{5}{3}\\) ways allocating five rolls appear.\nfive rolls,  must appear three times probability \\(1/6\\); two rolls must produce another number, probability \\(5/6\\).\nprobability \n\\[\n   \\Pr(\\text{3 ones}) = \\binom{5}{3} (1/6)^3 (5/6)^2 = 0.032,\n\\]\nassuming independence events.\nUsing approach, PMF binomial distribution can developed.binomial situation arises sequence Bernoulli trials observed, \\(\\Pr(\\{ s\\} ) = p\\) \\(\\Pr(\\{ f\\} ) = q = 1 - p\\).\n \\(n\\) trials, consider random variable \\(X\\),  \\(X\\) number successes \\(n\\) trials.\nNow \\(X\\) value set \\(\\mathcal{R}_X = \\{ 0, 1, 2, \\dots, n\\}\\).\n\\(p\\) must constant trial trial, \\(n\\) trials must independent.Consider event \\(X = r\\) (\\(0\\leq r\\leq n\\)).\ncorrespond sample point\n\\[\n   \\underbrace{s \\quad s \\quad s \\dots s \\quad s \\quad s \\quad s}_r\\quad\n   \\underbrace{f \\quad f \\dots f \\quad f}_{n - r}\n\\]\nintersection  \\(n\\) independent events comprising \\(r\\) successes \\(n - r\\) failures, hence probability \\(p^r q^{n - r}\\).Every sample point event \\(X = r\\) appear rearrangement  \\(s\\)’s  \\(f\\)’s sample point described therefore probability.Now number distinct arrangements  \\(r\\) successes \\(s\\) \\((n - r)\\) failures \\(f\\) \\(\\binom{n}{r}\\), \n\\[\n     \\Pr(X = r) = \\binom{n}{r} p^r q^{n - r}\n\\]\n\\(r = 0, 1, \\dots, n\\).\nbinomial distribution.Note sum probabilities  \\(1\\), binomial expansion \\((p + q)^n\\) (using (B.4)) \n\\[\\begin{equation}\n   \\sum_{r = 0}^n \\binom{n}{r} p^r q^{n - r} = (p + q)^n = 1\\label{EQN:sumbin}\n\\end{equation}\\]\nsince \\(p + q = 1\\).","code":""},{"path":"DiscreteDistributions.html","id":"BinomialDefinition","chapter":"7 Standard discrete distributions","heading":"7.4.2 Definition and properties","text":"definition binomial distribution can now given.Definition 7.6  (Binomial distribution) Let \\(X\\) number successes \\(n\\) independent Bernoulli trials \\(\\Pr(\\text{Success}) = p\\) (\\(0\\le p\\le 1\\)) constant trial.\n \\(X\\) binomial probability distribution parameters \\(n\\), \\(p\\) probability function given \n\\[\\begin{equation}\n   p_X(x; n, p) = \\binom{n}{x} p^x q^{n - x} \\quad\\text{$x = 0, 1, \\dots, n$}.\n   \\tag{7.3}\n\\end{equation}\\]\nwrite \\(X\\sim\\text{Bin}(n, p)\\).distribution function complicated given.\nFig. 7.2 shows probability function binomial distribution various parameter values.\nFIGURE 7.2: probability function binomial distribution various values  \\(p\\)  \\(n\\).\nfollowing basic properties binomial distribution.Theorem 7.3  (Binomial distribution properties) \\(X\\sim\\text{Bin}(n,p)\\) \\(\\operatorname{E}[X] = np\\).\\(\\operatorname{var}[X] = np(1 - p) = npq\\).\\(M_X(t) = (pe^t + q)^n\\).Proof. Using (B.4):\n\\[\\begin{align*}\n     \\operatorname{E}[X]\n     & = \\sum^n_{x = 0} x\\binom{n}{x} p^x q^{n - x}\\\\\n     & = \\sum^n_{x = 1} x \\frac{n}{x} \\binom{n - 1}{x - 1} p^x q^{n - x}\n          \\quad\\text{(note lower summation-index change)}\\\\\n     & = np\\sum^n_{x = 1} \\binom{n - 1}{x - 1} p^{x - 1} q^{n - x}\\\\\n     & = np \\sum^{n - 1}_{y = 0} \\binom{n - 1}{y}p^y q^{n - 1 - y}\\quad \\text{putting $y = x - 1$}.\n\\end{align*}\\]\nsummation  \\(1\\), since equivalent summing values  \\(y\\) binomial probability function \\(y\\) successes \\((n - 1)\\) Bernoulli trials, hence represents probability function sum one.\nsecond line, sum  \\(x\\)  \\(1\\)  \\(n\\) , \\(x = 0\\), probability multiplied \\(x = 0\\) makes contribution summation.\nThus,\n\\[\n   \\operatorname{E}[X] = np.\n\\]find variance, use computational formula \\(\\operatorname{var}[X] = \\operatorname{E}[X^2] - \\operatorname{E}[X]^2\\).\nFirstly, find \\(\\operatorname{E}[X^2]\\), write \\(\\operatorname{E}[X^2]\\) \\(\\operatorname{E}[X(X - 1) + X]\\) \\(\\operatorname{E}[X(X - 1)] + \\operatorname{E}[X]\\); :\n\\[\\begin{align*}\n     \\operatorname{E}[X^2]\n     &= \\sum^n_{x = 0} x(x - 1)\\Pr(X = x) + np\\\\\n     &= np + \\sum^n_{x = 2} x(x  -1)\\frac{n(n - 1)}{x(x - 1)} \\binom{n - 2}{x - 2} p^x q^{n - x}\\\\\n     &= np + \\sum^n_{x = 2} n(n - 1)\\binom{n - 2}{x - 2} p^x q^{n - x}\\\\\n     &= np + n(n - 1)p^2 \\sum^{n - 2}_{y = 0} \\binom{n - 2}{y} p^yq^{n - 2 - y},\n\\end{align*}\\]\nputting \\(y = x - 2\\).\nreason , summation  \\(1\\), \n\\[\n   \\operatorname{E}[X^2] = np + n^2 p^2 - np^2\n\\]\nhence\n\\[\n   \\operatorname{var}[X]\n   = \\operatorname{E}[X^2] - ([\\operatorname{E}[X])^2\n   = np + n^2p^2 - np^2 - n^2 p^2\n   = np (1 - p).\n\\]MGF  \\(X\\) \n\\[\\begin{align*}\n   M_X(t)\n   &= \\operatorname{E}\\big(\\exp(tX)\\big)\\\\\n   &= \\sum^n_{x = 0} e^{tx}\\binom{n}{x} p^x q^{n - x}\\\\\n   &= \\sum^n_{x = 0}\\binom{n}{x} (pe^t)^x q^{n - x}\n    = (pe^t + q)^n.\n\\end{align*}\\]\nProving third result first, using prove others, easier (using methods Sect. 5.5.3—try exercise.)Tables binomial probabilities commonly available, computers (e.g., using R) can also used generate probabilities.\nnumber ‘successes’ binomial distribution, number ‘failures’.\nSpecifically \\(X\\sim \\text{Bin}(n,p)\\), \\(Y = (n - X) \\sim \\text{Bin}(n, 1 - p)\\).R, many functions built-computing probability function, distribution function quantities common distributions (see Appendix D).four R functions working binomial distribution form [dpqr]binom(..., size, prob), prob\\({} = p\\) size\\({} = n\\).\nexample:function dbinom(x, size, prob) computes probability function binomial distribution \\(X = {}\\)x;function pbinom(q, size, prob) computes distribution function binomial distribution \\(X = {}\\)q;function qbinom(p, size, prob) computes quantiles binomial distribution function cumulative probability p; andThe function rbinom(n, size, prob) generates n random numbers given binomial distribution.Example 7.2  (Throwing dice) die thrown \\(4\\) times.\nfind probability rolling exactly \\(2\\) sixes, see \\(n = 4\\) Bernoulli trials \\(p = 1/6\\).\nLet random variable \\(X\\) number 6’s \\(4\\) tosses.\n\n\\[\n  \\Pr(X = 2) =\n  \\binom{4}{2} \\left(\\frac{1}{6}\\right)^2\\left(\\frac{5}{6}\\right)^2 = 150/1296 \\approx 0.1157.\n\\]\nR:binomial situation requires trials independent, probability success \\(p\\) constant throughout trials.example, drawing cards pack without replacing binomial situation; drawing one card, probabilities change drawing next card.\ncase, hypergeometric distribution used (Sect. 7.8).Example 7.3  (Freezing lake) Based Daniel S. Wilks (1995a) (p. 68), probability Cayuga Lake freezes can modelled binomial distribution \\(p = 0.05\\).Using information, number times lake freeze \\(n = 10\\) randomly chosen years given random variable \\(X\\), \\(X \\sim \\text{Bin}(10, 0.95)\\).\nprobability lake freeze ten years \\(\\Pr(X = 10) = \\binom{10}{10} 0.95^{10} 0.05^{0} \\approx 0.599\\), 60%.R:Note define random variable \\(Y\\) number times lake freeze ten randomly chosen years.\n, \\(Y\\sim\\text{Bin}(10, 0.05)\\) compute \\(\\Pr(Y = 0)\\) get answer.Binomial probabilities can sometimes approximated using normal distribution (Sect. 8.3.4) Poisson distribution (Sect. 7.7.3).\n","code":"\ndbinom(2,\n       prob = 1/6,\n       size = 4)\n#> [1] 0.1157407\ndbinom(0,\n       size = 10,\n       prob = 0.05)\n#> [1] 0.5987369\ndbinom(10,\n       size = 10,\n       prob = 0.95)\n#> [1] 0.5987369"},{"path":"DiscreteDistributions.html","id":"GeometricDistribution","chapter":"7 Standard discrete distributions","heading":"7.5 Geometric distribution","text":"","code":""},{"path":"DiscreteDistributions.html","id":"GeometricDerivation","chapter":"7 Standard discrete distributions","heading":"7.5.1 Derivation of a geometric distribution","text":"\nConsider now random process independent Bernoulli trials (Def. 7.5) repeated first success occurs.\ndistribution number failures first success observed?Let random variable \\(X\\) number failures first success observed.\nSince first success may occur first trial, second trial third trial, , \\(X\\) random variable range space \\(\\{0, 1, 2, 3, \\dots\\}\\) (theoretical) upper limit.Since probability failure \\(q = 1 - p\\) probability success  \\(p\\), probability \\(x\\) failures first success \n\\[\\begin{align*}\n   \\Pr(\\text{$x$ failures})          &\\times \\Pr(\\text{first success}) \\\\\n   q^x                               &\\times  p.\n\\end{align*}\\]\nderivation assumes events independent.","code":""},{"path":"DiscreteDistributions.html","id":"GeometricDefinition","chapter":"7 Standard discrete distributions","heading":"7.5.2 Definition and properties","text":"definition geometric distribution can now given.Definition 7.7  (Geometric distribution) random variable \\(X\\) geometric distribution probability function  \\(X\\) \n\\[\\begin{equation}\n   p_X(x; p) =  (1 - p)^x p = q^x p\\quad\\text{$x = 0, 1, 2, \\dots$}\n   \\tag{7.4}\n\\end{equation}\\]\n\\(q = 1 - p\\) \\(0 < p < 1\\) parameter distribution.\nwrite \\(X\\sim\\text{Geom}(p)\\).Definition 7.8  (Geometric distribution: distribution function) random variable \\(X\\) geometric distribution given (7.4), distribution function \n\\[\n  F_X(x; p) =\n  \\begin{cases}\n    0                                  & \\text{$x < 0$}\\\\\n    1 - (1 - p)^{\\lfloor x\\rfloor + 1} & \\text{$x\\ge 0$},\n  \\end{cases}\n\\]\n\\(\\lfloor z \\rfloor\\) floor function.parameterisation used R.\nprobability function geometric distribution various values  \\(p\\) shown Fig. 7.3.\nfollowing basic properties geometric distribution.Theorem 7.4  (Geometric distribution properties) \\(X\\sim\\text{Geom}(p)\\) defined Eq. (7.4), \\(\\operatorname{E}[X] = (1 - p)/p\\).\\(\\operatorname{var}[X] = (1 - p)/p^2\\).\\(M_X(t) = p/\\{1 - (1 - p)e^t\\}\\) \\(t < -\\log(1 - p)\\).Proof. first two result can proven directly proving third result, using MGF prove first two, easier.\nleft exercise (Ex. 7.21).four R functions working geometric distribution form [dpqr]geom(prob), prob\\({} = p\\) (see Appendix D).\nFIGURE 7.3: probability function geometric distribution \\(p = 0.1\\), \\(0.3\\), \\(0.5\\) \\(0.8\\).\nExample 7.4  (Netball shooting) Suppose netball goal shooter probability \\(p = 0.2\\) missing goal.\ncase ‘success’ refers missing shot \\(p = 0.2\\)Let \\(X\\) number shots made till first miss.\n, \\(X = \\{1, 2, 3, \\dots \\}\\), since first miss may occur first shot, subsequent shot.\nparameterisation used R.Instead, let \\(Y\\) number goals scored shooter first misses.\nNotice \\(Y = \\{0, 1, 2, \\dots \\}\\) parameterisation correspond used R, \\(Y\\sim \\text{Geom}(0.2)\\).probability shooter makes \\(4\\) goals first miss :probability shooter makes \\(4\\) fewer goals first miss \\(\\Pr(Y = 0, 1, 2, 3, \\text{} 4)\\):expected number shots first miss \\(\\operatorname{E}[Y] = (1 - p)/p  = 4\\).parameterisation number failures needed first success, \\(x = 0, 1, 2, 3, \\dots\\).\nalternative parameterisation number trials \\(X\\) observing success, \\(x = 1, 2, \\dots\\).\nway distinguish parameterisation used check range space probability function.Definition 7.9  (Geometric distribution: Alternative parameterisation) random variable \\(X\\) geometric distribution probability function  \\(X\\) \n\\[\\begin{equation}\n   p_X(x) =  (1 - p)^{x - 1}p = q^{x - 1} p\\quad\\text{$x = 1, 2, \\dots$}\n   \\tag{7.5}\n\\end{equation}\\]\n\\(q = 1 - p\\) \\(0 < p < 1\\) parameter distribution.\nwrite \\(X\\sim\\text{Geom}(p)\\).Theorem 7.5  (Geometric distribution properties) \\(X\\sim\\text{Geom}(p)\\) defined Eq. (7.5), \\(\\operatorname{E}[X] = 1/p\\).\\(\\operatorname{var}[X] = (1 - p)/p^2\\).\\(M_X(t) = p\\exp(t) /\\{ 1 - (1 - p)e^t \\}\\) \\(t < -\\log(1 - p)\\).Proof. first two result can proven Theorem 7.4.","code":"\ndgeom(4, prob = 0.2)\n#> [1] 0.08192\npgeom(4, prob = 0.2)\n#> [1] 0.67232"},{"path":"DiscreteDistributions.html","id":"NegativeBinomialDistribution","chapter":"7 Standard discrete distributions","heading":"7.6 Negative binomial distribution","text":"","code":""},{"path":"DiscreteDistributions.html","id":"NegBinStandard","chapter":"7 Standard discrete distributions","heading":"7.6.1 Derivation: standard parameterisation","text":"\nConsider random process independent Bernoulli trials repeated \\(r\\)th success occurs.\nLet random variable \\(X\\) number failures \\(r\\)th success observed, \\(X = 0, 1, 2, \\dots\\).observe \\(r\\)th success \\(x\\) failures, \\(x\\) failures \\(r - 1\\) successes observed first \\(x + r - 1\\) trials.\n\\(\\binom{x + r - 1}{r - 1}\\) ways allocate successes first \\(x + r - 1\\) trials.\n\\(r - 1\\) successes occur probability \\(p\\), \\(x\\) failures probability \\(1 - p\\) (assuming events independent).\nHence probability observing \\(r\\)th success trial \\(x\\) \n\\[\\begin{align*}\n   \\text{. ways}\n   &\\times \\Pr\\big(\\text{$x$ failures}\\big) \\times \\Pr(\\text{$r - 1$ successes}) \\\\\n   \\binom{x + r - 1}{r - 1}\n   &\\times (1 - p)^{x} \\times p^{r - 1}.\n\\end{align*}\\]","code":""},{"path":"DiscreteDistributions.html","id":"NegBinAlternative","chapter":"7 Standard discrete distributions","heading":"7.6.2 Definition and properties: standard parameterisation","text":"standard definition negative binomial distribution can now given.Definition 7.10  (Negative binomial distribution) random variable \\(X\\) probability function\n\\[\\begin{equation}\n   p_X(x; p, r) = \\binom{x + r - 1}{r - 1}(1 - p)^{x} p^{r - 1}\n\\quad\\text{$x = 0, 1, 2, \\dots$}\n   \\tag{7.6}\n\\end{equation}\\]\nnegative binomial distribution parameters \\(r\\) (positive integer)  \\(p\\) (\\(0\\le p\\le 1\\)).\nwrite \\(X\\sim\\text{NB}(r, p)\\).distribution function complicated given.\nfollowing basic properties negative binomial distribution.Theorem 7.6  (Negative binomial properties) \\(X\\sim \\text{NB}(r, p)\\) probability function Eq. (7.6) \\(\\operatorname{E}[X] = \\{r(1 - p)\\}/p\\).\\(\\operatorname{var}[X] = r(1 - p)/p^2\\).\\(M_X(t) = \\left[ p / \\{1 - (1 - p)\\exp(t)\\} \\right]^r\\) \\(t < -\\log p\\).Proof. Proving third statement left exercise.\nfirst two derived MGF.Example 7.5  (Negative binomial) telephone marketer invites customers, phone, product demonstration.\nTen people needed demonstration.\nprobability randomly-chosen person accepts invitation  \\(0.15\\)., ‘success’ acceptance attend demonstration.\nLet \\(Y\\) number failed calls necessary secure ten acceptances.\n \\(Y\\) negative binomial distribution \\(p = 0.15\\) \\(r = 10\\).\nmean number failures made \\(\\operatorname{E}[Y] = \\{r(1 - p)\\}/p = \\{10 \\times (1 - 0.15)\\}/0.15 \\approx  56.66667\\).Hence, including ten calls leads person accepting invitation, mean number calls made \\(10 + \\operatorname{E}[Y] = 66.66\\dots\\).Since parameterisation (7.6) defined non-negative integers, often used model count data.four R functions working negative binomial distribution based paramaterisation Eq. (7.6), form [dpqr]nbinom(size, prob), prob  \\(p\\) size  \\(r\\) (see Appendix D).\\(r = 1\\), negative binomial distribution geometric distribution: geometric distribution special case negative binomial.probability function negative binomial distribution various values  \\(p\\)  \\(r\\) shown Fig. 7.4.\nFIGURE 7.4: probability function negative binomial distribution \\(p = 0.2\\) \\(0.7\\) \\(r = 1\\) \\(r = 3\\).\nExample 7.6  (Negative binomial) Consider Example 7.5, concerning telephone marketer inviting customers, phone, product demonstration.\nTen people needed demonstration.\nprobability randomly chosen person accepts invitation  \\(0.15\\).Consider finding probability marketer need make \\(100\\) calls secure ten acceptances.\nsituation, ‘success’ acceptance attend demonstration.\nLet \\(Y\\) number failed calls securing ten acceptances.\n \\(Y\\) negative binomial distribution \\(Y\\sim\\text{NBin}(p = 0.15, r = 10)\\).determine \\(\\Pr(Y > 100) = 1 - \\Pr(Y\\le 100)\\), using computer easiest approach.\nR, command dnbinom() returns probabilities probability function negative binomial distribution, pnbinom() returns cumulative distribution probabilities:probability  \\(0.0244\\).Assuming call take average  \\(5\\) minutes, can determine long marketer expected calling find ten acceptances.\nLet \\(T\\) time make calls minutes.\n\\(T = 5Y\\).\nHence, \\(\\operatorname{E}[T] = 5\\operatorname{E}[Y] = 5 \\times 66.7 = 333.5\\), \\(5.56\\,\\text{h}\\).Assume call costs \\(25\\) cents, company pays marketer $\\(30\\) per hour.\ndetermine total cost, let \\(C\\) total cost dollars.\ncost employing marketer , average, \\(30\\times 5.56 = \\$166.75\\).\n\\(C = 0.25 Y + 166.75\\), \\(\\operatorname{E}[C] = 0.25 \\operatorname{E}[Y] + 166.75 = C = 0.25 \\times 66.7 + 166.75 = \\$183.43\\).","code":"\nx.values <- seq(1, 100, \n                by = 1)\n1 - sum(dnbinom( x.values, \n                 size = 10, \n                 prob = 0.15))\n#> [1] 0.02442528\n# Alternatively:\n1 - pnbinom(100, \n            size = 10, \n            prob = 0.15)\n#> [1] 0.02442528"},{"path":"DiscreteDistributions.html","id":"alternative-parameterisations","chapter":"7 Standard discrete distributions","heading":"7.6.3 Alternative parameterisations","text":"\nOften, different parameterisation negative binomial distribution; see Exercise 7.14.\nHowever, parameterisation presented Sect. 7.6.1 used R.negative binomial distribution can extended  \\(r\\) can positive number, just integer.\n \\(r\\) non-integer, interpretations lost, distribution flexible.\nRelaxing restriction  \\(r\\) gives probability function \n\\[\\begin{equation}\n   p_X(x; p, r) = \\frac{\\Gamma(x + r)}{\\Gamma(r)\\, x!} p^r (1 - p)^x,\n   \\tag{7.7}\n\\end{equation}\\]\n\\(x = 0, 1, 2 \\dots\\) \\(r > 0\\).\nexpression, \\(\\Gamma(r)\\) gamma function (Def. 7.11), related factorials  \\(r\\) integer: \\(\\Gamma(r) = (r - 1)!\\)  \\(r\\) positive integer.Definition 7.11  (Gamma function) function \\(\\Gamma(\\cdot)\\) called gamma function defined \n\\[\n   \\Gamma(r) = \\int_0^\\infty x^{r - 1}\\exp(-x)\\, dx\n\\]\n\\(r > 0\\).gamma function property \n\\[\\begin{equation}\n   \\Gamma(r) = (r - 1)!\n   \\tag{7.8}\n\\end{equation}\\]\n \\(r\\) positive integer (Fig. 7.5).\nImportant properties gamma function given .Theorem 7.7  (Gamma function properties) gamma function \\(\\Gamma(\\cdot)\\),\\(\\Gamma(r) = (r - 1)\\Gamma(r - 1)\\) \\(r > 0\\).\\(\\Gamma(1/2) = \\sqrt{\\pi}\\).\\(\\lim_{r\\0} \\\\infty\\).\\(\\Gamma(1) = \\Gamma(2) = 1\\).\\(\\Gamma(n) = (n - 1)\\) positive integers \\(n\\).Proof. first property, integration parts gives\n\\[\\begin{align*}\n   \\Gamma(r)\n   &= \\left. -\\exp(-x) \\frac{x^2}{r}\\right|_0^\\infty + \\int_0^\\infty \\exp(-x) (r - 1) x^{r - 2}\\,dx\\\\\n   &= 0 + \\ (r - 1) \\int_0^{\\infty}  e^{-x}  x^{r - 2} \\, dx\\\\\n   &= (r - 1) \\Gamma(r - 1).\n\\end{align*}\\]\nFIGURE 7.5: gamma function like factorial function continuous argument. line corresponds gamma function \\(\\Gamma(z)\\); solid points correspond factorial \\((z - 1)! = \\Gamma(z)\\) integer \\(z\\).\nExample 7.7  (Negative binomial) Consider computer system fails probability \\(p = 0.02\\) given day (system failure ‘success’).\nSuppose five failures, system upgraded.find probability upgrade happen within one year, let \\(D\\) number days fifth failure.\n, seek \\(\\Pr(X < 360)\\).\nUsing R:probability upgrading within one year \\(85\\)%.Example 7.8  (Mites) Bliss (1953) gives data counts adult European red mites leaves selected random six similar apple trees (Table 7.1).\nmean number mites per leaf  \\(1.14\\) variance  \\(3.57\\); since Poisson distribution equal mean variance, Poisson distribution may model count data well.\nHowever, mean number mites per leaf can modelled using negative binomial distribution \\(r = 1.18\\) \\(p = 0.5\\).estimated probability function Poisson negative binomial distributions given Table 7.1; negative binomial distribution fits better expected.\nTABLE 7.1: Counts mites leaves selected random six similar apple trees.\n","code":"\npnbinom(360, \n        size = 5, \n        prob = 0.02)\n#> [1] 0.8553145"},{"path":"DiscreteDistributions.html","id":"PoissonDistribution","chapter":"7 Standard discrete distributions","heading":"7.7 Poisson distribution","text":"\nPoisson distribution commonly used model number occurrences event occurs randomly time space.\nPoisson distribution arises result assumptions made random process:Events occur one time-interval (region) independent occurring non-overlapping time-interval (region).probability event occurs small time-interval proportional length interval.probability  \\(2\\) events occur small time-interval small negligible.Whenever assumptions valid, approximately , Poisson distribution appropriate.\nMany natural phenomena fall category.","code":""},{"path":"DiscreteDistributions.html","id":"PoissonDerivation","chapter":"7 Standard discrete distributions","heading":"7.7.1 Derivation of Poisson distribution","text":"binomial distribution (Sect. 7.4) applies situations event may occur certain number times fixed number trials, say \\(n\\).\n \\(n\\) gets increasingly large though, effectively upper limit?\nMathematically, might say: happens \\(n\\\\infty\\)?Let’s find .\nBegin binomial probability function\n\\[\\begin{equation}\n   p_X(x; n, p) = \\binom{n}{x} p^x (1 - p)^{n - x} \\quad\\text{$x = 0, 1, \\dots, n$}\n   \\tag{7.9}\n\\end{equation}\\]\nmean \\(\\lambda = np\\).\nFirst, re-writing (7.9) terms mean \\(\\lambda\\):\n\\[\\begin{equation}\n   p_X(x; n, \\lambda)\n   =\n   \\binom{n}{x} \\left(\\frac{\\lambda}{n}\\right)^x \\left(1 - \\frac{\\lambda}{n}\\right)^{n - x} \\quad\\text{$x = 0, 1, \\dots, n$}.\n   \\tag{7.10}\n\\end{equation}\\]\nNow consider case \\(n \\\\infty\\) (\\(x = 0, 1, \\dots\\)):\n\\[\\begin{align}\n   \\lim_{n\\\\infty}\n   p_X(x; n, \\lambda)\n   &=\n   \\lim_{n\\\\infty} \\binom{n}{x}\n   \\left(\\frac{\\lambda}{n}\\right)^x \\left(1 - \\frac{\\lambda}{n}\\right)^{n - x}\\\\\n   &= \\frac{\\lambda^x}{x!}\n   \\lim_{n\\\\infty} \\frac{n!}{(n - x)!} \\frac{1}{n^x} \\left(1 - \\frac{\\lambda}{n}\\right)^{n}  \\left(1 - \\frac{\\lambda}{n}\\right)^{-x}.\n\\end{align}\\]\nNow, since limit product functions equal product limits, component can considered turn.Looking first component, see \n\\[\\begin{align}\n  \\lim_{n\\\\infty} \\frac{n!}{(n - x)!} \\times \\frac{1}{n^x}\n  &= \\lim_{n\\\\infty} \\frac{n \\times (n - 1) \\times (n - 2) \\times\\cdots\\times 2 \\times 1}\n                            {(n - x)\\times (n - x - 1) \\times \\cdots \\times 2 \\times 1} \\times \\frac{1}{n^x}\\\\\n  &= \\lim_{n\\\\infty}\\frac{ \\overbrace{n \\times (n - 1) \\times \\cdots\\times (n - x + 1)}^{\\text{$x$ terms}}}\n                           { \\underbrace{n \\times n \\times\\cdots \\times n\\times n}_{\\text{$x$ terms}}}\\\\\n  &= \\lim_{n\\\\infty} \\frac{n}{n} \\times \\frac{n - 1}{n} \\times \\frac{n - 2}{n} \\times\\cdots\\times \\frac{n - x + 1}{n}\\\\\n  &= 1 \\times 1\\times\\cdots\\times 1 = 1.\n\\end{align}\\]\nnext component, see (using (B.10)):\n\\[\n  \\lim_{n\\\\infty} \\left(1 - \\frac{\\lambda}{n}\\right)^{n} = \\exp(-\\lambda).\n\\]\nlast term:\n\\[\n  \\lim_{n\\\\infty} \\left(1 - \\frac{\\lambda}{n}\\right)^{-x} = 1^{-x} = 1.\n\\]\nPutting pieces together:\n\\[\\begin{align}\n   \\lim_{n\\\\infty}\n   p_X(x; n, \\lambda)\n   &=\n   \\frac{\\lambda^x}{x!}\n   \\lim_{n\\\\infty} \\frac{n!}{(n - x)!} \\frac{1}{n^x}\n   \\left(1 - \\frac{\\lambda}{n}\\right)^{n}  \\left(1 - \\frac{\\lambda}{n}\\right)^{-x}\\\\\n   p_X(x; \\lambda)\n   &= \\frac{\\lambda^k}{x!}\n   1 \\times \\exp(-\\lambda) \\times 1\\\\\n   &= \\frac{\\exp(-\\lambda) \\lambda^x}{x!},\n\\end{align}\\]\n\\(x = 0, 1, 2, \\dots\\).\nprobability function Poisson distribution.Poisson process refers events occur rate \\(\\lambda\\), occur random.","code":""},{"path":"DiscreteDistributions.html","id":"PoissonDefinition","chapter":"7 Standard discrete distributions","heading":"7.7.2 Definition and properties","text":"definition Poisson distribution can now given.Definition 7.12  (Poisson distribution) random variable \\(X\\) said Poisson distribution probability function \n\\[\\begin{equation}\n   p_X(x; \\lambda) = \\frac{\\exp(-\\lambda) \\lambda^x}{x!}\\quad \\text{$x = 0, 1, 2, \\dots$}\n   \\tag{7.11}\n\\end{equation}\\]\nparameter \\(\\lambda > 0\\).\nwrite \\(X\\sim\\text{Pois}(\\lambda)\\).Definition 7.13  (Poisson distribution: distribution function) random variable \\(X\\) Poisson distribution given (7.11), distribution function \n\\[\n  F_X(x; \\lambda) =\n  \\begin{cases}\n    0                                  & \\text{$x < 0$}\\\\\n    \\displaystyle \\exp(-\\lambda) \\sum _{= 0}^{\\lfloor x \\rfloor }{\\frac {\\lambda ^{}}{!}} & \\text{$x\\ge 0$};\\\\\n  \\end{cases}\n\\]\n\\(\\lfloor z \\rfloor\\) floor function.PMF Poisson distribution different values  \\(\\lambda\\) shown Fig. 7.6.\nFIGURE 7.6: probability function Poisson distribution various values \\(\\lambda\\).\nfollowing basic properties Poisson distribution.Theorem 7.8  (Poisson distribution properties) \\(X\\sim\\text{Pois}(\\lambda)\\) \\(\\operatorname{E}[X] = \\lambda\\).\\(\\operatorname{var}[X] = \\lambda\\).\\(M_X(t) = \\exp[ \\lambda\\{\\exp(t) - 1\\}]\\).Proof. third result proven, results follow:\n\\[\\begin{align*}\n   M_X(t) = \\operatorname{E}[e^{tX}]\n     &= \\sum^\\infty_{x = 0} e^{xt} e^{-\\lambda} \\lambda^x / x!\\\\\n     &= e^{-\\lambda} \\sum^\\infty_{x = 0} \\frac{(\\lambda\\, e^t)^x}{x!} \\\\\n     &= e^{-\\lambda} \\left[ 1 + \\lambda\\, e^t + \\frac{(\\lambda\\,e^t)^2}{2!} + \\dots \\right]\\\\\n     &= e^{-\\lambda} e^{\\lambda\\, e^t}\\\\\n     &= e^{-\\lambda (1 - e^t)},\n\\end{align*}\\]\nusing (B.8).\nfirst two results follow differentiating MGF.Notice Poisson distribution variance equal mean.\nnegative binomial distribution two parameters Poison one, often negative binomial distribution produces better fit data.four R functions working Poisson distribution form [dpqr]poiss(lambda) (see Appendix D).Example 7.9  (Queuing) Customers enter service line ‘random’ rate 4 per minute.\nAssume number entering line given time interval Poisson distribution.\ndetermine probability least one customer enters line given \\(\\frac{1}{2}\\)-minute interval, use (since \\(\\lambda = 2\\) half-hour):\n\\[\n   \\Pr(X\\geq 1) = 1 - \\Pr(X = 0) = 1 - e^{-2} = 0.865.\n\\]\nR:Example 7.10  (Bomb hits) Clarke (1946) (quoted Hand et al. (1996), Dataset 289) discusses number flying bomb hits London World War II \\(36\\) square kilometre area South London.\narea gridded \\(0.25\\) km squares number bombs falling grid counted (Table 7.2).\nAssuming random hits, Poisson distribution can used model data using \\(\\lambda = 0.93\\).\nprobabilityfunction Poisson distribution can compared empirical probabilities computed ; see Table 7.2.\nexample, probability zero hits \n\\[\n   \\frac{\\exp(-0.93) (0.93)^0}{0!} \\approx 0.39.\n\\]\ntwo probabilities close; Poisson distribution fits data well.\nTABLE 7.2: number flying bomb hits London World War II \\(36\\) square kilometre area South London. proportion \\(576\\) grid squares receiving \\(0, 1, ...\\) hits also computed. observed empirical probabilities shown. Poisson distribution fits data well.\n","code":"\n1 - dpois(0,\n          lambda = 2)\n#> [1] 0.8646647"},{"path":"DiscreteDistributions.html","id":"PoissonBinomial","chapter":"7 Standard discrete distributions","heading":"7.7.3 Relationship to the binomial distribution","text":"Given derivation Poisson distribution (Sect. 7.7.1), relationship Poisson binomial distributions come surprise.Computing binomial probabilities tedious number trials \\(n\\) large probability success single trial small.\nexample, consider \\(X\\sim \\text{Bin}(n = 2000, p = 0.005)\\); probability function \n\\[\n   p_X(x) = \\binom{2000}{x}(0.005)^x 0.995^{2000 - x}.\n\\]\nUsing PMF, computing probabilities, \\(\\Pr(X > 101)\\), tedious.\n(Try computing \\(\\binom{2000}{102}\\) calculator, example.)\nHowever, Poisson distribution can used approximate probability.Set Poisson mean equal binomial mean (, \\(\\lambda = \\mu = np\\)).\nSince \\(\\operatorname{E}[Y] = \\operatorname{var}[Y] = \\lambda\\) Poisson distribution, means variance also set \\(\\sigma^2 = np\\).\ncourse, binomial mean \\(np(1 - p)\\), can (approximately) true  \\(p\\) close zero (\\(1 - p\\approx 1\\)).general guideline Poisson distribution can used approximate binomial \\(n\\) large (recall, derivation case \\(n\\\\infty\\)), \\(p\\) small \\(np \\le 7\\).random variable \\(X\\) binomial distribution \\(X \\sim \\text{Bin}(n, p)\\), probability function can approximated Poisson distribution \\(X \\sim \\text{Pois}(\\lambda)\\), \\(\\lambda = np\\).\napproximation good  \\(n\\) large, \\(p\\) small, \\(np\\le 7\\).\nFIGURE 7.7: Poisson distribution excellent approximation binomial distribution \\(p\\) small \\(n\\) large. binomial probability function shown using empty circles; Poisson probability function using crosses.\n","code":""},{"path":"DiscreteDistributions.html","id":"PoissonExtensions","chapter":"7 Standard discrete distributions","heading":"7.7.4 Extensions","text":"Poisson distribution commonly used model independent counts.\nHowever, sometimes counts explicitly exclude count zero.\nexample, consider modelling number nights patients spend hospital; patients must spend least one night data.\n, probability functions can expressed \n\\[\n   p_X(x; \\lambda) = \\frac{\\exp(-\\lambda) \\lambda^{x - 1}}{(x - 1)!}\\quad   \\text{$x = 1, 2, 3, \\dots$}\n\\]\nparameter \\(\\lambda > 0\\).\ncalled zero-truncated Poisson distribution.\ncase, \\(\\operatorname{E}[X] = \\lambda + 1\\) \\(\\operatorname{var}[X] = \\lambda\\) (Ex. 7.22).cases, random variable count, upper limit possible number counts.\ntruncated Poisson distribution.situations exist proportions zeros exceed proportions expected Poisson distribution (e.g., \\(\\exp(-\\lambda)\\)), Poisson distributions seems otherwise suitable.\nsituations, zero-inflated Poisson distribution may suitable.\n","code":""},{"path":"DiscreteDistributions.html","id":"HypergeometricDistribution","chapter":"7 Standard discrete distributions","heading":"7.8 Hypergeometric distribution","text":"","code":""},{"path":"DiscreteDistributions.html","id":"HypergeometricDerivation","chapter":"7 Standard discrete distributions","heading":"7.8.1 Derivation of a hypergeometric distribution","text":"\nselection items fixed number times done replacement, probability item selected stays binomial distribution can used.\nHowever, selection items done without replacement, trials independent, making binomial model unsuitable.\nsituations, hypergeometric distribution appropriate.Consider simple example.\nbag contains six light-coloured balls four dark-coloured balls (Fig. 7.8).\nvariable interest, say \\(X\\), number light-coloured balls drawn three random selections bag, without replacing balls.\nSince balls replaced, \\(\\Pr(\\text{draw light-coloured ball})\\) constant, binomial distribution used.\nFIGURE 7.8: Drawing balls bag.\nprobabilities can computed, however, using counting ideas Chap. 2.4.\ntotal \\(\\binom{10}{3}\\) ways selecting sample size \\(k = 3\\) bag.\nConsider case \\(X = 0\\).\nnumber ways drawing light-coloured balls \\(\\binom{6}{0}\\) number ways drawing three dark-coloured balls \\(\\binom{4}{3}\\), probability \n\\[\n   \\Pr(X = 0) = \\frac{\\binom{6}{0}\\binom{4}{3}}{\\binom{10}{3}} \\approx 0.00833.\n\\]\nLikewise, number ways draw one light-coloured ball (hence two dark-coloured balls; Fig. 7.8) \\(\\binom{6}{1}\\times \\binom{4}{2}\\), \n\\[\n   \\Pr(X = 1) = \\frac{ \\binom{6}{1}\\times \\binom{4}{2} }{\\binom{10}{3}}.\n\\]\nSimilarly,\n\\[\n   \\Pr(X = 2) =\\frac{ \\binom{6}{2}\\times \\binom{4}{1} }{\\binom{10}{3}}\n   \\quad\\text{}\\quad\n   \\Pr(X = 3) =\\frac{ \\binom{6}{3}\\times \\binom{4}{0} }{\\binom{10}{3}}.\n\\]","code":""},{"path":"DiscreteDistributions.html","id":"HypergeometricDefinition","chapter":"7 Standard discrete distributions","heading":"7.8.2 Definition and properties","text":"general, suppose bag contains \\(N\\) balls,  \\(m\\) light-coloured  \\(n\\) light-coloured; \\(N = m + n\\).\nSuppose sample size \\(k\\) drawn bag without replacement; probability finding \\(x\\) light-coloured balls sample size \\(k\\) \n\\[\n   \\Pr(X = x)\n   = \\frac{ \\binom{m}{x}{ \\binom{n}{k - x}}}{\\binom{N}{k}}\n   = \\frac{ \\binom{m}{x}{ \\binom{n}{k - x}}}{\\binom{m + n}{k}}\n\\]\n \\(X\\) number light-coloured balls sample size \\(k\\).\n(example, \\(k = 3\\), \\(m = 6\\) \\(N = 10\\).)formula, \\(\\binom{m}{x}\\) number ways selecting \\(x\\) light-coloured balls  \\(m\\) light-coloured balls bag; \\(\\binom{n}{k - x}\\) number ways selecting remaining \\(k - x\\) colour (\\(n = N - m\\) bag); \\(\\binom{N}{k}\\) number ways selecting sample size \\(k\\) \\(N\\) balls bag total.Definition 7.14  (Hypergeometric distribution) Consider set \\(N = m + n\\) items  \\(m\\) one kind (call ‘successes’) \\(n = N - m\\) another kind (call ‘failures’).\ninterested probability \\(x\\) successes \\(k\\) trials, selection (drawing) made without replacement.\nrandom variable \\(X\\) said hypergeometric distribution probability function\n\\[\\begin{equation}\n   p_X(x; n, m, k) = \\frac{ \\binom{m}{x}\\binom{n}{k - x}}{\\binom{m + n}{k}}\n   \\tag{7.12}\n\\end{equation}\\]\n\\(\\max(0, k - n) \\le x \\le \\min(n, m)\\).distribution function complicated given.\nfollowing basic properties hypergeometric distribution.Theorem 7.9  (Hypergeometric distribution properties)  \\(X\\) hypergeometric distribution probability function (7.12), (writing \\(N = m + n\\))\\(\\operatorname{E}[X] = km/N\\).\\(\\displaystyle\n\\operatorname{var}[X] = k \\left(\\frac{m}{N}\\right)\\left(\\frac{N - k}{N - 1}\\right)\\left(1 - \\frac{m}{N}\\right)\\).moment-generating function difficult considered.four R functions working hypergeometric distribution functions form [dpqr]hyper(m, n, k), k\\({} = k\\), m\\({} = m\\) m\\({} + {}\\)n\\({} = N\\), n\\({}={}\\)N\\({}-{}\\)m (see Appendix D).population much larger sample size (, \\(N\\) much larger  \\(k\\)), probability success approximately constant, binomial distribution can used give approximate probabilities.Consider example start section.\nprobability drawing light-coloured ball initially \\(6/10 = 0.6\\), probability next ball light-coloured \\(5/9 = 0.556\\).\nsuppose \\(10\\,000\\) balls bag, $6,000 $light-coloured.\nprobability drawing light-coloured ball initially \\(6000 / 10,000 = 0.6\\), probability next ball light-coloured becomes \\(5999/9999 = 0.59996\\); probability almost .\ncase, might consider using binomial distribution \\(p\\approx 0.6\\).general,  \\(N\\) much larger \\(k\\), population proportion approximately \\(p\\approx m/N\\), \\(1 - p \\approx (N - m)/N\\).\nUsing information,\n\\[\n   \\operatorname{E}[X] = k \\times (m/N) \\approx kp\n\\]\n\n\\[\\begin{align*}\n   \\operatorname{var}[X]\n   &= k\\left(\\frac{m}{N}\\right)\\left(1 - \\frac{m}{N}\\right)\\left(\\frac{N - k}{N - 1}\\right)\\\\\n   &\\approx k\\left( p \\right ) \\left( 1 - p \\right) \\left(1 \\right ) \\\\\n   &= k p (1 - p),\n\\end{align*}\\]\ncorrespond mean variance binomial distribution.Example 7.11  (Mice) Twenty mice available used experiment; seven mice female  \\(13\\) male.\nFive mice required sacrificed.\nprobability three mice males?Let \\(X\\) number male mice chosen sample size \\(k = 5\\).\n \\(X\\) hypergeometric distribution (since mice chosen without replacement) \\(N = 20\\), \\(k = 5\\), \\(m = 13\\) \\(n = 20 - 13 = 7\\), seek\n\\[\\begin{align*}\n   \\Pr(X > 3)\n   &= \\Pr(X = 4) + \\Pr(X = 5) \\\\\n   &= \\frac{ \\binom{13}{4} \\binom{7}{1}}{ \\binom{20}{5} } +\n       \\frac{ \\binom{13}{5} \\binom{7}{0}}{ \\binom{20}{5} } \\\\\n   & \\approx  0.3228 + 0.0830 = 0.4058.\n\\end{align*}\\]\nprobability  \\(41\\)%:","code":"\ndhyper(x = 4, m = 13, n = 7, k = 5) + \n  dhyper(x = 5, m = 13, n = 7, k = 5)\n#> [1] 0.4058308"},{"path":"DiscreteDistributions.html","id":"MultinomialDistribution","chapter":"7 Standard discrete distributions","heading":"7.9 Multinomial distribution","text":"\nspecific example discrete multivariate distribution multinomial distribution, generalization binomial distribution.Definition 7.15  (Multinomial distribution) Consider experiment sample space partitioned \\(S = \\{B_1, B_2, \\ldots, B_k\\}\\).\nLet \\(p_i = \\Pr(B_i), \\ = 1, 2,\\ldots k\\) \\(\\sum_{= 1}^k p_i = 1\\).\nSuppose \\(n\\) repetitions experiment  \\(p_i\\) constant.\nLet random variable \\(X_i\\) number times (\\(n\\) repetitions) event \\(B_i\\) occurs.\nsituation, random vector \\((X_1, X_2, \\dots, X_k)\\) said multinomial distribution probability function\n\\[\\begin{equation}\n   \\Pr(X_1 = x_1, X_2 = x_2, \\ldots, X_k = x_k)\n   = \\frac{n!}{x_1! \\, x_2! \\ldots x_k!}\n      p_1^{x_1}\\, p_2^{x_2} \\ldots p_k^{x_k},\n   \\tag{7.13}\n\\end{equation}\\]\n\\(\\mathcal{R}_X = \\{(x_1, \\ldots x_k) : x_i = 0,1,\\ldots,n, \\, = 1, 2, \\ldots k, \\, \\sum_{= 1}^k x_i = n\\}\\).part Eq. (7.13) involving factorials arises number ways arranging \\(n\\) objects, \\(x_1\\) first kind, \\(x_2\\) second kind, etc.\ndistribution \\((k - 1)\\)-variate since \\(x_k = n-\\sum_{= 1}^{k - 1}x_i\\).\nparticular \\(k = 2\\), multinomial distribution reduces binomial distribution univariate distribution.\\(X_i\\) number times ( \\(n\\)) event \\(B_i\\), probability \\(p_i\\), occurs.\nrandom variable \\(X_i\\) clearly binomial distribution parameters \\(n\\)  \\(p_i\\).\nsee marginal probability distribution one components multinomial distribution binomial distribution.Notice distribution Example 4.5 example trinomial distribution.\nprobabilities shown Table 4.3 can expressed algebraically \n\\[\n   \\Pr(X = x, Y = y)\n   = \\frac{2!}{x! \\, y!(2 - x - y)!}\n      \\left(\\frac{1}{6}\\right)^x\\left(\\frac{1}{6}\\right)^y\\left(\\frac{2}{3}\\right)^ {2 - x - y}\n\\]\n\\(x, y = 0 , 1 , 2\\); \\(x + y \\leq 2\\).following basic properties multinomial distribution.Theorem 7.10  (Multinomial distribution properties) Suppose \\((X_1, X_2, \\ldots, X_k)\\) multinomial distribution given Def. 7.15.\n\\(= 1, 2, \\ldots, k\\):\\(\\operatorname{E}[X_i] = np_i\\).\\(\\operatorname{var}[X_i] = n p_i(1 - p_i)\\).\\(\\operatorname{Cov}(X_i, X_j) = -n p_i p_j\\) \\(\\neq j\\).Proof. first two results follow fact \\(X_i \\sim \\text{Bin}(n, p_i)\\).use \\(x\\)  \\(x_1\\)  \\(y\\)  \\(x_2\\) third convenience.\nConsider case \\(k = 3\\), note \n\\[\n   \\sum_{(x, y) \\R}\n   \\frac{n!}{x! \\, y! (n - x - y)!} p_1^x p_2^y (1 - p_1 - p_2)^{n - x - y} = 1.\n\\]\n, putting \\(p_3 = 1 - p_1 - p_2\\),\n\\[\\begin{align*}\n  E(XY)\n  &= \\sum_{(x, y)}xy \\Pr(X = x, Y = y)\\\\\n  &= \\sum_{(x, y)}\\frac{n!}{(x - 1)!(y - 1)!(n - x - y)!} p_1^x  p_2^y p_3^{n - x - y}\\\\\n  &= n(n - 1) p_1 p_2\\underbrace{\\sum_{(x,y)}\\frac{(n - 2)!}{(x - 1)!(y - 1)!(n - x - y)!} p_1^{x - 1} p_2^{y - 1}p_3^{n - x - y}}_{ = 1}.\n\\end{align*}\\]\n\\(\\operatorname{Cov}(X, Y) = n^2 p_1 p_2 - n p_1 p_2 - (n p_1)(n p_2) = -n p_1 p_2\\).two R functions working multinomial distribution functions form [dr]multinom(size, prob) size\\({}= n\\) prob\\({} = (p_1, p_2, \\dots, p_k)\\) \\(k\\) categories (see App. D).\nNote functions qmultinom() pmultinom() defined.Example 7.12  (Multinomial distribution) Suppose four basic blood groups O, , B  AB known occur proportions \\(9:8:2:1\\).\nGiven random sample \\(8\\) individuals, probability \\(3\\) Types O  \\(1\\) Types B  AB?probabilities \\(p_1 = 0.45\\), \\(p_2 = 0.4\\), \\(p_3 = 0.1\\), \\(p_4 = 0.05\\), \n\\[\\begin{align*}\n   \\Pr(X_O = 3, X_A = 3, X_B = 1, X_{AB} = 1)\n   &= \\frac{8!}{3!\\,3!\\,1!\\,1!}(0.45)^3 (0.4)^3 (0.1)(.05)\\\\\n   &= 0.033.\n\\end{align*}\\]\nR:","code":"\ndmultinom(x = c(3, 3, 1, 1), \n          size = 8, \n          prob = c(0.45, 0.4, 0.1, 0.05))\n#> [1] 0.0326592"},{"path":"DiscreteDistributions.html","id":"OtherDiscreteDistributions","chapter":"7 Standard discrete distributions","heading":"7.10 Other notable discrete distributions","text":"distributions discussed chapter standard commonly-used discrete distributions.\ndiscrete uniform distribution used equally-likely outcomes (like coin tosses rolls die).\nBernoulli distribution used model single dichotomous (e.g., yes/; true/false) outcomes.\ntwo distributions nit often used practice, basis developing distributions.binomial distribution used model number successes fixed number Bernoulli repetitions.\ngeometric distribution used model number Bernoulli trials first success.\nnegative binomial distribution can used model number Bernoulli trials \\(r\\)th success, also mider uses modelling count data (theoretical upper limit); greater flexibility modelling counts Poisson distribution.\nPoisson distribution commonly used model count data (theoretical upper limit).hypergeometric distribution used model number successes sample size \\(n\\) n drawn without replacement population size size \\(N\\),  \\(K\\) successes.\nmultinomial distribution used model counts outcomes  \\(n\\) independent categorical trials (whereas binomial used two independent categorical trials).Countless useful discrete distributions exist; mention .\nZeta distribution (sometimes, Zipf distribution), defined \\(x = 1, 2, \\dots\\), often used linguistics, heavy-tailed counts.Poisson distribution commonly used model counts, two important variations Poisson distribution exist.\nzero-truncated Poisson distribution, range space \\(\\mathcal{R}_X = \\{1, 2, \\dots\\}\\), used model counts counts zero impossible (.e., number night hospital stay -patients).\ncontrast, zero-inflated Poisson distribution, range space \\(\\mathcal{R}_X = \\{0, 1, 2, \\dots\\}\\), used model counts proportion zeros higher expected using Poisson distribution.","code":""},{"path":"DiscreteDistributions.html","id":"SimulationDiscrete","chapter":"7 Standard discrete distributions","heading":"7.11 Simulation","text":"Simulation introduced Sect. 2.11 computer-based tool helping understand model specific scenarios.\nProbability distributions usually crucial simulations modelling phenomena.\nSimulation effectively connects theory real-world modelling.\nMany application (finance, epidemiology, engineering, AI) use simulation study uncertainty complex situations.\nbook provides building blocks complex situations.Distributions can used simulate practical situations, using random numbers generated distributions.\nR, function start letter r; example, generate three random numbers Poisson distribution, use rpois():Consider study insects females lays \\(E\\) eggs, fertile (based Ito (1977)).\nSuppose \\(E \\sim \\text{Pois}(8.5)\\), probability given egg fertile \\(p = 0.6\\) (assume fertility eggs one female independent).fertility one females can modelled R:Since random numbers generated, simulation produce different results, repeat many simulations:can draw histogram number fertile eggs laid females (Fig. 7.9):\nFIGURE 7.9: number fertile eggs laid, \\(5\\,000\\) simulations.\nUsing information, questions can answered number fertile eggs layed.\nexample:chance female lays  \\(6\\) fertile eggs 25.1%.\nmean variance number fertile eggs laid :","code":"\nrpois(3, # Generate three random numbers...\n      lambda = 4) # ... with mean = 4\n#> [1] 2 4 4\n# One female\nNumberOfEggs <- rpois(1, \n                      lambda = 8.5)\n\n# How many eggs are fertile?\nNumberFertileEggs <- rbinom(1, # Only one random number for this one female\n                            size = NumberOfEggs,\n                            prob = 0.6)\ncat(\"Number of eggs:\", NumberOfEggs,\n    \"| Number of fertile eggs:\", NumberFertileEggs, \"\\n\")\n#> Number of eggs: 5 | Number of fertile eggs: 3\n# Many simulations\n\nnumberSims <- 5000\n\nNumberOfEggs <- array( dim = numberSims )\nNumberFertileEggs <- array( dim = numberSims )\n\nfor (i in (1:numberSims)){ \n  NumberOfEggs[i] <- rpois(1, lambda = 8.5)\n  \n  # How many fertile?\n  NumberFertileEggs[i] <- rbinom(1, # Only one random number/female\n                                 size = NumberOfEggs[i],\n                                 prob = 0.6)\n}\nhist(NumberFertileEggs,\n     las = 1, # Horizontal tick mark labels\n     xlab = \"Number of fertile eggs\",\n     ylab = \"Count\",\n     main = \"Number of fertile eggs:\\n5000 simulations\")\nsum( NumberFertileEggs > 6) / numberSims\n#> [1] 0.2512\nmean( NumberFertileEggs )\n#> [1] 5.1074\nvar( NumberFertileEggs )\n#> [1] 4.97166"},{"path":"DiscreteDistributions.html","id":"DiscreteExercises","chapter":"7 Standard discrete distributions","heading":"7.12 Exercises","text":"Selected answers appear Sect. E.7.Exercise 7.1  \\(X \\sim \\text{Bin}(n, 1 - p)\\), show \\((n - X) \\sim \\text{Bin}(n, p)\\).Exercise 7.2  study Sutherland et al. (2012) found  \\(30\\)% Britons ‘generally added salt’ dinner table.group \\(25\\) Britons, probability least \\(10\\) generally added salt?group \\(25\\) Britons, probability  \\(9\\) generally added salt?group \\(25\\) Britons, probability  \\(5\\)  \\(10\\) (inclusive) generally added salt?probability \\(6\\) Britons need selected find one generally adds salt?probability least \\(8\\) Britons need selected find first generally add salt?probability least \\(8\\) Britons need selected find three generally add salt?assumptions made calculations?Exercise 7.3  study Loyeung et al. (2018) examined whether people identify potential placebos.\n\\(81\\) subjects presented five different supplements, asked select one legitimate herbal supplement based taste (rest placebos).probability  \\(15\\) correctly identify legitimate supplement?probability least \\(12\\) correctly identify legitimate supplement?probability first person identify legitimate supplement third person tested?probability fifth person identify legitimate supplement \\(10\\)th person tested?study, \\(50\\) people correctly selected true herbal supplement.\nsuggest?Exercise 7.4  study Maron (2007) used statistical modelling show mean number noisy miners (type bird) sites  \\(15\\) eucalypts per two hectares  \\(3\\).two hectare site \\(15\\) eucalypts, probability observing noisy miners?two hectare site \\(15\\) eucalypts, probability observing  \\(5\\) noisy miners?four hectare site \\(30\\) eucalypts, probability observing two noisy miners?Exercise 7.5  study rainfall disaggregation (extracting small-scale rainfall features large-scale measurements), number non-overlapping rainfall events per day Katherine modelled using Poisson distribution (\\(x = 0, 1, 2, 3, \\dots\\)) \\(\\lambda = 2.5\\) summer \\(\\lambda = 1.9\\) winter (Connolly, Schirmer, Dunn 1998).Denote number rainfall events summer  \\(S\\), winter  \\(W\\).Plot two distributions, compare summer winter (graph).\nCompare comment.probability  \\(3\\) rainfall events per day winter?probability  \\(3\\) rainfall events per day summer?Describe meant statement \\(\\Pr(S > 3 \\mid S > 1)\\), compute probability.Describe meant statement \\(\\Pr(W > 2 \\mid W > 1)\\), compute probability.Exercise 7.6  Consider population animals certain species unknown size \\(N\\) (Romesburg Marshall 1979).\ncertain number animals area trapped tagged, released.\nlater point, animals trapped, number tagged noted.Define \\(p\\) probability animal captured zero times study,  \\(n_x\\) number animals captured \\(x\\) times (\\(x = 0, 1, 2, \\dots\\)).\n(\\(n_0\\), write  \\(N\\), unknown.)\nstudy consist \\(s\\) trapping events, \\(n_1\\), \\(n_2, \\dots n_s\\)  \\(s\\) sufficiently ‘large’ truncation negligible.\n,\n\\[\\begin{equation}\n   n_x = N p (1 - p)^x \\quad \\text{$x = 0, 1, 2, \\dots$}.\n   \\tag{7.14}\n\\end{equation}\\]\n \\(p\\)  \\(N\\) unknown, value  \\(N\\) (effectively, population size) interest.Explain (7.14) arises understanding problem.Take logarithms sides (7.14), hence write equation form linear regression equation \\(\\hat{y} = \\beta_0 + \\beta_1 x\\).Using regression equation, identify estimate \\(p\\) knowing estimate slope regression line (\\(\\beta_1\\)), estimate \\(N\\) \\(y\\)-intercept (\\(\\beta_0\\)).Use data Table 7.3, study rabbits area Michigan (Eberhardt, Peterle, Schofield 1963), estimate rabbit population.(Hint: fit regression line  R, use lm(); example, fit regression line \\(\\hat{y} = \\beta_0 + \\beta_1 x\\), use lm(y ~ x).)\nTABLE 7.3: number rabbits \\(n_x\\) trapping previously trapped.\nExercise 7.7  Nigerian study using solar energy disinfect water (SODIS) modelled number days exposure needed disinfect water (Nwankwo Attama 2022).\nthreshold disinfection single day recording \\(4\\) kWh/m\\(2\\) daily cumulative solar irradiance.\nDefine \\(p\\) probability single day records threshold.Suppose \\(p = 0.5\\).\nmany days exposure needed, average, achieve disinfection?Suppose \\(p = 0.25\\).\nmany days exposure needed, average, achieve disinfection?\\(p = 0.25\\), variance number days needed achieve disinfection?\\(p = 0.25\\), probability disinfection achieved three days fewer?Exercise 7.8  negative binomial distribution used model number parasites feral cats Kerguelen Island (Hwang, Huggins, Stoklosa 2016).\nmodel used parameterised \\(\\mu = 8.7\\) \\(k = 0.4\\), \\(\\operatorname{var}[X] = \\mu + \\mu^2/k\\).Use information determine negative binomial parameters used parameterisation Sect. 7.9.Determine probability feral cat  \\(10\\) parasites.cats largest \\(10\\)% parasites many parasites?Exercise 7.9  study investigating bacterial sample preparation procedures single-cell studies (Koyama et al. 2016) studied, among bacteria, E. coli 110.\nnumber bacteria 2\\(\\mu\\)L samples modelled using Poisson distribution \\(\\lambda = 1.04\\).probability sample  \\(4\\) bacteria?probability sample  \\(4\\) bacteria, given bacteria?expect negative binomial distribution fit data better Poisson?Exercise 7.10  study accidents coal mines (Sari et al. 2009) used Poisson distribution \\(\\lambda = 12.87\\).Plot distribution.Compute probability one accident per day February.Exercise 7.11  study heat spells (Furrer et al. 2010) examined three cities.\nParis, ‘hot’ day defined day maximum  \\(27\\)\\(\\circ\\)C; Phoenix, ‘hot’ day defined day maximum \\(40.8\\)\\(\\circ\\)C.length heat spell \\(X\\) modelled using geometric distribution, \\(p = 0.40\\) Paris, \\(\\lambda = 0.24\\) Phoenix.graph, plot probability functions.city, probability heat spell last longer week?city, probability heat spell last longer week, given lasted two days?Exercise 7.12  study crashes intersections (Lord, Washington, Ivan 2005) examined high risk intersections, modelled number crashes Poisson distribution using \\(\\lambda = 11.5\\) per day.proportion intersection expected zero crashes?proportion intersection expected five crashes?Exercise 7.13  negative binomial distribution used model day eggs layed glaucous-winged gulls (Zador, Piatt, Punt 2006).\nauthors used different parameterisation negative binomial distribution, two parameters \\(\\mu\\) (mean)  \\(\\phi\\) (overdispersion parameter).\n( R, called mu size respectively calling dnbinom() friends.)\nexpected ‘lay date’ (Day \\(0\\)) \\(23.0\\) 1999, \\(19.5\\) 2000;\noverdispersion parameter \\(\\phi = 20.6\\) 1999 \\(\\phi = 8.9\\) 2000.Using information, plot probability function years (graph), comment.years, compute probability lay date exceeds \\(30\\), comment.years, find lay date earliest \\(15\\)% birds.data Table 7.4 shows clutch size (number eggs) found birds’ nest.\nCompute mean standard deviation number eggs per clutch.\nTABLE 7.4: numbers eggs per clutch.\nExercise 7.14  negative binomial distribution defined Sect 7.6 random variable \\(X\\), represented number failures \\(r\\)th success.\nalternative parameterisation define random variable \\(Y\\) number trials necessary obtain \\(r\\) successes.Define range space  \\(Y\\); explain.Deduce probability function  \\(Y\\).Determine mean, variance MGF  \\(Y\\), using results already available  \\(X\\).Exercise 7.15  Suppose typos made typist typing test form Poisson process mean rate \\(2.5\\) typos per minute test lasts five minutes.Determine probability typist makes exactly \\(10\\) errors test.Determine probability typist makes exactly \\(6\\) errors first 3 minutes, exactly \\(4\\) errors last \\(2\\) minutes test.Determine probability typist makes exactly \\(6\\) errors first 3 minutes, exactly \\(6\\) last \\(3\\) minutes test.\n(Note: times overlap.)Exercise 7.16  depth river varies ‘normal’ level, say \\(Y\\) (metres), specific location PDF given \\(f(y) = 1/4\\) \\(-2 \\le y \\le 2\\).Find probability  \\(Y\\) greater \\(1\\,\\text{m}\\).readings four different days taken probability exactly two greater \\(1\\,\\text{m}\\)?Exercise 7.17  Poisson distributions used queuing theory model formation queues.\nSuppose certain queue modelled Poisson distribution mean \\(0.5\\) arrivals per minute.Use R simulate arrivals 8AM 9AM (one simulation, plot queue length minute).\nProduce \\(100\\) simulations, hence compute mean standard deviation people queue 9AM.Suppose server begins work 8:30AM serving customers (e.g., removing queue) rate \\(0.7\\) per minute.\n, use R simulate arrivals \\(60\\,\\text{mins}\\) (one simulation, plot queue length minute).\nProduce \\(100\\) simulations, hence compute mean standard deviation people queue 9AM.Suppose one server begins work 8:30AM , another servers begins 8:45; together two can serve customers (e.g., removing queue) rate \\(1.3\\) per minute.\n, use R simulate arrivals 60 minutes (one simulation, plot queue length minute).\nProduce \\(100\\) simulations, hence compute mean standard deviation people queue 9AM.Exercise 7.18  Poisson distribution (7.11), determine values \\(\\lambda\\) \\(\\Pr(X) = \\Pr(X + 1)\\).Exercise 7.19  Consider mean variance hypergeometric distribution.\nexercise, write \\(p = m/N\\).Show mean hypergeometric binomial distributions .Show variance hypergeometric binomial distributions connected Finite Population Correction factor (seen later Def. 12.4): \\((N - k)/(N - 1)\\).Exercise 7.20  exercise, find variance discrete uniform distribution, using definitions  \\(Y\\)  \\(X\\) Sect. 7.2.First find \\(\\operatorname{E}[Y^2]\\)find \\(\\operatorname{var}[X]\\).Exercise 7.21  Prove results Theorem 7.4, first finding MGF.Exercise 7.22  Prove results Sect. 7.7.4: \\(\\operatorname{E}[X] = \\lambda + 1\\) \\(\\operatorname{var}[X] = \\lambda\\) zero-truncated Poisson distribution.Exercise 7.23  Prove result Theorem 7.7: \\(\\Gamma(1) = \\Gamma(2) = 1\\).","code":""},{"path":"ContinuousDistributions.html","id":"ContinuousDistributions","chapter":"8 Standard continuous distributions","heading":"8 Standard continuous distributions","text":"completion chapter, able :recognise probability functions underlying parameters uniform, exponential, gamma, beta, normal random variables.know basic properties continuous distributions.apply distributions appropriate problem solving.approximate binomial distribution normal distribution.","code":""},{"path":"ContinuousDistributions.html","id":"introduction-2","chapter":"8 Standard continuous distributions","heading":"8.1 Introduction","text":"chapter, popular continuous distributions discussed.\nProperties definitions applications considered.","code":""},{"path":"ContinuousDistributions.html","id":"ContinuousUniform","chapter":"8 Standard continuous distributions","heading":"8.2 Continuous uniform distribution","text":"\ncontinuous uniform distribution constant PDF given range.Definition 8.1  (Continuous uniform distribution) random variable \\(X\\) range space \\([, b]\\) PDF\n\\[\\begin{equation}\n   f_X(x; , b) = \\displaystyle\\frac{1}{b - }\\quad\\text{$\\le x\\le b$},\n   \\tag{8.1}\n\\end{equation}\\]\n \\(X\\) continuous uniform distribution.\nwrite \\(X\\sim U(, b)\\) \\(X\\sim\\text{Unif}(, b)\\).distribution also called rectangular distribution.\nnotation used denote discrete continuous uniform distribution; context make clear meant.\nplot PDF continuous uniform distribution shown Fig. 8.1.\nFIGURE 8.1: PDF continuous uniform distribution \\(U(,b)\\).\nDefinition 8.2  (Continuous uniform distribution: distribution function) random variable \\(X\\) continuous uniform distribution given Eq. (8.1), distribution function \n\\[\n  F_X(x; , b) =\n  \\begin{cases}\n    0                                  & \\text{$x < $};\\\\\n    \\displaystyle \\frac{x - }{b - }  & \\text{$\\le x \\le b$};\\\\\n    1                                  & \\text{$x > b$}.\n  \\end{cases}\n\\]following basic properties continuous uniform distribution.Theorem 8.1  (Continuous uniform distribution properties) \\(X\\sim\\text{Unif}(,b)\\) \\(\\operatorname{E}[X]  = (+ b)/2\\).\\(\\operatorname{var}[X] = (b - )^2/12\\).\\(M_X(t) = \\{ \\exp(bt) - \\exp() \\} / \\{t(b - )\\}\\).Proof. proofs left exercises.four R functions working continuous uniform distribution form [dpqr]unif(min, max) (see App. D).Example 8.1  (Continuous uniform)  \\(X\\) uniformly distributed \\([-2, 2]\\), \\(\\Pr(|X| > \\frac{1}{2})\\) can found.\nNote \n\\[\n  f_X(x; = -2, b = 2) = \\frac{1}{4} \\quad\\text{$-2 < x < 2$.}\n\\]\n:\n\\[\\begin{align*}\n     \\Pr\\left(|X| > \\frac{1}{2}\\right)\n     &= \\Pr\\left(X > (1/2)\\right) + \\Pr\\left(X < -(1/2)\\right)\\\\\n     &= \\int^2_{1/2} f(x)\\,dx  +  \\int^{-1/2}_{-2} f(x)\\,dx\\\\\n     &= 3/4.\n\\end{align*}\\]\nprobability also computed finding area appropriate rectangle.\nAlternatively, R:","code":"\npunif(-0.5, -2, 2) + \n  1 - punif(0.5, -2, 2)\n#> [1] 0.75"},{"path":"ContinuousDistributions.html","id":"Normal","chapter":"8 Standard continuous distributions","heading":"8.3 Normal distribution","text":"","code":""},{"path":"ContinuousDistributions.html","id":"NormalUsual","chapter":"8 Standard continuous distributions","heading":"8.3.1 Definition and properties","text":"\nwell-known continuous distribution probably normal distribution (Gaussian distribution), sometimes called bell-shaped distribution.\nnormal distribution many applications (especially sampling; see Sect. 12), many natural quantities (heights weights humans) follow normal distributions.Definition 8.3  (Normal distribution) random variable \\(X\\) PDF\n\\[\\begin{equation}\n   f_X(x; \\mu, \\sigma^2) =\n   \\displaystyle \\frac{1}{\\sigma \\sqrt{2\\pi}}\n                 \\exp\\left\\{ -\\frac{1}{2}\\left( \\frac{x-\\mu}{\\sigma}\\right)^2 \\right\\}\n   \\tag{8.2}\n\\end{equation}\\]\n\\(-\\infty<x<\\infty\\),  \\(X\\) normal distribution.\ntwo parameters mean \\(\\mu\\) \\(-\\infty < \\mu < \\infty\\); variance \\(\\sigma^2\\) \\(\\sigma^2 > 0\\).\nwrite \\(X\\sim N(\\mu, \\sigma^2)\\).authors—especially non-theoretical work—use notation \\(X\\sim N(\\mu,\\sigma)\\) wise check article book notation used.\nexamples normal distribution PDFs shown Fig. 8.2.\nFIGURE 8.2: examples normal distributions. solid lines correspond \\(\\sigma = 0.5\\) dashed lines \\(\\sigma = 1\\). left panel, \\(\\mu = -3\\); right panel, \\(\\mu = 2\\).\ndrawing graph normal PDF, note \\(f_X(x)\\) symmetrical \\(\\mu\\): \\(f_X(\\mu - x) = f_X(\\mu + x)\\).\\(f_X(x) \\0\\) asymptotically \\(x\\\\pm \\infty\\).\\(f_X'(x) = 0\\) \\(x = \\mu\\), maximum occurs .\\(f_X''(x) = 0\\) \\(x = \\mu \\pm \\sigma\\) (points inflection).proof \\(\\displaystyle \\int^\\infty_{-\\infty} f_X(x)\\,dx = 1\\) obvious given.\nproof relies first squaring integral changing polar coordinates.Definition 8.4  (Normal distribution: distribution function) random variable \\(X\\) normal distribution given Eq. (8.2), distribution function \n\\[\\begin{align}\n  F_X(x; \\mu, \\sigma^2)\n  &= \\frac{1}{\\sigma \\sqrt{2\\pi}}\n    \\int_{-\\infty}^x\n    \\exp\\left\\{\\frac{(t - \\mu)^2}{2\\sigma^2}\\right\\}\\, dt\\\\\n  &= \\frac{1}{2}\n    \\left\\{1 + \\operatorname{erf} \\left( \\frac{x - \\mu}{\\sigma {\\sqrt{2}} }\\right)\\right\\}\n\\end{align}\\]\n\\(\\operatorname{erf}(\\cdot)\\) error function\n\\[\\begin{equation}\n  \\operatorname{erf}(x) =\n  \\frac{2}{\\sqrt{\\pi}} \\int _{0}^{x} \\exp\\left( -t^{2}\\right)\\,dt,\n  \\tag{8.3}\n\\end{equation}\\]\n\\(x \\\\mathbb{R}\\).\nfunction \\(\\operatorname{erf}(\\cdot)\\) appears numerous places, commonly tabulated, available many computer packages.following basic properties normal distribution.Theorem 8.2  (Normal distribution properties) \\(X\\sim N(\\mu,\\sigma^2)\\) \\(\\operatorname{E}[X] = \\mu\\).\\(\\operatorname{var}[X] = \\sigma^2\\).\\(M_X(t) = \\displaystyle \\exp\\left(\\mu t + \\frac{t^2\\sigma^2}{2}\\right)\\).Proof. proof delayed Theorem 8.3.four R functions working normal distribution form [dpqr]norm(mean, sd) (see App. D).\nNote normal distribution R specified giving standard deviation variance.error function \\(\\operatorname{erf}(\\cdot)\\) available directly R, can evaluated using\n\\[\n  \\operatorname{erf}(x) = 2\\times \\texttt{pnorm}(x\\sqrt{2}) - 1.\n\\]","code":""},{"path":"ContinuousDistributions.html","id":"StandardNormal","chapter":"8 Standard continuous distributions","heading":"8.3.2 The standard normal distribution","text":"\nspecial case normal distribution standard normal distribution, normal distribution mean zero variance one.Definition 8.5  (Standardard normal distribution) PDF random variable \\(Z\\) standard normal distribution, sometimes denoted \\(\\phi(x)\\), \n\\[\\begin{equation}\n   f_Z(z) = \\displaystyle \\frac{1}{\\sqrt{2\\pi}}\n                                \\exp\\left\\{ -\\frac{z^2}{2}\\right\\}\n   \\tag{8.4}\n\\end{equation}\\]\n\\(-\\infty < z < \\infty\\).\nwrite \\(Z\\sim N(0, 1)\\).Since \\(f_Z(z)\\) PDF, \n\\[\\begin{equation}\n   \\frac 1{\\sqrt{2\\pi}}\\int^\\infty_{-\\infty} \\exp\\left\\{-\\frac{1}{2} z^2\\right\\}\\,dz = 1,\n   \\tag{8.5}\n\\end{equation}\\]\nresult proves useful many contexts (proof second statement Theorem 7.7, proof ).Definition 8.6  (Standard normal distribution: distribution function) random variable \\(X\\) standard normal distribution given Eq. (8.4), distribution function \n\\[\\begin{align}\n  F_X(x)\n  &= \\frac{1}{2}\n     \\left\\{1 + \\operatorname{erf} \\left( \\frac {x}{\\sqrt{2}} \\right)\\right\\}\\nonumber\\\\\n  &= \\int_{-\\infty}^z \\frac{1}{\\sqrt{2\\pi}} \\exp\\left\\{ -\\frac{t^2}{2}\\right\\}\\, dt \\\\\n  &=  \\Phi(z)\n  \\tag{8.6}\n\\end{align}\\]\n\\(\\operatorname{erf}(\\cdot)\\) error function (8.3).Definition 8.7  (Notation) density distribution functions standard normal distribution used often notation.\ndensity function denoted \\(\\phi(\\cdot)\\), distribution function denoted \\(\\Phi(\\cdot)\\).\n, density function normal distribution mean \\(\\mu\\) standard deviation \\(\\sigma\\) \\(\\phi\\big( (x - \\mu)/\\sigma\\big)\\), corresponding distribution function \\(\\Phi\\big( (x - \\mu)/\\sigma\\big)\\).Note: \\(\\displaystyle\\frac{d}{dx} \\Phi(x) = \\phi(x)\\).following basic properties standard normal distribution.Theorem 8.3  (Standard normal distribution properties) \\(Z\\sim N(0, 1)\\) \\(\\operatorname{E}[Z] = 0\\).\\(\\operatorname{var}[Z] = 1\\).\\(M_Z(t) = \\displaystyle \\exp\\left(\\frac{t^2}{2}\\right)\\).Proof. Part 3 proven first, used prove Parts 1  2.\nHowever, proving Parts 1  2 directly constructive.\nexpected value:\n\\[\\begin{align*}\n    \\operatorname{E}[Z]\n    &= \\frac {1}{\\sqrt{2\\pi}}\n       \\int^\\infty_{-\\infty} z \\exp\\left\\{-\\frac{1}{2}z^2 \\right\\}\\,dz\\\\\n    &= \\int^\\infty_{-\\infty} -\\frac{d}{dz} \\left(\\exp\\left\\{-\\frac{1}{2}z^2\\right\\} \\right)\\, dz\\\\\n    &= \\left[ -\\exp\\left\\{-\\frac{1}{2}z^2\\right\\}\\right]^\\infty_{-\\infty} = 0,\n\\end{align*}\\]\nsince integrand symmetric  \\(0\\).variance, first see \\(\\operatorname{var}[Z] = \\operatorname{E}[Z^2] - \\operatorname{E}[Z]^2 = \\operatorname{E}[Z^2]\\) since \\(\\operatorname{E}[Z] = 0\\).\n:\n\\[\n   \\operatorname{var}[X] = \\operatorname{E}[Z^2]\n   = \\frac {1}{\\sqrt{2\\pi}}\\int^\\infty_{-\\infty}z^2e^{-\\frac{1}{2}z^2}\\,dz.\n\\]\nintegrate parts (.e., \\(\\int u\\,dv = uv - \\int v\\, du\\)), set \\(u = z\\) (\\(du = 1\\)) set\n\\[\n   dv = ze^{-\\frac{1}{2}z^2}\n   \\quad\\text{}\\quad\n   v = -e^{-\\frac{1}{2}z^2}.\n\\]\nHence,\n\\[\\begin{align*}\n   \\operatorname{var}[X]\n   &= \\frac {1}{\\sqrt{2\\pi}} \\left\\{ -z \\exp\\left\\{-\\frac{1}{2}z^2\\right\\} - \\int^\\infty_{-\\infty}-\\exp\\left\\{-\\frac{1}{2}z^2\\right\\}\\, dz \\right\\}\\\\\n   &= \\frac {1}{\\sqrt{2\\pi}}\\left(\\left. -z\\,\\exp\\left\\{-\\frac{1}{2} z^2\\right\\}\\right|^\\infty_{-\\infty}\\right) + \\frac{1}{\\sqrt{2\\pi}}\\int^\\infty_{-\\infty} \\exp\\left\\{-\\frac{1}{2}z^2\\right\\}\\,dz = 1\n\\end{align*}\\]\nsince first term zero, second term uses Eq. (8.5).MGF:\n\\[\n   M_Z(t) = \\operatorname{E}[\\exp\\left\\{tZ\\right\\}]\n   = \\int^\\infty_{-\\infty} \\exp\\left\\{tz\\right\\}\n     \\frac{1}{\\sqrt{2\\pi}} \\exp\\left\\{-\\frac{1}{2}z^2\\right\\}\\,dz.\n\\]\nCollecting together terms exponent completing square,\n\\[\\begin{equation*}\n     -\\frac{1}{2}[z^2 -2tz] = -\\frac{1}{2}(z - t)^2+\\frac{1}{2} t^2.\n\\end{equation*}\\]\nTaking constants outside integral:\n\\[\n   M_Z(t)\n   = \\exp\\left\\{\\frac{1}{2}t^2\\right\\}\\int^\\infty_{-\\infty}\n     \\frac{1}{\\sqrt{2\\pi}} \\exp\\left\\{-\\frac{1}{2}[z - t]^2\\right\\}\\,dz.\n\\]\nintegral  \\(1\\), since area \\(N(t, 1)\\) PDF.\nHence\n\\[\n   M_Z(t) = \\exp\\left\\{\\frac{1}{2} t^2\\right\\}.\n\\]distribution important practice since normal distribution can rescaled standard normal distribution using\n\\[\\begin{equation}\n   Z = \\frac{X - \\mu}{\\sigma}.\n   \\tag{8.7}\n\\end{equation}\\]\nSince \\(Z = (X - \\mu)/\\sigma\\), \\(X = \\mu + \\sigma Z\\), \n\\[\n   \\operatorname{E}[X] = \\operatorname{E}[\\mu + \\sigma Z] = \\mu + \\sigma \\operatorname{E}[Z] = \\mu\n\\]\n\\(\\operatorname{E}[Z] = 0\\).\nAlso\n\\[\n   \\operatorname{var}[X] = \\operatorname{var}[\\mu  +\\sigma Z] = \\sigma^2\\operatorname{var}[Z] = \\sigma^2\n\\]\n\\(\\operatorname{var}[Z] = 1\\).\nFinally\n\\[\n   M_X(t) = \\operatorname{E}[e^{tX}] = \\operatorname{E}\\big[\\exp\\{t(\\mu + \\sigma Z)\\}\\big] = \\exp(\\mu t)\\operatorname{E}\\big[\\exp(t\\sigma Z)\\big].\n\\]\nHowever, \\(\\operatorname{E}\\big[\\exp(t\\sigma Z)\\big] = M_Z(t\\sigma) = \\exp\\left\\{\\frac{1}{2}(\\sigma t)^2\\right\\}\\) \n\\[\n   M_Z(t) = \\displaystyle \\exp\\left(\\frac{t^2}{2}\\right).\n\\]four R functions working normal distribution form [dpqr]norm(mean, sd) (see App. D).\ndefault, mean = 0 sd = 1, corresponding standard normal distribution.\nNote normal distribution specified giving standard deviation variance.","code":""},{"path":"ContinuousDistributions.html","id":"FindNormalProbs","chapter":"8 Standard continuous distributions","heading":"8.3.3 Determining normal probabilities","text":"\nprobability \\(\\Pr(< X \\le b)\\) \\(X\\sim N(\\mu, \\sigma^2)\\) can written\n\\[\n   \\Pr(< X \\le b) = F_X(b) - F_X().\n\\]\n \\(F_X(x)\\) distribution function Def. 8.6.\nintegral distribution function written terms standard functions general must evaluated particular \\(x\\) numerically (e.g., using Simpson’s rule similar).\nHowever, statistical packages built-procedures evaluate \\(F_X(x)\\)  \\(z\\) (pnorm() R).Also, since normal distribution can transformed standard normal distribution, can write\n\\[\n   z_1 = (- \\mu)/\\sigma\n   \\quad\\text{}\\quad\n   z_2 = (b - \\mu)/\\sigma\n\\]\nhence probability \n\\[\n   \\Pr( z_1 < Z \\le z_2) = \\Phi(z_2) - \\Phi(z_1),\n\\]\n\\(\\Phi(z)\\) distribution function standard normal distribution, Eq. (8.3).process converting value \\(x\\) \\(z\\) using \\(z = (x - \\mu)/\\sigma\\) called standardising.\nTables \\(\\Phi(z)\\) (sometimes \\(1 - \\Phi(z)\\)) commonly available.\ntables can used compute probabilities associated normal distributions (see examples ).\ncourse, R can used .addition, tables often used reverse sense, probability event given value random variable sought.\ncase, tables used ‘backwards’; appropriate area found body table corresponding \\(z\\)-value found table margins.\nR, function qnorm() used.\n\\(z\\)-value converted value original random variable using\n\\[\\begin{equation}\n   x = \\mu + z\\sigma.\n   \\tag{8.8}\n\\end{equation}\\]\nprocess sometimes referred unstandardising.following examples illustrate use R.\nDrawing rough graphs showing relevant areas encouraged.Example 8.2  (Walking speeds) study stadium evacuations used simulation compare scenarios (H. Xie, Weerasekara, Issa 2017).\nwalking speed people modelled using \\(N(1.15, 0.2^2)\\) distribution; , \\(\\mu = 1.15\\) m/s standard deviation \\(0.2\\) m/s.\nsituation can shown Fig. 8.3, top left panel.Many questions can asked walking speeds, R used compute answers.\nUsing model:probability person walks faster  \\(1.5\\) m/s?probability person walks slower  \\(1.0\\) m/s?probability person walks \\(1.0\\) m/s  \\(1.5\\) m/s?speed slowest \\(15\\)% people walk?speed quickest \\(5\\)% people walk?Sketches situations shown Fig. 8.3.\nUsing R:answers , respectively:4%;22.7%;73.3%;0.894 m/s;1.479m/s.\nFIGURE 8.3: normal distribution used modelling walking speeds. vertical dotted lines mean \\(\\pm1\\), \\(\\pm2\\), \\(\\pm3\\) \\(\\pm4\\) standard deviations mean.\nExample 8.3  (Using normal distributions) Scores examination normal distribution, mean score  \\(50\\) standard deviation  \\(10\\).\nSuppose top \\(75\\)% candidates taking examination passed; call minimum passing score \\(x^*\\).Since \\(X\\sim N(50, 10^2)\\):\n\\[\\begin{align*}\n   \\Pr(X > x^*)\n   &= 0.75\\\\\n   \\Pr\\left(Z > \\frac{x^* - 50}{10}\\right)\n   &= 0.75\\\\\n   \\Pr\\left(Z < \\frac{50 - x^*}{10}\\right)\n   & = 0.75.\n\\end{align*}\\]\ntables, \\(0.75 = \\Phi(0.675)\\) \\((50 - x^*)/10 = 0.675\\) \\(x^* = 43.25\\).\nUsing R:","code":"\n# Part 1\n1 - pnorm(1.5, mean = 1.15, sd = 0.2)\n#> [1] 0.04005916\n\n# Part 2\npnorm(1, mean = 1.15, sd = 0.2)\n#> [1] 0.2266274\n\n# Part 3\npnorm(1.5, mean = 1.15, sd = 0.2) - \n   pnorm(1, mean = 1.15, sd = 0.2)\n#> [1] 0.7333135\n\n# Part 4\nqnorm(0.15, mean= 1.15, sd = 0.2)\n#> [1] 0.9427133\n\n# Part 5\nqnorm(0.95, mean = 1.15, sd = 0.2)\n#> [1] 1.478971\nqnorm(0.25, # 'Top 75%' is the same as the 'bottom 25%' \n      mean = 50,\n      sd = 10)\n#> [1] 43.2551"},{"path":"ContinuousDistributions.html","id":"NormalApproxBinomial","chapter":"8 Standard continuous distributions","heading":"8.3.4 Normal approximation to the binomial","text":"Sect. 7.4, binomial distribution considered.\nSometimes using binomial distribution tedious; consider binomial random variable \\(X\\) \\(n = 1000\\), \\(p = 0.45\\) \\(\\Pr(X > 650)\\) sought: calculate \\(\\Pr(X = 651) + \\Pr(X = 652) + \\cdots + \\Pr(X = 1000)\\).However, sometimes normal distribution can used approximate binomial probabilities.\ncertain parameter values, binomial pf starts take normal distribution shape (Fig. 8.4).binomial probability function close enough use normal approximation?\ndefinitive answer; common guideline suggests \\(np \\ge 5\\) \\(n(1 - p) \\ge 5\\) approximation satisfactory.\n(guidelines, texts may suggest different guidelines.)Figure 8.4 shows picture various binomial probability functions overlaid corresponding normal distribution; approximation visibly better guidelines given satisfied.\nFIGURE 8.4: normal distribution approximating binomial distribution. guidelines suggest approximation good \\(np \\ge 5\\) \\(n (1 - p) \\ge 5\\); evident pictures. top row, significant amount approximating normal distribution even appears \\(Y < 0\\).\nnormal distribution can used approximate probabilities situations actually binomial.\nfundamental difficulty approach discrete distribution modelled continuous distribution.\nbest explained example.\nexample explains principle; idea extends situations normal distribution used approximate binomial distribution.random variable \\(X\\) binomial distribution \\(X \\sim \\text{Bin}(n, p)\\), probability function can approximated normal distribution \\(Y \\sim N(\\mu, \\sigma^2)\\), \\(\\mu = np\\) \\(\\sigma^2 = np(1 - p)\\).approximation good \\(np \\ge 5\\) \\(n(1 - p) \\ge 5\\).Example 8.4  (Normal approximation binomial) Consider mdx mice (strain muscular dystrophy) particular source  \\(30\\)% mice survive least \\(40\\) weeks.\nOne particular experiment requires least \\(35\\) \\(100\\) mice live beyond \\(40\\) weeks.\nprobability \\(35\\) group survive beyond \\(40\\) weeks?First see situation binomial;  \\(X\\) number mice group  \\(100\\) survive, \\(X \\sim \\text{Bin}(100, 0.3)\\).\napproximated normal distribution \\(Y\\sim N(30, 21)\\), variance \\(np(1 - p) = 100\\times 0.3\\times 0.7 = 21\\).\n\\(np = 30\\) \\(n(1 - p) = 70\\) much larger  \\(5\\), approximation expected adequate.Figure 8.5 shows upper tail distribution near \\(X = 35\\):\nusing normal approximation \\(Y = 35\\), half original bar binomial pf included.\nHowever, since number mice discrete, want entire bar corresponding \\(X = 35\\).\ncompute correct answer, normal distribution must evaluated \\(\\Pr(Y > 34.5)\\).\nchange \\(Y \\ge 34.5\\) \\(Y > 34.5\\) called using continuity correction.exact answer (using binomial distribution)  \\(0.1629\\) (rounded four decimal places).\nUsing normal distribution continuity correction gives answer  \\(0.1631\\); using normal distribution without continuity correction, answer  \\(0.1376\\).\nsolution accurate, expected, using continuity correction.\nFIGURE 8.5: normal distribution approximating binomial distribution \\(n = 100\\), \\(p = 0.3\\) finding probability \\(X > 35\\).\nExample 8.5  (Continuity correction) Consider rolling standard die \\(100\\) times, counting number times  appear uppermost.\nrandom variable \\(X\\) number times  appears; , \\(X\\sim \\text{Bin}(n = 100, p = 1/6)\\).Since \\(np = 16.667\\) \\(n(1 - p) = 83.333\\) greater  \\(5\\), normal approximation accurate, define \\(Y\\sim N(16.6667, 13.889)\\) (variance \\(np(1 - p) = 13.889\\)).\nVarious probabilities (Table 8.1) show accuracy approximation, way continuity correction used.understand concept continuity correction applied situation, able compute probabilities normal approximation.\nTABLE 8.1: events probabilities die-rolling example, computed using binomial distribution (exact) normal approximation using continuity correction. accuracy approximation generally good.\n","code":"\n# Exact\nExactP <- sum( dbinom(x = 35:100,\n                      size = 100,\n                      prob = 0.30))\nExactP2 <- 1 - pbinom(34, \n                      size = 100,\n                      prob = 0.30)\n# Normal approx\nNormalP <- 1 - pnorm(35, \n                     mean = 30,\n                     sd = sqrt(21))\n# Normal approx with continuity correction\nContCorrP <- 1 - pnorm(34.5, \n                       mean = 30,\n                       sd = sqrt(21))\ncat(\"Exact:\" = round(ExactP, 6), \"\\n\") # \\n means \"new line\" \n#> 0.162858\ncat(\"Exact (alt):\" = round(ExactP2, 6), \"\\n\")\n#> 0.162858\ncat(\"Normal approx:\" = round(NormalP, 6), \"\\n\")\n#> 0.137617\ncat(\"With correction:\" = round(ContCorrP, 6), \"\\n\")\n#> 0.163055"},{"path":"ContinuousDistributions.html","id":"ExponentialDistribution","chapter":"8 Standard continuous distributions","heading":"8.4 Exponential distribution","text":"\nnormal distribution defined real values, many quantities defined positive real numbers .\nalways problem, especially observations far zero, heights adult humans.\nHowever, many random variables observations close zero, data often skewed right (positively skewed).Many distributions can used modelling right-skewed data defined positive real numbers; simplest exponential distribution.","code":""},{"path":"ContinuousDistributions.html","id":"ExponentialDerivation","chapter":"8 Standard continuous distributions","heading":"8.4.1 Derivation","text":"exponential distribution closely related Poisson distribution.\nshow , consider Poisson process, events occur average rate \\(\\lambda\\) events per unit time (e.g., \\(\\lambda = 0.5\\) per minute).consider fixed length time \\(t > 0\\); , number occurrences \\(Y\\) average rate  \\(\\lambda t\\) (e.g., 12 minutes, ’d expect average rate \\(0.5\\times 12 = 6\\) occurrences).\n, number events occurring interval \\((0, t)\\) Poisson distribution mean \\(\\lambda t\\), probability function\n\\[\\begin{equation}\n  p_Y(y) = \\frac{\\exp(-\\lambda t) (\\lambda t)^y}{y!}\n  \\quad\n  \\text{$y = 0, 1, 2, \\dots$}.\n   \\tag{8.9}\n\\end{equation}\\]\nHence, probability zero events occur interval \\((0, t)\\) \n\\[\n  \\Pr(Y = 0)\n  = \\frac{\\exp(-\\lambda t) (\\lambda t)^0}{0!}\n  = \\exp(-\\lambda t)\n\\]\nusing \\(Y = 0\\) (8.9).Now, introduce new random variable \\(T\\) represent time first event occurs.\n, \\(Y = 0\\) (.e., events occur interval \\((0, t)\\)) corresponds \\(T > t\\) (.e., time till first event occurs must great time interval \\(t\\)).\n,\n\\[\n  \\Pr(Y = 0) = \\exp(-\\lambda t) = \\Pr(T  > t).\n\\]\nRewriting terms distribution function \\(T\\),\n\\[\n  F_T(t) = 1 - \\Pr(T > t) = 1 - \\exp(-\\lambda t)\n\\]\n\\(t > 0\\).\nHence, probability density function \n\\[\n  f_T(t) = \\lambda \\exp(-\\lambda t)\n\\]\n\\(t > 0\\).\nprobability density function exponential distribution.shows events occur random according Poisson distribution, time (space) events follows exponential distribution (Theorem 8.4).\nHence exponential distribution used describe interval consecutive randomly occurring events follow Poisson distribution.Theorem 8.4  (Poisson process exponential distributions) Consider Poisson process rate \\(\\lambda\\) suppose observation starts arbitrary time point.\ntime \\(T\\) first event exponential distribution mean \\(\\operatorname{E}[T] = 1/\\lambda\\); .e.,\n\\[\n   f_T(t) = \\lambda e^{-\\lambda t},\\quad t > 0.\n\\]Although theorem refers ‘time’, variable interest may distance continuous variable measuring interval events.events occur according Poisson distribution, “time” Poisson events can modelled using exponential distribution.","code":""},{"path":"ContinuousDistributions.html","id":"ExponentialDefinition","chapter":"8 Standard continuous distributions","heading":"8.4.2 Definition and properties","text":"exponential distribution usually written follows.Definition 8.8  (Exponential distribution) random variable \\(X\\) PDF\n\\[\\begin{equation}\n   f_X(x; \\beta) = \\displaystyle \\frac{1}{\\beta}  \\exp(-x/\\beta)\n   \\quad\n   \\text{$x > 0$}\n   \\tag{8.10}\n\\end{equation}\\]\n \\(X\\) exponential distribution parameter \\(\\beta > 0\\).\nwrite \\(X\\sim\\text{Exp}(\\beta)\\).parameter \\(\\lambda = 1/\\beta\\) often used place  \\(\\beta\\), called rate parameter.\nSometimes notation \\(X\\sim\\text{Exp}(\\lambda)\\), checking notation used context wise.\nPlots PDF various exponential distributions given Fig. 8.6}.\nFIGURE 8.6: Exponential distributions various values rate parameter \\(\\lambda = 1/\\beta\\).\nDefinition 8.9  (Exponential distribution: distribution function) random variable \\(X\\) exponential distribution given Eq. (8.10), distribution function \n\\[\n  F_X(x; \\lambda)\n  = 1 - \\exp\\left\\{-x/\\beta\\right\\}\n  = 1 - \\exp\\left\\{-\\lambda x\\right\\}\n\\]\n\\(x > 0\\).following basic properties exponential distribution.Theorem 8.5  (Exponential distribution properties) \\(X\\sim\\text{Exp}(\\beta)\\) \\(\\operatorname{E}[X] = \\beta = 1/\\lambda\\).\\(\\operatorname{var}[X] = \\beta^2 = 1/\\lambda^2\\).\\(M_X(t) = (1 - \\beta t)^{-1}\\) \\(t < 1/\\beta\\) (, \\(M_X(t) = \\lambda/(\\lambda - t)\\) \\(t < \\lambda\\)).Proof. proofs left exercise.parameter \\(\\beta\\) represents mean exponential distribution.\nalternative parameter \\(\\lambda = 1/\\beta\\) represents mean rate events occur.\nLike Poisson distribution, variance defined mean defined .four R functions working exponential distribution form [dpqr]exp(rate) rate\\({}= \\lambda = 1/\\beta\\) (see App. D).Example 8.6  (Exponential distributions) Allen et al. (1975) use exponential distribution model daily rainfall Kentucky.Example 8.7  (Exponential distributions) Cox Lewis (1966) give data collected Fatt Katz concerning time intervals successive nerve pulses along nerve fibre.\n \\(799\\) observations give .\nmean time pulses \\(\\beta = 0.2186\\) seconds.\nexponential distribution might expected model data well.\nindeed case (Fig. 8.7).Define \\(X\\) time successive nerve pulses (seconds); \\(X\\sim \\text{Exp}(\\beta = 0.2186)\\) (rate parameter \\(\\lambda = 1/0.2186 = 4.575\\) per second).\nfind proportion time intervals longer \\(1\\) second:\n\\[\\begin{align*}\n   \\Pr(X > 1)\n   &=  \\int_1^\\infty \\frac{1}{0.2186}\\exp(-x/0.2186)\\, dx \\\\\n   &= -\\exp(-x/0.2186)\\Big|_1^\\infty \\\\\n   &= (-0) + (\\exp\\{-1/0.2186\\})\\\\\n   &=  0.01031.\n\\end{align*}\\]\n\\(1\\)% chance nerve pulse exceeding one second.\nFIGURE 8.7: time successive nerve pulses. exponential distribution fits well.\nrelationship Poisson exponential distribution explored Sect. 8.4.1 (see, example, Theorem 8.4).\nnext example explores relationshipExample 8.8  (Relationship exponential Poisson distributions) Suppose Poisson process occurs mean rate \\(5\\) events per hour.\nLet \\(N\\) represent number events one day  \\(T\\) time consecutive events.\ncan describe distribution time consecutive events distribution number events one day (\\(24\\,\\text{h}\\)).Since events occur mean rate \\(\\lambda = 5\\) events per hour, mean time consecutive events \\(\\beta = 1/\\lambda = 0.2\\,\\text{h}\\).\nHence, mean number events one day \\(\\mu = 24\\times 5 = 120\\).Consequently, \\(N\\sim\\text{Pois}(\\mu = 120)\\) \\(X\\sim\\text{Exp}(\\beta = 0.2)\\) (, equivalently, \\(X\\sim\\text{Exp}(\\lambda = 5)\\)).important feature Poisson process, hence exponential distribution, memoryless Markov property: future process time point depend history process.\nproperty captured following theorem.Theorem 8.6  (Memoryless property) \\(T \\sim \\text{Exp}(\\lambda)\\) \\(\\lambda\\) rate parameter, \\(s > 0\\) \\(t > 0\\),\n\\[\n    \\Pr(T > s + t \\mid T > s) = \\Pr(T > t)\n\\]Proof. Using Definition 2.13,\n\\[\n   \\Pr(T > s + t \\mid T > s)\n   = \\frac{ \\Pr( \\{T > s + t\\} \\cap \\{T > s\\})} {\\Pr(T > s)}\n\\]\n\\(T > s + t\\), \\(T > s\\).\nConsequently \\(\\Pr( \\{T > s + t \\}\\cap \\{T > s \\} ) = \\Pr(T > s + t)\\) \n\\[\\begin{align*}\n   \\Pr(T > s + t \\mid T > s )\n   &= \\frac{\\Pr(T > s +t )}{\\Pr(T > s)}\\\\\n   &= \\frac{\\exp\\left\\{-\\lambda(s + t)\\right\\}}{\\exp\\left\\{-\\lambda s\\right\\}}\\\\\n   &= \\exp\\left\\{-\\lambda t\\right\\}\\\\\n   &= \\Pr(T > t).\n\\end{align*}\\]theorem states probability time next event greater  \\(t\\) depend time \\(s\\) back previous event.\ncalled memoryless property exponential distribution.Example 8.9  (Memoryless property exponential distribution) Suppose lifespan component \\(\\) modelled exponential distribution mean \\(12\\) months.\n, \\(T \\sim \\text{Exp}(\\beta = 6)\\).probability component \\(\\) fails less \\(6\\) months \n\\[\n   \\Pr(T < 6) = 1 - \\exp(-6/12) = 0.3935.\n\\]Now, suppose component \\(\\) place \\(12\\) months.\nprobability fail less \\(6\\) months \n\\[\n   \\Pr(T < 18 \\mid T > 12) = \\Pr(T < 6) = 1 - \\exp(-6/12) = 0.3935\n\\]\nmemoryless property.Example 8.9 shows exponential process ‘ageless’: risk ‘mortality’ remains constant age.\n, probability event occurring next small interval, whether failure component occurrence accident, remains constant regardless age component length time since last accident.\nsense, exponential lifetime different human lifetime, lifetime many man-made objects, risk ‘death’ next small interval increases age.\n","code":""},{"path":"ContinuousDistributions.html","id":"GammaDistribution","chapter":"8 Standard continuous distributions","heading":"8.5 Gamma distribution","text":"\nmean exponential distribution defined, variance defined.\nflexibility sometimes needed, provided gamma distribution.Definition 8.10  (Gamma distribution) random variable \\(X\\) PDF\n\\[\n   f_X(x; \\alpha, \\beta)\n   = \\frac{1}{\\beta^\\alpha \\Gamma(\\alpha)} x^{\\alpha - 1} \\exp(-x/\\beta)\n\\]\n \\(X\\) gamma distribution, \\(\\Gamma(\\cdot)\\) gamma function (see Sect. 7.11) \\(\\alpha, \\beta > 0\\).\nwrite \\(X \\sim \\text{Gam}(\\alpha, \\beta)\\).parameter \\(\\alpha\\) called shape parameter  \\(\\beta\\) called scale parameter.\ntexts use different notation shape scale parameters.\nbroad terms, shape parameter dictates general shape distribution; scale parameter dictates ‘stretched ’ distribution .R, gamma function \\(\\Gamma(x)\\) evaluated using gamma(x).four R functions working gamma distribution form [dpqr]gamma(shape, rate, scale = 1/rate), shape\\({}= \\alpha\\) scale\\({}= \\beta\\) (see App. D).Plots gamma PDF various values parameters given Fig. 8.8.\nbottom right panel, gamma distributions starting look bit like normal distributions.exponential distribution special case gamma distribution \\(\\alpha = 1\\).\nmeans properties exponential distribution can obtained substituting \\(\\alpha = 1\\) formulae gamma distribution.\nFIGURE 8.8: PDF gamma distribution various values \\(\\alpha\\) \\(\\beta\\).\nNotice \n\\[\\begin{align}\n   \\int_0^\\infty f_X(x)\\,dx\n   &= \\int_0^\\infty\\frac{e^{-x/\\beta}x^{\\alpha - 1}}{\\beta^\\alpha\\Gamma(\\alpha)}\\,dx\\nonumber\\\\\n   &= \\frac{1}{\\Gamma(\\alpha)} \\int_0^\\infty e^{-y} y^{\\alpha-1}\\,dy  \\quad \\text{(putting $y = x/\\beta$)}\\nonumber \\\\\n   &= 1,\n   \\tag{8.11}\n\\end{align}\\]\n\\(\\int_0^\\infty \\exp(-y) y^{\\alpha-1}\\,dy = \\Gamma(\\alpha)\\).distribution function gamma distribution complicated, involving incomplete gamma functions, given.\nfollowing basic properties gamma distribution.Theorem 8.7  (Gamma distribution properties) \\(X\\sim\\text{Gam}(\\alpha,\\beta)\\) \\(\\operatorname{E}[X] = \\alpha\\beta\\).\\(\\operatorname{var}[X] = \\alpha\\beta^2\\).\\(M_X(t) = (1-\\beta t)^{-\\alpha}\\) \\(t < 1/\\beta\\).Proof. expected value:\n\\[\n   \\operatorname{E}[X]\n   = \\int_0^{\\infty}x\\, f_X(x)\\,dx\n   = \\beta \\frac{\\Gamma(\\alpha + 1)}{\\Gamma(\\alpha)}\n     \\underbrace{\\int_0^{\\infty}\\frac{\\exp(-x/\\beta) x^{(\\alpha + 1) - 1}}{\\beta^{\\alpha + 1}\\Gamma(\\alpha + 1)} \\, dx}_{= 1}\n   = \\alpha\\beta.\n\\]\nresult follows using Eq. (8.11) Theorem 8.7\n\\[\n   \\operatorname{E}[X^2]\n   = \\int_0^{\\infty}x^2\\,  f_X(x) \\, dx\n   = \\beta^2\\frac{\\Gamma(\\alpha + 2)}{\\Gamma(\\alpha)}\n     \\underbrace{\\int_0^{\\infty}\\frac{\\exp(-x/\\beta) x^{(\\alpha + 2) - 1}}{\\beta^{\\alpha + 2}\\Gamma(\\alpha + 2)}\\,dx}_{= 1}\n   = \\alpha(\\alpha + 1)\\beta^2\n\\]\nresult follows writing \\(\\Gamma(\\alpha + 2) = (\\alpha + 1)\\alpha\\Gamma(\\alpha)\\).\nHence\n\\[\n   \\operatorname{var}[X]\n   = \\operatorname{E}[X^2] - (\\operatorname{E}[X])^2\n   = \\alpha(\\alpha + 1)\\beta^2 - (\\alpha\\beta)^2\n   = \\alpha\\beta^2\n\\]\nAlso:\n\\[\\begin{align*}\n   M_X(t)\n    = \\operatorname{E}[e^{Xt}]\n   &= \\int_0^{\\infty}\\exp(tx) \\frac{\\exp(-x/\\beta) x^{\\alpha - 1}}{\\beta^{\\alpha} \\Gamma(\\alpha)} \\, dx\\\\\n   &= \\int_0^{\\infty} \\frac{\\exp\\{-x(1 - \\beta t)/\\beta\\}x^{\\alpha - 1}}{\\beta^{\\alpha}\\Gamma(\\alpha)} \\, dx\\\\\n   &= \\int_0^{\\infty} \\frac{\\exp(-z) z^{\\alpha - 1}}{\\Gamma(\\alpha)(1 - \\beta t)^{\\alpha - 1}} \\, \\frac{dz} {1 - \\beta t},\n      \\text{putting $z = x(1 - \\beta t)/\\beta$}\\\\\n   &= (1 - \\beta t)^{-\\alpha},\n\\end{align*}\\]\nsince integral remaining  \\(1\\).usual moments can found expanding \\(M_X(t)\\) series.\n,\n\\[\n   M_X(t) = 1 \\ + \\ \\alpha\\beta t \\ + \\ \\frac{\\alpha(\\alpha+1)\\beta^2}{2!}  t^2 \\ + \\ \\cdots\n\\]\n\n\\[\\begin{align*}\n   \\operatorname{E}[X]   &= \\text{coefficient }t =\\alpha\\beta,\\\\\n   \\operatorname{E}[X^2] &= \\text{coefficient }t^2/2!=\\alpha(\\alpha+1)\\beta^2,\n\\end{align*}\\]\nfound earlier.normal distribution, distribution function gamma , general, computed without using numerical integration, tables (although see Example 8.11) software.Example 8.10  (Rainfall) Das (1955) used (truncated) gamma distribution modelling daily precipitation Sydney.\nsimilar approach adopted D. S. Wilks (1990).Larsen Marx (1986) (Case Study 4.6.1) use gamma distribution model daily rainfall Sydney, Australia using parameter estimates \\(\\alpha = 0.105\\) \\(\\beta = 76.9\\) (based Das (1955)).\ncomparison data model (Table 8.2) indicates good agreement data theoretical distribution.\nTABLE 8.2: gamma distribution used model Sydney daily rainfall.\nExample 8.11  (Electrical components) lifetime electrical component hours, say \\(T\\), can well modelled distribution \\(\\text{Gam}(2, 1)\\).\nprobability component last three hours?information, \\(T\\sim \\text{Gam}(\\alpha = 2, \\beta = 1)\\).\nrequired probability therefore\n\\[\\begin{align*}\n   \\Pr(T > 3)\n   &= \\int_3^\\infty \\frac{1}{1^2 \\Gamma(2)}t^{2 - 1} \\exp(-t/1)\\,dt \\\\\n   &= \\int_3^\\infty t \\exp(-t)\\,dt \\\\\n\\end{align*}\\]\nsince \\(\\Gamma(2) = 1! = 1\\).\nexpression can integrated using integration parts:\n\\[\\begin{align*}\n   \\Pr(T > 3)\n   &= \\int_3^\\infty t \\exp(-t)\\,dt \\\\\n   &= \\left\\{ -t \\exp(-t)\\right\\}\\Big|_3^\\infty - \\int_3^\\infty -\\exp(-t)\\, dt \\\\\n   &= [ (0) - \\{-3\\exp(-3)\\}] - \\left\\{ \\exp(-t)\\Big|_3^\\infty\\right\\} \\\\\n   &= 3\\exp(-3) + \\exp(-3)\\\\\n   &= 0.1991\n\\end{align*}\\]\nIntegration parts possible since \\(\\alpha\\) integer.\n \\(\\alpha = 2.5\\), example, integration parts possible.general approach use tables incomplete gamma function evaluate integral, numerical integration, software.\nuse R:Using method, probability 20%.Example 8.12  (Gamma) \\(X\\sim \\text{Gam}(\\alpha, \\beta)\\), find distribution \\(Y = kX\\) constant \\(k\\).One way approach question use moment-generating functions.\nSince \\(X\\) gamma distribution, \\(M_X(t) = (1-\\beta t)^{-\\alpha}\\).\nNow,\n\\[\\begin{align*}\n   M_Y(t)\n   &= \\operatorname{E}[\\exp(tY)] \\qquad\\text{definition MGF}\\\\\n   &= \\operatorname{E}[\\exp(t kX)] \\qquad\\text{since $X = kX$}\\\\\n   &= \\operatorname{E}[\\exp(s X)] \\qquad\\text{letting $s = kt$}\\\\\n   &= M_X(s) \\qquad\\text{definition MGF}\\\\\n   &= M_X(kt) \\\\\n   &= (1 - \\beta kt)^{-\\alpha}.\n\\end{align*}\\]\njust MGF random variable \\(X\\)  \\(k\\beta\\) place  \\(\\beta\\), distribution  \\(Y\\) \\(\\text{Gam}(\\alpha, k\\beta)\\).","code":"\n# Integrate\nintegrate(dgamma, # The gamma distribution prob. fn\n          lower = 3,\n          upper = Inf,  # \"Inf\" means \"infinity\"\n          shape = 2,\n          scale = 1)\n#> 0.1991483 with absolute error < 9.3e-05\n\n# Directly\n1 - pgamma(3,\n           shape = 2,\n           scale = 1)\n#> [1] 0.1991483"},{"path":"ContinuousDistributions.html","id":"BetaDistribution","chapter":"8 Standard continuous distributions","heading":"8.6 Beta distribution","text":"\ncontinuous random variables constrained finite interval.\nbeta distribution useful situations.Definition 8.11  (Beta distribution) random variable \\(X\\) probability density function\n\\[\\begin{equation*}\n   f_X(x; m, n)\n   = \\frac{x ^ {m - 1}(1 - x)^{n - 1}}{B(m, n)} \\quad \\text{$0\\leq x \\leq 1$ $m > 0$, $n > 0$},\n\\end{equation*}\\]\n\\(B(m, n)\\) beta function\n\\[\\begin{align}\n   B(m, n)\n   &= \\int_0^1  x^{m - 1}(1 - x)^{n - 1} \\, dx, \\quad \\text{$m > 0$ $n > 0$}\\notag\\\\\n   &= \\frac{\\Gamma(m)\\, \\Gamma(n)}{\\Gamma(m + n)},\n   \\tag{8.12}\n\\end{align}\\]\nsaid beta distribution parameters \\(m\\)  \\(n\\).\nwrite \\(X \\sim \\text{Beta}(m, n)\\).\\(B(m, n)\\) defined (8.12) known beta function parameters \\(m\\)  \\(n\\).\nSince \\(\\int_0^1 f_X(x)\\,dx = 1\\) \n\\[\n   \\int_0^1  \\frac{x^{m - 1}(1 - x)^{n - 1}}{B(m, n)}\\,dx = 1.\n\\]R, beta function evaluated using beta(, b), \\({} = m\\) b\\({} = n\\).four R functions working beta distribution form [dpqr]beta(shape1, shape2), shape1\\({}= m\\) shape2\\({}= n\\) (see App. D).distribution function beta distribution complicated, involving incomplete beta functions, given.\nproperties beta function follow.Theorem 8.8  (Beta function properties) beta function Eq. (8.12) satisfies following:beta function symmetric \\(m\\) \\(n\\).\n, \\(m\\) \\(n\\) interchanged, function remains unaltered; .e., \\(B(m, n) = B (n, m)\\).\\(B(1, 1) = 1\\).\\(B\\left(\\frac{1}{2}, \\frac{1}{2}\\right) = \\pi\\).Proof. prove first, put \\(z = 1 - x\\) hence \\(dz = -dx\\) Eq. (8.12).\n\n\\[\\begin{align*}\n   B(m, n)\n   &= -\\int_1^0(1 - z)^{m - 1}  z^{n - 1} \\, dz\\\\\n   &= \\int_0^1 z^{n - 1}(1 - z)^{m - 1} \\, dz\\\\\n   &= B(n, m).\n\\end{align*}\\]second, put \\(x = \\sin^2\\theta\\), \\(dx = 2\\sin\\theta\\cos\\theta\\,d\\theta\\), Eq. (8.12).\n\n\\[\n   B(m, n) = 2 \\int_0^{\\pi/2} \\sin^{2m - 1}\\theta \\cos ^{2n - 1}\\theta\\,d\\theta.\n\\]\n, \\(m = n = \\frac{1}{2}\\),\n\\[\n   B\\left(\\frac{1}{2}, \\frac{1}{2}\\right) = 2\\int_0^{\\pi/2} d\\theta = \\pi.\n\\]third, note \\(\\Gamma(1/2) = \\sqrt{\\pi}\\) Theorem 8.7.\n, since \\(\\Gamma(1) = 0! = 1\\), results follows.Typical graphs beta PDF given Fig. 8.9.\n\\(m = n\\), distribution symmetric \\(x = \\frac{1}{2}\\).\\(m = n = 1\\), beta distribution becomes uniform distribution \\((0, 1)\\).\nFIGURE 8.9: Various beta-distribution PDFs.\nbasic properties beta distribution follow.Theorem 8.9  (Beta distribution properties) \\(X \\sim \\text{Beta}(m, n)\\) \\(\\operatorname{E}[X] = m/(m + n)\\).\\(\\displaystyle\\operatorname{var}[X] = \\frac{mn}{(m + n)^2 (m + n + 1)}\\).mode occurs \\(\\displaystyle x = \\frac{m - 1}{m + n - 2}\\) \\(m, n > 1\\).Proof. Assume \\(X \\sim \\text{Beta}(m, n)\\), \n\\[\\begin{align*}\n   \\mu_r'\n   =\\operatorname{E}[X^r]\n   &= \\int_0^1  \\frac{x^r  x^{m - 1}(1 - x)^{n - 1}} {B(m, n)}\\,dx\\\\\n   &= \\frac{B(m + r, n)}{B(m, n)}  \\int_0^1 \\frac{x^{m + r - 1}(1 - x)^{n - 1}}{B(m + r, n)}\\,dx\\\\\n   &= \\frac{\\Gamma(m + r)\\,\\Gamma(n)}{\\Gamma(m + r + n)} \\frac{\\Gamma(m + n)}{\\Gamma(m)\\,\\Gamma(n)}\\\\\n   &= \\frac{\\Gamma(m + r)\\,\\Gamma(m + n)}{\\Gamma(m + r + n)\\,\\Gamma(m)}\n\\end{align*}\\]\nPutting \\(r = 1\\) \\(r = 2\\) expression, using \\(\\operatorname{var} = \\operatorname{E}[X^2] - \\operatorname{E}[X]^2\\), mean variance \\(\\operatorname{E}[X] = m/(m + n)\\) \\(\\operatorname{var}[X] = \\frac{mn}{(m + n)^2(m + n + 1)}\\).mode \\(\\theta\\) distribution \\(0 < \\theta < 1\\) satisfy \\(f'_X(\\theta) = 0\\).\ndefinition see \\(m, n>1\\) \\(0 < x < 1\\),\n\\[\n   f'_X(x) = (m - 1)x^{m - 2} (1 - x)^{n - 1} - (n - 1)x^{m - 1}(1 - x)^{n - 1}/B(m, n) = 0\n\\]\nimplying \\((m - 1)(1 - x) = (n - 1)x\\) satisfied \\(x = (m - 1) / (m + n - 2)\\).MGF beta distribution written terms standard functions.\ndistribution function beta must evaluated numerically general, except  \\(m\\)  \\(n\\) integers, shown example .\\(X\\sim \\text{Beta}(m, n)\\),  \\(X\\) defined \\([0, 1]\\).\nSometimes , beta distribution used model proportions, though proportions total number (binomial distribution used).\nInstead, beta distribution used, example, model percentage cloud cover (instance, Falls (1974)).random variable \\(Y\\) defined different closed interval \\([, b]\\), define \\(Y = X(b - ) + \\).Example 8.13  Damgaard Irvine (2019) use beta-distribution model relative areas covered plants different species.\nbeta distribution suitable model, since relative areas proportions.\naddition, authors state (p. 2747) …plant cover data tend left-skewed (J-shaped), right skewed (L-shaped) U-shapedwhich align plots Fig. 8.9.Example 8.14  (Beta distributions) bulk storage tanks fuel retailed filled Monday.\nretailer observed many weeks proportion available fuel supply sold well modelled beta distribution \\(m = 4\\) \\(n = 2\\). \\(X\\) denotes proportion total supply sold given week, mean proportion fuel sold week \n\\[\n   \\operatorname{E}[X] = m/(m + n) = 4/6 = 2/3.\n\\]find probability least \\(90\\)% supply sell given week, compute:\n\\[\\begin{align*}\n   \\Pr(X > 0.9)\n   &= \\int_{0.9}^1\\frac{\\Gamma(4 + 2)}{\\Gamma(4)\\Gamma(2)}x^3(1 - x)dx\\\\\n   &= 20\\int_{0.9}^1 (x^3 - x^4))\\,dx\\\\\n   &= 20(0.004)\\\\\n   &= 0.08\n\\end{align*}\\]\nunlikely  \\(90\\)% supply sold given week.\nR:","code":"\n1 - pbeta(0.9, \n          shape1 = 4, \n          shape2 = 2)\n#> [1] 0.08146"},{"path":"ContinuousDistributions.html","id":"TransformationsChiSquared","chapter":"8 Standard continuous distributions","heading":"8.7 The chi-squared distribution","text":"USE EXMAPLE??Examples 6.8  6.13 produce chi-square distribution, important model statistical theory (Theorem 12.6).Definition 8.12  (Chi-squared distribution) continuous random variable \\(X\\) probability density function\n\\[\\begin{equation}\n   f_X(x)\n   = \\frac{x^{(\\nu/2) - 1}e^{-x/2}}{2^{\\nu/2}\\Gamma(\\nu/2)}\\quad\\text{$x > 0$}\n\\end{equation}\\]\nsaid chi-square distribution parameter \\(\\nu > 0\\).\nparameter \\(\\nu\\) called degrees freedom.\nwrite \\(X \\sim \\chi^2(\\nu)\\).plots \\(\\chi^2\\)-distributions shown Fig. 8.10.chi-squared distribution special case gamma distribution, \\(\\alpha = \\nu/2\\) \\(\\beta = 2\\) (Exercise 6.19).\nmeans properties chi-squared distribution can obtained directly gamma distribution (Sect. 8.5).\nFIGURE 8.10: \\(\\chi^2\\)-distributions.\nbasic properties chi-square follow directly gamma distribution (Theorem 8.7).Theorem 8.10  (Properties chi-squared distribution) \\(X\\sim\\chi^2(\\nu)\\) \\(\\operatorname{E}(X) = \\nu\\).\\(\\operatorname{var}(X) = 2\\nu\\).\\(M_X(t) = (1 - 2t)^{-\\nu/2}\\).Proof. See Theorem 8.7.importance chi-square distribution hinted Examples 6.8  6.13, essentially prove following theorem.Theorem 8.11  (Chi-square distribution 1 df) \\(Z\\sim N(0, 1)\\) \\(Z^2\\) chi-square distribution one degree freedom.Proof. Exercise; see Example 6.8.useful property chi-square distribution sum independent random variables, chi-square distribution, also chi-square distribution.\nproperty given following theorem, used later.Theorem 8.12  (Chi-squared distribution) \\(Z_1, Z_2,\\dots, Z_n\\) independently identically distributed (iid) \\(N(0, 1)\\), sum squares \\(S = \\sum_i Z_i^2\\) \\(\\chi^2(n)\\) distribution.Proof. Since \\(S\\) linear combination known distributions, using MGF method appropriate.\nSince \\(Z_i \\sim \\chi^2(1)\\), Theorem 8.10\n\\[\n   M_{Z_i}(t)\n   = (1 - 2t)^{-1/2}.\n\\]\nTheorem 5.6 , \\(S = \\sum_{= 1}^n Z_i^2\\) MGF\n\\[\\begin{align*}\n   M_{S}(t)\n   &= \\prod_{= 1}^n (1 - 2t)^{-1/2}\\\\\n   &= \\left[(1 - 2t)^{-1/2}\\right]^n\n    = (1 - 2t)^{-n/2},\n\\end{align*}\\]\nMGF \\(\\chi^2(n)\\).Chi-square probabilities general calculated without computers tables.four R functions working chi-squared distribution form [dpqr]chisq(df), df\\({} = \\nu\\) refers degrees freedom (see Appendix D).Example 8.15  (Chi-squared distributions) variable \\(X\\) chi-square distribution \\(12\\) df.\nDetermine value  \\(X\\) lies 90% distribution.seek value \\(c\\) \\(\\Pr(X < c) = F_X(c) = 0.90\\) \\(X\\sim\\chi^2(12)\\).\nR:,  \\(90\\)% distribution lies 18.549.","code":"\nqchisq(0.9, df = 12)\n#> [1] 18.54935"},{"path":"ContinuousDistributions.html","id":"BVNormalDistribution","chapter":"8 Standard continuous distributions","heading":"8.8 The bivariate normal distribution","text":"\nspecific example continuous multivariate distribution bivariate normal distribution.Definition 8.13  (bivariate normal distribution) pair random variables \\(X\\)  \\(Y\\) joint PDF\n\\[\\begin{equation}\n   f_{X, Y}(x, y; \\mu_x, \\mu_Y, \\sigma^2_X, \\sigma^2_Y, \\rho) =\n   \\frac{1}{2\\pi\\sigma_X\\sigma_Y\\sqrt{1 - \\rho^2}}\\exp(-Q/2)\n   \\tag{8.13}\n\\end{equation}\\]\n\n\\[\n   Q = \\frac{1}{1-\\rho^2}\\left[\n                      \\left(\\frac{x-\\mu_X}{\\sigma_X}\\right)^2 -\n                2\\rho\\left( \\frac{x-\\mu_X}{\\sigma_X}\\right)\\left(\\frac{y-\\mu_Y}{\\sigma_Y}\\right)\n                +     \\left(\\frac{y-\\mu_Y}{\\sigma_Y}\\right)^2 \\right],\n\\]\n \\(X\\)  \\(Y\\) bivariate normal distribution.\nwrite\n\\[\n   (X, Y) \\sim N_2(\\mu_X, \\mu_Y, \\sigma^2_X, \\sigma^2_Y, \\rho ).\n\\]typical graph bivariate normal surface \\(x\\)–\\(y\\) plane shown\n.\nShowing \\(\\int^\\infty_{-\\infty}\\!\\int^\\infty_{-\\infty}f_{X,Y}(x,y)\\,dx\\,dy = 1\\) straightforward involves writing Eq. (8.13) using polar coordinates.important facts bivariate normal distribution contained theorem .Theorem 8.13  (Bivariate normal distribution properties) \\((X, Y)\\) PDF given Eq. (8.13):marginal distributions  \\(X\\)  \\(Y\\) \\(N(\\mu_X, \\sigma^2_X)\\) \\(N(\\mu_Y, \\sigma^2_Y)\\) respectively.parameter \\(\\rho\\) appearing Eq. (8.13) correlation coefficient  \\(X\\)  \\(Y\\).conditional distributions  \\(X\\) given \\(Y = y\\),  \\(Y\\) given \\(X = x\\), respectively\n\\[\\begin{align*}\n  &N\\left( \\mu_X + \\rho \\sigma_X(y - \\mu_Y)/\\sigma_Y, \\sigma^2_X(1 - \\rho^2)\\right); \\\\\n  &N\\left( \\mu_Y + \\rho \\sigma_Y(x - \\mu_X)/\\sigma_X, \\sigma^2_Y(1 - \\rho^2)\\right).\n  \\end{align*}\\]Proof. Recall marginal PDF  \\(X\\) \\(f_X(x) = \\int^\\infty_{-\\infty} f_{X, Y}(x, y)\\,dy\\).\nintegral, put \\(u = (x - \\mu_X)/\\sigma_X, v = (y - \\mu_Y)/\\sigma_Y,\\, dy = \\sigma_Y\\,dv\\) complete square exponent  \\(v\\):\n\\[\\begin{align*}\n     g(x)\n     &= \\frac{1}{2\\pi\\sigma_X\\sqrt{1 - \\rho^2}\\sigma_Y}\\int^\\infty_{-\\infty}\\exp\\left\\{ -\\frac{1}{2(1 - \\rho^2)}\\left[ u^2 - 2\\rho uv + v^2\\right]\\right\\} \\sigma_Y\\,dv\\\\[2mm]\n     &= \\frac{1}{2\\pi \\sigma_X\\sqrt{1 - \\rho^2}}\\int^\\infty_{-\\infty} \\exp\\left\\{ -\\frac{1}{2(1 - \\rho^2)}\\left[ (v - \\rho u)^2 + u^2 - \\rho^2u^2\\right]\\right\\}\\,dv\\\\[2mm]\n     &= \\frac{e^{-u^2/2}}{\\sqrt{2\\pi} \\sigma_X} \\ \\underbrace{\\int^\\infty_{-\\infty} \\frac{1}{\\sqrt{2\\pi (1 - \\rho^2)}} \\exp\\left\\{ -\\frac{1}{2(1 - \\rho^2)}(v - \\rho u)^2\\right\\}\\,dv}_{=1}.\n\\end{align*}\\]\nReplacing \\(u\\) \\((x - \\mu_X )/\\sigma_X\\), see PDF \\(X \\sim N(\\mu_X, \\sigma^2_X)\\).\nSimilarly marginal PDF  \\(Y\\); .e., \\(f_Y(y)\\).show  \\(\\rho\\) Eq. (8.13) correlation coefficient  \\(X\\)  \\(Y\\), recall \n\\[\\begin{align*}\n     \\rho_{X,Y}\n     &= \\operatorname{Cov}(X,Y)/\\sigma_X\\sigma_Y=\\operatorname{E}[(X-\\mu_X)(Y - \\mu_Y)]/\\sigma_X\\sigma_Y   \\\\[2mm]\n     & = \\int^\\infty_{-\\infty}\\!\\int^\\infty_{-\\infty} \\frac{(x - \\mu_X)}{\\sigma_X}\\frac{(y - \\mu_Y)}{\\sigma_Y}f(x,y)\\,dx\\,dy\\\\[2mm]\n     &= \\int^\\infty_{-\\infty}\\!\\int^\\infty_{-\\infty} uv\\frac{1}{2\\pi\\sqrt{1 - \\rho^2} \\sigma_X\\sigma_Y}\\exp\\left\\{ -\\frac {1}{2(1 - \\rho^2)}[u^2 - 2\\rho uv + v^2]\\right\\} \\sigma_X\\sigma_Y\\,du\\,dv.\n\\end{align*}\\]exponent \n\\[\n   -\\frac{[(u - \\rho v)^2 + v^2 - \\rho^2 v^2]}{2(1 - \\rho^2)}\n   = - \\frac{1}{2} \\left\\{\\frac{(u - \\rho v)^2}{(1 - \\rho^2)} + v^2\\right\\}.\n\\]\n:\n\\[\\begin{align*}\n   \\rho_{X, Y}\n   &=\\int^\\infty_{-\\infty}\\frac{v e^{-v^2/2}}{\\sqrt{2\\pi}}\\underbrace{\\int^\\infty_{-\\infty} \\frac{u}{\\sqrt{2\\pi\n  (1 - \\rho^2)}}\\exp\\{ -(u - \\rho v)^2/2(1 - \\rho^2)\\}\\,du}_{\\displaystyle{= \\operatorname{E}[U]\\text{ } u \\sim N(\\rho\n  v, 1 - \\rho^2) = \\rho v}}\\,dv \\\\[2mm]\n   &= \\rho \\int^\\infty_{-\\infty} \\frac{v^2}{\\sqrt{2\\pi}}e^{-v^2/2}\\,dv\\\\[2mm]\n   &= \\rho\\quad \\text{since integral $\\operatorname{E}[V^2]$ $V \\sim N(0,1)$.}\n\\end{align*}\\]finding conditional PDF  \\(X\\) given \\(Y = y\\), use\n\\[\n   f_{X \\mid Y = y}(x) = f_{X, Y}(x, y)/f_Y(y).\n\\]\nratio, constant \n\\[\n  \\frac{\\sqrt{2\\pi} \\sigma_Y}{2\\pi \\sigma_X\\sigma_Y \\sqrt{1 - \\rho^2}}\n  = \\frac{1}{\\sqrt{2\\pi}\\sigma_X\\sqrt{1 - \\rho^2}}.\n\\]\nexponent \n\\[\\begin{align*}\n       & \\frac{\\exp\\left\\{ -\\left[ \\displaystyle{\\frac{(x - \\mu_X)^2}{\\sigma^2_X}} -\n  \\displaystyle{\\frac{2\\rho(x - \\mu_X)(y - \\mu_Y)}{\\sigma_X\\sigma_Y}} +\n  \\displaystyle{\\frac{(y - \\mu_Y)^2}{\\sigma^2_Y}}\\right] / 2(1 - \\rho^2) \\right\\}  }{\\exp\\left[ -(y - \\mu_Y)^2 / 2\\sigma^2_Y\\right]}\\\\[2mm]\n       &= \\exp\\left\\{ - \\frac{1}{2(1 - \\rho^2)} \\left[ \\frac{(x - \\mu_X)^2}{\\sigma^2_X} - \\frac{2\\rho (x - \\mu_X)(y - \\mu_Y)}{\\sigma_X\\sigma_Y} +\n  \\frac{(y - \\mu_Y)^2}{\\sigma^2_Y} (1 - 1 + \\rho^2)\\right] \\right\\}\\\\[2mm]\n       &= \\exp\\left\\{ - \\frac{1}{2\\sigma^2_X(1 - \\rho^2)}\n       \\left[ (x - \\mu_X)^2 - 2\\rho \\frac{\\sigma_X}{\\sigma_Y} (x - \\mu_X)(y - \\mu_Y) +\n       \\frac{\\rho^2 \\sigma^2_X}{\\sigma^2_Y}(y - \\mu_Y)^2\\right]\\right\\}\\\\[2mm]\n       &= \\exp \\left\\{ - \\frac{1}{2(1 - \\rho^2)\\sigma^2_X} \\left[ x - \\mu_X - \\rho\\frac{\\sigma_X}{\\sigma_Y}(y - \\mu_Y)\\right]^2\\right\\}.\n\\end{align*}\\]\nconditional distribution  \\(X\\) given \\(Y = y\\) \n\\[\n   N\\left( \\mu_X + \\rho\\frac{\\sigma_X}{\\sigma_Y}(y - \\mu_Y), \\sigma^2_X(1 - \\rho^2)\\right).\n\\]\nRecall interpretation conditional distribution  \\(X\\) given \\(Y = y\\) (Sect. 4.5) note shape density\n.Comments Theorem 8.13:first third parts, \\(\\operatorname{E}[X] = \\mu_X\\) \\(\\operatorname{E}[X \\mid Y = y] = \\mu_X + \\rho \\sigma_X (y - \\mu_Y)/\\sigma_Y\\) (similarly  \\(Y\\)).\nNotice \\(\\operatorname{E}[X \\mid Y = y]\\) linear function  \\(y\\); .e., \\((X, Y)\\) bivariate normal, regression line  \\(Y\\)  \\(X\\) ( \\(X\\)  \\(Y\\)) linear.important result follows second part.\n \\(X\\)  \\(Y\\) uncorrelated (.e., \\(\\rho = 0\\)) \\(f_{X, Y}(x, y) = f_X(x) f_Y(y)\\) thus \\(X\\)  \\(Y\\) independent.\n, two normally distributed random variables uncorrelated, also independent.Example 8.16  (Bivariate normal: Heights) (Badiou, Marsh, Gauchet 1988) gives data \\(200\\) married men wives OPCS study heights weights adults Great Britain 1980.\nHistograms husbands’ wives’ heights given Fig. 8.11 (left centre panels); marginal distributions approximately normal.\nscatterplot heights shown Fig. 8.11 (right panel).histograms, reason suspect bivariate normal distribution appropriate.\nUsing \\(H\\) refer heights (mm) husbands  \\(W\\) heights (mm) wives, sample statistics shown (estimate correlation  \\(+0.364\\)):Note  \\(\\rho\\) positive; implies taller men marry taller women average.\nUsing sample information, bivariate normal distribution can estimated.\n\\(3\\)-dimensional density function can difficult plot two-dimensional page, see\n.PDF bivariate normal distribution heights husbands wives written form (8.13) values  \\(\\mu_H\\), \\(\\mu_W\\), \\(\\sigma^2_H\\), \\(\\sigma^2_W\\)  \\(\\rho\\) , tedious.Given information, probability randomly chosen man UK 1980 \\(173\\,\\text{cm}\\) tall married woman taller ?information implies \\(H = 1730\\) given (remembering data given millimetres).\nneed conditional distribution \\(W \\mid H = 1730\\).\nUsing results , conditional distribution mean\n\\[\\begin{align*}\n   b\n   &= \\mu_W + \\rho\\frac{\\sigma_W}{\\sigma_H}(y_H - \\mu_H) \\\\\n   &= 1602 + 0.364\\frac{62.4}{68.8}(1730 - 1732) \\\\\n   &= 1601.34\n\\end{align*}\\]\nvariance\n\\[\n   \\sigma_2^2(1 - \\rho^2) = 62.4^2(1 - 0.364^2) = 377.85.\n\\]\nsummary, \\(W \\mid  (H = 1730) \\sim N(1601.34, 3377.85)\\).\nNote conditional distribution univariate normal distribution, probabilities \\(W > 1730\\) easily determined.\n,\n\\[\\begin{align*}\n   \\Pr(W > 1730 \\mid H = 1730)\n   &= \\Pr\\left( Z > \\frac{1730 - 1601.34}{\\sqrt{3377.85}}\\right) \\\\\n   &= \\Pr( Z > 2.2137)\\\\\n   &= 0.013.\n\\end{align*}\\]\nApproximately \\(1.3\\)% males \\(173\\,\\text{cm}\\) tall married women taller UK 1980.\nFIGURE 8.11: Plots heights data.\n","code":"#> INSIDE"},{"path":"ContinuousDistributions.html","id":"OtherContinuousDistributions","chapter":"8 Standard continuous distributions","heading":"8.9 Other notable continuous distributions","text":"distributions discussed chapter standard commonly-used continuous distributions.\ncontinuous uniform distribution used model complete randomness values interval equally likely.\nGenerally, computers can generate continuous uniform (pseudo-) random numbers; random numbers distributions must produced transforming (pseudo-) random number produced continuous uniform distribution.normal distribution well-known continuous distribution often kown ‘bell-shaped curve’)\ndefined entire real line.\nMany natural processes, many measurement errors, normal distribution.\nImportantly, often used statistics, due importance Central Limit Theorem*.Many natural process onky defined non-negative values, rather entire real line (heights, times distances).\nexponential gamma distributions defined suitable region.\nexponential distribution right-skewed, often used modelling waiting times Poisson arrivals, time memoryless events.\ngamma distribution flexible exponential distribution, often used ot model insurance claims, rainfall, waiting times.\nexponential distribution special case gamma distribution.beta distribution used modelling proportions (probabilities) counts fied number (binomial distribution appropriate), cloud cover, fractional resource consumption, etc.Countless useful discrete distributions exist; mentioned .\nWeibull distribution defined \\([0, \\infty)\\) (notice includes zero) used model lifetimes failures, used reliability analysis survival analysis.von Mises distribution used model angles, defined \\([-\\pi, \\pi)\\); sometimes called ‘circular normal distribution’.\nused model wind directions animal movements.Cauchy distribution defined entire real-line, heavier tails normal distribution.\nused model ratio two independent normal variables.lognormal distribution right-skewed distribution defined positive real numbers.\n\\(\\log X\\sim N(\\mu, \\sigma^2)\\),  \\(X\\) lognormal distribution.\nused model incomes stock prices (using Black–Scholes equation).arcsine distribution special case beta distribution hence defined \\((0, 1)\\), probability concentrated near limits range \\(x = 0\\) \\(x = 1\\).Numerous continuous distributions emerge studying sampling (Chap. 12), usually related normal distribution.\n\\(t\\)-distribution similar normal distribution (defined entire real line), heavier tals.\n\\(\\chi^2\\) distribution, defined \\((0, \\infty)\\), related sums independent squared normal distributions.\n\\(F\\)-distribution, defined \\((0, \\infty)\\) related ratio independent \\(\\chi^2\\) distributions.","code":""},{"path":"ContinuousDistributions.html","id":"SimulationContinuous","chapter":"8 Standard continuous distributions","heading":"8.10 Simulation","text":"","code":""},{"path":"ContinuousDistributions.html","id":"examples","chapter":"8 Standard continuous distributions","heading":"8.10.1 Examples","text":"discrete distributions (Sect. 7.11), simulation can used continuous distributions.\n, R, random numbers specific distribution use functions start letter r; example, generate random numbers normal distribution, use rnorm():Suppose seeking women \\(173\\,\\text{cm}\\) tall study; percentage women \\(173\\,\\text{cm}\\) tall?\nAssume adult females heights modelled normal distribution mean \\(163\\,\\text{cm}\\) standard deviation  \\(5\\,\\text{cm}\\) (Australian Bureau Statistics 1995).\npercentage taller \\(173\\,\\text{cm}\\) :Simulations can also used, allows us consider complex situations.\nsimple question initially (Fig. 8.12, left panel):Now consider complex situation.\nSuppose adult males heights mean  \\(175\\,\\text{cm}\\) standard deviation  \\(7\\,\\text{cm}\\), constitute \\(44\\)% population.\nNow, seeking women men \\(173\\,\\text{cm}\\) tall (Fig. 8.12, right panel):\nFIGURE 8.12: distribution heights women (left panel) men women combined (right panel).\n","code":"\nrnorm(3, # Generate three random numbers...\n      mean = 4,     # ... with mean = 4\n      sd = 2)       # ... with sd = 2\n#> [1] 1.748524 4.082457 2.624341\n1 - pnorm(173, mean = 163, sd = 5)\n#> [1] 0.02275013\nhtW <- rnorm(1000,  # One thousand women\n             mean = 163,  # Mean ht\n             sd = 5)      # Sd of height\npcTallWomen <- sum( htW > 173 )/1000 * 100\n\ncat(\"Percentage 'tall' women:\", pcTallWomen, '%\\n')\n#> Percentage 'tall' women: 2.3 %\npersonSex <- rbinom(1000,  # Select sex of each person\n                    1,\n                    0.44) # So 0 = Female; 1 = Male\n\nhtP <- rnorm(1000,  # change ht parameters according to sex\n             mean = ifelse(personSex == 1,\n                           175,\n                           163),\n             sd = ifelse(personSex == 1,\n                         7, \n                         5))\n\npcTallPeople <- sum( htP > 173 )/1000 * 100\npcFemales <- sum( personSex == 0 ) / 1000 * 100\n\ncat(\"Percentage 'tall' people:\", pcTallPeople, '%\\n\\n')\n#> Percentage 'tall' people: 28.1 %\ncat(\"Percentage females:\", pcFemales, '%\\n\\n')\n#> Percentage females: 56.8 %\ncat(\"Percentage of 'tall' people that are female:\", \n    round( sum(htP > 173 & personSex == 0) / sum(htP > 173) * 100, 1),\n    '%\\n\\n')\n#> Percentage of 'tall' people that are female: 4.3 %\ncat(\"Variance of heights: \", var(htP), \"\\n\")\n#> Variance of heights:  74.2899"},{"path":"ContinuousDistributions.html","id":"relationships-between-distributions","chapter":"8 Standard continuous distributions","heading":"8.10.2 Relationships between distributions","text":"seen (Sect. 3.7), many statistical distributions generated existing distributions.\nrelationships can shown using computer simulation.example, gamma distribution related exponential distribution.\nrandom variable \\(Y\\) exponential distribution rate parameter \\(\\lambda\\), distribution \n\\[\n  X = \\sum_{= 1}^k Y\n\\]\ngamma distribution, shape parameter \\(\\alpha = k\\) rate parameter \\(\\lambda\\).\ncan shown using simulation (Fig. 8.13):\nFIGURE 8.13: Left: histogram \\(5000\\) simulated values exponential distribution. Right: histogram sum five independent exponential distributions. solid lines right panel theoretical distribution gamma distribution shape parameter  \\(5\\).\n","code":"\nset.seed(67433)\n\n### An exponential variate with  rate = 2 \nY <- rexp(n = 5000,\n          rate = 2)\nX <- matrix( data = Y,\n             ncol = 5, # Place 5 values in each of 1000 rows\n             nrow = 1000)\nX <- rowSums(X) # Sum five values in each row\n\n### Set up for two plots, side-by-side\npar(mfrow = c(1, 2) )\n\n### Plot two histograms: Y and X\n###  truehist  is part of the MASS package,which must be loaded first, using: \n###  library(MASS)\ntruehist(Y,\n         las = 1,\n         main = \"Exponential density\",\n         xlab = expression(italic(Y) ),\n         ylab = \"Density\")\ntruehist(X,\n         las = 1,\n         main = \"Gamma density\",\n         xlab = expression(italic(X) ),\n         ylab = \"Density\")\n\n### Now plot the theoretical gamma distribution\n# Use these values of X:\nx_Plot <- seq(0, 10,\n              length = 100)\n# Add lines to the histogram of chi-square random values\nlines( dgamma(x_Plot, shape = 5, rate = 2) ~ x_Plot,\n       type = \"l\",\n       lwd = 3)\n# dgamma()  is the density function for a gamma distribution"},{"path":"ContinuousDistributions.html","id":"ContinuousExercises","chapter":"8 Standard continuous distributions","heading":"8.11 Exercises","text":"Selected answers appear Sect. E.8.Exercise 8.1  Write beta distribution parameters \\(m\\)  \\(n\\) terms mean variance.Exercise 8.2  study modelling pedestrian road-crossing behaviour (Shaaban Abdel-Warith 2017), uniform distribution used simulation model vehicle speeds.\nspeeds minimum value  \\(30\\) km.h\\(-1\\), maximum value  \\(72\\) km.h\\(-1\\).Using model, compute mean standard deviation vehicle speeds.Compute plot PDF distribution function. \\(X\\) vehicle speed, compute \\(\\Pr(X > 60)\\), \\(60\\) km.h\\(-1\\) posted speed limit.Compute \\(\\Pr(X > 65\\mid X > 60)\\).Exercise 8.3  study modelling pedestrian road-crossing behaviour (Shaaban Abdel-Warith 2017), normal distribution used simulation model vehicle speeds.\nnormal model mean \\(48\\) km/h, standard deviation2 \\(8.8\\) km.h\\(-1\\).article ignores vehicles travelling slower \\(30\\) km.h\\(-1\\) faster \\(72\\) km.h\\(-1\\).\nproportion vehicles excluded using criterion?Write PDF truncated normal distribution.Plot PDF df truncated normal distribution.Compute mean variance truncated normal distribution.Suppose vehicle caught speeding (.e., exceeding \\(60\\) km.h\\(-1\\)); probability vehicle exceeding speed limit less \\(5\\) km.h\\(-1\\)?Exercise 8.4  study service life concrete various conditions (Liu Shi 2012) modelled diffusion coefficients using gamma distribution, \\(\\alpha = 27.05\\) \\(\\beta = 1.42\\) (unitless).Plot PDF df.Determine mean standard deviation chloride diffusion coefficients used simulation. \\(C\\) represents chloride diffusion coefficients, compute \\(\\Pr(C > 30\\mid C < 50)\\).Exercise 8.5  study service life concrete various conditions (Liu Shi 2012) used normal distributions model surface chloride concentrations.\none model, mean set \\(\\mu = 2\\) kg.m\\(-3\\) standard deviation \\(\\sigma = 0.2\\) kg.m\\(-3\\).Plot PDF df.Let \\(Y\\) surface chloride concentration.\nCompute \\(\\Pr(Y > 2.4)\\).\\(80\\)% time, surface chloride concentrations value?\\(15\\)% time, surface chloride concentrations exceed value?Exercise 8.6  alternative beta distribution closed form Kumaraswamy distribution, PDF\n\\[\n   p_X(x) =\n   \\begin{cases}\n      ab x^{- 1} (1 - x^)^{b - 1} & \\text{$0 < x < 1$};\\\\\n      0                              & \\text{elsewhere}\n   \\end{cases}\n\\]\n\\(> 0\\) \\(b > 0\\).\nwrite \\(X\\sim \\text{Kumaraswamy}(, b)\\).Plot densities Kumaraswamy distribution showing different shapes.Show df \\(F_X(x) = 1 - (1 - x^)^b\\) \\(0 < x < 1\\).\nPlot distribution functions corresponding PDFs Part 1.\\(X\\sim \\text{Kumaraswamy}(, 1)\\), show \\(X\\sim \\text{Beta}(, 1)\\).\\(X\\sim \\text{Kumaraswamy}(1, 1)\\), show \\(X\\sim \\text{Unif}(0, 1)\\).\\(X\\sim \\text{Kumaraswamy}(, 1)\\), show \\((1 - X) \\sim \\text{Kumaraswamy}(1, )\\).Exercise 8.7  Show gamma distribution constant coefficient variation, coefficient variation standard deviation divided mean.Exercise 8.8  study modelling waiting times hospital (Khadem et al. 2008), patients classified one three categories:Red: Critically ill injured patients.Yellow: Moderately ill \ninjured patients.Green: Minimally injured \nuninjured patients.‘Green’ patients, service time \\(S\\) modelled \\(S = 4.5 + 11V\\), \\(V \\sim \\text{Beta}(0.287, 0.926)\\).Find mean standard deviation service times \\(S\\).Produce well-labelled plots PDF distribution function  \\(V\\), showing important features.Exercise 8.9  study modelling waiting times hospital (Khadem et al. 2008), patients classified one three categories:Red: Critically ill injured patients.Yellow: Moderately ill \ninjured patients.Green: Minimally injured \nuninjured patients.time (minutes) spent reception ‘Yellow’ patients, say \\(T\\), modelled \\(T = 0.5 + W\\), \\(W\\sim \\text{Exp}(\\beta = 16.5)\\).Find mean standard deviation waiting times \\(T\\).Plot PDF distribution function  \\(W\\).Determine \\(\\Pr(T > 1)\\).Exercise 8.10  study modelling waiting times hospital (Khadem et al. 2008), patients classified one three categories:Red: Critically ill injured patients.Yellow: Moderately ill \ninjured patients.Green: Minimally injured \nuninjured patients.time (minutes) spent reception ‘Green’ patients, say \\(T\\), modelled normal mean \\(\\mu = 45.4\\,\\text{mins}\\) standard deviation \\(\\sigma = 23.4\\,\\text{mins}\\).Find mean standard deviation waiting times \\(T\\).Plot PDF distribution function  \\(T\\).proportion ‘Green’ patients wait longer hour?long slowest \\(10\\)% patients need wait?Exercise 8.11  study rainfall (Watterson Dix 2003), rainfall wet days SE Australia (37\\(\\circ\\)S; 146\\(\\circ\\)E) simulated using gamma distribution \\(\\alpha = 0.62\\) \\(\\beta = 7.1\\)mm.Rainfall \\(0.0017\\,\\text{mm}\\) recorded equipment used.\nproportion days represent?Plot PDF df.probability \\(3\\,\\text{mm}\\) falls wet day?proportion wet days  \\(0.43\\).\nproportion days receive \\(3\\,\\text{mm}\\)?Exercise 8.12  study rainfall disaggregation (Connolly, Schirmer, Dunn 1998) (extracting small-scale rainfall features large-scale measurements), duration (fractions day) non-overlapping rainfall events per day Katherine modelled using Gamma distribution \\(\\alpha = 2\\), \\(\\beta = 0.04\\) summer \\(\\beta = 0.03\\) winter.Denote duration rainfall events, fractions day, denoted summer  \\(S\\), winter  \\(W\\).Plot rainfall event duration distributions, compare summer winter.probability rainfall event lasting \\(6\\,\\text{h}\\) winter?probability rainfall event lasting \\(6\\,\\text{h}\\) summer?Describe meant statement \\(\\Pr(S > 3/24 \\mid S > 1/24)\\), compute probability.Describe meant statement \\(\\Pr(W < 2/24 \\mid W > 1/24)\\), compute probability.Exercise 8.13  study rainfall disaggregation (Connolly, Schirmer, Dunn 1998) (extracting small-scale rainfall features large-scale measurements), starting time first rainfall event Katherine day (scaled 0 1) modelled using beta distribution parameters \\((1.16, 1.50)\\) summer \\((0.44, 0.56)\\) winter.Denote number rainfall events summer  \\(S\\), winter  \\(W\\).Plot two starting-time distributions, compare summer winter.Compute mean standard deviation starting times seasons.probability first rain event winter 6am?probability first rain event summer 6am?Describe meant statement \\(\\Pr(S > 3 \\mid S > 1)\\), compute probability.Describe meant statement \\(\\Pr(W > 2 \\mid W > 1)\\), compute probability.Exercise 8.14  study impact diarrhoea (Schmidt, Genser, Chalabi 2009) used gamma distribution model duration symptoms.\nGuatemala, duration (days) modelled using gamma distribution \\(\\alpha = 1.11\\) \\(\\beta = 3.39\\).\nLet \\(X\\) refer duration symptoms.Compute mean standard deviation duration.value  \\(\\alpha\\) close one.\nPlot PDF gamma distribution near-equivalent exponential distribution, comment.Compute \\(\\Pr(X > 4)\\) using gamma exponential distributions, comment.patients \\(5\\)% symptoms longest symptoms long?\n, compare models, comment.Exercise 8.15  study office occupancy university (Luo et al. 2017), leaving-times professors modelled normal distribution, mean leaving time 6pm, standard deviation \\(1.5\\,\\text{h}\\).Using model, proportion professors leave 5pm?Suppose professor still work 5pm; probability professor leaves 7pm?latest-leaving \\(15\\)% professors leave time?Exercise 8.16  percentage clay content soil modelled using beta distribution.\none study (Haskett, Pachepsky, Acock 1995), two counties Iowa modelled beta-distributions: County parameters \\(m = 11.52\\) \\(n = 4.75\\), County B parameters \\(m = 3.85\\) \\(n = 3.65\\).Plot two distributions, comment (context).Compute mean standard deviation county.percentage soil samples exceed \\(50\\)% clay two counties?\nComment.Exercise 8.17  study water uptake plants grasssland savanna ecosystems (Nippert Holdo 2015) used beta distribution model distribution root depth \\(D\\).\nMRD (maximum root density) \\(70\\,\\text{cm}\\), beta distribution used defined interval \\([0, 70]\\).\none simulation, beta-distribution parameters  \\(m = 1\\)  \\(n = 5\\).Plot PDF, explain means context.Determine probability root depth deeper \\(50\\,\\text{cm}\\).Determine root depth plants deepest \\(20\\)% roots.Exercise 8.18  Suppose measured voltage certain electrical circuit normal distribution mean \\(120\\,\\text{V}\\) standard deviation \\(2\\,\\text{V}\\).probability measurement  \\(116\\)  \\(118\\,\\text{V}\\)?five independent measurements voltage made, probability three five measurements  \\(116\\) \\(118\\,\\text{V}\\)?Exercise 8.19  firm buys \\(500\\) identical components, whose failure times independent exponentially distributed mean \\(100\\,\\text{h}\\).Determine probability one component survive least \\(150\\,\\text{h}\\).probability least \\(125\\) components survive least \\(150\\,\\text{h}\\)?Exercise 8.20  headway time gap (front-bumper front-bumper) separating consecutive motor vehicles lane road traffic.\nSuppose headways \\(X\\) (seconds) section traffic lane described shifted gamma distribution, PDF\n\\[\n   f_X(x) =\n   \\frac{1}{\\beta^\\alpha \\Gamma(\\alpha)}\n   (x - \\Delta)^{\\alpha - 1}\n   \\exp\\left( -(x - \\Delta)/\\beta\\right)\n   \\quad\\text{$x > \\Delta$},\n\\]\n\\(\\alpha = 2.5\\), \\(\\beta = 0.8\\) \\(\\Delta = 1.2\\).Determine theoretical mean variance distribution.Simulate \\(1000\\) headways using R, plot histogram data.\nComment.Using simulation results, estimate probability headway within two standard deviations mean.Confirm Tchebyshev’s inequality holds probability.Exercise 8.21  Another commonly-used distribution Weibull distribution:\n\\[\n   f_Y(y) = \\frac{k}{\\lambda} \\left( \\frac{x}{\\lambda} \\right)^{k - 1} \\exp( -(x/\\lambda)^k)\\quad\\text{$x > 0$},\n\\]\n\\(\\lambda > 0\\) called scale parameter, \\(k > 0\\) called shape parameter.Produce sketches R show variety shapes density function.Derive mean variance Weibull distribution.Derive cdf Weibull distribution.Exercise 8.22  Raschke (2011) modelled humidity Haarweg Wageningen (Netherlands) weather station May 2007  2008 using beta distributions.\nMay 2007, relative humidity modelled \\(\\text{Beta}(6.356, 1.970)\\) distribution; May 2008, \\(\\text{Beta}(2.803, 1.456)\\) distribution.given models, determine mean variance humidity months.graph, plot distributions, comment.May 2008, compute \\(\\Pr(X > 60)\\),  \\(X\\) relative humidity, based model.May 2008, compute \\(\\Pr(X > 60 \\mid X > 50)\\),  \\(X\\) relative humidity, based model.May 2008, \\(80\\)% days relative humidity less value, based model?Exercise 8.23  Based Pfizer Australia (2008), mean height \\(Y\\) Australian girls aged  \\(2\\)  \\(5\\) years age based age \\(X\\) approximately given \n\\[\n   \\hat{y} = 7.5x + 70.\n\\]\nSimilarly, standard deviation \\(s\\) heights age \\(x\\) approximately given \n\\[\n   s = 0.4575x + 1.515.\n\\]Suppose , Australia-wide, day-care facilities  \\(32\\)% children aged \\(2\\), \\(33\\)% children aged \\(3\\), \\(25\\)% children aged \\(4\\), remainder aged \\(5\\).\nUse R simulate distribution heights children day-care facilities.Using normal distributions implied, percentage children taller \\(100\\,\\text{cm}\\) age?Use simulation determine proportion children taller \\(100\\,\\text{cm}\\).Use simulation determine mean variance height children day-care facility.Use simulation determine heights tallest \\(15\\)% children facility.Exercise 8.24  Show variance normal, Poisson gamma distributions can written form \\(\\operatorname{var}[X] = \\phi \\operatorname{E}[X]^p\\) \\(p = 0, 1\\) \\(2\\) respectively (Dunn Smyth 2018).\nspeical cases Tweedie distributions.Exercise 8.25  Chia Hutchinson (1991) used beta distribution model cloud duration Darwin, defined (p. 195)…fraction observable daylight hours receiving bright sunshine.January, fitted parameters \\(m = 0.859\\) \\(n = 0.768\\); July, fitted parameters \\(m = 0.110\\) \\(b = 1.486\\).Plot PDFs graph, comment tells cloud cover DarwinWriting \\(C\\) cloud duration, compute \\(\\Pr(C > 0.5)\\) January July.Exercise 8.26  Suppose spinning wheel circumference one metre, point outer edge wheel point interest (Fig. 8.14).wheel spun hard, probability point finishes locations \\(\\)  \\(B\\) stops?\nFIGURE 8.14: wheel spin.\nExercise 8.27  Suppose random variable \\(Y\\) beta distribution (Sect. 8.6), can used modelling proportions.\nProportions related odds, \n\\[\n  \\text{odds} = \\frac{p}{1 - p}\n\\]\n\\(0 \\le p < 1\\).Using simulation, find plot density function distribution odds \\(p = 0.5\\).Using simulation, find plot density function distribution odds \\(p = 0.25\\).(beta prime distribution.)","code":""},{"path":"ChapterMixedDistributions.html","id":"ChapterMixedDistributions","chapter":"9 Mixed distributions","heading":"9 Mixed distributions","text":"completion chapter, able :recognise mixed random variables.recognise censored models, hurdle models compound Poisson–gamma models modelling mixed random variables.know basic properties mixed distributions.apply distributions appropriate problem solving.","code":""},{"path":"ChapterMixedDistributions.html","id":"introduction-3","chapter":"9 Mixed distributions","heading":"9.1 Introduction","text":"Mixed random variables commonly occur, often studied.\nMixed random variables appear diverse applications insurance, agriculture, climatology, fishing, etc.practical cases, mixed random variable \\(X\\) discrete probability mass \\(X = 0\\), continuous \\(X > 0\\); type mixed random variable consider (though extensions discrete masses occurring values random variable often (always) similar).\nexample:modelling insurance, zero probability corresponds portfolios zero claims; however, claims made, total claim amount continuous distribution.modelling rainfall, zero probability corresponds receiving zero rainfall; however, rain fall, total rainfall recorded continuous distribution.modelling fish catch, zero probability corresponds catching zero fish; however, fish caught, mass fish catch continuous distribution.Continuous data discrete probability zero can modelled various ways, depending discrete probability \\(X = 0\\) incorporated.\nThree approaches considered chapter.Censored models (Sect. 9.2) assume existence underlying latent variable, defined real values.\nHowever, values latent variable zero observed (called censoring).\nprobabilities corresponding values latent variable zero accumulated, assigned probability observing value zero.\nvalue \\(X = 0\\) represents censored observations latent variable.Hurdle models (Sect. 9.3) treat zeros emerging two-step process.\nStep one binary process, models whether event interest (.e., insurance claim made; rainfall recorded; fish caught) occurs.\nStep two model continuous data interest, conditional event interest occurring.Compound Poisson-gamma models (Sect. 9.4) combine discrete continuous components single probability model.\nUnlike hurdle model, discrete continuous parts modelled separately; entire distribution modelled using single probability model specific modeling mixed random variables.","code":""},{"path":"ChapterMixedDistributions.html","id":"CensoredModels","chapter":"9 Mixed distributions","heading":"9.2 Censored model","text":"","code":""},{"path":"ChapterMixedDistributions.html","id":"CensoredModelsDefinitions","chapter":"9 Mixed distributions","heading":"9.2.1 Definitions","text":"\nCensored models assume existence unobserved latent variable, say \\(Y\\), \\(Y\\\\mathbb{R}\\) probability function \\(f_Y(y)\\).\nValues  \\(Y\\) threshold, say \\(y^*\\) (commonly, \\(y^* = 0\\)), directly observed.\nInstead, \\(X = \\max(Y^*, Y)\\) observed; , values  \\(Y\\) less  \\(y^*\\) recorded  \\(y^*\\).\nfocus case \\(y^* = 0\\).Example 9.1  (Latent variables censoring) Consider latent variable defined ‘desire purchase alcohol’ \\(Y\\), value \\(Y = 0\\) represents indifference purchasing alcohol.\npeople may strong aversion purchasing alcohol, value  \\(Y\\) large negative.\npeople may mild aversion purchasing alcohol, value  \\(Y\\) small still negative.\ncases, however, aversion purchasing alcohol present, alcohol purchased.observed variable interest  \\(X\\), average weekly spend buying alcohol dollars.\n\\(Y \\le 0\\), observe \\(X = 0\\); however, \\(Y > 0\\), continuous amount spent purchasing alcohol (Fig. 9.1).\nFIGURE 9.1: Left: latent variable \\(Y\\), desire buy alcohol. Right: observed variable \\(X\\), average weekly spend alcohol. solid dot sum probabilities \\(Z \\le 0\\).\ncontext, value observed variable \\(X\\) said censored.Definition 9.1  (Censoring) variable called censored value observed certain threshold.Left censoring occurs true (latent) value less equal known threshold, exact value unknown.\nknow true (latent) value certain threshold.Right censoring occurs true (latent) value greater equal known threshold, exact value unknown.\nknow true (latent) value certain threshold.Example 9.2  (Left censoring) latent variable ‘desire purchase alcohol’ used Example 9.1 left censored.Example 9.3  survival analysis, time takes insects die (say \\(X\\)) may studied.\nstudy ends time \\(t^*\\), time death insects still alive value larger \\(t^*\\); , \\(X\\ge t^*\\) exact time death remains unknown.\ntime death right censored.Censoring truncation.Censoring retains observations, exact value remains unknown.\nTruncation removes observations whose value falls threshold.Usually \\(y^* = 0\\) (Example 9.1), value  \\(Y \\le 0\\) recorded  \\(y = 0\\).\n, observe random variable \\(X\\) \n\\[\n  X = \\max(Y, 0).\n\\]\nmeans probability function  \\(X\\) \n\\[\n  f_X(x) =\n  \\begin{cases}\n    0       & \\text{$X < 0$}\\\\\n    F_Y(0)  & \\text{$X = 0$}\\\\\n    f_Y(y)  & \\text{$X > 0$}\n  \\end{cases}\n\\]\n\\(F_Y(y)\\) distribution function  \\(Y\\).\nprobability function \\(X\\mid X > 0\\), continuous part distribution , \n\\[\\begin{equation}\n  f_{X\\mid X > 0}(x)\n  = \\frac{f_X(x)}{\\Pr(X > 0)}\n  = \\frac{f_Y(y)}{1 - F_Y(0)}\n  \\tag{9.1}\n\\end{equation}\\]\n\\(F_Y(y)\\) distribution function  \\(Y\\).\ndistribution function  \\(X\\) \n\\[\n  F_{X\\mid X>0}(x) =\n  \\begin{cases}\n    0             & \\text{$x < 0$}\\\\\n    F_Y(0)        & \\text{$x = 0$}\\\\\n    F_Y(x)        & \\text{$x > 0$}\n  \\end{cases}\n\\]latent distribution normal distribution, Equation (9.1) can written terms standard normal density function \\(\\phi(\\cdot)\\) standard normal distribution function \\(\\Phi(\\cdot)\\) (see Sect. 8.3.2).\ncase , notice \n\\[\\begin{align*}\n   \\Pr(X > 0)\n   &= \\Pr(Z > (0 - \\mu)/\\sigma)\\quad\\text{($Z$ standard normal variate)} \\\\\n   &= \\Pr(Z > -\\mu/\\sigma) \\\\\n   &= 1 - \\Phi( -\\mu/\\sigma) \\\\\n   &= \\Phi(\\mu/\\sigma) \\quad\\text{(using symmetry normal distribution)},\n\\end{align*}\\]\nhence Equation (9.1) can written \n\\[\\begin{equation}\n  f_{X\\mid X>0}(x)\n  = \\frac{\\phi\\big((x-\\mu)/\\sigma\\big)}{\\Phi(\\mu/\\sigma)}.\n  \\tag{9.2}\n\\end{equation}\\]\nwrite \\(t = \\mu/\\sigma\\) define \\(\\lambda(t) = \\phi(t)/\\Phi(t)\\), expression becomes\n\\[\n  f_{X\\mid X>0}(x) = ?????????????\n\\]\nLOST:\n\\[\n  f_{X\\mid X>0}(x)\n  = \\frac{f_Z(x)}{\\Pr(Z > 0)}\n  = \\frac{f_Z(x)}{1 - F_Z(0)}\n\\]Example 9.4  (Latent variables censoring) Consider number hours worked (can observed), must take non-negative value.\nassume underlying, unobserved latent variable ‘willingness work’.\nNegative values ‘willingness work’, matter small large, result zero observed hours work.","code":""},{"path":"ChapterMixedDistributions.html","id":"CensoredModelsProperties","chapter":"9 Mixed distributions","heading":"9.2.2 Properties","text":"expected value censored random variable \\(X\\) can found first separating discrete continuous parts distribution:\n\\[\\begin{align*}\n  \\operatorname{E}[X]\n  &= \\operatorname{E}[X\\mid X = 0] + \\operatorname{E}[X\\mid X > 0]\\\\\n  &= \\Pr(X = 0)\\times 0 + \\operatorname{E}[X\\mid X > 0]\\\\\n  &= \\operatorname{E}[X\\mid X > 0]\\\\\n  &= \\int_0^\\infty x\\cdot f_{X\\mid X > 0}(x)\\,dx,\\\\\n  &= \\frac{1}{\\Phi()} \\int_0^\\infty x\\cdot f_{X}(x)\\,dx,\n\\end{align*}\\]\n\\(f_{X\\mid X > 0}(x)\\) given Equation (9.2)).censoring happens \\(x^* = 0\\), standardised score \\((x^* - \\mu)/\\sigma = -t\\), defining \\(t = \\mu/\\sigma\\) proves useful.\n\\(1 - \\Phi()\\)\nThus,\n\\[\\begin{align*}\n  \\operatorname{E}[X]\n  &= \\int_0^\\infty x\\cdot f_{X\\mid X > 0}(x)\\,dx,\n  &= \\int_0^\\infty x\\cdot f_{X\\mid X > 0}(x)\\,dx,\n\\end{align*}\\]Similarly,\n\\[\n  \\operatorname{E}[X^2]\n  = \\operatorname{E}[X^2 \\mid X > 0]\n  = \\int_0^\\infty x^2 \\cdot f_{X\\mid X>0}(x)\\, dx,\n\\]\nvariance can obtained.\napproach also gives MGF \n\\[\n  M_X(t) = M_{X\\mid X > 0}(t).\n\\]\nClearly, progress expressions requires knowing distribution  \\(X\\) hence distribution latent variable \\(Z\\).\ncommon, means universal, latent variable described normal distribution (models also called Tobit models).Example 9.5  (Censored model) Consider latent variable \\(Y\\), observed variable \\(X = \\text{max}(0, Z)\\).\nSuppose  \\(Z\\) normal distribution (Fig. 9.2, left panel))\n\\[\n  Z \\sim N(\\mu = 0, \\sigma^2 = 1),\n\\]\n\\(\\operatorname{E}[Z] = 0\\) \\(\\operatorname{var}[Z] = 1\\).\n, \\(Z\\) standard normal distribution (Sect 8.3.2), \n\\[\n  \\Pr(Z < 0) = \\Phi(0) = 0.5.\n\\]\nRecall notation: \\(\\Phi(\\cdot)\\) distribution function standard normal variate, \\(\\phi(\\cdot)\\) density function standard normal variate., define random variable \\(X\\) \n\\[\n  f_X(x) =\n  \\begin{cases}\n    0          & \\text{$Z < 0$};\\\\\n    \\Phi(0)    & \\text{$Z = 0$};\\\\\n    \\phi(x)    & \\text{$Z > 0$},\n  \\end{cases}\n\\]\nshown Fig. 9.2 (right panel).Equation (9.1),\n\\[\n  f_{X\\mid X > 0}(x)\n  = \\frac{\\phi(x)}{1 - \\Phi(0)}\n  = 2 \\phi(x)\n\\]\n\\(X > 0\\).\nexpression,\n\\[\n  \\operatorname{E}[X]\n   = \\operatorname{E}[X \\mid X > 0]\n  = 2 \\int_0^\\infty x\\cdot \\phi(x)\\, dx\n  = \\frac{2}{\\sqrt{2\\pi}}\n  \\approx 0.7979,\n\\]\nintegrating expression \\(\\phi(x)\\) (Equation (8.4)).\nSimilarly, using integration parts,\n\\[\n  \\operatorname{E}[X^2]\n   = \\operatorname{E}[X^2 \\mid X > 0]\n  = 2 \\int_0^\\infty x^2\\cdot \\phi(x)\\, dx\n  = 1/2.\n\\]\nCombining, means \n\\[\n  \\operatorname{var}[X]\n  = \\frac{1}{2} - \\left(\\frac{2}{\\sqrt{2\\pi}}\\right)^2\n  = \\frac{1}{2} - \\frac{1}{\\pi}\n  \\approx 0.182.\n\\]\nFIGURE 9.2: censored model, using normal distribution continuous component, threshold value \\(Z = 0\\).\n","code":""},{"path":"ChapterMixedDistributions.html","id":"HurdleModels","chapter":"9 Mixed distributions","heading":"9.3 Hurdle models","text":"","code":""},{"path":"ChapterMixedDistributions.html","id":"HurdleModelsDefinitions","chapter":"9 Mixed distributions","heading":"9.3.1 Definitions","text":"\nhurdle model treats zero values random variable \\(X\\) emerging two-step process.\nfirst step uses Bernoulli distribution (Sect. 7.3) model random variable \\(Y\\) defines whether event interest occurs:\n\\[\n  \\Pr(Y) =\n  \\begin{cases}\n    1 - p    & \\text{$Y = 0$; .e., event interest \\emph{} occur;}\\\\\n    p        & \\text{$Y = 1$; .e., event interest \\emph{} occur.}\n  \\end{cases}\n\\]\nevent occur (.e., conditional \\(X > 0\\)), probability \\(1 - p\\), continuous component modelled using probability density function \\(f^+_X(x)\\) defined positive real values (exponential distribution gamma distribution).probability function mixed random variable \\(X\\) therefore\n\\[\n   f_X(x) =\n   \\begin{cases}\n      1 - p           & \\text{$x = 0$}\\\\\n      p\\cdot f^+_X(x)    & \\text{$x > 0$,}\n   \\end{cases}\n\\]\n\\(f^+_X(x) = f_{X \\mid X > 0}(x)\\) density function continuous distribution defined \\(x\\\\mathbb{R}_{+}\\).\ndistribution function \n\\[\n   F_X(x) =\n   \\begin{cases}\n      0                             & \\text{$x < 0$}\\\\\n      1 - p                         & \\text{$x = 0$}\\\\\n      (1 - p) + p\\cdot F^+_X(x)    & \\text{$x > 0$,}\n   \\end{cases}\n\\]\n\\(F^+_X(x) = F_{X\\mid X > 0}(x)\\) distribution function continuous distribution defined \\(x\\\\mathbb{R}_{+}\\).Example 9.6  (Hurdle model) Consider mixed random variable \\(X\\), probability function \\(f_X(x)\\).\nSuppose , \\(X > 0\\), exponential distribution used describe distribution; \n\\[\n  f^+_X(x) = \\frac{1}{\\lambda}\\exp(-x/\\lambda)\\quad\\text{$x > 0$}.\n\\]\n, using \\(p = 0.7\\) \\(\\lambda = 1/2\\), probability function  \\(X\\) \n\\[\n  f_X(x)\n  =\n    \\begin{cases}\n       0.3                   & \\text{$x = 0$}\\\\\n       0.35\\cdot \\exp(-x/2)  & \\text{$x > 0$;}\n    \\end{cases}\n\\]\nsee Fig. 9.3.\nFIGURE 9.3: hurdle model, showing exponential distribution continuous component.\n","code":""},{"path":"ChapterMixedDistributions.html","id":"HurdleModelsProperties","chapter":"9 Mixed distributions","heading":"9.3.2 Properties","text":"expected value random variable \\(X\\) can found first separating discrete continuous parts distribution:\n\\[\\begin{align*}\n  \\operatorname{E}[X]\n  &= \\operatorname{E}[X\\mid X = 0] + \\operatorname{E}[X\\mid X > 0]\\\\\n  &= (1 - p)\\times 0 + p \\operatorname{E}[X\\mid X > 0]\\\\\n  &= p \\mu^+,\n\\end{align*}\\]\n\\(\\mu^+\\) denotes \\(\\operatorname{E}[X\\mid X > 0]\\), expected value distribution defined \\(X > 0\\).\nSimilarly,\n\\[\n  \\operatorname{E}[X^2]\n  = \\operatorname{E}[X^2 \\mid X > 0]\n  = p (\\mu^2)^+,\n\\]\n\\((\\mu^2)^+\\) denotes \\(\\operatorname{E}[X^2\\mid X > 0]\\).\n, variance  \\(X\\) \n\\[\\begin{align*}\n  \\operatorname{var}[X]\n  &=  p[(\\mu^2)^+ - p(\\mu^+)^2]\\\\\n  &=  p(\\sigma^2)^+ + p(1 - p)(\\mu^+)^2.\n\\end{align*}\\]\n\\(\\sigma^2 = \\operatorname{var}[X \\mid X > 0]\\)\napproach also gives MGF \n\\[\\begin{align*}\n  M_X(t)\n  &= (1 - p) + p\\cdot M_{X^+}(t)\\\\\n  &= (1 - p) + p\\cdot M_{X\\mid X > 0}(t),\n\\end{align*}\\]\n\\(M_{X^+}(t)\\) MGF distribution defined \\(X > 0\\).Clearly, progress expressions requires knowing distribution  \\(X | X > 0\\)Example 9.7  (Hurdle model, using gamma distribution) Consider random variable \\(X\\) \\(p = 0.75\\) (hence \\(\\Pr(X = 0) = 1 - p = 0.25\\)), distribution \\(X > 0\\) follows gamma distribution (Sect. 8.5) \\(\\alpha = 2\\) \\(\\beta = 1\\), density function\n\\[\n  f_X^{+}(x; \\alpha, \\beta)\n  = f_{X\\mid X > 0}(x; \\alpha, \\beta)\n  = \\frac{1}{\\Gamma(2)} x \\exp(-x)\n  \\quad\\text{$x > 0$}.\n\\]\ngamma distribution, using results Sect. 8.5, \\(\\mu^+ = \\operatorname{E}[X\\mid X > 0] = \\alpha\\beta = 2\\) \\(\\operatorname{var}[X\\mid X > 0] = \\alpha\\beta^2 = 2\\).\n, probability function  \\(X\\) \n\\[\n  f_{X}(x)\n  =\n  \\begin{cases}\n     0.25                                & \\text{$x = 0$}\\\\\n     0.75 \\times f_X^{+}(x; \\alpha, \\beta) & \\text{$x > 0$.}\n  \\end{cases}\n\\]\n\n\\[\\begin{align*}\n  \\operatorname{E}[X]\n   &= p\\cdot \\mu^+\n    = 0.75 \\times 2\n    = 1.5\\\\\n  \\operatorname{var}[X]\n   &= p(\\sigma^2)^+ + p(1 - p)(\\mu^+)^2\n   = 0.75\\times 2 + 0.75\\times 0.25\\times 1.5^2\n   \\approx 1.921.\n\\end{align*}\\]\nFIGURE 9.4: hurdle model, using gamma distribution continuous component.\n","code":""},{"path":"ChapterMixedDistributions.html","id":"TweedieModels","chapter":"9 Mixed distributions","heading":"9.4 Compound Poisson-gamma distributions","text":"\nCompound Poisson–gamma distribution take different approach modelling mixed random variables: Compound Poisson–gamma distributions model mixed random variables Poisson sum independent gamma distributions.\nSuppose number events observed (discrete)  \\(N\\), \n\\[\\begin{equation}\n  N \\sim \\text{Pois}(\\lambda).\n   \\tag{9.3}\n\\end{equation}\\]\n, event occurs \\(N\\) times, occurs random following Poisson distribution.\nGiven  \\(N\\) Poisson distribution, follows (Sect. 7.7) \n\\[\n  \\Pr(N = 0) = \\exp(-\\lambda)\n\\]\nprobability zero events observed (.e., \\(N = 0\\)).\n, however, \\(N > 0\\), \\(= 1, 2, \\dots, N\\), continuous random variable \\(Y_i\\) observed, \n\\[\n   Y_i \\sim \\text{Gam}(\\alpha_i, \\beta).\n\\]\n\\(N = 0\\), events \\(Y_i\\) observed ; however, \\(N > 0\\)  \\(N\\) events observed \n\\[\n  X = \\sum_{= 1}^N Y_i\n\\]\ncontinuous distribution.\n,\n\\[\n  X =\n  \\begin{cases}\n    0                   & \\text{$N = 0$};\\\\\n    \\sum_{= 1}^N Y_i  & \\text{$N > 0$}.\n  \\end{cases}\n\\]\ndefinition \\(X\\) compound Poisson-gamma distribution.\ndistribution  \\(X\\) probability mass \\(X = 0\\) \\(\\Pr(Y = 0) = \\Pr(N = 0) = \\exp(-\\lambda)\\).probability functions compound Poisson-gamma distribution written closed form.\nHowever, probability function can expressed infinite sum (Dunn Smyth 2005) inverting MGF (Dunn Smyth 2008) discussed Sect. 5.5.5.probability function infinite series can determined development compound Poisson-gamma distribution .\n\\(N = 1\\) \\(\\Pr(N = 1)\\) (Poisson distribution), , \\(X = Y\\) \\(\\text{Gam}(\\alpha, \\beta)\\) distribution.\n,\n\\[\n  f_{X\\mid N = 1}(x)\n  = \\frac{\\beta^{\\alpha}}{\\Gamma(\\alpha)} x^{\\alpha -1}\\exp(-\\beta x) \\quad \\text{$x > 0$}.\n\\]\\(N = 2\\) \\(\\Pr(N = 2)\\) (Poisson distribution), , \\(X = Y_1 + Y_2\\), \\(Y_i \\sim\\text{Gam}(\\alpha, \\beta)\\) \\(= 1, 2\\).\nmeans \\(X \\sim \\text{Gam}(2\\alpha, \\beta)\\), easily shown using MGFS. (; CROSS REF).\n,\n\\[\n  f_{X\\mid N = 2} (x)\n  = \\frac{\\beta^{2\\alpha}}{\\Gamma(2\\alpha)} x^{2\\alpha -1}\\exp(-\\beta x) \\quad \\text{$x > 0$}.\n\\]generally, \\(N = n\\) \\(\\Pr(N = n)\\) (Poisson distribution), , \\(X = Y_1 + Y_2+\\cdots + Y_n\\), \\(X \\sim\\text{Gam}(n\\alpha, \\beta)\\), \n\\[\n  f_{X\\mid N = n} (x)\n  = \\frac{\\beta^{n\\alpha}}{\\Gamma(n\\alpha)} x^{n\\alpha -1}\\exp(-\\beta x) \\quad \\text{$x > 0$}.\n\\], using Law total probability (Theorem 2.4),\n\\[\\begin{align*}\n  f_X(x)\n  &= \\sum_{n = 1}^\\infty \\Pr(N = n) \\cdot F_{X\\mid N = n}(x)\\\\\n  &= \\sum_{n = 1}^\\infty \\frac{\\exp(-\\lambda)\\lambda^n}{n!} \\cdot \\frac{\\beta^{n\\alpha}}{\\Gamma(n\\alpha)} x^{n\\alpha -1}\\exp(-\\beta x)\\\\\n  &= \\exp(-\\lambda - \\beta x) \\sum_{n = 1}^\\infty \\frac{\\lambda^n}{n!} \\frac{\\beta^{n\\alpha}}{\\Gamma(n\\alpha)} x^{n\\alpha - 1}\n\\end{align*}\\]\n\\(x > 0\\).\naddition, course, \n\\[\n  \\Pr(X = 0) = \\exp(-\\lambda).\n\\]Even though probability function closed form, MGF reasonably simple.\nfind MGF, first consider value  \\(N\\) fixed; , \\(X = Y_1 + Y_2 + \\cdots + Y_N\\) MGF  \\(X\\) \n\\[\n  M_{X\\mid N}(t) = \\operatorname{E}[\\exp(tX)\\mid N].\n\\]\nSince MGF sum independent random variables product MGFs (Theorem 5.6), can assume \\(N\\) takes specific value \\(n\\) write\n\\[\\begin{align*}\n  M_{X\\mid N}(t)\n  &= \\operatorname{E}\\left[ \\exp\\big(t(Y_2 + Y_2 + \\cdots + Y_n)\\big)\\right] \\\\\n  &= \\prod_{= 1}^n \\operatorname{E}[\\exp(t Y_i)] \\\\\n  &= \\left( M_Y(t)\\right)^n\n\\end{align*}\\]\nMGF \\(X\\) (rather \\(X\\mid N\\) ) \n\\[\\begin{align*}\n  M_X(t)\n  &= \\operatorname{E}\\left[ \\operatorname{E}[ \\exp(tX\\mid N)]\\right] \\\\\n  &= \\operatorname{E}\\left[ M_{X\\mid N}(t))\\right] \\\\\n  &= \\operatorname{E}\\left[ M_Y(t))^N\\right] \\\\\n\\end{align*}\\]\n\n\\[\n  M_X(t)=\\exp\\!\\left\\{\\lambda\\left[\\Big(\\frac{\\beta}{\\beta-t}\\Big)^{\\!\\alpha}-1\\right]\\right\\},\\qquad t<\\beta\n\\]\n\\[\nM_X(t) = \\mathbb{E}[e^{tX}]\n        = \\mathbb{E}\\!\\left[ \\big(M_Y(t)\\big)^{N} \\right].\n\\]\nWriting expectation sum possible values  \\(N\\),\n\\[\n  M_X(t)\n  = \\sum_{n=0}^\\infty \\big(M_Y(t)\\big)^{n} \\, \\Pr(N = n).\n\\]\nSince \\(N \\sim \\mathrm{Pois}(\\lambda)\\),\n\\[\n  \\Pr(N = n) = \\frac{\\exp(-\\lambda) \\lambda^n}{n!}.\n\\]\nSubstituting expression XREF\n\\[\\begin{align*}\n  M_X(t)\n  &= \\sum_{n = 0}^\\infty\n  \\big(M_Y(t)\\big)^{n} \\cdot \\frac{\\exp(-\\lambda) \\lambda^n}{n!}\\\\\n  &= \\exp(-\\lambda) \\sum_{n = 0}^\\infty \\frac{\\left[\\lambda \\, M_Y(t)\\right]^n}{n!}.\n\\end{align*}\\]\nUsing series expansion \\(\\sum_{n = 0}^\\infty z^n/n! = \\exp(z)\\) (Equation (B.8)) \\(z = \\lambda M_Y(t)\\), gives\n\\[\\begin{align*}\n  M_X(t)\n  &= \\exp(-\\lambda) \\cdot \\exp\\big(\\lambda M_Y(t)\\big)\\\\\n  &= \\exp\\{\\lambda(M_Y(t) - 1) \\\\\n  &= \\exp\\!\\left\\{\\lambda\\left[\\Big(\\frac{\\beta}{\\beta-t}\\Big)^{\\!\\alpha}-1\\right]\\right\\},\n\\end{align*}\\]\nprovided \\(t < \\beta\\).MGF can deduce\n\\[\\begin{align*}\n  \\operatorname{E}[X]\n  &= \\lambda\\alpha\\/\\beta\\\\\n  \\operatorname{var}[X]\n  &= \\lambda\\alpha(\\alpha + 1)\\beta^2.\n\\end{align*}\\]Example 9.8  (Poisson--gamma distribution) Suppose\n\\[\n  N\\sim\\text{Pois}(\\lambda = 3)\n  \\quad\\text{}\\quad\n  Y_i \\sim\\text{Gam}(\\alpha = 1/2, \\beta = 2/3)\n\\]\n(\\(\\beta\\) scale parameter).\nmean variance  \\(X\\) \n\\[\\begin{align*}\n  \\operatorname{E}[X]\n  &= \\lambda\\alpha\\/\\beta\n   = 3\\times \\frac{1}{2}\\times\\frac{2}{3} = 1;\\\\\n  \\operatorname{var}[X]\n  &= \\lambda\\alpha(\\alpha + 1)\\beta^2\n   = 3\\times \\left(\\frac{1}{2}\\times \\frac{3}{2}\\right)\\left(\\frac{5}{3}\\right)^2 = 1.\n\\end{align*}\\]\nFurthermore, Equation (9.3),\n\\[\n  \\Pr(X = 0) = \\exp(-\\lambda) = 0.0498.\n\\]\nPoisson-gamma distribution shown Fig. 9.5 (right panel).\nFIGURE 9.5: Two compound Poisson–gamma models, mean varaince set \\(1\\). solid dot \\(X = 0\\) represents discrete probability.\nExample 9.9  (Modelling monthly rainfall) Modelling rainfall.\nUsing hurdle Stern Coe, Chandler Wheater? Tobit thing IWSM, Poisson–gamma,","code":"#> alpha, beta, lambda, p0\n#> 20 0.04761905 1.05 0.3499377\n#> 0.5 0.6666667 3 0.04978707"},{"path":"ChapterMixedDistributions.html","id":"MixedVariablesSimulation","chapter":"9 Mixed distributions","heading":"9.5 Simulation","text":"","code":""},{"path":"ChapterMixedDistributions.html","id":"general","chapter":"9 Mixed distributions","heading":"9.5.1 General","text":"\nFIGURE 9.6: FIX!!!!! solid dot \\(X = 0\\) represents discrete probability.\n…hurdle model used model daily rainfall (e.g., Stern Coe (1984)), using Bernoulli distribution distinguish wet dry days, gamma distribution model rainfall amounts wet days.…compound Poisson–gamma model also used model daily rainfall (e.g., Yunus et al. (2017)).Example 9.10  (Simulations censored models) Censored:Example 9.11  (Simulations hurdle models) Hurdle model; see Fig. 9.7\nFIGURE 9.7: simulation hurdle model. FIX!!!!! solid dot \\(X = 0\\) represents discrete probability.\nExample 9.12  (Simulations compound Poisson--gamma models) Compound P–G; see Fig. 9.8\nFIGURE 9.8: simulation compound Poisson–gamma models, mean variance set \\(1\\). solid dot \\(X = 0\\) represents discrete probability.\n","code":""},{"path":"ChapterMixedDistributions.html","id":"rainfall","chapter":"9 Mixed distributions","heading":"9.5.2 Rainfall","text":"Simulation censored, hurdle compound Poisson-gamma models can achieved directly modelling process zeros incorporated.\ndemonstrate, consider modelling monthly rainfall certain location one specific month year.\nMonthly rainfall mixed random variable; months record exactly zero rainfall, months rain falls continuous rainfall amount recorded.Simulating monthly rainfall uses agriculture, hydrology, forestry elsewhere (often input models).\ndistribution monthly rainfall obviously varies depending location month selected modelling; however, monthly rainfall many locations typically contain months (historically) record zero rainfall (see, example, Hasan Dunn (2010a)).Suppose seek model monthly rainfall (im mm) \\(R\\) given location certain month,  \\(30\\)% months record zero rainfall (.e., \\(\\Pr(X = 0) = 0.30\\)), mean monthly rainfall (rain recorded) \\(\\mu_R = 50\\).model monthly rainfall using censored model (e.g., Sansó Guenni (2000)), hurdle model (e.g, Kenabatho et al. (2012)) Poisson-gamma distribution (e.g., Hasan Dunn (2010a)).First consider censored model.\nlatent variable represent favourable conditions rainfall’ say \\(Y\\sim N(\\mu_R = 50, \\sigma_R^2)\\), \\(\\Phi(0) = \\Pr(Y < 0) = 0.30\\), variance \\(\\sigma_R^2\\).\nFirst, can find value standard normal variate \\(Z\\) \\(\\Phi(z) = 0.30 = \\Pr(Y < 0)\\):value  \\(z\\) value \n\\[\n  z = -0.5244 = \\frac{x^* - \\mu_R}{\\sigma^2_R} = \\frac{0 - 50}{\\sigma_R^2},\n\\]\nfind \\(\\sigma_R = 95.35\\).\nThus, monthly rainfall can simulated:HURDLE:model rainfall using hurdle model, use \\(1 - p = 0.3\\) probability monthly rainfall recorded; , \\(p = 0.7\\).\ninformation also gives \\(\\mu = \\operatorname{E} = 50\\); Sect. 9.3.2\n\\[\n  \\mu = p \\mu^+,\n\\]\n\\(\\mu^+ = \\mu/p = 50/0.7 approx 71.43\\) mean rainfall, conditional rain recorded.\nsuppose exponential distribution (rate parameter \\(\\lambda\\)) rainfall amounts rain recorded; thus \\(\\operatorname{E}[X] = 1\\lambda = 71.43\\) \\(\\lambda = 0.01399972\\).\nhurdle model :\n\\[\\begin{align*}\n  f_R(r)\n  &=\n  \\begin{cases}\n    0.3 & \\\\\n    0.3\\lambda \\exp(-r/\\lambda)\n  \\end{cases}\\\\\n  &=\n  \\begin{cases}\n    0.3 & \\\\\n    0.004199916 \\exp(-r/0.01399972).\n  \\end{cases}\n\\end{align*}\\]\nFIGURE 9.9: HURDLE MODEL\nGAMMA SEEMS FAIL PARAMETERS ANYWAY: checkIf use variance censored model (.e., \\(\\sigma^2 = 95.35\\)), can find variance gamma distribution:\n\\[\\begin{align*}\n  \\operatorname{var}[X]\n  &= p(\\sigma^2)^+ + p(1 - p)(\\mu^+)^2\\\\\n  95.35\n  &= 0.7(\\sigma^2)^+ + 0.7\\times 0.3(71.43)^2;\n\\end{align*}\\]\nsolving shows use \\((\\sigma^2)^ = ??\\) variance gamma distribution (\\(X > 0\\)).suppose gamma distribution rainfall amounts rain recorded; thus \\(\\alpha\\beta = 71.43\\).\nuse variance censored model (.e., \\(\\sigma^2 = 95.35\\)), can find variance gamma distribution:\n\\[\\begin{align*}\n  \\operatorname{var}[X]\n  &= p(\\sigma^2)^+ + p(1 - p)(\\mu^+)^2\\\\\n  95.35\n  &= 0.7(\\sigma^2)^+ + 0.7\\times 0.3(71.43)^2;\n\\end{align*}\\]\nsolving shows use \\((\\sigma^2)^ = ??\\) variance gamma distribution (\\(X > 0\\)).OOPS!!P–G DSTN:Need convert \\(p_0\\), \\(\\mu\\) \\(\\sigma^2\\) \\(\\lambda, \\alpha, \\beta\\). (easily)?? JUST USE THEORETICAL DISTN! Well, wan rainfall values use upstream simulation (eg) cropping.\nMAXIMUM RAINFALL, CI-like interval? … distributionPERHAPS CONNECT SOI","code":"\np_Zero_Rainfall <- 0.30\nqnorm(p_Zero_Rainfall) # This find z such that Phi(z) = 0.30\n#> [1] -0.5244005\nset.seed(76103) # For reproducibility\nnum_Sims <- 1000\n\nmean_R <- 50\nsd_R <- -mean_R / qnorm(p0) \n\nY <- rnorm(num_Sims,\n           mean = mean_R,\n           sd = sd_R)\np0 <- 0.30\n\n# To get sd_R, note: z = (0 - mu)/sigma = \\Phi(p0) => -mu/sigma = qnorm(p0)\nthreshold_R <- mean_R  - qnorm(p0) * sd_R # SHOULD BE AT ZERO\n\n###\n\npar(mfrow = c(1, 2))\nlatent_Breaks <- seq(-250, 450,\n                     by = 50)\n###  truehist  is part of the MASS package,which must be loaded first, using: \n###  library(MASS)\nMASS::truehist(Y,\n         las = 1,\n         breaks = latent_Breaks,\n         col = c( rep( \"grey\", length(latent_Breaks[latent_Breaks < 0])),\n                  rep(plotColour1, length(latent_Breaks[latent_Breaks >= 0])) ),  \n         main = expression(Latent~variable),\n         ylab = \"Probabiity fn\",\n         xlab = expression(Observed~variable~italic(X)) )\n\nabline(v = 0,\n       lwd = 2,\n       col = \"grey\",\n       lty = 2)\n\ntruehist(Y[Y>0],\n         las = 1,\n         breaks = latent_Breaks[latent_Breaks>=0],\n         main = expression(Observed~variable),\n         ylab = \"Probabiity fn\",\n         xlab = expression(Observed~variable~italic(X)) )\npoints(x = 0,\n       y = p0,\n       pch = 19)\nnum_Sims <- 1000\n\n#Simulate\n\n# Compute max monthly rainfall\nset.seed(76103) # For reproducibility\nnum_Sims <- 1000\n\np0 <- 0.30\np <- 1 - p0\n\nmean_R <- 50\nmean_Rpositive <- mean_R/p\nlambda_Exp <- 1 / mean_Rpositive\n  \nR <- matrix( NA, ncol = 1, nrow = num_Sims)\np_Each_Month <- rbinom(num_Sims,\n                       size = 1,\n                       prob = p)\n\nfor (i in 1:num_Sims){\n  if ((p_Each_Month[i]) == 0){\n    R[i] <- 0 # No rainfall recorded\n  }  else {\n    R[i] <- rexp(1, rate = lambda_Exp)    \n  }\n}\n\n\n\n###\n###  truehist  is part of the MASS package,which must be loaded first, using: \n###  library(MASS)\ntruehist(R,\n         las = 1,\n         main = expression(Hurdle~model),\n         ylab = \"Probabiity fn\",\n         xlab = expression(Observed~rainfall~italic(R)) )\n\npoints(x = 0,\n       y = p0,\n       pch = 19)\n( 95.35 - 0.7*0.3*(71.43)^2 )/0.7\n#> [1] -1394.459"},{"path":"ChapterMixedDistributions.html","id":"MixedDistributionsExercises","chapter":"9 Mixed distributions","heading":"9.6 Exercises","text":"Selected answers appear Sect. E.9.Exercise 9.1  Medical Costs\nObserved variable: Annual medical expenses.Latent variable (\\(z\\)): Underlying health need risk.Censoring: Many people incur \\(0\\) expenses given year, despite possibly latent health risks needs.Interpretation: People latent need threshold never seek care (e.g. due costs, access, asymptomatic cases), leading observed \\(0\\).Exercise 9.2  Credit Card Balances\nObserved variable: Balance end month.Latent variable (\\(z\\)): Willingness propensity use credit.Censoring: users consistently pay full carry balance.Interpretation: people low latent demand borrowing; observed balance zero latent borrowing desire threshold.Exercise 9.3  Consider latent variable \\(Y\\), observed variable \\(X = \\text{max}(0, Y)\\).\nSuppose  \\(Y\\) normal distribution\n\\[\n  Y \\sim N(\\mu = 1.5, \\sigma^2 = 1),\n\\]\n\\(\\operatorname{E}[Y] = 1.5\\) \\(\\operatorname{var}[Y] = 1\\).Determine \\(\\Pr(Y < 0)\\), \\(Z\\) standard normal variate (Sect. 8.3.2).Using functions \\(\\Phi(\\cdot)\\) \\(\\phi(\\cdot)\\), determine probability function  \\(X\\).Plot probability function.","code":""},{"path":"ChapMultivariate.html","id":"ChapMultivariate","chapter":"10 Multivariate distributions*","heading":"10 Multivariate distributions*","text":"completion chapter able :apply concept bivariate random variables.compute joint probability functions distribution function two random variables.find marginal conditional probability functions random variables discrete continuous cases.apply concept independence two random variables.compute expectation variance linear combinations random variables.interpret compute covariance coefficient correlation two random variables.compute conditional mean conditional variance random variable given value another random variable.use multinomial bivariate normal distributions.","code":""},{"path":"ChapMultivariate.html","id":"MultivariateIntroduction","chapter":"10 Multivariate distributions*","heading":"10.1 Introduction","text":"random processes sufficiently simple outcome denoted single variable \\(X\\).\nMany situations require observing two numerical characteristics simultaneously.\nchapter mainly discusses two-variable (bivariate) case, also discusses multivariable (two variables) case using matrix notation (Sect. 10.2).","code":""},{"path":"ChapMultivariate.html","id":"Multivariate","chapter":"10 Multivariate distributions*","heading":"10.2 Multivariate random variables and matrix notation","text":"","code":""},{"path":"ChapMultivariate.html","id":"RandomVectors","chapter":"10 Multivariate distributions*","heading":"10.3 Random vectors","text":"far, studied univariate random variables (.e., single random variable)\nbivariate random variables (two jointly distributed random variables).\nideas can extended random variables; case multivariate random variables, several random variables considered simultaneously., using random vectors convenient.\nrandom vector column vector \\(n\\) random variables:\n\\[\n  \\mathbf{X} = [X_1, X_2, \\dots, X_n]^T,\n\\]\nsuperscript \\(T\\) means ‘transpose’.\n \\(X_i\\) random variable, together form \\(n\\)-dimensional random vector.\nbivariate case, example, write\n\\[\n   \\mathbf{X}\n   = \\begin{bmatrix}\n       X_1 \\\\\n       X_2\n     \\end{bmatrix}\n\\]\n(column) vector \\(\\mathbf{} = \\begin{bmatrix} a_1 \\\\ a_2 \\end{bmatrix}\\).","code":""},{"path":"ChapMultivariate.html","id":"MultivariateProbFn","chapter":"10 Multivariate distributions*","heading":"10.4 Joint probability functions","text":"joint probability function \\(\\mathbf{X}\\) describes probability distribution \\(n\\) random variables simultaneously.Definition 10.1  (Joint probability function (discrete)) Let \\(\\mathbf{X} = (X_1, \\dots, X_n)\\) \\(n\\)-dimensional discrete random variable.\nrandom variable \\(X_j\\) (\\(j = 1, \\dots, n\\)) takes values set \\(S_j\\) (range)., range space random vector \n\\[\n  \\mathcal{X} \\subseteq S_1\\times \\cdots \\times S_n.\n\\]\n, joint probability mass function \n\\[\n  p_{X_1, \\dots, X_n}(x_1, \\dots, x_n)\n  = \\Pr(X_1 = x_1,\\, \\dots,\\, X_n = x_n) \\quad \\text{$(x_1, \\dots, x_n) \\\\mathcal{X}$},\n\\]\n\n\\[\n  p_{X_1, \\dots, X_n}(x_1, \\dots, x_n) \\geq 0\n  \\quad \\text{$(x_1, \\dots, x_n) \\\\mathcal{X}$},\n\\]\n\n\\[\n  \\sum_{(x_1, \\dots, x_n) \\\\mathcal{X}} p_{X_1, \\dots, X_n}(x_1, \\dots, x_n) = 1.\n\\]Example 10.1  (Three dice) Consider random vector \\(\\mathbf{X} = (X_1, X_2, X_3)\\) representing outcome rolling three fair six-sided dice.Since \\(X_j \\\\{1,2,3,4,5,6\\}\\), range space \n\\[\n  \\mathcal{X} = \\{1,2,3,4,5,6\\} \\times \\{1,2,3,4,5,6\\} \\times \\{1,2,3,4,5,6\\}.\n\\]\nAssuming dice independent, joint probabuility mass function \n\\[\\begin{align*}\n  p_{X_1, X_2, X_3}(x_1, x_2, x_3)\n  &=\n  P(X_1 = x_1, X_2 = x_2, X_3 = x_3)\\\\\n  &= \\frac{1}{6^3} = \\frac{1}{216}\n\\end{align*}\\]\n\\((x_1, x_2, x_3) \\\\mathcal{X}\\).instance, probability sum three dice equals \\(10\\) \n\\[\n  \\Pr(X_1 + X_2 + X_3 = 10)\n  = \\sum_{\\substack{(x_1, x_2, x_3) \\\\mathcal{X}\\\\ x_1 + x_2 + x_3 = 10}} p(x_1,x_2,x_3).\n\\]obtain answer R, use:case continuous random variables, definition similar.Definition 10.2  (Joint probability function (continuous)) Let \\(\\mathbf{X} = (X_1, \\dots, X_n)\\) \\(n\\)-dimensional continuous random vector.\n\\(j = 1, \\dots, n\\), random variable \\(X_j\\) takes values subset \\(S_j \\subseteq \\mathbb{R}\\).range space vector therefore\n\\[\n  \\mathcal{X} \\subseteq S_1 \\times \\cdots \\times S_n \\subseteq \\mathbb{R}^n.\n\\]\njoint probability density function \\(\\mathbf{X}\\) \n\\[\n  f_{X_1, \\dots, X_n}(x_1, \\dots, x_n), \\quad \\text{$(x_1, \\dots, x_n) \\\\mathcal{X}$},\n\\]\n\n\\[\n  f_{X_1, \\dots, X_n}(x_1, \\dots, x_n) \\geq 0\n  \\quad \\text{$(x_1, \\dots, x_n) \\\\mathcal{X}$},\n\\]\n\n\\[\n  \\int_{\\mathcal{X}} f_{X_1, \\dots, X_n}(x_1, \\dots, x_n)\\, dx_1 \\cdots dx_n = 1.\n\\]Example 10.2  (Three-dimensional uniform example (continuous)) Consider random vector \\(\\mathbf{X} = (X_1, X_2, X_3)\\), uniformly distributed unit cube \\([0, 1]^3\\).\nrange space \n\\[\n  \\mathcal{X} = [0, 1] \\times [0, 1] \\times [0, 1] \\subset \\mathbb{R}^3.\n\\]joint probability density function :\n\\[\n  f_{X_1, X_2, X_3}(x_1,x_2,x_3) =\n  \\begin{cases}\n    1 & \\text{$0 \\le x_j \\le 1$ $j = 1, 2, 3$},\\\\\n    0 & \\text{otherwise}.\n  \\end{cases}\n\\]marginal distributions uniform \\([0, 1]\\); example:\n\\[\n  f_{X_1}(x_1) = \\int_0^1 \\int_0^1 f(x_1, x_2, x_3)\\, dx_2\\, dx_3 = 1, \\quad 0\\le x_1 \\le 1.\n\\]instance, probability sum three variables less equal  \\(1\\) \n\\[\n  P(X_1 + X_2 + X_3 \\le 1)\n  = \\text{volume tetrahedron } \\{(x_1,x_2,x_3)\\\\mathcal{X}: x_1 + x_2 + x_3 \\le 1\\}\n  = \\frac{1}{6}.\n\\]answer can obtained using integration, knowledge volumes, using simulation R:","code":"\n# List all possible outcomes of rolling three dice\ndice_Outcomes <- expand.grid(x1 = 1:6, \n                             x2 = 1:6, \n                             x3 = 1:6)\n\n# Show the first few rows (outcomes)\nprint(head(dice_Outcomes))\n#>   x1 x2 x3\n#> 1  1  1  1\n#> 2  2  1  1\n#> 3  3  1  1\n#> 4  4  1  1\n#> 5  5  1  1\n#> 6  6  1  1\n\n# Find where the rows sum to 10\nfavourable_Outcome <- subset(dice_Outcomes, \n                             x1 + x2 + x3 == 10)\n\n# Probability\nprob <- nrow(favourable_Outcome) / nrow(dice_Outcomes)\nprob\n#> [1] 0.125\nset.seed(30991) # For reproducibility\n\n# Generate 1 000 000 random points in each dimension of the cube\nN <- 1e6\nX <- matrix(runif(3 * N), \n            ncol = 3)  # N rows of (x1,x2,x3)\nprint(head(X))\n#>            [,1]        [,2]      [,3]\n#> [1,] 0.01910142 0.876909934 0.1347004\n#> [2,] 0.41801207 0.146094920 0.5405445\n#> [3,] 0.86790140 0.008614253 0.6262534\n#> [4,] 0.20781771 0.648519075 0.2852823\n#> [5,] 0.07213365 0.119805842 0.6374308\n#> [6,] 0.70971854 0.642130898 0.1407279\n\n# How many of these random points satisfy: sum less than one?\nprob_Est <- mean(rowSums(X) <= 1)\nprob_Est\n#> [1] 0.166457"},{"path":"ChapMultivariate.html","id":"MultivariateDistFn","chapter":"10 Multivariate distributions*","heading":"10.5 Joint distribution functions","text":"multivariate distribution function (CDF) \n\\[\n  F_{X_1, \\dots, X_n}(x_1, \\dots, x_n)\n  = \\Pr(X_1 \\leq x_1, \\dots, X_n \\leq x_n).\n\\](#exm:Dice3D_CDF) (Three dice) Let \\(\\mathbf{X} = (X_1, X_2, X_3)\\) random vector representing outcome rolling three fair six-sided dice, Example 10.1 (joint probability mass function given).joint cumulative distribution function \n\\[\n  F_{X_1, X_2, X_3}(x_1, x_2, x_3)\n  = P(X_1 \\le x_1, X_2 \\le x_2, X_3 \\le x_3)\n  = \\sum_{= 1}^{\\lfloor x_1 \\rfloor}\n    \\sum_{j = 1}^{\\lfloor x_2 \\rfloor} \\sum_{k = 1}^{\\lfloor x_3 \\rfloor} p(, j, k),\n\\]\n\\(\\lfloor x \\rfloor\\) floor function (greatest integer less equal  \\(x\\)).instance, probability dice shows less equal  \\(3\\) \n\\[\n  F_{X_1, \\dots, X_n}(3, 3, 3)\n  = \\sum_{= 1}^{3} \\sum_{j = 1}^{3} \\sum_{k = 1}^{3} \\frac{1}{216}\n  = \\frac{27}{216} = \\frac{1}{8}.\n\\](#exm:UniformCube3D_CDF) (Three-Dimensional Uniform Cube: CDF) Let \\(\\mathbf{X} = (X_1, X_2, X_3)\\) uniformly distributed unit cube \\([0, 1]^3\\), Example 10.2 (joint probability density function given)joint cumulative distribution function \n\\[\\begin{align*}\n  F_{X_1, \\dots, X_n}(x_1, x_2, x_3)\n  &= P(X_1 \\le x_1, X_2 \\le x_2, X_3 \\le x_3)\\\\\n  &= \\int_0^{x_1}\\!\\! \\int_0^{x_2}\\!\\! \\int_0^{x_3} f(u_1, u_2, u_3)\\, du_3\\, du_2\\, du_1,\n\\end{align*}\\]\n\\(0 \\le x_1, x_2, x_3 \\le 1\\).instance, probability variable less  \\(0.5\\) \n\\[\n  Fvs(0.5, 0.5, 0.5) = \\int_0^{0.5}\\!\\! \\int_0^{0.5}\\!\\! \\int_0^{0.5} 1 \\, du_3\\, du_2\\, du_1 = 0.5^3 = 0.125.\n\\]","code":""},{"path":"ChapMultivariate.html","id":"marginal-and-conditional-distributions","chapter":"10 Multivariate distributions*","heading":"10.6 Marginal and conditional distributions","text":"Marginal distributions obtained summing integrating unwanted variables. example,\n\\[\n  f_{X_1}(x_1) = \\int_{\\mathbb{R}^{n-1}} f(x_1, x_2, \\dots, x_n)\\, dx_2 \\cdots dx_n.\n\\]Conditional distributions defined natural way:\n\\[\n  f_{X_1 \\mid X_2, \\dots, X_n}(x_1 \\mid x_2, \\dots, x_n) =\n  \\frac{f(x_1, x_2, \\dots, x_n)}{f_{X_2,\\dots,X_n}(x_2, \\dots, x_n)}.\n\\]EXAMPLES","code":""},{"path":"ChapMultivariate.html","id":"MultivariateIndependence","chapter":"10 Multivariate distributions*","heading":"10.7 Multivariate independence","text":"multivariate case, \\(n\\) random variables \\(X_1, \\dots, X_n\\) independent knowing value subset gives information others.\nFormally, let \\(\\mathbf{X} = (X_1, \\dots, X_n)\\) \\(n\\)-dimensional random vector joint probability distribution.discrete case, random variables independent joint probability mass function factors product marginal probability functions:\n\\[\n  p_{X_1, \\dots, X_n}(x_1, \\dots, x_n)\n  = \\prod_{= 1}^{n} p_{X_i}(x_i),\n  \\quad (x_1, \\dots, x_n) \\\\mathcal{X}.\n\\]continuous case similar; random variables independent joint probability density function factors product marginal densities:\n\\[\n  f_{X_1, \\dots, X_n}(x_1, \\dots, x_n)\n  = \\prod_{= 1}^{n} f_{X_i}(x_i),\n  \\quad (x_1, \\dots, x_n) \\\\mathcal{X}.\n\\]Independence can also determined using distribution functions:\n\\[\n  F_{X_1, \\dots, X_n}(x_1, \\dots, x_n) = \\prod_{=1}^{n} F_{X_i}(x_i),\n\\]\n\\(F\\) joint cumulative distribution function.practice, independence allows joint probabilities volumes computed multiplying corresponding marginal probabilities integrating products marginal densities.(#exm:Dice3D_Indep) (Three dice example: independence) Consider outcomes rolling three fair six-sided dice, represented random vector \\(\\mathbf{X} = (X_1, X_2, X_3)\\).\n\\(X_j \\\\{1,2,3,4,5,6\\}\\), \\(\\mathcal{X} = \\{1,\\dots,6\\}^3\\).\ndice independent, joint probability mass function can written product marginal probability functions:\n\\[\n  p_{X_1, X_2, X_3}(x_1, x_2, x_3)\n  = P(X_1 = x_1, X_2 = x_2, X_3 = x_3)\n  = \\prod_{= 1}^3 P(X_i = x_i)\n  = \\frac{1}{6}\\cdot   \\frac{1}{6} \\cdot \\frac{1}{6} = \\frac{1}{216}.\n\\]\nEquivalently, joint distribution factors \n\\[\n  F_{X_1, X_2, X_3}(x_1, x_2, x_3) = P(X_1\\le x_1, X_2 \\le x_2, X_3 \\le x_3)\n  = \\prod_{= 1}^3 F_{X_i}(x_i).\n\\]\nprobability dice show value less equal  \\(3\\) \n\\[\n  F_{X_1, \\dots, X_n}(3, 3, 3)\n  = \\prod_{= 1}^3 F_{X_i}(3)\n  = \\left(\\frac{3}{6}\\right)^3\n  = \\frac{27}{216} = \\frac{1}{8}.\n\\](#exm:UniformCube3D_Indep) (Three-dimensional uniform cube: independence) Let \\(\\mathbf{X} = (X_1, X_2, X_3)\\) uniformly distributed unit cube \\([0, 1]^3\\), joint probability density function\n\\[\n  f_{X_1, X_2, X_3}(x_1, x_2, x_3) = 1, \\quad (x_1, x_2, x_3) \\[0, 1]^3.\n\\]variables \\(X_1, X_2, X_3\\) independent, joint density function can written \n\\[\n  f(x_1, x_2, x_3)\n  = f_{X_1}(x_1)\\cdot f_{X_2}(x_2)\\cdot f_{X_3}(x_3)\n  = 1 \\cdot 1 \\cdot 1.\n\\]Equivalently, joint distribution can written \n\\[\n  F_{X_1, X_2, X_3}(x_1, x_2, x_3)\n  = \\Pr(X_1 \\le x_1, X_2 \\le x_2, X_3 \\le x_3)\n  = \\prod_{= 1}^3 F_{X_i}(x_i).\n\\]example, probability values three variables less  \\(0.5\\) \n\\[\n  F_{X_1, X_2, X_3}(0.5, 0.5, 0.5) = 0.5^3 = 0.125.\n\\]","code":""},{"path":"ChapMultivariate.html","id":"MultivariateExercises","chapter":"10 Multivariate distributions*","heading":"10.8 Exercises","text":"Selected answers appear Sect. E.10.Exercise 10.1  PLACE??Suppose \\(X \\sim \\text{Pois}(\\lambda)\\).\nShow \\(\\lambda\\sim\\text{Gam}(, p/(1 - p) )\\), distribution  \\(X\\) negative binomial distribution.","code":""},{"path":"MultivariateExtensions.html","id":"MultivariateExtensions","chapter":"11 Expectations for multivariate distributions*","heading":"11 Expectations for multivariate distributions*","text":"","code":""},{"path":"MultivariateExtensions.html","id":"MultivariateExpectation","chapter":"11 Expectations for multivariate distributions*","heading":"11.1 Expectation","text":"Results involving expectations naturally generalise bivariate multivariate case.\nFirstly, expectation linear combination random variables.Theorem 11.1  (Expectation linear combinations) \\(X_1, X_2,\\dots, X_n\\) random variables \\(a_1, a_2,\\ldots a_n\\) constants \n\\[\n   \\operatorname{E}\\left[\\sum_{= 1}^n a_i X_i \\right]\n   =\\sum_{= 1}^n a_i \\, \\operatorname{E}[X_i]\n\\]Proof. proof follows directly Theorem 5.8 induction.variance linear combination random variables given following theorem.Theorem 11.2  (Variance linear combination) \\(X_1, X_2, \\dots, X_n\\) random variables \\(a_1, a_2,\\ldots a_n\\) constants \n\\[\n   \\operatorname{var}\\left[\\sum_{= 1}^n a_i X_i \\right]\n   =\n   \\sum^n_{= 1}^2_i\\operatorname{var}[X_i] + 2{\\sum\\sum}_{<j}a_i a_j\\operatorname{Cov}(X_i, X_j).\n\\]Proof. convenience, put \\(Y = \\sum_{= 1}^n a_iX_i\\).\ndefinition variance\n\\[\\begin{align*}\n     \\operatorname{var}[Y]\n     &= \\operatorname{E}\\big[Y - \\operatorname{E}(Y)\\big]^2\\\\\n     &= \\operatorname{E}[a_1 X_1 + \\dots + a_n X_n - a_1\\mu_1 - \\dots a_n\\mu_n]^2\\\\\n     &= \\operatorname{E}[a_1(X_1 - \\mu_1) + \\dots + a_n(X_n - \\mu_n)]^2\\\\\n     &= \\operatorname{E}\\left[ \\sum_i ^2_i(X_i - \\mu_i)^2 + 2\\mathop{\\sum\\sum}_{< j}a_i a_j(X_i - \\mu_i)X_j - \\mu_j)\\right]\\\\\n     &= \\sum_i ^2_i\\operatorname{E}[X_i - \\mu_i]^2 + 2\\mathop{\\sum\\sum}_{< j}a_i a_j\\operatorname{E}[X_i - \\mu_i] (X_j - \\mu_j)\\\\\n        %\\quad \\text{using Theorem\\ \\@ref(thm:ExpLinear)}\\\\\n     &= \\sum_i ^2_i\\sigma^2_i + 2\\mathop{\\sum\\sum}_{<j}a_i a_j\\operatorname{Cov}(X_i, X_j).\n\\end{align*}\\]statistical theory, important special case Theorem 11.2 occurs  \\(X_i\\) independently identically distributed (IID).\n, \\(X_1, X_2, \\dots, X_n\\) distribution independent .\n(see relevance Chap. 12.)\nimportance special case called corollary Theorems 11.1  11.2.Corollary 11.1  (IID rvs) \\(X_1, X_2, \\dots, X_n\\) independently distributed (IID) random variables, mean \\(\\mu\\) variance \\(\\sigma^2\\), \\(a_1, a_2,\\ldots a_n\\) constants, \n\\[\\begin{align*}\n   \\operatorname{E}\\left[\\sum_{= 1}^n a_i X_i \\right]\n   &= \\mu\\sum_{= 1}^n a_i;\\\\\n   \\operatorname{var}\\left[\\sum_{= 1}^n a_i X_i \\right]\n   &= \\sigma^2\\sum^n_{= 1}^2_i.\n\\end{align*}\\]Proof. Exercise!","code":""},{"path":"MultivariateExtensions.html","id":"Vectors","chapter":"11 Expectations for multivariate distributions*","heading":"11.2 Vector formulation","text":"standard rules matrix multiplication, Theorems 11.1  11.2 applied Eq. (11.1) give respectively (check!)\n\\[\\begin{equation}\n   \\operatorname{E}[Y]\n   = \\operatorname{E}[\\mathbf{}^T\\mathbf{X}]\n   = [a_1, a_2]\n   \\begin{bmatrix}\n      \\mu_1 \\\\\n      \\mu_2\n    \\end{bmatrix}\n   = \\mathbf{}^T\\boldsymbol{\\mu}\n\\end{equation}\\]\n\n\\[\\begin{align}\n  \\operatorname{var}[Y]\n  &= \\operatorname{var}[\\mathbf{}^T\\mathbf{X}\\mathbf{}]\\nonumber\\\\\n  &= [a_1, a_2]\n  \\begin{bmatrix}\n     \\sigma^2_1 & \\sigma_{12} \\\\\n     \\sigma_{21}& \\sigma^2_2\n  \\end{bmatrix}\n  \\begin{bmatrix}\n     a_1 \\\\\n     a_2\n  \\end{bmatrix} \\nonumber\\\\\n  &= \\mathbf{}^T\\mathbf{\\Sigma}\\mathbf{}.\n\\end{align}\\]vector formulation results apply directly multivariate case described .\nWrite\n\\[\\begin{align*}\n  \\mathbf{X}\n  &= [X_1, X_2, \\ldots, X_n]^T; \\\\\n     \\operatorname{E}[\\mathbf{X}]\n  &= [\\mu_1, \\ldots, \\mu_n]^T = \\boldsymbol{\\mu}^T; \\\\\n     \\operatorname{var}[\\mathbf{X}]\n  &= \\mathbf{\\Sigma}; \\\\\n     \\mathbf{}^T & = \\left[a_1,a_2,\\ldots, a_n \\right].\n\\end{align*}\\]\nNow Theorems 11.1  11.2 can expressed vector form.Theorem 11.3  (Bivariate mean variance (vector form))  \\(\\mathbf{X}\\) random vector length \\(n\\) mean \\(\\boldsymbol{\\mu}\\) variance \\(\\mathbf{\\Sigma}\\),  \\(\\mathbf{}\\) constant vector length \\(n\\), \n\\[\\begin{align*}\n   \\operatorname{E}[\\mathbf{}^T\\mathbf{X}]\n   &= \\mathbf{}^T\\boldsymbol{\\mu}; \\\\\n   \\operatorname{var}[\\mathbf{}^T\\mathbf{X}]\n   &= \\mathbf{}^T\\mathbf{\\Sigma}\\mathbf{}.\n\\end{align*}\\]Proof. Exercise.elegant statements concerning linear combinations feature vector formulations extend many statistical results theory statistics.\nOne obvious advantage formulation implementation vector-based computer programming used packages R.One result presented (without proof) involving two linear combinations.Theorem 11.4  (Covariance combinations)  \\(\\mathbf{X}\\) random vector length \\(n\\) mean \\(\\boldsymbol{\\mu}\\) variance \\(\\mathbf{\\Sigma}\\),  \\(\\mathbf{}\\)  \\(\\mathbf{b}\\) constant vectors, length \\(n\\), \n\\[\n   \\operatorname{Cov}(\\mathbf{}^T\\mathbf{X},\\mathbf{b}^T\\mathbf{X})\n   =\n   \\mathbf{}^T\\mathbf{\\Sigma}\\mathbf{b}.\n\\]Example 11.1  (Expectations using vectors) Suppose random variables \\(X_1, X_2, X_3\\) respective means \\(1\\), \\(2\\),  \\(3\\), respective variances \\(4\\), \\(5\\),  \\(6\\), covariances \\(\\operatorname{Cov}(X_1, X_2) = -1\\), \\(\\operatorname{Cov}(X_1, X_3) = 1\\) \\(\\operatorname{Cov}(X_2, X_3) = 0\\).Consider random variables \\(Y_1 = 3X_1 + 2X_2 - X_3\\) \\(Y_2 = X_3 - X_1\\).\nDetermine \\(\\operatorname{E}[Y_1]\\), \\(\\operatorname{E}[Y_2]\\), \\(\\operatorname{var}[Y_1]\\), \\(\\operatorname{var}[Y_2]\\) \\(\\operatorname{Cov}(Y_1,Y_2)\\).vector formulation problem allows us use Theorems 11.3  11.4 directly.\nPutting \\(\\mathbf{}^T = [3, 2, -1]\\) \\(\\mathbf{b}^T = [-1, 0, 1]\\):\n\\[\n   Y_1 = \\mathbf{}^T\\mathbf{X}\n   \\quad\\text{}\\quad\n   Y_2 = \\mathbf{b}^T\\mathbf{X}\n\\]\n\\(\\mathbf{X}^T = [X_1, X_2, X_3]\\).\nAlso define \\(\\boldsymbol{\\mu}^T = [1, 2, 3]\\) \n\\(\\mathbf{\\Sigma} =\n\\begin{bmatrix}\n  4 & -1 & 1\\\\\n-1 & 5 & 0\\\\\n  1 & 0 & 6\n\\end{bmatrix}\\)\nmean variance–covariance matrix respectively \\(\\mathbf{X}\\).\n\n\\[\n   \\operatorname{E}[Y_1]\n   = \\mathbf{}^T\\boldsymbol{\\mu}\n   = [3, 2, -1]\n   \\begin{bmatrix}\n      1\\\\\n      2\\\\\n      3\n      \\end{bmatrix}\n   = 4\n\\]\n\n\\[\n   \\operatorname{var}[Y_1]\n   = \\mathbf{}^T\\mathbf{\\Sigma}\\mathbf{}\n   = [3, 2, -1]\n   \\begin{bmatrix}\n      4 & -1 & 1\\\\\n      -1 & 5 & 0\\\\\n       1 & 0 & 6\n    \\end{bmatrix}\n    \\begin{bmatrix}\n      3\\\\\n      2\\\\\n      -1\n   \\end{bmatrix}\n   = 44.\n\\]\nSimilarly \\(\\operatorname{E}[Y_2] = 2\\) \\(\\operatorname{var}[Y_2] = 8\\).\nFinally:\n\\[\n   \\operatorname{Cov}(Y_1, Y_2)\n   = \\mathbf{}^T\\mathbf{\\Sigma}\\mathbf{b}\n   = [3, 2, -1]^T\n   \\begin{bmatrix}\n      4 & -1 & 1\\\\\n      -1 & 5 & 0\\\\\n      1 & 0 & 6\n      \\end{bmatrix}\n      \\begin{bmatrix}\n        -1\\\\\n        0\\\\\n        1\n      \\end{bmatrix}\n  = -12.\n\\]","code":""},{"path":"MultivariateExtensions.html","id":"expectation-and-covariance","chapter":"11 Expectations for multivariate distributions*","heading":"11.3 Expectation and covariance","text":"Consider random vector \\(n\\) random variables\n\\[\n  \\mathbf{X} = [X_1, X_2, \\dots, X_n]^T.\n\\]\nmean vector \\(\\mathbf{X}\\) \n\\[\n  \\boldsymbol{\\mu}\n  = E[\\mathbf{X}]\n  =\n  \\begin{bmatrix}\n    E[X_1] \\\\\n    \\vdots \\\\\n    E[X_n]\n  \\end{bmatrix}.\n\\]covariance matrix \n\\[\n  \\Sigma = \\text{Cov}(\\mathbf{X})\n  = E\\!\\left[(\\mathbf{X} - \\boldsymbol{\\mu})(\\mathbf{X} - \\boldsymbol{\\mu})^T\\right].\n\\]entries \\(\\Sigma_{ij} = \\text{Cov}(X_i, X_j)\\).\ndiagonal entries variances, -diagonal entries covariances.correlation matrix obtained normalisation:\n\\[\n  \\rho_{ij} = \\frac{\\Sigma_{ij}}{\\sqrt{\\Sigma_{ii}\\,\\Sigma_{jj}}}.\n\\]","code":""},{"path":"MultivariateExtensions.html","id":"independence","chapter":"11 Expectations for multivariate distributions*","heading":"11.4 Independence","text":"components \\(X_1, \\dots, X_n\\) independent \n\\[\n  f(x_1, \\dots, x_n) = \\prod_{= 1}^n f_{X_i}(x_i).\n\\]Linear combinations random variables elegantly dealt using methods notation vectors matrices, especially dimension grows beyond bivariate case.\nbivariate case define\n\\[\\begin{align}\n   \\mathbf{X}\n   &= \\begin{bmatrix}\n       X_1 \\\\\n       X_2\n      \\end{bmatrix};\n  \\label{EQN:matrix1} \\\\\n  \\operatorname{E}[\\mathbf{X}]\n  & =\n   \\operatorname{E}\n   \\left[\n   \\begin{bmatrix}\n      X_1 \\\\\n      X_2\n   \\end{bmatrix}\n   \\right]\n   =\n   \\begin{bmatrix}\n      \\operatorname{E}[X_1] \\\\\n      \\operatorname{E}[X_2]\n   \\end{bmatrix}\n   =  \n   \\begin{bmatrix}\n     \\mu_1 \\\\\n     \\mu_2\n   \\end{bmatrix} =\n  \\boldsymbol{\\mu};\\label{EQN:matrix2} \\\\\n  \\operatorname{var}[\\mathbf{X}]\n  & = \\operatorname{var}\n  \\left[\n  \\begin{bmatrix}\n     X_1 \\\\\n     X_2\n  \\end{bmatrix}\n  \\right]\n  =\n  \\begin{bmatrix}\n     \\sigma^2_1  & \\sigma_{12} \\\\\n     \\sigma_{21} & \\sigma^2_2\n  \\end{bmatrix}\n  = \\mathbf{\\Sigma}.\\label{EQN:matrix3}\n\\end{align}\\]vector \\(\\boldsymbol{\\mu}\\) mean vector, matrix \\(\\mathbf{\\Sigma}\\) called variance–covariance matrix, square symmetric since \\(\\sigma_{12} = \\sigma_{21}\\) (covariance).linear combination \\(Y = a_1 X_1 + a_2 X_2\\) can expressed \n\\[\\begin{equation}\n   Y\n   = a_1 X_1 + a_2 X_2\n   = [a_1, a_2]\n   \\begin{bmatrix}\n      X_1 \\\\\n      X_2\n   \\end{bmatrix}\n   = \\mathbf{}^T\\mathbf{X}\n   \\tag{11.1}\n\\end{equation}\\]\n(column) vector \\(\\mathbf{} = \\begin{bmatrix} a_1 \\\\ a_2 \\end{bmatrix}\\), superscript \\(T\\) means ‘transpose’.","code":""},{"path":"MultivariateExtensions.html","id":"IndependentRVs","chapter":"11 Expectations for multivariate distributions*","heading":"11.5 Independent random variables","text":"Recall events \\(\\)  \\(B\\) independent , ,\n\\[\n   something nmissing!\n\\]","code":""},{"path":"MultivariateExtensions.html","id":"SimulationBivariate","chapter":"11 Expectations for multivariate distributions*","heading":"11.6 Simulation","text":"univariate distributions (Sects. 7.11  8.10), simulation can used bivariate distributions.\nRandom numbers bivariate normal distribution (Sect. 8.8) generated using function dmnorm() library mnorm.\nRandom numbers multinomial distribution (Sect. 7.9) generated using function rmultinom().\ncommonly, univariate distributions combined.Monthly rainfall, example, commonly modelled using gamma distributions (example, Husak, Michaelsen, Funk (2007)).\nSimulating rainfall used others models (cropping simulations; example Ines Hansen (2006)).\nexample, consider location monthly rainfall well-modelled gamma distribution shape parameter \\(\\alpha = 1.6\\) scale parameter \\(\\beta = 220\\) (Fig. 11.1, left panel):percentage months rainfall exceeding \\(1000\\,\\text{mm}\\) also computed.\nHowever, now suppose shape parameter \\(\\alpha\\) also varies, exponential distribution mean \\(2\\) (Fig. 11.1, centre panel):Using simulation, also easy simulate impact scale parameter \\(\\beta\\) varying also, suppose normal distribution mean  \\(200\\) variance  \\(16\\) (Fig. 11.1, right panel):\nFIGURE 11.1: Three simulations using gamma distribution.\n","code":"\nlibrary(\"mnorm\") # Need to explicitly load  mnorm  library\nMRain <- rgamma( 2000,\n                 shape = 1.6,\n                 scale = 220)\n\ncat(\"Rainfall exceeding 900mm:\",\n    sum(MRain > 900) / 2000 * 100, \"%\\n\")\n#> Rainfall exceeding 900mm: 4.95 %\n# Directly:\nround( (1 - pgamma(900, shape = 1.6, scale = 220) ) * 100, 2)\n#> [1] 4.95\nMRain2 <- rgamma( 1000,\n                  shape = rexp(1000, rate = 1/2),\n                  scale = 220)\n\ncat(\"Rainfall exceeding 900mm:\",\n    sum(MRain2 > 900) / 1000 * 100, \"%\\n\")\n#> Rainfall exceeding 900mm: 13.3 %\nMRain3 <- rgamma( 1000,\n                  shape = rexp(1000, rate = 1/2),\n                  scale = rnorm(1000, mean = 200, sd = 4))\n\ncat(\"Rainfall exceeding 900:\",\n    sum(MRain3 > 900) / 1000 * 100, \"%\\n\")\n#> Rainfall exceeding 900: 11.4 %"},{"path":"MultivariateExtensions.html","id":"MultivariateExpectationExercises","chapter":"11 Expectations for multivariate distributions*","heading":"11.7 Exercises","text":"Selected answers appear Sect. E.11.Exercise 11.1  Suppose \\(X_1, X_2, \\dots, X_n\\) independently distributed random variables, mean \\(\\mu\\) variance \\(\\sigma^2\\).\nDefine sample mean \\(\\overline{X} = \\left( \\sum_{=1}^n X_i\\right)/n\\).Prove \\(\\operatorname{E}[\\overline{X}] = \\mu\\).Find variance  \\(\\overline{X}\\).Exercise 11.2  LONGER NEEDED:Poisson-gamma distributions (e.g., see Hasan Dunn (2010b)), used modelling rainfall, can developed follows:Let \\(N\\) number rainfall events month, \\(N\\sim \\text{Pois}(\\lambda)\\).\nrainfall events recorded month, monthly rainfall \\(Z = 0\\).rainfall event (, \\(N > 0\\)), say \\(\\) \\(= 1, 2, \\dots N\\), amount rain event \\(\\), say \\(Y_i\\), modelled using gamma distribution parameter \\(\\alpha\\)  \\(\\beta\\).total monthly rainfall \\(Z = \\sum_{= 1}^N Y_i\\).Suppose monthly rainfall station particular station can modelled using \\(\\lambda = 0.78\\), \\(\\alpha = 0.5\\) \\(\\beta = 6\\).Use simulation produce one-month rainfall total using model.Repeat \\(1\\,000\\) simulations (.e., simulate \\(1\\,000\\) months), plot distribution.type random variable monthly rainfall: discrete, continuous, mixed?\nExplain.Based \\(1\\,000\\) simulations, approximately often month exactly zero rainfall?Based \\(1\\,000\\) simulations, mean monthly rainfall?Based \\(1\\,000\\) simulations, mean monthly rainfall months rain recorded?Exercise 11.3  \\(X\\), \\(Y\\)  \\(Z\\) uncorrelated random variables expected values \\(\\mu_x\\), \\(\\mu_y\\)  \\(\\mu_z\\) standard deviations \\(\\sigma_x\\), \\(\\sigma_y\\)  \\(\\sigma_z\\).\n\\(U\\)  \\(V\\) defined \n\\[\\begin{align*}\n   U&= X - Z;\\\\\n   V&= X - 2Y + Z.\n\\end{align*}\\]Find expected values  \\(U\\)  \\(V\\).Find variance  \\(U\\)  \\(V\\).Find covariance  \\(U\\)  \\(V\\).conditions  \\(\\sigma_x\\), \\(\\sigma_y\\)  \\(\\sigma_z\\)  \\(U\\)  \\(V\\) uncorrelated?Exercise 11.4  Suppose \\((X, Y)\\) joint probability function given \n\\[\n  \\Pr(X = x, Y = y) = \\frac{|x - y|}{11}\n\\]\n\\(x = 0, 1, 2\\) \\(y = 1, 2, 3\\).Find \\(\\operatorname{E}[X \\mid Y = 2]\\).Find \\(\\operatorname{E}[Y \\mid X\\ge 1]\\).Exercise 11.5  PDF \\((X, Y)\\) given \n\\[\n   f_{X, Y}(x, y) = 1 - \\alpha(1 - 2x)(1 - 2y),\n\\]\n\\(0 \\le x \\le 1\\), \\(0 \\le y \\le 1\\) \\(-1 \\le \\alpha \\le 1\\).Find marginal distributions  \\(X\\)  \\(Y\\).Evaluate correlation coefficient \\(\\rho_{XY}\\).value  \\(\\alpha\\)  \\(X\\)  \\(Y\\) independent?Find \\(\\Pr(X < Y)\\).Exercise 11.6  random variables \\(X_1\\), \\(X_2\\), \\(X_3\\) y\\(X_1\\): mean \\(\\mu_1 = 5\\) standard deviation \\(\\sigma_1 = 2\\).\\(X_2\\): mean \\(\\mu_2 = 3\\) standard deviation \\(\\sigma_2 = 3\\).\\(X_3\\): mean \\(\\mu_3 = 6\\) standard deviation \\(\\sigma_3 = 4\\).correlations : \\(\\rho_{12} = -\\frac{1}{6}\\), \\(\\rho_{13} = \\frac{1}{6}\\) \\(\\rho_{23} = \\frac{1}{2}\\).random variables \\(U\\)  \\(V\\) defined \\(U = 2X_1 + X_2 - X_3\\) \\(V = X_1  - 2X_2 - X_3\\), find\\(\\operatorname{E}[U]\\).\\(\\operatorname{var}[U]\\).\\(\\operatorname{Cov}(U, V)\\).Exercise 11.7  Let \\(X\\)  \\(Y\\) body mass index (BMI) percentage body fat netball players attending AIS.\nAssume \\(X\\)  \\(Y\\) bivariate normal distribution \\(\\mu_X = 23\\), \\(\\mu_Y = 21\\), \\(\\sigma_X = 3\\), \\(\\sigma_Y = 6\\) \\(\\rho_{XY} = 0.8\\).\nFindthe expected BMI netball player percent body fat  \\(30\\).expected percentage body fat netball player BMI  \\(19\\).Exercise 11.8  Let \\(X\\)  \\(Y\\) bivariate normal distribution parameters \\(\\mu_x = 1\\), \\(\\mu_y = 4\\), \\(\\sigma^2_x = 4\\), \\(\\sigma^2_y = 9\\) \\(\\rho = 0.6\\).\nFind\\(\\Pr(-1.5 < X < 2.5)\\).\\(\\Pr(-1.5 < X < 2.5 \\mid Y = 3)\\).\\(\\Pr(0 < Y < 8)\\).\\(\\Pr(0 < Y < 8 \\mid X = 0)\\).Exercise 11.9  Consider \\(n\\) random variables \\(Z_i\\) \\(Z_i \\sim \\text{Exp}(\\beta)\\) every \\(= 1, \\dots, n\\).\nShow distribution \\(Z_1 + Z_2 + \\cdots + Z_n\\) gamma distribution \\(\\text{Gam}(n, \\beta)\\), determine parameters gamma distribution.\n(Hint: See Theorem 5.6.)Exercise 11.10  Daniel S. Wilks (1995b) (p. 101) states maximum daily temperatures measured Ithaca (\\(\\)) Canandaigua (\\(C\\)) January 1987 symmetrical.\nalso says two temperatures modelled bivariate normal\ndistribution \\(\\mu_I = 29.87\\), \\(\\mu_C = 31.77\\), \\(\\sigma_I = 7.71\\), \\(\\sigma_C = 7.86\\) \\(\\rho_{IC} = 0.957\\).\n(measurements degrees Fahrenheit.)Explain, context, correlation coefficient  \\(0.957\\) means.Determine marginal distributions  \\(C\\)  \\(\\).Find conditional distribution \\(C\\mid \\).Plot PDF Canandaigua maximum temperature.Plot conditional PDF Canandaigua maximum temperature given maximum temperature Ithaca  \\(25^\\circ\\)F.Comment differences two PDFs plotted .Find \\(\\Pr(C < 32 \\mid = 25)\\).Find \\(\\Pr(C < 32)\\).Comment differences last two answers.temperature measured degrees Celsius instead degrees Fahrenheit, value  \\(\\rho_{IC}\\) change?Exercise 11.11  discrete random variables \\(X\\)  \\(Y\\) joint probability distribution shown following table:Determine marginal distribution  \\(X\\).Calculate \\(\\Pr(X \\ne Y)\\).Calculate \\(\\Pr(X + Y = 2 \\mid X = Y)\\). \\(X\\)  \\(Y\\) independent?\nJustify answer.Calculate correlation  \\(X\\)  \\(Y\\); .e., compute \\(\\text{Cor}(X,Y)\\).Exercise 11.12  Suppose \\(X\\)  \\(Y\\) joint PDF\n\\[\n   f_{X, Y}(x, y) = \\frac{2 + x + y}{8} \\quad\\text{$-1 < x < 1$ $-1 < y < 1$}.\n\\]Sketch distribution using R.Determine marginal PDF  \\(X\\). \\(X\\)  \\(Y\\) independent?\nGive reasons.Determine \\(\\Pr(X > 0, Y > 0)\\).Determine \\(\\Pr(X \\ge 0, Y \\ge 0, X + Y \\le 1)\\).Determine \\(\\operatorname{E}[XY]\\).Determine \\(\\operatorname{var}[Y]\\).Determine \\(\\operatorname{Cov}(X, Y)\\).Exercise 11.13  Historically, final marks certain course approximately normally distributed mean  \\(64\\) standard deviation \\(8\\).\nfifteen students completing course, probability \\(2\\) obtain HDs, \\(3\\) Distinctions, \\(4\\) Credits, \\(5\\) Passes \\(2\\) Fails?Exercise 11.14  Let \\(X_1, X_2, X_3, \\dots, X_n\\) denote independently identically distributed PDF\n\\[\n   f_X(x) = 4x^3\\quad \\text{$0 < x < 1$}.\n\\]Write expression joint PDF distribution \\(X_1, X_2, X_3, \\dots, X_n\\).Determine probability first observation \\(X_1\\) less  \\(0.5\\).Determine probability observations less  \\(0.5\\).Use result deduce probability largest observation less  \\(0.5\\).Exercise 11.15  Let \\(X\\)  \\(Y\\) bivariate normal distribution \\(\\operatorname{E}[X] = 5\\), \\(\\operatorname{E}[Y] = -2\\), \\(\\operatorname{var}[X] = 4\\), \\(\\operatorname{var}[Y] = 9\\), \\(\\operatorname{Cov}(X, Y) = -3\\).\nDetermine joint distribution \\(U = 3X + 4Y\\) \\(V = 5X - 6Y\\).Exercise 11.16  Two fair dice rolled.\nLet \\(X\\)  \\(Y\\) denote, respectively, maximum minimum numbers spots showing two dice.Construct table enumerates sample space.Determine \\(E(Y\\mid X = 4)\\) \\(1\\le x\\le 6\\).Simulate experiment using R.\nCompare simulated results theoretical results found .Exercise 11.17  Let \\(X\\) random variable \\(\\operatorname{E}[X] = \\mu\\) \\(\\operatorname{var}[X] = \\sigma^2\\), let \\(c\\) arbitrary constant.Show \n\\[\n   \\operatorname{E}[(X - c)^2] = (\\mu - c)^2 + \\sigma^2.\n\\]result tell us possible size \\(\\operatorname{E}[(X - c)^2]\\)?Exercise 11.18  Sect. 11.6, simulation used monthly rainfall.\nDaily rainfall, however, difficult model days exactly zero rainfall, days continuous amount rainfalls; .e., daily monthly mixed random variable (Sect. 3.2.3).One way model daily rainfall use two-step process (Chandler Wheater 2002).\nFirstly, model whether day records rainfall (using binomial distribution): occurrence model.\n, days rain falls, model amount rainfall using gamma distribution: amounts model.Use information model daily rainfall one year (\\(365\\) days), location probability wet day  \\(0.32\\), amount rainfall wet days follows gamma distribution \\(\\alpha = 2\\) \\(\\beta = 20\\).\nProduce histogram distribution annual rainfall \\(1\\,000\\) simulations.Suppose probability rainfall, \\(p\\), depends day year, :\n\\[\np =  \\left\\{1 + \\cos[ 2\\pi\\times(\\text{day year})/365]\\right\\} / 2.2.\n\\]\nPlot change  \\(p\\) day year.Revise first model using value  \\(p\\).\nProduce histogram distribution annual rainfall \\(1\\,000\\) simulations.Revise initial model (probability rain Day 1 still \\(0.32\\)), value  \\(p\\) depends happened day :\nday \\(\\) receives rain, probability following day receives rain \\(p = 0.55\\); day \\(\\) receive rain, probability following day receives rain just \\(p = 0.15\\).\nProduce histogram distribution annual rainfall \\(1\\,000\\) simulations.Exercise 11.19  company produces \\(500\\,\\text{g}\\) packet ‘trail mix’ includes certain weight nuts \\(X\\), dried fruit \\(Y\\), seeds \\(Z\\).\nactually weights ingredient vary randomly, depending seasonality availability.Explain really just two variables problem.Suppose weight nuts plus dried fruit must  \\(300\\,\\text{g}\\) \\(400\\,\\text{g}\\).\nDraw sample space.addition, suppose weight nuts must least \\(100\\,\\text{g}\\), weight dried fruit must least \\(100\\,\\text{g}\\).\nDraw sample space.conditions, assume weights ingredients used trail mix uniformly distributed.\nDetermine probability function.Exercise 11.20  Mixed nuts come \\(250\\,\\text{g}\\) packets, comprise walnuts, almonds peanuts.\nactual weights ingredient vary randomly, depending seasonality availability.Explain really just two variables problem.peanuts cheapest ingredient, guidelines state weight peanuts must exceed \\(100\\,\\text{g}\\).\nDraw sample space.addition, suppose weight almonds must least \\(25\\,\\text{g}\\), weight walnuts must least \\(25\\,\\text{g}\\).\nDraw sample space.conditions, assume weights ingredients used mixed nuts packets uniformly distributed.\nDetermine probability function.Exercise 11.21  Suppose \\(X\\) concentration biomarker (mmol.L\\(-1\\)),  \\(Y\\) disease indicator \\(Y = 1\\) refers patient disease \\(Y = 0\\) refers patient without disease.\nprevalence disease population \n\\[\n  \\Pr(Y = 1) = 0.10, \\quad \\Pr(Y = 0) = 0.90.\n\\]\nConditional disease status, biomarker concentrations follow different ??????normal?????? distributions.\n\\(Y = 1\\):\\[\n  X \\mid Y = 1 \\sim \\mathcal{N}(\\mu_1=8, \\, \\sigma_1^2 = 1^2),\n\\]\n\\(Y = 0\\):\n\\[\n  X \\mid Y = 0 \\sim \\mathcal{N}(\\mu_0=5, \\, \\sigma_0^2 = 1^2).\n\\]\n1. Write joint density–mass function \\(f_{X, Y}(x, y)\\).\n2. Derive marginal density \\(X\\), \\(f_X(x)\\).\n3. Compute \\(\\Pr(Y = 1 \\mid X = 7)\\) (posterior probability disease given biomarker level \\(7\\)).\n4. Interpret probability plain language.Exercise 11.22  machine component can fail one three reasons:mechanical failure (\\(Y = 1\\)), probability \\(0.5\\);electrical failure (\\(Y = 2\\)), probability \\(0.3\\);thermal failure (\\(Y = 3\\)), probability \\(0.2\\).random variable \\(X\\) denotes time failure (hundreds hours).\nConditional knowing mode failure, time--failure distribution\n\\[\n  f_{X\\mid Y}(x\\mid Y = y)\n  =  \\lambda_y \\exp(-\\lambda_y x)\n    \\quad\\text{$x > 0$}\n\\]\n\\(\\lambda_y = 1\\) failure mechanical;\\(\\lambda_y = 0.5\\) failure electrical; \\(\\lambda_y = 0.25\\) failure thermal.Write joint density–mass function \\(f_{X, Y}(x, y)\\).Find marginal density time failure \\(f_X(x)\\).Compute \\(\\Pr(Y = 1 \\mid X \\leq 2)\\), probability failure mode mechanical given component failed within \\(200\\) hours.Interpret result words.","code":""},{"path":"SamplingDistributions.html","id":"SamplingDistributions","chapter":"12 Describing samples","heading":"12 Describing samples","text":"use appropriate techniques determine sampling distributions  \\(t\\), \\(F\\),  \\(\\chi^2\\) distributions.explain distributions related normal distribution.apply concepts Central Limit Theorem appropriate circumstances.use Central Limit Theorem approximate binomial probabilities normal probabilities appropriate circumstances.","code":""},{"path":"SamplingDistributions.html","id":"SamplingIntro","chapter":"12 Describing samples","heading":"12.1 Introduction","text":"chapter introduces application distribution theory discipline statistics.\nRecall (Sect. 1.1): distribution theory describing random variables using probability, statistics data collection extracting information data.","code":""},{"path":"SamplingDistributions.html","id":"from-theoretical-distributions-to-practical-observations","chapter":"12 Describing samples","heading":"12.2 From theoretical distributions to practical observations","text":"now, results concerned theoretical probability distributions (.e., distribution theory) describe ideal distributions infinite populations.\npractice, course, infinite populations, observe finite (usually relatively small) samples unknown distributions.Example 12.1  Suppose population interest female residents currently living local nursing home.\n(Usually, populations much broader .)\nheights \\(X\\) residents may described theoretically , example, \\(X\\sim N(170, 3^2)\\), actual heights \\(N = 86\\) female residents one value theoretical distribution, rounded (convenience) nearest centimetre.example, may find heights : \\(160\\,\\text{cm}\\); \\(173\\,\\text{cm}\\); \\(169\\,\\text{cm}\\); \\(\\ldots\\); \\(171\\,\\text{cm}\\).heights (theory) continuous, observe rounded measurements, must discrete values.\nUsing values , expected value (population mean) height can found \n\\[\n  \\operatorname{E}[X] = \\mu = \\sum_{= 1}^{86} p_X(x_i) \\times x_i\n\\]\nusing formula discrete random variable (using Def. 5.1).\nSince population contains \\(N = 86\\) equally-important observations, expected value \n\\[\n  \\operatorname{E}[X] = \\mu = \\sum_{= 1}^{86} \\frac{1}{86} \\times x_i.\n\\]\nUsing idea generally, population mean can computed \n\\[\n  \\mu = \\frac{1}{N} \\sum_{= 1}^N x_i.\n\\]\nSimilarly, variance population can found (Def. 5.3):\n\\[\n  \\sigma^2 = \\frac{1}{N} \\sum_{= 1}^N (x_i - \\mu)^2.\n\\]","code":""},{"path":"SamplingDistributions.html","id":"estimating-the-population-mean","chapter":"12 Describing samples","heading":"12.2.1 Estimating the population mean","text":"\nvalues population available almost never possible, instead study sample size \\(n\\) probably-unknown distribution: \\(x_1, x_2, \\ldots, x_n\\).\nGiven formula computing value population mean, sensible approach computing value sample seem using\n\\[\n  \\bar{X} = \\frac{1}{n}(X_1 + X_2 + \\cdots X_n) = \\frac{1}{n}\\sum_{=1}^n X_i.\n\\]\nturns ‘good’ estimate  \\(\\mu\\), sense unbiased estimate  \\(\\mu\\).Definition 12.1  (Unbiased estimator) \nparameter \\(\\theta\\), estimate \\(\\hat{\\theta}\\) called unbiased estimator  \\(\\theta\\) \n\\[\n  \\operatorname{E}[\\hat{\\theta}] = \\theta.\n\\]means unbiased estimator produces estimates , average, correct (population) value estimating.\nseems sensible property ‘good’ estimator.\n(properties ‘good’ estimators , explore .)\nsay  \\(\\bar{X}\\) unbiased estimator  \\(\\mu\\) , mean \\(\\operatorname{E}[\\bar{X}] = \\mu\\),Theorem 12.1  (unbiased estimator $\\mu$) sample mean\n\\[\n  \\bar{X} = \\frac{1}{n} \\sum_{= 1}^n X_i\n\\]\nunbiased estimator population mean \\(\\mu\\).Proof. Using definition unbiased estimator:\n\\[\\begin{align*}\n  \\operatorname{E}[\\bar{X}]\n  &= \\operatorname{E}\\left[ \\frac{1}{n}(X_1 + X_2 + \\cdots X_n) \\right]\\\\\n  &= \\frac{1}{n} \\operatorname{E}[ X_1 + X_2 + \\cdots X_n ]\\\\\n  &= \\frac{1}{n} \\left( \\operatorname{E}[ X_1 ] + \\operatorname{E}[X_2] + \\cdots \\operatorname{E}[X_n] \\right) \\\\\n  &= \\frac{1}{n} (n\\times \\mu) = \\mu.\n\\end{align*}\\]\n, expected value  \\(\\bar{X}\\) equal  \\(\\mu\\).\nestimator \\(\\bar{X}\\) unbiased  \\(\\mu\\).","code":""},{"path":"SamplingDistributions.html","id":"EstimatePopVariance","chapter":"12 Describing samples","heading":"12.2.2 Estimating the population variance","text":"\nSimilarly, given formula population variance, might suggest following estimate population variance.Theorem 12.2  (unbiased estimator $\\sigma^2$) sample variance\n\\[\\begin{equation}\n  S^2_n = \\frac{1}{n} \\sum_{=1}^n (X_i - \\mu)^2.\n   \\tag{12.1}\n\\end{equation}\\]\nunbiased estimator population variance \\(\\sigma^2\\).Proof. starting definition unbiased estimator:\n\\[\\begin{align*}\n    \\operatorname{E}[S^2_n]\n    &= \\operatorname{E}\\left[ \\frac{1}{n} \\sum_{=1}^n (X_i - \\mu)^2\\right]\\\\\n    &= \\frac{1}{n} \\operatorname{E}\\left[  (X_1 - \\mu)^2 + (X_2 - \\mu)^2 + \\cdots + (X_n - \\mu)^2\\right]\\\\\n    &= \\frac{1}{n} \\left( \\operatorname{E}[  (X_1 - \\mu)^2] + \\operatorname{E}[(X_2 - \\mu)^2] + \\cdots + \\operatorname{E}[X_n - \\mu]^2\\right)\\\\\n    &= \\frac{1}{n} \\left( \\sigma^2 + \\sigma^2 + \\cdots + \\sigma^2 \\right)\\\\\n    &= \\frac{1}{n} \\times n\\sigma^2 = \\sigma^2.\n\\end{align*}\\]\nThus, \\(S^2_n\\) unbiased estimator \\(\\sigma^2\\).Equation (12.1) practical difficulty however; equation used estimate unknown value population variance, relies knowing value population mean \\(\\mu\\).\nKnowing value \\(\\mu\\) value \\(\\sigma^2\\) seems unlikely.\nAlmost always, estimate population mean \\(\\bar{X}\\).may suggest using\n\\[\n  \\frac{1}{n} \\sum_{=1}^n (X_i - \\bar{X})^2.\n\\]\nestimate population variance.\nHowever, estimate unbiased (.e., biased).\nSince \\(\\bar{X}\\) estimates \\(\\mu\\) imprecisely (just one many possible samples), introduces extra source variation estimation  \\(\\sigma^2\\).see , start definition unbiased estimator:\n\\[\\begin{align*}\n  \\operatorname{E}\\left[ \\frac{1}{n} \\sum_{=1}^n (X_i - \\bar{X})^2 \\right]\n  &= \\frac{1}{n}\\operatorname{E}\\left[ \\sum_{=1}^n (X_i - \\bar{X})^2\\right]\\\\\n  &= \\frac{1}{n}\\operatorname{E}\\left[ \\sum_{=1}^n \\left(X^2_i - 2X_i\\bar{X} + \\bar{X}^2\\right)\\right]\\\\\n  &= \\frac{1}{n}\\operatorname{E}\\left[ \\sum_{=1}^n X^2_i - \\bar{X}\\sum_{=1}^n 2X_i + \\sum_{=1}^n \\bar{X}^2\\right].\n\\end{align*}\\]\nNow observe since \\(\\bar{X} = \\sum_i X_i/n\\), \\(\\sum_i X_i = n \\bar{X}\\); hence\n\\[\\begin{align*}\n  \\operatorname{E}\\left[ \\frac{1}{n} \\sum_{=1}^n (X_i - \\bar{X})^2 \\right]\n  &= \\frac{1}{n}\\operatorname{E}\\left[ \\sum_{=1}^n X^2_i - \\bar{X}^2 + n \\bar{X}^2\\right]\\\\\n  &= \\frac{1}{n}\\operatorname{E}\\left[ \\sum_{=1}^n X^2_i - n \\bar{X}^2 \\right]\\\\\n  &= \\frac{1}{n}\\left(\\operatorname{E}\\left[ \\sum_{=1}^n X^2_i\\right] - n\\operatorname{E}\\left[ \\bar{X}^2 \\right]\\right).\n\\end{align*}\\]\nexpression can simplified noting \n\\[\n  \\operatorname{var}[\\bar{X}]\n  = \\operatorname{E}[\\bar{X}^2] - \\operatorname{E}[\\bar{X}]^2\n  = \\operatorname{E}[\\bar{X}^2] - \\mu^2,\n\\]\n\\(\\operatorname{E}[\\bar{X}^2] = \\operatorname{var}{\\bar{X}} + \\mu^2 = \\sigma^2/n + \\mu^2\\).\nAlso,\n\\[\n  \\operatorname{var}[X]\n  = \\operatorname{E}[X^2] - \\operatorname{E}[X]^2\n  = \\operatorname{E}[X^2] - \\mu^2,\n\\]\n\\(\\operatorname{E}[X^2] = \\sigma + \\mu^2\\).\nHence,\n\\[\\begin{align*}\n  \\operatorname{E}\\left[ \\frac{1}{n} \\sum_{=1}^n (X_i - \\bar{X})^2 \\right]\n  &= \\frac{1}{n}\\left(\\operatorname{E}\\left[ \\sum_{=1}^n X^2_i\\right] - n\\operatorname{E}\\left[\\sum_{=1}^n \\bar{X}^2 \\right]\\right)\\\\\n  &= \\frac{1}{n}\\left\\{ \\sum(\\sigma^2 + \\mu^2) - n\\left(\\frac{\\sigma^2}{n} + \\mu^2\\right)\\right\\}\\\\\n  &= \\frac{1}{n}\\left\\{ n\\sigma^2 + n\\mu^2 - \\sigma^2 - n\\mu^2\\right\\}\\\\\n  &= \\frac{1}{n}\\left\\{ n\\sigma^2  - \\sigma^2\\right\\}\\\\\n  &= \\frac{n - 1}{n} \\sigma^2.\n\\end{align*}\\]\nThus estimator biased estimator  \\(\\sigma^2\\).\nHowever, multiplying estimator \\((n - 1)/n\\) produce unbiased estimator.Theorem 12.3  (unbiased estimator $\\sigma^2$ ($\\mu$ unknown)) value  \\(\\mu\\) unknown, unbiased estimator  \\(\\sigma^2\\) \n\\[\\begin{equation}\n  S^2_{n - 1} = \\frac{1}{n - 1} \\sum_{=1}^n (X_i - \\bar{X})^2.\n   \\tag{12.2}\n\\end{equation}\\]\nNotice divisor front summation now \\(n - 1\\) rather  \\(n\\).Proof. Easily shown using result .","code":""},{"path":"SamplingDistributions.html","id":"a-example-using-a-small-population","chapter":"12 Describing samples","heading":"12.3 A example using a small population","text":"","code":""},{"path":"SamplingDistributions.html","id":"some-results-for-samples","chapter":"12 Describing samples","heading":"12.3.1 Some results for samples","text":"\nSuppose simple random sample size \\(n\\) taken population.\nmain interest mean sample estimate unknown mean population \\(\\mu\\).\nTypically, population large, often even possible list elements population.\nHowever, demonstration purposes, suppose population contains \\(N = 5\\) elements:\n\\[\n   2\\qquad 3\\qquad 3\\qquad 4\\qquad 8.\n\\]\n, mean population \n\\[\n  \\mu = \\frac{2 + 3 + 3 + 4 + 8}{5} = 4.\n\\]\nUsing R:Similarly variance population \n\\[\n  \\sigma^2 = \\frac{1}{N} \\sum_{= 1}^n  (X_i - \\mu)^2 = 4.4:\n\\]cumbersome second formula R population variance necessary R function var() computes variance sample, rather variance population .\nfactor \\((n - 1)/n\\) comes difference formulas (Sect. 12.2.2).","code":"\nPopulation <- c(2, 3, 3, 4, 8)\nmean(Population)\n#> [1] 4\nN <- length(Population)\nsum( (Population - mean(Population))^2 ) / N\n#> [1] 4.4\n\nvar(Population) * (N - 1) / N\n#> [1] 4.4"},{"path":"SamplingDistributions.html","id":"distribution-of-the-sample-mean-n-2-without-replacement","chapter":"12 Describing samples","heading":"12.3.2 Distribution of the sample mean: \\(n = 2\\) (without replacement)","text":"\nAssume know values  \\(\\mu\\)  \\(\\sigma^2\\), usually case.\n, estimate value  \\(\\mu\\), take sample size \\(n = 2\\) population, without replacement.\nsamples taken random, know elements sample, know value  \\(\\bar{X}\\) .Simple random sampling refers possible samples size \\(n\\) equally likely.\nSince \\(\\binom{N}{n}\\) samples size \\(n\\) sampling without replacement  \\(N\\) objects, \\(\\binom{5}{2} = 10\\) equally-likely possible samples size \\(n = 2\\) possible example.\ninstance, sample \\(\\{2, 3\\}\\) probability \\(1/10\\) selected, mean sample  \\(2.5\\).can use R list possible samples size \\(n = 2\\) population:\\(10\\) possible samples size \\(n = 2\\) can listed, mean sample:sample mean varies sample sample.\nmean sample depends sample selected random.\ninformation, distribution sample mean can determined (Fig. 12.1):\nFIGURE 12.1: distribution sample means.\ndistribution known sampling distribution sample mean, recognising distribution sample mean.sample mean distribution: value sample mean varies, depending sample obtained.mean sampling distribution Fig.  12.1 \\(\\mu_{\\bar{X}} = 4\\):variance sampling distribution Fig.  12.1 \\(\\sigma^2_{\\bar{X}} = 1.65\\):Note since mean possible samples, ‘population’ sample means.","code":"\n### All possible combinations of 2 elements:\nAllSamples <- combn(Population,\n                    m = 2)\nAllSamples\n#>      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9]\n#> [1,]    2    2    2    2    3    3    3    3    3\n#> [2,]    3    3    4    8    3    4    8    4    8\n#>      [,10]\n#> [1,]     4\n#> [2,]     8\n\n### Mean of each possible sample:\nMeanAllSamples <- colMeans(AllSamples)\nMeanAllSamples\n#>  [1] 2.5 2.5 3.0 5.0 3.0 3.5 5.5 3.5 5.5 6.0\ncbind(                 t(AllSamples), \n       \"Sample mean\" = MeanAllSamples,\n       \"Probability\" = 1/10)\n#>           Sample mean Probability\n#>  [1,] 2 3         2.5         0.1\n#>  [2,] 2 3         2.5         0.1\n#>  [3,] 2 4         3.0         0.1\n#>  [4,] 2 8         5.0         0.1\n#>  [5,] 3 3         3.0         0.1\n#>  [6,] 3 4         3.5         0.1\n#>  [7,] 3 8         5.5         0.1\n#>  [8,] 3 4         3.5         0.1\n#>  [9,] 3 8         5.5         0.1\n#> [10,] 4 8         6.0         0.1\nprop.table(table(MeanAllSamples))\n#> MeanAllSamples\n#> 2.5   3 3.5   5 5.5   6 \n#> 0.2 0.2 0.2 0.1 0.2 0.1\n\nplot( prop.table(table(MeanAllSamples)),\n      las = 1,\n      ylim = c(0, 0.22),\n      xlab = \"Sample mean\",\n      ylab = \"Probability\",\n      main = \"The distribution of sample means\")\npoints( x = names(prop.table(table(MeanAllSamples))),\n        y = prop.table(table(MeanAllSamples)),\n        pch = 19)\nmean(MeanAllSamples)\n#> [1] 4\nn <- length(MeanAllSamples)\nvar(MeanAllSamples) * (n - 1) / n\n#> [1] 1.65"},{"path":"SamplingDistributions.html","id":"distribution-of-the-sample-mean-larger-samples-without-replacement","chapter":"12 Describing samples","heading":"12.3.3 Distribution of the sample mean: larger samples (without replacement)","text":"\ncan increase sample size \\(n = 3\\), repeat:mean possible samples (\\(\\mu_{\\bar{X}} = 4\\)), variance smaller (\\(\\sigma^2_{\\bar{X}} = 0.7333\\) rather \\(1.65\\)).\n\\(n = 4\\), mean possible samples remains , variance smaller :","code":"\n### All possible combinations of 3 elements:\nAllSamples <- combn(Population,\n                    m = 3)\n\n### Mean of each possible sample:\nMeanAllSamples <- colMeans(AllSamples)\nmean(MeanAllSamples)\n#> [1] 4\nn <- length(MeanAllSamples)\nvar(MeanAllSamples) * (n - 1) / n\n#> [1] 0.7333333\n### All possible combinations of 3 elements:\nAllSamples <- combn(Population,\n                    m = 4)\n\n### Mean of each possible sample:\nMeanAllSamples <- colMeans(AllSamples)\nmean(MeanAllSamples)\n#> [1] 4\nn <- length(MeanAllSamples)\nvar(MeanAllSamples) * (n - 1) / n\n#> [1] 0.275"},{"path":"SamplingDistributions.html","id":"SamplingWithReplacement","chapter":"12 Describing samples","heading":"12.3.4 Distribution of the sample mean: \\(n = 2\\) (with replacement)","text":"\nexamples use sampling without replacement.\nHowever, sampling randomly replacement used .\n, \\(5\\times 5 = 25\\) equally-likely samples possible:table, distribution sample mean can determined (Fig. 12.2):\nFIGURE 12.2: distribution sample means.\nmean variance sampling distribution case \\(\\mu_{\\bar{X}} = 4\\) \\(\\sigma^2_{\\bar{X}} = 2.2\\) respectively:","code":"\nAllSamplesWithReplacement <- expand.grid(Population, Population)\nnames(AllSamplesWithReplacement) <- c(NA, \n                                      NA)\nMeanAllSamplesWithReplacement <- rowMeans(AllSamplesWithReplacement)\n\ncbind( AllSamplesWithReplacement[, 1],\n       AllSamplesWithReplacement[, 2],\n       \"Sample mean\" = MeanAllSamplesWithReplacement, \n       \"Probability\" = 1/25)\n#>           Sample mean Probability\n#>  [1,] 2 2         2.0        0.04\n#>  [2,] 3 2         2.5        0.04\n#>  [3,] 3 2         2.5        0.04\n#>  [4,] 4 2         3.0        0.04\n#>  [5,] 8 2         5.0        0.04\n#>  [6,] 2 3         2.5        0.04\n#>  [7,] 3 3         3.0        0.04\n#>  [8,] 3 3         3.0        0.04\n#>  [9,] 4 3         3.5        0.04\n#> [10,] 8 3         5.5        0.04\n#> [11,] 2 3         2.5        0.04\n#> [12,] 3 3         3.0        0.04\n#> [13,] 3 3         3.0        0.04\n#> [14,] 4 3         3.5        0.04\n#> [15,] 8 3         5.5        0.04\n#> [16,] 2 4         3.0        0.04\n#> [17,] 3 4         3.5        0.04\n#> [18,] 3 4         3.5        0.04\n#> [19,] 4 4         4.0        0.04\n#> [20,] 8 4         6.0        0.04\n#> [21,] 2 8         5.0        0.04\n#> [22,] 3 8         5.5        0.04\n#> [23,] 3 8         5.5        0.04\n#> [24,] 4 8         6.0        0.04\n#> [25,] 8 8         8.0        0.04\nprop.table(table(MeanAllSamplesWithReplacement))\n#> MeanAllSamplesWithReplacement\n#>    2  2.5    3  3.5    4    5  5.5    6    8 \n#> 0.04 0.16 0.24 0.16 0.04 0.08 0.16 0.08 0.04\n\nplot( prop.table(table(MeanAllSamplesWithReplacement)),\n      las = 1,\n      ylim = c(0, 0.25),\n      xlab = \"Sample mean\",\n      ylab = \"Probability\",\n      main = \"The distribution of sample means\\n(sampling with replacement)\")\npoints( x = names(prop.table(table(MeanAllSamplesWithReplacement))),\n        y = prop.table(table(MeanAllSamplesWithReplacement)),\n        pch = 19)\nmean(MeanAllSamplesWithReplacement)\n#> [1] 4\nn <- length(MeanAllSamplesWithReplacement)\nvar(MeanAllSamplesWithReplacement) * (n - 1) / n\n#> [1] 2.2"},{"path":"SamplingDistributions.html","id":"the-sampling-distribution-of-other-statistics","chapter":"12 Describing samples","heading":"12.3.5 The sampling distribution of other statistics","text":"principle, distribution statistic found using approach example ; , median, variance, range, etc. sampling distribution.\nexample, can display distribution sample range:\nFIGURE 12.3: distribution sample range.\n","code":"\n## Define a function to compute the range\nmyRange <- function(x){ max(x) - min(x) }\n\n## The range of the population:\nmyRange(Population)\n#> [1] 6\n\n## Sampling:\nAllSamplesWithReplacement <- expand.grid(Population, Population)\nnames(AllSamplesWithReplacement) <- c(NA, \n                                      NA)\nRangeAllSamplesWithReplacement <- apply(AllSamplesWithReplacement,\n                                        1,\n                                       \"myRange\")\nprop.table(table(RangeAllSamplesWithReplacement))\n#> RangeAllSamplesWithReplacement\n#>    0    1    2    4    5    6 \n#> 0.28 0.32 0.08 0.08 0.16 0.08\n\nplot( prop.table(table(RangeAllSamplesWithReplacement)),\n      las = 1,\n      ylim = c(0, 0.35),\n      xlab = \"Sample range\",\n      ylab = \"Probability\",\n      main = \"The distribution of sample range\\n(sampling with replacement)\")\npoints( x = names(prop.table(table(RangeAllSamplesWithReplacement))),\n        y = prop.table(table(RangeAllSamplesWithReplacement)),\n        pch = 19)"},{"path":"SamplingDistributions.html","id":"SamplingDistributionsIntro","chapter":"12 Describing samples","heading":"12.4 Sampling distributions","text":"previous section displays numerous sampling distributions: distributions sample statistics.\npractice, populations almost always large number possible samples huge, infinite, exact elements population difficult impossible list.\ncases, listing possible samples computing exact sampling distributions impossible.terms distribution theory, probability distributions Figs. 12.1  12.2 just distribution random variable, since sample mean random variable.\nStatistically, distribution describe expect sample means randomly sample population.quantity computed sample random variable, since value unknown beforehand.\nobserved value quantity depends sample selected (random).Consequently, sample means, sample standard deviations, sample medians etc. random variables.knowledge essential extracting information samples.\nObserve mean sampling distribution example (taken without replacement) mean population : \\(\\mu = (2 + 3 + 3 + 4 + 8 )/5 = 4\\).\nAlso, variance sampling distribution smaller variance population (\\(\\sigma^2 = 4.4\\)).\nexact relationship \\(\\sigma^2_{\\bar{X}}\\) \\(\\sigma^2\\) expanded Theorem 12.4.Definition 12.2  (Statistic) statistic real-valued function observations sample.\\(X_1, X_2, \\dots, X_n\\) represents numerical sample size \\(n\\) population, \\(T = g(X_1, X_2, \\dots, X_n)\\) represents statistic.\nExamples include sample mean, sample standard deviation, sample variance sample proportion.\nindividual observations sample also statistics.\nHence, example, minimum value sample (usually denoted \\(\\min\\) \\(X_{[1]}\\)), maximum value, range \\(\\max - \\min\\) statistics.\nFurthermore, even first observation sample \\(X_1\\) statistic.random variable numeric variable fully determined values , even members sample.Example 12.2  (Sampling distribution statistic) sampling distribution statistic theoretical probability distribution statistic associated possible samples particular size drawn particular population.Notice definition confined random samples.\nHowever, practically applications sample involved assumed random sense.\nRandom sampling imposes probability structure possible values statistic, allowing sampling distribution defined.use \\(\\bar{X}\\) represent mean sample whose value yet unknown (depends sample selected).\nsymbol \\(\\overline{x}\\) used denoted value sample mean, computed data found particular sample.","code":""},{"path":"SamplingDistributions.html","id":"RandomSamples","chapter":"12 Describing samples","heading":"12.4.1 Random sampling","text":"purposes, random sample defined follows:Definition 12.3  (Random sample) set random variables \\(X_1, X_2,\\dots,X_n\\) said random sample size \\(n\\) population df \\(F_X(x)\\)  \\(X_i\\) identically independently distributed (iid) df \\(F_X(x)\\).definition uses distribution function accommodate discrete continuous random variables.definition random sample standard theoretical statistics.\nstatistical practice, however, many sampling designs called “random”, simple random sampling, stratified sampling, systematic sampling.\nsampling methods produce ‘random samples’, even though samples produced don’t necessarily satisfy Def. 12.3.\nUsually context makes clear whether strict loose meaning ‘random sample’ intended.sample size \\(n\\) assumed chosen ‘random’ without replacement population size \\(N\\), sample constitute random sample sense Def. 12.3.\nEven though member sample distribution population, members sample independent.\nexample, consider second element \\(X_2\\) samples Sect. 12.1.\n, \\(\\Pr(X_2 = 2) = 0.2\\) \\(\\Pr(X_2 = 2 \\mid X_1 = 2) = 0\\).\nTherefore \\(\\Pr(X_2 = 2 \\mid X_1 = 2) \\ne\\Pr(X_2 = 2)\\) \\(X_2\\) \\(X_1\\) dependent.However, sampling random without replacement approximates random sample sample size \\(n\\) small compared population size \\(N\\).\ncase, impact removing members population minimal impact distribution population remaining, observations approximately independent.\nPopulations sizes commonly much larger sample sizes (write \\(N>\\!>n\\)).Sect. 12.1, sampling performed replacement \\(\\Pr(X_2 = 2 \\mid X_1 = 2) = \\Pr(X_2 = 2) = \\Pr(X_1 = 2)\\).\nSimilar statements can made sample members values population.\ncase, sample random sample according Def. 12.3.Sampling random replacement gives rise random sample, sampling random without replacement approximates random sample provided \\(N >\\!> n\\).\nSampling without replacement sample size much smaller population size give rise random sample.\nsituation often referred sampling finite population.Sampling random replacement also known repeated sampling.\ntypical example sequence independent trials (Bernoulli otherwise) occur coin die tossed repeatedly.\ntrial involves random sampling population, case coin comprises H  T equal probability.Sampling continuous distribution really theoretical concept population assumed uncountably infinite.\nConsequently, sampling random situation assumed produce random sample.","code":""},{"path":"SamplingDistributions.html","id":"SamplingDistributionMean","chapter":"12 Describing samples","heading":"12.4.2 The sampling distribution of the mean","text":"powerful results exist describe sampling distribution mean random sample.Theorem 12.4  (Sampling distribution mean) \\(X_1, X_2, \\dots, X_n\\) random sample size \\(n\\) population mean \\(\\mu\\) variance \\(\\sigma^2\\), sample mean \\(\\bar{X}\\) sampling distribution mean \\(\\mu\\) variance \\(\\sigma^2 / n\\).Proof. application Corollary 11.1 \\(a_i = 1/n\\).Theorem 12.4 applies random sample defined Sect. 12.4.1.\nremarkable feature Theorem 12.4 distribution  \\(X\\) important.Theorem 12.4, distribution population sample drawn irrelevant.Theorem 12.4 applies samples randomly selected replacement finite population described Sect. 12.3.4.\ncase sample taken without replacement, \\(\\mu_{\\bar{X}}\\) still equals \\(\\mu\\) (result depend observations independent), \n\\[\\begin{equation}\n   \\operatorname{var}[\\bar{X}]\n   = \\frac{\\sigma^2}{n} \\times \\frac{(N - n)}{(N - 1)}\n\\end{equation}\\]\nsmaller \\(\\sigma^2/n\\) factor \\((N - n)/(N - 1)\\).\nmeans standard deviation  \\(\\bar{X}\\) smaller \\(\\sigma/\\sqrt{n}\\) factor \\(\\sqrt{(N - n)/(N - 1)}\\).\n(factor also seen Exercise 7.19.)Definition 12.4  (Finite Population Correction (FPC)) finite population correction (FPC) factor \n\\[\n   \\sqrt{ \\frac{N - n}{N - 1} }.\n\\]Example 12.3  (Sampling without replacement) Sect. 12.1, \\(\\mu = 4.4\\) \\(\\sigma^2 = 5.04\\) population.\nFPC \n\\[\n   \\text{FPC} = \\sqrt{ \\frac{5 - 2}{5 - 1} } = 0.8660\\dots.\n\\]\nsampling done replacement, \\(\\mu_{\\bar{X}} = \\mu = 4.4\\) \\(\\operatorname{var}[\\bar{X}] = 2.52 = 5.04/2\\), agreement theorem.sampling done without replacement, sampling distribution mean mean \\(\\mu_{\\bar{X}} = \\mu = 4.4\\) variance \\(\\operatorname{var}[\\bar{X}] = 1.89 = \\frac{5.04}{2} \\times \\text{FPC}^2\\), agreement note following Theorem 12.13.Theorem 12.13 describes location spread sampling distribution mean, shape sampling distribution.\nRemarkably, shape sampling distribution depends shape population distribution sample small (Sect. 12.6).","code":""},{"path":"SamplingDistributions.html","id":"SamplingDistributionsRelatedToNormal","chapter":"12 Describing samples","heading":"12.5 Sampling distributions related to the normal distribution","text":"","code":""},{"path":"SamplingDistributions.html","id":"NormalGeneralResults","chapter":"12 Describing samples","heading":"12.5.1 General results","text":"normal distribution used model many naturally occurring phenomena (Sect. 8.3).\nConsequently, begin studying sampling distributions statistics random samples come normal populations.First, present useful result.Theorem 12.5  (Linear combinations) Let \\(X_1, X_2, \\dots, X_n\\) set independent random variables \\(X_i\\sim N(\\mu_i,\\sigma^2_i)\\).\nDefine linear combination \\(Y\\) \n\\[\n   Y = a_1 X_1 + a_2 X_2 + \\dots + a_nX_n.\n\\]\n\\(Y \\sim N(\\Sigma a_i\\mu_i, \\Sigma ^2_i\\sigma^2_i)\\).Proof. Theorem 8.2, MGF random variable \\(X_i\\) shown \n\\[\n   M_{X_i} (t)\n   = \\exp\\left\\{\\mu_i t + \\frac{1}{2} \\sigma^2_i t^2\\right\\}\\quad\\text{$= 1, 2, \\dots, n$}.\n\\]\n, constant \\(a_i\\) (using Theorem 5.1 \\(\\beta = 0\\), \\(\\alpha = a_i\\)):\n\\[\\begin{align*}\n   M_{a_i X_i}(t)\n   &= M_{X_i}(a_it)\\\\\n   &= \\exp\\left\\{\\mu_i a_i t + \\frac{1}{2} \\sigma^2_i ^2_i t^2\\right\\}.\n\\end{align*}\\]\nSince MGF sum independent random variables equal product MGFs, \n\\[\\begin{align*}\n     M_Y(t)   \n     &= \\prod^n_{= 1} \\exp\\left( \\mu_i a_i t + \\frac{1}{2} \\sigma^2_i ^2_i t^2 \\right)\\\\\n     &= \\exp\\left( t\\Sigma a_i\\mu_i +\\frac{1}{2} t^2 \\Sigma ^2_i \\sigma^2_i \\right).\n\\end{align*}\\]\nMGF normal random variable mean \\(\\operatorname{E}[Y] = \\Sigma a_i\\mu_i\\) variance \\(\\operatorname{var}[Y] = \\Sigma ^2_i\\sigma^2_i\\).Example 12.4  Theorem 12.5 can demonstrated R; see .sampling distribution sum mean random sample normal population follows directly Theorem 12.5.Corollary 12.1  (Sum mean random sample) Let \\(X_1, X_2, \\dots, X_n\\) random sample size \\(n\\) \\(N(\\mu,\\sigma^2)\\).\nDefine sum \\(S\\) mean \\(\\bar{X}\\) respectively \n\\[\\begin{align*}\n   S\n   &= X_1 + X_2 + \\dots + X_n\\\\\n   \\bar{X}\n   &= (X_1+X_2 + \\dots + X_n)/n\n\\end{align*}\\]\n\\(S\\sim N(n\\mu, n\\sigma^2)\\) \\(\\bar{X}\\sim N(\\mu, \\sigma^2/n)\\).Proof. Exercise.Corollary 12.1 relies population sample drawn normal distribution, properties expectation; depend sample size \\(n\\).\nbasis inference population mean normal distribution known variance.Example 12.5  (Sums rvs) Suppose envelopes collected packs  \\(25\\) weighing .\nweight envelope distributed normally mean \\(3\\,\\text{g}\\) standard deviation \\(0.6\\,\\text{g}\\).\nweighed pack envelopes declared containing \\(25\\) weighs \\(70\\,\\text{g}\\) \\(80\\,\\text{g}\\).\nprobability pile  \\(25\\) counted ?Let random variable \\(X_i\\) weight \\(\\)th envelope; \\(X_i \\sim N(3, 0.36)\\).\nLet \\(Y = X_1 + X_2 + \\dots + X_{25}\\); \\(Y \\sim N(75, 9)\\).\nrequire \\(1 - \\Pr(70 < Y < 80)\\); see Fig. 12.4.\nSince normal distribution symmetric, proceed:\n\\[\\begin{align*}\n   1 - \\Pr(70 < Y < 80)\n   &= 2\\times \\Pr\\left( Z < \\frac{70 - 75}{\\sqrt{9}} \\right)\\\\\n   &= 2\\times \\Phi(-1.667) \\\\\n   &= 0.10.\n\\end{align*}\\]\nR:\nFIGURE 12.4: envelope question. Left panel: distribution weight individual envelopes; right panel: distribution weight packs \\(25\\) envelopes.\nExample 12.6  (CLT) IQs large population \\(10\\) year-old boys (assumed normally distributed) determined found mean  \\(110\\) variance  \\(144\\).\nlarge sample taken order probability  \\(0.9\\) mean IQ sample differ expected value \\(110\\)  \\(5\\)?Let \\(X_i\\) IQ \\(\\)th boy; \\(X_i \\sim N(110, 144)\\).\nConsider sample size \\(n\\) let \\(\\bar{X} = \\sum^n_{= 1}X_i/n\\); \\(\\bar{X}\\sim N(110, 144/n)\\).\n\n\\[\n   \\Pr(|\\bar{X} - 110|\\leq 5) = 0.90.\n\\]\n\n\\[\n   \\Pr\\left(\\frac{|\\bar{X} - 110|\\sqrt{n}}{12} \\leq \\frac{5\\sqrt{n}}{12}\\right) = 0.90\n\\]\nhence\n\\[\n   \\Pr(Z \\leq 5\\sqrt{n} /12) = 0.90.\n\\]\nUsing R:smallest size sample, , sample \\(n = 16\\).Example 12.7  (Two random variable) certain product involves plunger fitting cylindrical tube.\ndiameter plunger can considered normal random variable mean \\(2.1\\,\\text{cm}\\) standard deviation \\(0.1\\,\\text{cm}\\).\ninside diameter cylindrical tube normal random variable mean \\(2.3\\,\\text{cm}\\) standard deviation \\(0.05\\,\\text{cm}\\).\nplunger tube chosen randomly day’s production run, find probability plunger fit cylinder.Let \\(X\\)  \\(Y\\) diameter plunger cylinder respectively.\n\\(X\\sim N\\big(2.1, (0.1)^2\\big)\\) \\(Y\\sim N\\big( 2.3, (0.05)^2\\big)\\), seek \\(\\Pr(Y < X)\\).\ndistribution \\(Y - X\\) \\(N(2.3 - 2.1, 0.0025 + 0.01)\\) \n\\[\\begin{align*}\n   \\Pr(Y - X < 0)\n   &= \\Pr\\left(Z <\\frac{0 - 0.2}{\\sqrt{0.0125}}\\right) \\quad\\text{$Z\\sim N(0,1)$}\\\\\n   &= \\Pr(Z < -1.78)\\\\\n          &= 0.0375.\n\\end{align*}\\]\nR:","code":"\nset.seed(8979704) # For reproducibility\nnumberVars <- 1000 # That is: n\n\n# The values of a_i, the coefficients in the linear combination\nai <- sample(x = seq(-2, 3),    # We select values of a_i between -2 and 3\n             size = numberVars, # Need one for each X value\n             replace = TRUE)    # Select with replacement\n\n# The means of the individual distributions\nmui <- runif(numberVars, # We take means from a uniform dist; one for each X\n             min = -5,   # Min value is -5\n             max = 5)    # Max value is 5\n# The variances of the distributions\nsigma2i <- runif(numberVars,  # Variance from a uniform distn also\n                min = 1,    # Minimum values of 1\n                max = 3)    # Maximum value of 3\n\n# Compute the values directly:\nXi <- rnorm(numberVars,      # X_i with \n            mean = mui,   # means \n            sd = sqrt(sigma2i) )\n\n# What we expect for Y = a_1 X_2 + a_2 X_2 + ...\n# according to the Theorem:\nY <- ai * Xi  # The linear combination Y\n\n# Show some:\nsomeInfo <- cbind(ai, mui, sigma2i, Xi, Y)\nsomeInfo[1:5, ]\n#>      ai        mui  sigma2i        Xi         Y\n#> [1,] -1  0.1690571 2.685062  1.993591 -1.993591\n#> [2,]  1 -4.8907282 1.712808 -5.100795 -5.100795\n#> [3,] -1 -0.4511475 1.562472 -1.184639  1.184639\n#> [4,] -1  0.6878348 2.445416  4.870628 -4.870628\n#> [5,] -2 -1.9851899 1.032315 -2.497384  4.994768\n\n\n# Compare:\ncompareTable <- array( dim = c(2, 2) )\ncolnames(compareTable) <- c(\"Means:\",\n                            \"Variances:\")\nrownames(compareTable) <- c(\"Using theorem\",\n                            \"Computing directly\")\n\ncompareTable[, 1] <- c( mean(ai * Xi), \n                        mean(Y) )\ncompareTable[, 2] <- c( var(ai * Xi), \n                        var(Y) )\ncompareTable\n#>                        Means: Variances:\n#> Using theorem      -0.2656963    32.6838\n#> Computing directly -0.2656963    32.6838\n2 * ( pnorm( -1.667 ) )\n#> [1] 0.0955144\nz <- qnorm(0.95)\nz\n#> [1] 1.644854\n\nn <- (z * 12 / 5) ^ 2\nn\n#> [1] 15.58393\npnorm( -1.78 )\n#> [1] 0.03753798"},{"path":"SamplingDistributions.html","id":"ChiSquareDistribution","chapter":"12 Describing samples","heading":"12.5.2 The \\(\\chi^2\\) distribution","text":"\nstudied sampling distribution mean, now study sampling distribution variance normal population.\nFirst, present useful result variation Theorem 8.12.Theorem 12.6  (Chi-square distribution) Let \\(X_1, X_2, \\dots, X_n\\) random sample size \\(n\\) \\(N(\\mu, \\sigma^2)\\).\n\\(\\sum_{= 1}^n \\left(\\displaystyle{\\frac{X_i - \\mu}{\\sigma}}\\right)^2\\) chi-square distribution \\(n\\) degrees freedom (df), written \\(\\chi^2_n\\).Proof. \\(Z_i = \\displaystyle{\\frac{X_i - \\mu}{\\sigma}}\\), \n\\[\n   \\sum_{= 1}^n \\left(\\frac{X_i - \\mu}{\\sigma}\\right)^2 = \\sum_{= 1}^n Z_i^2.\n\\]\n, \\(Z_i\\) standardised version  \\(X_i\\) standard normal \\(N(0, 1)\\) distribution.\nAlso, random variables \\(Z_i\\) independent  \\(X_i\\) independent (\\(= 1, 2, \\dots, n\\)).\nrequired result follows directly Theorem 8.12.four R functions working \\(\\chi^2\\) distribution form [dpqr]chisq(df), df degrees freedom (see Appendix  D).sample variance given \n\\[\\begin{equation}\n   S^2\n   = \\frac{1}{n - 1}\\sum_{= 1}^n (X_i - \\bar{X})^2.\n   \\tag{12.3}\n\\end{equation}\\]\nsample estimates (‘statistics’) vary sample sample, sample variance distribution.\nsampling distribution sample variance follows.Theorem 12.7  (Distribution sample variance) Let \\(X_1, X_2, \\dots, X_n\\) random sample size \\(n\\) \\(N(\\mu, \\sigma^2)\\).\n\n\\[\n   \\frac{(n - 1)S^2}{\\sigma^2} \\sim \\chi^2_{n - 1}.\n\\]Proof. partial proof provided.\nUsing (12.3),\n\\[\n   \\frac{(n - 1)S^2}{\\sigma^2}\n   = \\sum_{= 1}^n \\left(\\frac{X_i - \\bar{X}}{\\sigma}\\right)^2,\n\\]\nlooks much like conditions Theorem 12.6.\ndifference \\(\\bar{X}\\) replaces \\(\\mu\\).\ndifference important:\n\\(\\mu\\) fixed value vary, whereas \\(\\bar{X}\\) random variable distribution.Writing\n\\[\\begin{align*}\n  \\sum_{= 1}^n \\left( \\frac{X_i - \\bar{X}}{\\sigma} \\right)^2\n    &= \\sum_{= 1}^n \\left( \\frac{X_i - \\bar{X} + \\mu - \\mu}{\\sigma} \\right)^2\\\\\n    &= \\sum_{= 1}^n \\left( \\frac{X_i - \\mu}{\\sigma} \\right)^2 -\n       n\\frac{(\\bar{X} - \\mu)^2}{\\sigma^2}.\n\\end{align*}\\]\n(Check!)\nRewrite \\(V = W - U\\) \n\\[\n   V\n   = \\sum_{= 1}^n \\left(\\frac{X_i - \\bar{X}}{\\sigma}\\right)^2;\\qquad\n   W\n   = \\sum_{= 1}^n \\left( \\frac{X_i - \\mu}{\\sigma} \\right)^2;\\quad\\text{}\\qquad\n   U\n   = n\\frac{(\\bar{X} - \\mu)^2}{\\sigma^2}.\\qquad\n\\]\ncan write \\(W = U + V\\).\nobserve:\\(W \\sim \\chi^2_n\\) Theorem 12.6, MGF  \\(W\\) \\(M_W(t) = (1 - 2t)^{-n}\\).\\(V \\sim \\chi^2_1\\)  \\(V\\) square standard normal random variable, since \\(\\bar{X}\\sim N(\\mu, \\sigma^2/n)\\).\nTherefore, MGF \\(V\\) \\(M_V(t) = (1 - 2t)^{-1}\\).Provided \\(W\\)  \\(V\\) independent, MGF  \\(W\\) product MGFs  \\(U\\)  \\(V\\).\nmeans MGF  \\(U\\) \n\\[\n   M_U(t)\n   = \\frac{M_W(t)}{M_V(t)}\n   = (1 - 2t)^{n - 1}\n\\]\nMGF chi-square distribution \\(n - 1\\) df.partial proof, independence  \\(U\\)  \\(V\\) shown.Example 12.8  (Distribution sample variance) random sample size \\(n = 10\\) selected \\(N(20, 5)\\) distribution.\nprobability variance sample exceeds \\(10\\)?Theorem 12.7,\n\\[\n   \\frac{(n - 1)S^2}{\\sigma^2}\n   = \\frac{9 S^2}{5} \\sim \\chi^2_9.\n\\]\nTherefore\n\\[\n   \\Pr(S^2 > 10)\n   = \\Pr\\left( \\frac{9S^2}{5} > \\frac{9\\times10}{5} \\right)\n   = \\Pr(X^2 > 18),\n\\]\n\\(X^2 \\sim \\chi^2(9)\\).\nUsing R, \\(\\Pr(S^2 > 10) = 0.03517\\):","code":"\n1 - pchisq(18, df = 9)\n#> [1] 0.03517354\npchisq(18, df = 9, lower.tail = FALSE)   # Alternatively\n#> [1] 0.03517354"},{"path":"SamplingDistributions.html","id":"TDistribution","chapter":"12 Describing samples","heading":"12.5.3 The \\(t\\) distribution","text":"\nbasis independence assumed proof Theorem 12.7 (perhaps surprising) result mean variance random sample normal population independent random variables.Theorem 12.8  (Sample mean variance: Independent) Let \\(X_1, X_2,\\dots, X_n\\) random sample size \\(n\\) \\(N(\\mu, \\sigma^2)\\).\nsample mean\n\\[\n   \\bar{X}\n   =\n   \\frac1n\\sum_{=1}^n X_i\n\\]\nsample variance\n\\[\n   S^2\n   = \\frac{1}{n - 1}\\sum_{=1}^n (X_i - \\bar{X})^2\n\\]\nindependent.Proof. proof given.Another distribution derives sampling normal population \\(t\\) distribution.Definition 12.5  (T distribution) Suppose \\(Z \\sim N(0, 1)\\) (.e., standard normal distribution) \\(V \\sim \\chi^2_n\\) (.e., chi-squared distribution \\(n\\) degrees freedom).\n\n\\[\\begin{equation}\n  T = \\frac{Z}{\\sqrt{V/n}}\n  \\tag{12.4}\n\\end{equation}\\]\ndefined \\(T\\) distribution \\(n\\) degrees freedom.Definition 12.6  (T distribution) continuous random variable \\(X\\) probability density function\n\\[\\begin{equation}\n   f_X(x)\n   = \\frac{\\Gamma\\left((\\nu + 1)/2\\right)}{\\sqrt{\\pi \\nu}\\,\\Gamma(\\nu/2)}\n     \\left(1 + \\frac{x^2}{\\nu}\\right)^{-(\\nu + 1)/2}\n     \\quad\\text{$x$}\n\\end{equation}\\]\nsaid \\(t\\) distribution parameter \\(\\nu > 0\\).\nparameter \\(\\nu\\) called degrees freedom.\nwrite \\(X \\sim t_\\nu\\).Proof. proof given.PDF \\(T\\)-distribution similar standard normal distribution (Fig. 12.5): bell-shaped symmetric zero.\nKURTOSIS?? SKEWNESS??However, variance greater one (Theorem 12.9) dependent  \\(\\nu\\).four R functions working \\(t\\) distribution form [dpqr]t(df), df degrees freedom (see Appendix D).\nFIGURE 12.5: \\(t\\) distributions (normal distributions dashed lines), mean \\(0\\) variance \\(1\\).\nTheorem 12.9  (Properties $T$ distribution) \\(X\\sim t_\\nu\\) thenFor \\(\\nu > 1\\), \\(\\operatorname{E}[X] = 0\\).\\(\\nu > 2\\), \\(\\operatorname{var}[X] = \\displaystyle{\\frac{\\nu}{\\nu - 2}}\\).MGF exist first \\(\\nu - 1\\) moments exist.Proof. proof given.Although won’t prove , \\(\\nu\\\\infty\\) \\(t\\) distribution converges standard normal (can seen Fig. 12.5).\naddition, Theorem 12.9, see \\(\\nu \\\\infty\\), \\(\\operatorname{var}[X] \\1\\) normal distribution.usefulness \\(t\\) distribution derives following important result.Theorem 12.10  ($T$-scores) Let \\(X_1, X_2, \\dots, X_n\\) random sample size \\(n\\) \\(N(\\mu, \\sigma^2)\\).\nrandom variable\n\\[\n   T = \\frac{\\bar{X} - \\mu}{S/\\sqrt{n}}\n\\]\nfollows \\(t_{n - 1}\\) distribution  \\(\\bar{X}\\) sample variance  \\(S^2\\) sample variance defined  q.(12.3).Proof. give partial proof .\nstatistic \\(T\\) can written \n\\[\\begin{align*}\n  T\n    &= \\frac{\\bar{X} - \\mu}{S/\\sqrt{n}}\\\\\n    &= \\left(\\frac{\\bar{X} - \\mu}{\\sigma/\\sqrt{n}}\\right)   \\frac{1}{\\sqrt{\\frac{(n - 1)S^2}{\\sigma^2}/(n - 1)}}\\\\\n    &= \\frac{Z}{Y/(n - 1)},\n\\end{align*}\\]\n\\(Z\\) \\(N(0, 1)\\) variable \\(Y\\) chi-square variable \\((n - 1)\\) df.\nUsing Def. 12.5 , \\(T\\) \\(T\\) distribution.Notice  \\(T\\) represents standardised version sample mean.\n, \\(T\\) statistic important statistical inference.\nprobably used hypothesis testing, example.Example 12.9  (Calculating $T$) random sample \\(21, 18, 16, 24, 16\\) drawn normal population mean  \\(20\\).value  \\(T\\) sample?random samples population, probability  \\(T\\) less value found ?sample: \\(\\overline{x} = 19.0\\) \\(s^2 = 12.0\\).\nTherefore \\(t = \\frac{19.0 - 20}{\\sqrt{12.0/5}} = -0.645\\).\n(Notice lower-case symbols used specific values statistics, upper-case symbols random variables.)Interest \\(\\Pr(T < -0.645)\\) \\(T\\sim t_4\\).\nR, answer  \\(0.277\\):","code":"\npt(-0.645, df = 4)\n#> [1] 0.2770289"},{"path":"SamplingDistributions.html","id":"FDistribution","chapter":"12 Describing samples","heading":"12.5.4 The \\(F\\) distribution","text":"\nDefinition 12.7 introduces \\(F\\) distribution, describes ratio two sample variances normal populations.\n\\(F\\) distribution used inferences comparing two sample variances.\ndistribution also used analysis variance, common technique used test equality several means.Definition 12.7  ($F$ distribution) continuous random variable \\(X\\) probability density function\n\\[\\begin{equation}\n  f_X(x)\n  = \\frac{\\Gamma\\big( (\\nu_1 + \\nu_2)/2 \\big)\n    \\nu_1^{\\nu_1/2} \\nu_2^{\\nu_2/2} x^{(\\nu_1/2) - 1}}\n   {\\Gamma(\\nu_1/2)\\Gamma(\\nu_2/2)(\\nu_1 x + \\nu_2)^{(\\nu_1 + \\nu_2)/2}}\\quad\\text{$x > 0$}\n\\end{equation}\\]\nsaid \\(F\\) distribution parameters \\(\\nu_1 > 0\\) \\(\\nu_2 > 0\\).\nparameters known respectively numerator denominator degrees freedom.\nwrite \\(X \\sim F_{\\nu_1, \\nu_2}\\).\\(F\\) distributions shown Fig. 12.6.\nbasic properties \\(F\\) distribution follows.Theorem 12.11  (Properties $F$ distribution) \\(X\\sim F(\\nu_1, \\nu_2)\\) thenFor \\(\\nu_2 > 2\\), \\(\\operatorname{E}[F] = \\displaystyle{\\frac{\\nu_2}{\\nu_2-2}}\\).\\(\\nu_2 > 4\\), \\(\\operatorname{var}[F] = \\displaystyle{\\frac{2\\nu_2^2(\\nu_1 + \\nu_2-2)} {\\nu_1(\\nu_2 - 2)^2(\\nu_2 - 4)}}\\).MGF exist.Proof. covered.\nFIGURE 12.6: \\(F\\) distributions.\nrelationship  \\(T\\) \\(F\\) distribution can established.\nUsing Eq. (12.4),\n\\[\n  T^2\n  = \\frac{Z^2}{V/n}\n  = \\frac{Z^2/1}{V/n}\n\\]\nnumerator \\(\\chi^2_1\\) distribution; hence, \\(T^2\\) \\(F_{1, n}\\) distribution.Theorem 12.12  ($F$ distribution) Let \\(X_1, X_2, \\dots, X_{n_1}\\) random sample size \\(n_1\\) \\(N(\\mu_1, \\sigma_1^2)\\) \\(Y_1, Y_2,\\dots, Y_{n_2}\\) independent random sample size \\(n_2\\) \\(N(\\mu_2, \\sigma_2^2)\\).\nrandom variable\n\\[\n   F\n   = \\frac{S_1^2/\\sigma_1^2}{S_2^2/\\sigma_2^2}\n\\]\nfollows \\(F_{n_1 - 1, n_2 - 1}\\) distribution.Proof. give partial proof .\n\\(F\\) statistic can rewritten \n\\[\n   F\n   = \\frac{S_1^2/\\sigma_1^2}{S_2^2/\\sigma_2^2}\n   = \\frac{U_1/\\nu_1}{U_2/\\nu_2}\n\\]\n\n\\[\\begin{align*}\n   U_1\n     &= \\displaystyle{\\frac{(n_1 - 1)S_1^2}{\\sigma_1^2}}\n   \\nu_1\n     = n_1 - 1, \\\\\n   U_2\n     &= \\displaystyle{\\frac{(n_2 - 1)S_2^2}{\\sigma_2^2}},\n   \\nu_2\n     = n_2 - 1.\n\\end{align*}\\]\n\\(U_1\\)  \\(U_2\\) chi-square distributions.\nPDF \\(\\frac{U_1/\\nu_1}{U_2/\\nu_2}\\) rest proof given.four R functions working \\(F\\) distribution form [dpqr]f(df1, df2), df1\\({} = \\nu_1\\) df2\\({} = \\nu2\\) (see Appendix D).Example 12.10  ($F$ distribution probabilities) Suppose \\(X\\sim F_{2, 10}\\).\nFind:\\(\\Pr(X < 1)\\)\\(x\\) \\(\\Pr(X > x) = 0.01\\)Using R:","code":"\npf(1, \n   df1 = 2, \n   df2 = 10)\n#> [1] 0.5981224\nqf(0.99, \n   df1 = 2, \n   df2 = 10)\n#> [1] 7.559432"},{"path":"SamplingDistributions.html","id":"CentralLimitTheorem","chapter":"12 Describing samples","heading":"12.6 The Central Limit Theorem","text":"Sampling distributions various statistics interest well-developed sampling normal distribution (Sect. 12.5).\nAlthough results important, know distribution population random sample drawn (usually case)?Sect. 12.4.2 general results given describing mean variance sample mean hold population distribution, say anything distribution.main result section following theorem, called Central Limit Theorem (CLT).Central Limit Theorem one important theorems statistics.Theorem 12.13  (Central Limit Theorem (CLT)) Let \\(X_1, X_2, \\dots, X_n\\) random sample distribution mean \\(\\mu\\) variance \\(\\sigma^2\\).\nrandom variable\n\\[\n   Z_n\n   = \\frac{\\bar{X} - \\mu}{\\sigma / \\sqrt{n}}\n\\]\nconverges distribution standard normal variable \\(n\\\\infty\\).Proof. Let random variable \\(X_i\\) \\((= 1, \\dots, n)\\) MGF \\(M_{X_i}(t)\\).\n\n\\[\n   M_{X_i}(t)\n   = 1 + \\mu'_1t + \\mu'_2\\frac{t^2}{2!} + \\dots .\n\\]\n\\(\\bar{X} = X_1/n + \\dots + X_n/n\\), \n\\[\n   M_{\\bar{X}}(t)\n   = \\prod^n_{= 1} M_{X_i/n}(t)\n   = \\left[ M_{X_i} (t/n)\\right]^n.\n\\]\nNow \\(\\displaystyle{Z_n = \\frac{\\sqrt{n}}{\\sigma}\\bar{X} - \\frac{\\sqrt{n}\\mu}{\\sigma}}\\), form \\(Y = aX + b\\) \n\\[\\begin{align*}\n  M_{Z_n}(t)\n    &= e^{-\\sqrt{n}\\mu  t/\\sigma}M_{\\bar{X}} (\\sqrt{n}t/\\sigma )\\\\\n    &= e^{-\\sqrt{n}\\mu t / \\sigma} \\left[ M_{X_i}\\left(\\frac{\\sqrt{n}t}{\\sigma n} \\right)\\right]^n\\\\\n    &  = e^{-\\sqrt{n}\\mu t /\\sigma}\\left[ M_{X_i}(t / \\sigma \\sqrt{n})\\right]^n\n\\end{align*}\\]\n\n\\[\\begin{align*}\n     \\log M_{Z_n}(t)\n     &= \\frac{-\\sqrt{n}\\mu t}{\\sigma} +\n        n\\log \\left[ 1 + \\frac{\\mu'_1 t}{\\sigma\\sqrt{n}} + \\frac{\\mu'_2 t^2}{2! \\, n\\sigma^2}\n        + \\frac{\\mu'_3 t^3}{3! \\, n\\sqrt{n}\\sigma^3}  + \\dots \\right] \\\\\n     &= \\frac{-\\sqrt{n}\\mu t}{\\sigma} + n\\left[ \\mu'_1 \\frac{t}{\\sigma\n\\sqrt{n}} + \\frac{\\mu'_2 t^2}{2n\\sigma^2} + \\frac{\\mu'_3t^3}{6n\\sqrt{n}\\sigma^3} + \\dots\n\\right]\\\\\n       & \\quad \\text{} - \\frac{n}{2} \\left[ \\frac{\\mu'_1t}{\\sigma\\sqrt{n}} +\n         \\dots \\right]^2 + \\frac{n}{3} \\left[\\phantom{\\frac{n}{3}}\\dots\\phantom{\\frac{n}{3}}\\right]^3 - \\dots \\\\\n     &=  \\frac{\\mu'_2 t^2}{2\\sigma^2} - \\frac{(\\mu'_1)^2 t^2}{2\\sigma^2} +\n\\text{ terms $1 / \\sqrt{n}$, etc}.\n\\end{align*}\\]\nNow,\n\\[\n   \\lim_{n\\\\infty} \\log M_{Z_n}(t)\n   =\n   \\frac{(\\mu'_2- (\\mu'_1)^2)}{\\sigma^2}\\frac{t^2}{2} = \\frac{t^2}{2}\n\\]\nterms \\((1/\\sqrt{n}\\,)\\), etc. tend zero \\(n\\\\infty\\), \\(\\mu'_2 - (\\mu'_1)^2 = \\sigma^2\\).\nThus \\(\\displaystyle{ \\lim_{n\\\\infty} M_{Z_n}(t) = e^{\\frac{1}{2} t^2}}\\), MGF \\(N(0, 1)\\) random variable.\n \\(Z_n\\) converges distribution standard normal.partial proof since concept convergence distribution defined, MGF  \\(X_i\\) assumed exists, need case.\nHowever, arguments used proof powerful worth understanding.‘Convergence distribution’ means  \\(Z_n\\) approaches normality \\(n\\\\infty\\).\n \\(n\\) ‘large’, \\(Z_n\\) expected approximate standard normal distribution.\nTransforming \\(Z_n\\) back sample mean, \\(\\bar{X}\\) can expected approximate \\(N(\\mu, \\sigma^2/n)\\) distribution.practice, distribution  \\(\\bar{X}\\) sufficiently close normal distribution  \\(n\\) larger  \\(25\\) situations.\nHowever, population distribution severely skewed, larger samples sizes may necessary approximate adequate practice.Central Limit Theorem states , random variable \\(X\\), distribution sample means :approximate normal distribution,mean \\(\\mu\\) (mean  \\(X\\)), andwith variance \\(\\sigma^2/n\\) ( \\(\\sigma^2\\) variance  \\(X\\)).approximation improves sample size \\(n\\) gets larger.applies random variable \\(X\\), whatever distribution.\nmany distributions highly skewed, approximation reasonable sample sizes larger  \\(20\\)  \\(30\\).\ndata comes normal distribution, distribution sample mean always normal distribution  \\(n\\).Example 12.11  R code , data come uniform distribution (Fig. 12.7, left panel).\nHowever, mean samples size \\(n = 10\\) (centre panel) \\(n = 20\\) (right panel) approximately distributed normal distribution.\nvariance sample means larger sample size smaller \\(n = 10\\), distribution looks like normal distribution larger sample size.\nFIGURE 12.7: Central Limit Theorem. Left: distribution individual observations. Centre: sampling distribution sample mean samples size \\(n = 10\\). Right: sampling distribution sample mean samples size \\(n = 20\\).\nExample 12.12  (CLT) soft-drink vending machine set amount drink dispensed random variable mean \\(200\\,\\text{mL}\\) standard deviation \\(15\\,\\text{mL}\\).\nprobability average amount dispensed random sample size \\(36\\) least \\(204\\,\\text{mL}\\)? \\(X\\) amount drink dispensed mL, distribution  \\(X\\) given.\nHowever, mean standard deviation  \\(X\\) given.\ndistribution sample mean (average), \\(\\bar{X}\\), can approximated normal distribution mean \\(\\mu = 200\\) standard error \\(\\sigma_{\\bar{X}} = \\sigma/\\sqrt{n} = 15/\\sqrt{36} = 15/6\\), according CLT.\n, \\(\\bar{X} \\sim N(200, (15/6)^2)\\).\nNow\n\\[\n   \\Pr(\\bar{X} \\ge 204) \\approx P_N\\left(Z \\ge \\frac{204 - 200}{15/6}\\right)\n   = \\Pr(Z \\ge 1.6).\n\\]\nR:Hence, \\(\\Pr(\\bar{X}\\ge 204) \\approx 0.0548\\),  \\(5.5\\)%.\n(, \\(P_N()\\) used denote probability event \\(\\) involving random variable assumed normal distribution.)Example 12.13  (Throwing dice) Consider experiment throwing fair die \\(n\\) times observe sum faces showing.\n\\(n = 12\\), find probability sum faces least \\(52\\).Let random variable \\(X_i\\) number showing \\(\\)th throw.\n, define \\(Y = X_1 + \\dots + X_{12}\\).\nseek \\(\\Pr(Y\\geq 52)\\).order use Theorem 12.13, note event ‘\\(Y\\geq 52\\)’ equivalent ‘\\(\\bar{X}\\geq 52/12\\)’, \\(\\bar{X} = Y/12\\) mean number showing \\(12\\) tosses.Since distribution  \\(X_i\\) rectangular \\(\\Pr(X_i = x) = 1/6\\) (\\(x = 1, 2, \\dots, 6\\)), \\(\\operatorname{E}[X_i] = 7/2\\) \\(\\operatorname{var}[X_i] = 35/12\\).follows \\(\\operatorname{E}[\\bar{X}] = 7/2\\) \\(\\operatorname{var}[\\bar{X}] = 35/(12^2)\\).\nTheorem 12.13,\n\\[\\begin{align*}\n  \\Pr(Y\\geq 52)\n    & \\simeq \\Pr(\\bar{X}\\geq 52/12)\\\\\n    &= P_N \\left(Z\\geq \\frac{52/12 - 7/2}{\\sqrt{35/144}}\\right)\\\\\n    &= 1 - \\Phi(1.690)\\\\\n    &= 0.0455\n\\end{align*}\\]\nprobability approximately \\(4.5\\)%.Example 12.13 can also solved using generalisation Central Limit Theorem.\ngeneralisation indicates normal distribution plays important role statistical theory.\nstates large class random variables converge distribution standardized normal.Theorem 12.14  (Convergence standard normal distribution) Let \\(X_1, X_2, \\dots, X_n\\) sequence independent random variables \\(\\operatorname{E}[X_i] = \\mu_i\\), \\(\\operatorname{var}[X_i] = \\sigma^2_i\\).\nDefine \\(Y = a_1X_1 + a_2X_2 + \\dots + a_nX_n\\).certain general conditions, including \\(n\\) large, \\(Y\\) distributed approximately \\(N(\\sum_i a_i\\mu_i, \\sum_i ^2_i\\sigma^2_i)\\).Proof. given.Theorem 12.14 makes assumptions distribution  \\(X_i\\).\n \\(X_i\\) normally distributed, \\(Y = \\sum_{= 1}^n a_i X_i\\) normal distribution  \\(n\\), large small.Example 12.14  (CLT (Voltages)) Suppose number independent noise voltages \\(V_i\\) (\\(= 1, 2, \\dots, n\\)).\nLet \\(V\\) sum voltages, suppose  \\(V_i\\) distributed \\(U(0, 10)\\).\n\\(n = 20\\), find \\(\\Pr(V > 105)\\).example Theorem 12.14 \\(a_i = 1\\).\nfind \\(\\Pr(V > 105)\\), distribution  \\(V\\) must known.Since \\(\\operatorname{E}[V_i] = 5\\) \\(\\operatorname{var}[V_i] = 100/12\\), Theorem 12.14 \\(V\\) approximate normal distribution mean \\(20\\times 5 = 100\\) variance \\(20\\times 100/12\\).\n, \\(\\displaystyle{\\frac{V - 100}{10\\sqrt{5/3}}}\\) distributed \\(N(0, 1)\\) approximately.\n\n\\[\n   \\Pr(V > 105)\n   \\approx P_N\\left (Z > \\frac{105 - 100}{12.91}\\right )\n   = 1 - \\Phi(0.387)\n   = 0.352.\n\\]\nprobability approximately \\(35\\)%.","code":"\n# The distribution of individuals has a uniform distribution:\nset.seed(4516) # For reproducibility\nsampleSize1 <- 10\nsampleSize2 <- 20\nsampleMeans1 <- NULL\nsampleMeans2 <- NULL\n\nnumberOfSimulations <- 1000\nfor (i in 1:numberOfSimulations){\n  # Take a sample of size 10\n  Xs1 <- runif(sampleSize1, \n               min = 0,\n               max = 5)\n  # Take a sample of size 10\n  Xs2 <- runif(sampleSize2, \n               min = 0,\n               max = 5)\n\n  # Find the sample mean\n  sampleMeans1 <- c(sampleMeans1, mean(Xs1) )\n  sampleMeans2 <- c(sampleMeans2, mean(Xs2) )\n}\n\npar(mfrow = c(1, 3))\n\nplot( x = c(-1, 0, 0, 5, 5, 6), \n      y = c(0, 0, 0.2, 0.2, 0, 0),\n      lwd = 2,\n      las = 1,\n      type = \"l\",\n      xlab = expression(italic(X)),\n      ylab = \"Prob. fn\",\n      main = \"Distribution of the\\nindividuals observations\")\n\nhist( sampleMeans1,\n      las = 1,\n      xlab = expression(Sample~means~bar(italic(X)) ),\n      main = expression( atop(Distribution~of~sample~means,\n                              \"for\"~italic(n)==10) ) ) \nhist( sampleMeans2,\n      las = 1,\n      xlab = expression(Sample~means~bar(italic(X)) ),\n      main = expression( atop(Distribution~of~sample~means,\n                              \"for\"~italic(n)==20) ) )\n1 - pnorm(1.6)\n#> [1] 0.05479929"},{"path":"SamplingDistributions.html","id":"NormalApproxCLT","chapter":"12 Describing samples","heading":"12.7 The normal approximation to the binomial","text":"\nnormal approximation binomial distribution (Sect. 8.3.4 can seen application Central Limit Theorem.\nessential observation sample proportion sample mean.\nConsider sequence independent Bernoulli trials resulting random sample \\(X_1, X_2, \\dots, X_n\\) \n\\[\n  X_i\n  =\n  \\begin{cases}\n     0  &  \\text{failure}\\\\\n     1  &  \\text{success}\n  \\end{cases}\n\\]\ndenotes whether \\(\\)th trial success.\nsum\n\\[\n   Y = \\sum_{= 1}^n X_i\n\\]\nrepresents number successes \\(n\\) trials \n\\[\n   \\bar{X}\n   =\n   \\frac{1}{n}\\sum_{= 1}^n X_i\n   =\n   \\frac{Y}{n}\n\\]\nsample mean representing proportion fraction trials successful.\ncontext, \\(\\bar{X}\\) usually denoted sample proportion \\(\\widehat{p}\\).Note \\(\\operatorname{E}[X_i] = p\\) \\(\\operatorname{var}[X_i] = p(1 - p)\\).\nTherefore\n\\[\n   \\operatorname{E}[Y] = np\n   \\quad\\text{}\\quad\n   \\operatorname{var}[Y] = np(1 - p)\n\\]\n\n\\[\n   \\operatorname{E}[\\bar{X}] = p\n   \\quad\\text{}\\quad\n   \\operatorname{var}[\\bar{X}] = \\frac{p(1 - p)}{n}.\n\\]\nTheorems 12.13  12.14 applicable  \\(\\bar{X}\\)  \\(Y\\) respectively.\nHence\n\\[\n   \\bar{X} = \\widehat{p}\n   \\sim N\\left(p, \\frac{p(1 - p)}{n}\\right)\\text{ approximately}\n\\]\n\n\\[\n   Y = n\\widehat{p}\n   \\sim N(np, np(1 - p))\\text{ approximately}.\n\\]","code":""},{"path":"SamplingDistributions.html","id":"exercises-1","chapter":"12 Describing samples","heading":"12.8 Exercises","text":"Selected answers appear Sect. E.12.Exercise 12.1  Consider FPCF (Def. 12.4).happens sample size population size?\nExplain sensible result.Show , large \\(n\\), FPCF approximately \\(\\sqrt{ 1 - n/N}\\).Exercise 12.2  random sample size \\(81\\) taken population mean \\(128\\) standard deviation \\(6.3\\).probability individual observation fall  \\(126.6\\)  \\(129.4\\)?probability sample mean fall  \\(126.6\\)  \\(129.4\\)?probability sample mean fall  \\(126.6\\)  \\(129.4\\)?Exercise 12.3  Let \\(Y_1\\), \\(Y_2\\), \\(\\dots\\), \\(Y_n\\) \\(n\\) independent random variables, PDF\n\\[\n   f_Y(y) = \\begin{cases}\n               3y^2 & \\text{$0\\le y \\le 1$};\\\\\n               0 & \\text{otherwise}.\n            \\end{cases}\n\\]Determine probability single observation within one standard deviation population mean.Determine probability sample mean within one standard deviation population mean, using Central Limit Theorem.Determine probability sample mean within one standard deviation population mean, using Central Limit Theorem.Exercise 12.4  Suppose weights eggs dozen carton weight normally distributed mean \\(59\\,\\text{g}\\) variance \\(0.7\\,\\text{g}\\).Find probability , sample \\(20\\) cartons, sample mean weight exceed \\(59.5\\,\\text{g}\\).Find probability sample twelve eggs produce sample variance greater  \\(1\\).Exercise 12.5  carton dozen eggs, number broken Poisson distribution mean \\(0.2\\).Find probability , sample \\(20\\) cartons, sample mean number broken eggs per carton one.\n(Use Central Limit Theorem.)Find probability , single carton, probability one egg broken.Exercise 12.6  random variable \\(M\\) following probability density function\n\\[\n    f_M(m) =\n    \\begin{cases}\n       3m^2 & \\text{$0 < m < 1$};\\\\\n       0    & \\text{otherwise}.\n    \\end{cases}\n\\]\nrandom sample size \\(9\\) taken distribution, sample mean \\(\\overline{M}\\) computed.Compute mean  \\(M\\).Compute variance  \\(M\\).State approximate distribution  \\(\\overline{M}\\) including parameters.Compute probability sample mean within \\(0.1\\) true mean.Exercise 12.7  Generate random sample size \\(n = 9\\) \\(N(10, 36)\\) distribution hundreds times.\nObtain mean variance  \\(\\sum X_i\\)  \\(\\overline{X}\\) sample.Verify distribution sample means approximately normally distributed expected.\n1, Explain expected.Exercise 12.8  Consider random variable \\(\\), defined \n\\[\n    = \\frac{Z}{\\sqrt{W/\\nu}},\n\\]\n \\(Z\\) standard normal distribution independent  \\(W\\), \\(\\chi^2\\) distribution \\(\\nu\\) degrees freedom.\nAlso, consider random variable \\(B\\), defined \n\\[\n    B = \\frac{W_1/\\nu_1}{W_2/\\nu_2},\n\\]\n \\(W_1\\)  \\(W_2\\) independent \\(\\chi^2\\) variables  \\(\\nu_1\\)  \\(\\nu_2\\) degrees freedom respectively.Write (derive) distribution  \\(\\), including parameters distribution.Write (derive) distribution  \\(B\\), including parameters distribution.Deduce distribution  \\(^2\\) special case distribution  \\(B\\), state values parameters true.Exercise 12.9  manufacturing plant produces \\(2\\) tonnes waste product given day, standard deviation \\(0.2\\) tonnes per day.\nFind probability , \\(20\\) day period, plant produces less \\(25\\) tonnes waste daily productions can assumed independent.Exercise 12.10  Let \\(Y\\) change depth river one day next measured (cms) specific location.\nAssume \\(Y\\) uniformly distributed \\(y \\[-70, 70]\\).Find probability mean change depth period \\(30\\) days greater \\(10\\,\\text{cm}\\).Use simulation estimate probability .Exercise 12.11  number deaths per year due typhoid fever assumed Poisson distribution rate \\(\\lambda = 4.1\\) per year.deaths year year can assumed independent, distribution \\(20\\) year period?Find probability \\(100\\) deaths due typhoid fever period \\(20\\) years.Exercise 12.12  probability cell lymphocyte  \\(0.2\\).Write exact expression probability sample containing \\(150\\) cells least \\(40\\) lymphocytes.\nEvaluate expression using R.Write approximate expression probability, evaluate .Exercise 12.13  Suppose probability person aged \\(80+\\) years dying receiving influenza vaccine  \\(0.006\\).\nsample \\(200\\) persons aged \\(80+\\) years:Write exact expression probability  \\(5\\) die vaccination influenza.\nEvaluate expression using R.Write approximate expression probability evaluate .\\(4\\) persons died sample  \\(200\\), conclusion make probability dying vaccination?\nJustify answer.Exercise 12.14  Illustrate Central Limit Theorem uniform distribution \\([-1, 1]\\) simulation repeated sampling, various sample sizes.Exercise 12.15  Illustrate Central Limit Theorem exponential distribution mean \\(1\\), simulation repeated sampling, various sample sizes.Exercise 12.16  Demonstrate Central Limit Theorem using R:uniform distribution, \\(-1 < x < 1\\) (symmetric distribution).exponential distribution parameter \\(1\\) (skewed distribution).","code":""},{"path":"OrderStatisticsChapter.html","id":"OrderStatisticsChapter","chapter":"13 Order statistcs","heading":"13 Order statistcs","text":"completion chapter, able :determine sampling distributions given order statistics.explain find distributions order statistics.use order statistics practical problems.","code":""},{"path":"OrderStatisticsChapter.html","id":"OrderStatisticsIntro","chapter":"13 Order statistcs","heading":"13.1 Introduction","text":"\nmany applications, information minimum value, maximum value, sample useful.\nexample, knowing maximum water height dam useful modelling controlled water releases; mean water height little use .\nminimum maximum values sample examples order statistics.generally, order statistics refer values sample arranged increasing order.\ngiven random sample \\(X_1, X_2, \\dots, X_n\\), order statistics defined denoted \n\\[\n  X_{(1)} \\le X_{(2)} \\le \\cdots \\le X_{(n)}\n\\]\n\\(X_{(k)}\\) \\(k\\)th order statistic, \\(k\\)th smallest value sample.Important order statistics sample size \\(n\\) include minimum value \\(X_{(1)}\\), maximum value \\(X_{(n)}\\), range \\(X_{(n)} - X_{(1)}\\).median also important order statistic:\n\\[\n   \\text{median} =\n   \\begin{cases}\n      \\displaystyle\\frac{X_{(n/2)} + X_{(n/2 + 1)}}{2} & \\text{$n$ even;}\\\\\n      X_{([n + 1]/2)}                     & \\text{$n$ odd.}\n  \\end{cases}\n\\]\ncircumstances, median \\(n\\) even uses slightly formula.inter-quartile range (IQR)index{Inter-quartile range} also order statistic:\n\\[\n   \\text{IQR}\n   =\n   X_{(3n/4)} - X_{(n/4 )}.\n\\]\nUnless sample size \\(n\\) multiple four, exact order statistics \\(X_{(3n/4)}\\) \\(X_{(n/4 )}\\) may defined, situation different ways exist approximate order statistics.Example 13.1  (Median: $n$ odd) Consider sample five die rolls, say \\(X_i\\):\n\\[\n  x_1 = 5; \\quad x_2 = 3; \\quad  x_3 = 5; \\quad  x_4 = 2; \\quad  x_5 = 6.\n\\]\norder statistics \n\\[\n  x_{(1)} = 2; \\quad  x_{(2)} = 3; \\quad  x_{(3)} = 5; \\quad  x_{(4)} = 5; \\quad  x_{(5)} = 6.\n\\]\nminimum value \\(X_{(1)} = 2\\), maximum value \\(X_{(5)} = 6\\).\nmedian value \\(X_{(3)} = 5\\).Example 13.2  (Median: $n$ even) Consider sample six die rolls, say \\(Y_i\\):\n\\[\n  y_1 = 1; \\quad y_2 = 4; \\quad  y_3 = 6; \\quad  y_4 = 2; \\quad  y_5 = 5 \\quad  y_6 = 1.\n\\]\norder statistics \n\\[\n  y_{(1)} = 1; \\quad  y_{(2)} = 1; \\quad  y_{(3)} = 2; \\quad  y_{(4)} = 4; \\quad  y_{(5)} = 5; \\quad  y_{(6)} = 6.\n\\]\nminimum value \\(y_{(1)} = 1\\), maximum value \\(y_{(4)} = 6\\).\nmedian value \\(\\left(X_{(3)} + X_{(4)}\\right)/2 = (2 + 4)/2 = 3\\), one observations sample.statistic, order statistics sampling distribution.\neasiest order statistics find distribution maximum value (\\(n\\)th order statistic; Sect. 13.2) minimum value (first order statistic; Sect. 13.3).","code":""},{"path":"OrderStatisticsChapter.html","id":"OrderStatisticsMaxDistribution","chapter":"13 Order statistcs","heading":"13.2 Sampling distribution of the maximum value","text":"Consider sample size \\(n\\), say \\(X_1, X_2, \\dots, X_n\\), \\(X_i\\) drawn independently distribution density function \\(f_X(x)\\) distribution function \\(F_X(x)\\).\ndefine \\(X_{(n)} = \\operatorname{max}\\{X_1, X_2, \\dots, X_n\\}\\) maximum value sample.distribution function \\(X_{(n)}\\) \\(F_{X_{(n)}}(x) = \\Pr(X_{{(n)}} \\le x)\\), definition: probability maximum value less given value \\(x\\).\nvalue \\(X_{(n)}\\) less equal value \\(x\\), values sample must less value \\(x\\).\nSince observations \\(X_1, X_2, \\dots X_n\\) independent identically distributed, probability observations less equal value \\(x\\) \n\\[\n   F_{X_{(n)}}(x) = [F_X(x)]^{n}\n\\]\n\\(x \\R_X\\).density function found differentiating \\(F_{X_{(n)}}(x)\\) respect  \\(x\\) (Sect. 3.4), obtain\n\\[\n   f_{X_{(n)}}(x) = n [F_X(x)]^{n - 1} \\times f_X(x)\n\\]\n\\(x \\R_X\\).Definition 13.1  (Distribution maximum value) Consider sample size \\(n\\), say \\(X_1, X_2, \\dots, X_n\\), \\(X_i\\) drawn independently continuous distribution density function \\(f_X(x)\\) distribution function \\(F_X(x)\\).\ndistribution function maximum value (order statistic \\(X_{(n)}\\)) \n\\[\n   F_{X_{(n)}}(x) =\n       \\begin{cases}\n      0                   & \\text{$x \\le 0$}\\\\\n      [F_X(x)]^{n}        & \\text{$0 < x < 1$}\\\\\n      1                   & \\text{$x \\ge 1$.}\n    \\end{cases}\n\\]\ndensity function \n\\[\n   f_{X_{(n)}}(x) =\n      n [F_X(x)]^{n - 1} \\times f_X(x)\n\\]\n\\(x\\R_X\\).Example 13.3  (Uniform distribution: maximum value) continuous uniform distribution (Sect. 8.2) defined \\([0, 1]\\), density function \n\\[\\begin{equation}\n   f_X(x; 0, 1)  = 1 \\quad\\text{$0\\le x\\le 1$},\n   \\tag{13.1}\n\\end{equation}\\]\ndistribution function \n\\[\n  F_X(x; 0, 1) =\n  \\begin{cases}\n    0                & \\text{$x < 0$};\\\\\n    \\displaystyle x  & \\text{$0 \\le x \\le 1$};\\\\\n    1                & \\text{$x > 1$}.\n  \\end{cases}\n\\]\nHence, density function maximum value sample size \\(n\\) , Def. 13.1,\n\\[\n  f_{X_{(n)}}(x)\n  = n\\, x^{n - 1} \\times 1\n  = n \\, x^{n - 1},\n\\]\n\\(0 < x < 1\\).\naddition, distribution function , Def. 13.1,\n\\[\n  F_{X_{(n)}}(x)\n  =\n  \\begin{cases}\n    0       & \\text{$x < 0$}\\\\\n    x^n     & \\text{$0 \\le x \\le 1$}\\\\\n    1       & \\text{$x > 1$.}\n  \\end{cases}\n\\]Using density function maximum value given , expected value variance maximum value can found definitions; example,\n\\[\\begin{align*}\n  \\operatorname{E}[X_{(n)}]   &= \\int_0^1 x \\cdot n x^{n - 1}\\, dx   = \\frac{n}{n + 1};\\\\\n  \\operatorname{E}[X^2_{(n)}] &= \\int_0^1 x^2 \\cdot n x^{n - 1}\\, dx = \\frac{n}{n + 2};\\\\\n  \\operatorname{var}[X_{(n)}] &= \\frac{n}{n + 2} - \\left( \\frac{n}{n + 1} \\right)^2.\n\\end{align*}\\]case \\(n = 10\\), :\n\\[\n  f_{X_{(n)}}(x)\n  = 10 \\, x^{9},\n\\]\n\\(0 < x < 1\\).\naddition, distribution function , Def. 13.1,\n\\[\n  F_{X_{(n)}}(x)\n  =\n  \\begin{cases}\n    0               & \\text{$x < 0$}\\\\\n    x^{10}          & \\text{$0 \\le x \\le 1$}\\\\\n    1               & \\text{$x > 1$.}\n  \\end{cases}\n\\]\nFig. 13.1 shows probability density function (left panel) distribution function (right panel).\nexample, probability largest value sample size \\(10\\) smaller  \\(0.8\\) \n\\[\n  F_{(10)}(0.8) = 0.8^{10} = 0.1073742\\dots\n\\]expected value variance maximum value :\n\\[\\begin{align*}\n  \\operatorname{E}[X_{(n)}]   &= \\frac{10}{11} \\approx 0.90909;\\\\\n  \\operatorname{var}[X_{(n)}] &= \\frac{10}{12} - \\left( \\frac{10}{11} \\right)^2\\approx 0.006887.\n\\end{align*}\\]\nresults can confirmed using quick computer simulation:\nFIGURE 13.1: distribution maximum value uniform distribution \\([0, 1]\\) sample size \\(10\\). Left: probability density function maximum value \\(X_{(10)}\\). Right: distribution function maximum value \\(X_{(10)}\\). grey, dotted lines show \\(F_{(10)}(0.8)\\).\n","code":"\nnum_Sims <- 5000\nsample_Size <- 10\n\nx_Max <- array( dim = num_Sims)\nfor (i in 1:num_Sims){\n  x <- runif(sample_Size,\n             min = 0,\n             max = 1)\n  x_Max[i] <- max(x) \n}\ncat(\"Mean of the maximum value\", mean(x_Max), \"\\n\")\n#> Mean of the maximum value 0.9072658\ncat(\"Variance of the maximum value\", var(x_Max), \"\\n\")\n#> Variance of the maximum value 0.007492214"},{"path":"OrderStatisticsChapter.html","id":"OrderStatisticsMinDistribution","chapter":"13 Order statistcs","heading":"13.3 Sampling distribution of the minimum value","text":"Consider sample size \\(n\\), say \\(X_1, X_2, \\dots, X_n\\), \\(X_i\\) drawn independently distribution density function \\(f_X(x)\\) distribution function \\(F_X(x)\\).\ndefine \\(X_{(1)} = \\operatorname{min}\\{X_1, X_2, \\dots, X_n\\}\\) minimum value sample.distribution function \\(X_{(1)}\\) \\(F_{X_{(1)}}(x) = \\Pr(X_{{(1)}} \\le x)\\).\nvalue \\(X_{(1)}\\) less equal value \\(x\\), must least one value less value \\(x\\).\nAlternatively, means sample values larger value \\(x\\) (must none less  \\(x\\)).probability values larger  \\(x\\) \\([1 - F_X(x)]^n\\) (since  \\(n\\) values identical, independent distributions).\nHence, probability values larger  \\(x\\) \n\\[\n  1 - [1 - F_X(x)]^n\n\\]\n\\(x \\R_X\\).\ndensity function found differentiating \\(F_{X_{(1)}}(x)\\) respect  \\(x\\) (Sect. 3.4), obtain\n\\[\n  f_{X_{(1)}}(x) = n [1 - F_X(x)]^{n-1} f_X(x)\n\\]\n\\(x \\R_X\\).Definition 13.2  (Distribution minimum value) Consider sample size \\(n\\), say \\(X_1, X_2, \\dots, X_n\\), \\(X_i\\) drawn independently continuous distribution density function \\(f_X(x)\\) distribution function \\(F_X(x)\\).\ndistribution function minimum value (order statistic \\(X_{(1)}\\)) \n\\[\n   F_{(X_1)}(x) = 1 - [1 - F_X(x)]^{n}\n\\]\ndensity function \n\\[\n   f_{(X_1)}(x) = n [1 - F_X(x)]^{n - 1} \\times f_X(x)\n\\]\n\\(x\\R_X\\).Example 13.4  (Uniform distribution: minimum value) continuous uniform distribution (Sect. 8.2) defined \\([0, 1]\\), density function distribution function giuven Example 13.3.\nUsing , density function minimum value sample size \\(n\\) \n\\[\n  f_{X_{(1)}}(x)\n  = n(1 - x)^{n - 1} \\times 1\n  = n (1- x)^{n - 1},\n\\]\n\\(0 < x < 1\\).\naddition, distribution function \n\\[\n  F_{X_{(1)}}(x)\n  =\n  \\begin{cases}\n    0             & \\text{$x < 0$}\\\\\n    1 - (1 - x)^n & \\text{$0 \\le x \\le 1$}\\\\\n    1             & \\text{$x > 1$.}\n  \\end{cases}\n\\]case \\(n = 10\\),\n\\[\n  f_{X_{(1)}}(x)\n  = 10 (1- x)^9,\n\\]\n\\(0 < x < 1\\), distribution function \n\\[\n  F_{X_{(1)}}(x)\n  =\n  \\begin{cases}\n    0                 & \\text{$x < 0$}\\\\\n    1 - (1 - x)^{10}  & \\text{$0 \\le x\\le 1$}\\\\\n    1                 & \\text{$x > 0$.}\n  \\end{cases}\n\\]\nFigure 13.2 shows probability density function (left panel) distribution function (right panel).\nexample, probability smallest value sample size \\(10\\) smaller  \\(0.2\\) \n\\[\n  F_{(1)}(0.2) = 1 - (1 - 0.2)^{10} = 0.8926\\dots\n\\]\ncan confirmed using small simulation:Using density function minimum value, expected value variance minimum value can found ; example,\n\\[\n  \\operatorname{E}[X_{(10)}] = \\int_0^1 x \\cdot 10(1 - x)^9\\, dx = 1/11 = 0.0909\\dots\n\\]\nFIGURE 13.2: distribution minimum value uniform distribution \\([0, 1]\\) sample size \\(10\\). Left: probability density function minimum value \\(X_{(1)}\\). Right: distribution function minimum value \\(X_{(1)}\\). grey, dotted lines show \\(F_{X_{(1)}}(0.2)\\).\n","code":"\nnum_Sims <- 5000\nsample_Size <- 10\n\nx_Min <- array( dim = num_Sims)\nfor (i in 1:num_Sims){\n  x <- runif(sample_Size,\n             min = 0,\n             max = 1)\n  x_Min[i] <- max(x) \n}\n\ncat(\"Proportion of smallest values less than 0.2: \", \n    round(sum(x_Min) / num_Sims, 3), \"\\n\")\n#> Proportion of smallest values less than 0.2:  0.91"},{"path":"OrderStatisticsChapter.html","id":"OrderStatisticsDistribution","chapter":"13 Order statistcs","heading":"13.4 Sampling distribution of the \\(k\\)th order statistic","text":"ideas developed distribution minimum maximum value sample can applied order statistics also.\nConsider sample continuous distribution density function \\(f_X(x)\\) distribution function \\(F_X(x)\\).\n\\(k\\)th order statistic \\(X_{(k)}\\) distribution function \\(F_{X_(k)}(x) = \\Pr(X_{(k)}\\le x)\\).\ndefinition, probability \\(k\\)th smallest value sample less (equal ) value \\(x\\).Writing \\(X_{(k)} \\le x\\) means least \\(k\\)  \\(n\\) observations smaller value \\(x\\); , \\(k, k + 1, \\dots n\\)  \\(n\\) observations smaller  \\(x\\).First, consider case exactly \\(k\\) observations smaller  \\(x\\).\nmeans must \\(k\\) observations value less equal \\(x\\); probability \n\\[\n  [F_X(x)]^k\n\\]\naddition, remaining \\(n - k\\) observations must value larger \\(x\\); probability \n\\[\n  [1 - F_{X}(x)]^{n -k}\n\\]\n \\(k\\) observations smaller  \\(x\\) located many different places among  \\(n\\) values;  \\(k\\) observations \\(\\binom{n}{k}\\) different arrangements among sample size \\(n\\).Combining, probability exactly \\(k\\) observations smaller  \\(x\\) \n\\[\n  \\binom{n}{k} \\cdot [F_X(x)]^k \\cdot [1 - F_{X}(x)]^{n - k}.\n\\]However, noted , may  \\(k\\) observations smaller  \\(x\\) also.\nFollowing ideas, probability exactly \\(k + 1\\) observations smaller  \\(x\\) \n\\[\n  \\binom{n}{k + 1} \\cdot [F_X(x)]^{k+1} \\cdot [1 - F_{X}(x)]^{n - (k+1)}\n\\]\n,  \\(n\\) observations less  \\(x\\).\nSumming probabilities gives\n\\[\n  \\Pr(X_{(k)} \\le x) = \\sum_{= k}^n \\binom{n}{} \\cdot [F_X(x)]^\\cdot [1 - F_{X}(x)]^{n - }.\n\\]distribution function, probability density function can obtained \n\\[\n  f_{X_{(k)}}(x)\n  = \\frac{d}{dx} F_{X_{(k)}}(x)\n  = \\frac{n!}{(k - 1)!\\,(n - r)!}f_{X}(x) [F_{X}(x)]^{k - 1} [1 - F_{X}(x)]^{n - r},\n\\]\n\\(n > k \\ge 1\\)  \\(n\\)  \\(k\\) integers.Definition 13.3  (Distribution order statistic: continuous rv) Consider sample size \\(n\\), say \\(X_1, X_2, \\dots, X_n\\), \\(X_i\\) drawn independently continuous distribution density function \\(f_X(x)\\) distribution function \\(F_X(x)\\).\ndistribution function \\(k\\)th order statistic \\(X_{(k)}\\) \n\\[\\begin{equation}\n  F_{X_{(k)}}(x) = \\Pr(X_{(k)}\\le x)\n  =\n  \\sum_{= k}^n \\binom{n}{k}\\times [F_{X}(x)]^\\times [1 - F_{X}(x)]^{n - }\n   \\tag{13.2}\n\\end{equation}\\]\ndensity function \\(k\\)th order statistic \\(X_{(k)}\\)\n:\n\\[\\begin{equation}\n   f_{X_{(k)}}(x) = \\frac{n!}{(k - 1)! (n - k)!} [F_X(x)]^{k-1} [1 - F_X(x)]^{n-k} f(x),\n   \\tag{13.3}\n\\end{equation}\\]\n\\(n > k > 0\\)  \\(n\\)  \\(k\\) integers.Example 13.5  (Uniform distribution: order statistics) Consider second sixth order statistics sample size \\(n = 10\\) Unif\\((0, 1)\\) distribution (\\(f_X(x)\\) \\(F_X(x)\\) given Example 13.3); using Equation (13.3):\n\\[\\begin{align*}\n   f_{X_{(2)}}(x)\n   &= \\frac{10!}{1!\\, 8!} x (1 - x)^8\n    = 90 x (1 - x)^8\\quad\\text{}\\\\\n   f_{X_{(6)}}(x)\n   &= \\frac{10!}{5!\\, 4!} x^5 (1 - x)^4\n    = 1260 x^5 (1 - x)^4.\n\\end{align*}\\]Consider simulating \\(5\\,000\\) samples size \\(n = 10\\) Unif\\((0, 1)\\) distribution:distribution \\(X_{(2)}\\) (.e., samples2) \\(X_{(6)}\\) (.e., samples6) shown Fig.  13.3.\nFIGURE 13.3: Simulating distribution two order statistics uniform distribution \\([0, 1]\\) sample size \\(10\\). Left: distribution \\(X_{(2)}\\). Right: distribution \\(X_{(6)}\\). solid lines show theoretical distributions.\nExample 13.6  (Air conditioner servicing) new server room four air-conditioning units installed.\nlifetime unit (hours) follows exponential distribution mean \\(\\lambda = 2000\\,\\text{h}\\).\nunits operate independently.\nservice technician called two units fail.\nseek distribution time call technician made.Let \\(T_1, T_2, T_3, T_4\\) denote lifetimes four units.\n:\n\\[\n  T_i \\sim \\text{Exponential}(\\lambda)\\quad\\text{$= 1, 2, 3, 4$},\n\\]\n\\(\\lambda = 1/2000\\), \\(T_i\\) independent.\n,\n\\[\n  f_{T_i}(t)= \\frac{1}{\\lambda}\\exp(-t/\\lambda)\n  \\quad\\text{}\\quad\n  F_{T_i}(t)= \\exp(-t/\\lambda)\n\\]\n\\(0 < x < \\infty\\).Define order statistics \\(T_{(1)} < T_{(2)} < T_{(3)} < T_{(4)}\\).\ntechnician called time \\(T_{(2)}\\), second failure.\nSince \\(n = 4\\) \\(k = 2\\), distribution \\(T_{(2)}\\) (REF)\n\\[\n   f_{T_{(2)}}(t) =\n   6 [\\exp(-t/\\lambda)]^1 [1 - \\exp(-t/\\lambda)] ^2 \\times \\frac{1}{\\lambda}\\exp(-t/\\lambda),\n\\]\ncomplicated.However, exponential distribution memoryless property (REF), different approach much simpler.\ntime first failure \\(T_{(1)}\\) probability density function\n\\[\n   T_{(1)} \\sim \\text{Exponential}(4\\lambda)\n\\]\nsince \\(4\\) units can fail first.\ntime first second failure also exponential distribution, due memoryless property, means \n\\[\n  T_{(2)} - T_{(1)} \\sim \\text{Exponential}(3\\lambda)\n\\]\n\\(3\\) units remain first failure.Thus, total time second failure :\n\\[\n  T_{(2)} = E_1 + E_2\n\\]\n\n\\[\n  E_1 \\sim \\text{Exp}(4\\lambda) \\quad\\text{}\\quad E_2 \\sim \\text{Exp}(3\\lambda).\n\\]distribution exponential, sum two independent exponentials different rates.expected value \n\\[\n\\operatorname{E}[T_{(2)}] = \\frac{1}{4\\lambda} + \\frac{1}{3\\lambda} = \\frac{1}{\\lambda} \\left( \\frac{1}{4} + \\frac{1}{3} \\right) = 2000 \\cdot \\frac{7}{12} = 1166.6\\dots\\,\\text{h}.\n\\]average, technician called  \\(1166.67\\,\\text{h}\\).PDF \\(T_{(2)}\\) can found using convolution:\n\\[\n  f_{T_{(2)}}(t) = \\int_0^t f_{E_1}(x) f_{E_2}(t - x)\\, dx\n\\]\n\\(f_{E_1}(x) = 4\\lambda e^{-4\\lambda x}\\), \\(f_{E_2}(x) = 3\\lambda e^{-3\\lambda x}\\).\napproach can extended \\(k\\)th failure  \\(n\\) components.","code":"\nn <- 10   # Take samples of size 10\n\nsamples2 <- replicate(5000,               # 1000 repetitions\n                      sort(runif(n))[2])  # 2nd order statistic\nsamples6 <- replicate(5000,               # 1000 repetitions\n                      sort(runif(n))[6])  # 6th order statistic"},{"path":"OrderStatisticsChapter.html","id":"OrderStatisticsDiscrete","chapter":"13 Order statistcs","heading":"13.5 Order statistics for discrete random variables","text":"Working order statistics easier continuous random variables discrete random variables, ties possible continuous random variables (since \\(\\Pr(X = x) = 0\\) value \\(x\\) \\(X\\) continuous random variable).\nresult, formulas discrete random variables complicated continuous random variables, since need accommodate possibility ties sample.Let \\(X_1, X_2, \\dots, X_n\\) ..d. discrete distribution probability mass function\n\\[\n   p_X(x) = \\Pr(X = x),\n\\]\ndistribution function\n\\[\n   F_X(x) = \\Pr(X \\le x).\n\\]\ndiscrete case, defining\n\\[\n   F_X(x^-) = \\lim_{t \\uparrow x} F_X(t) = \\Pr(X < x).\n\\]\nproves helpful follows (see Fig. 13.4).\ndiscrete random variables defined consecutive integers, \\(x^- = x - 1\\).\nFIGURE 13.4: Explaining meaning \\(F_X(b^-)\\).\nbegin first order statistic (minimum value) establish general ideas working discrete random variables.\nfirst order statistic \\(X_{(1)} = \\min\\{ X_1, X_2, \\dots, X_n \\}\\); , following similar logic continuous case,\n\\[\\begin{align*}\nF_{X_{(1)}}(x)\n  &= \\Pr(X_{(1)} \\le x) \\\\\n  &= 1 - \\Pr(X_{(1)} > x) \\\\\n  &= 1 - \\Pr(X_1 > x, X_2 > x, \\dots, X_n > x) \\\\\n  &= 1 - \\big[ 1 - F_X(x) \\big]^n.\n\\end{align*}\\]\nprobability mass function probability observations least \\(x\\), greater  \\(x\\):\n\\[\\begin{align*}\n  p_{X_{(1)}}(x)\n    &= \\Pr(X_{(1)} \\ge x) - \\Pr(X_{(1)} \\ge x+1) \\\\\n    &= \\big[ 1 - F_X(x^-) \\big]^n - \\big[ 1 - F_X(x) \\big]^n.\n\\end{align*}\\]Example 13.7  (Rolling fair die: minimum) Suppose fair die rolled \\(n = 10\\) times, number \\(X\\) appears roll recorded.\n\\(X \\sim\\text{Unif}(1, 6)\\) \n\\[\n  p_X(x)\n  =\n  \\begin{cases}\n    1/6 & \\text{$x = 1, 2, \\dots 6$}\\\\\n    0   & \\text{elsewhere}\n  \\end{cases}\n  \\qquad\n  \\text{}\n  \\qquad\n  F_X(x)\n  =\n  \\begin{cases}\n    0                   & \\text{$x < 1$}\\\\\n    \\lfloor x \\rfloor/6 & \\text{$1 \\le x \\le 6$}\\\\\n    1                   & \\text{$x > 6$.}\n  \\end{cases}\n\\]distribution function minimum value (.e., \\(X_{(1)}\\)) \n\\[\\begin{align*}\n  \\Pr\\big(X_{(1)} \\le x\\big)\n  &= \\big[1 - F_X(x - 1)\\big]^{10} \\\\\n  &=\n  \\begin{cases}\n    0 & \\text{$x < 1$}\\\\\n    \\displaystyle 1 - \\left[1 -\\frac{\\lfloor{x}\\rfloor}{6}\\right]^{10} & \\text{$1 \\le x\\le 6$}\\\\\n    1 & \\text{$x > 6$}\n  \\end{cases}\n\\end{align*}\\]\n\\(1 \\le x\\le 6\\).\nfocus distribution function integer outcomes ,\n\\[\n  F_{X_{(1)}}(x)\n  =\n  1 - \\left(\\frac{6 - x}{6}\\right)^{10} \\qquad\\text{$x = 1, 2\\dots, 6$}.\n\\]\nSince, dice, outcomes rolling consecutive integers, \\(x^- = x - 1\\); hence, corresponding probability mass function :\n\\[\n  \\Pr\\left(X_{(1)} = s\\right)\n  = \\left(\\frac{6 - x}{6}\\right)^{10} - \\left(\\frac{5 - x}{6}\\right)^{10}\n  \\quad\n  \\text{$x = 1, \\dots, 6$.}\n\\]\ncan computed R, plotted (dots Fig. 13.5, left panel):situation can simulated plotted also (bars Fig. 13.5, left panel):example, means set \\(10\\) rolls, probability minimum roll two \n\\[\n  \\Pr( X_{(7)} = 2) \\approx 0.14 =  14\\%.\n\\]generally, \\(k\\)th order statistic \\(X_{(k)}\\) discrete random variable \\(X\\), let \\(X_1, X_2, \\dots, X_n\\) ..d. discrete distribution \n\\[\\begin{align*}\n  p_X(x)   &= \\Pr(X = x);\\\\\n  F_X(x)   &= \\Pr(X \\le x); \\text{}\\\\\n  F_X(x^-) &=\\Pr(X < x).\n\\end{align*}\\]First, define number observations less equal  \\(x\\) \n\\[\n  Y = \\sum_{j = 1}^n \\{X_j\\le x\\} \\sim \\mathrm{Binom}\\!\\left(n,\\,F_X(x)\\right).\n\\]\n\n\\[\\begin{align*}\n  F_{X_{(k)}}(x)\n  &= \\Pr\\bigl(X_{(k)} \\le x\\bigr)\n   = \\Pr(Y \\ge k) \\\\\n  &= \\sum_{= k}^n \\binom{n}{}\\,F_X(x)^{}\\,[1 - F_X(x)]^{\\,n - }.\n\\end{align*}\\]discrete random variable \\(X\\), probability mass function much distributions function ‘jumps’ values  \\(x\\).\n, probability mass function \\(X_{(k)}\\) can found distribution function \n\\[\\begin{align*}\n  p_{X_{(k)}}(x)\n    &= \\Pr\\bigl(X_{(k)}=x\\bigr)\\\\\n    &= F_{X_{(k)}}(x) - F_{X_{(k)}}(x^-) \\notag\\\\\n    &= \\sum_{=k}^n \\binom{n}{}\\,[F_X(x)]^{}\\,[1-F_X(x)]^{\\,n-}\n     \\;- \\notag\\\\\n    &\\qquad \\qquad\n       \\sum_{= k}^n \\binom{n}{}\\,[F_X(x^-)]^{}\\,[1 - F_X(x^-)]^{\\,n - }.\n\\end{align*}\\]Definition 13.4  (Distribution order statistic: discrete rv) Consider sample size \\(n\\), say \\(X_1, X_2, \\dots, X_n\\), \\(X_i\\) drawn independently discrete distribution probability mass function \\(f_X(x)\\) distribution function \\(F_X(x)\\).\ndistribution function \\(k\\)th order statistic \\(X_{(k)}\\) \n\\[\\begin{equation}\n  F_{X_{(k)}}(x)\n  = \\Pr\\big(X_{(k)} \\le x\\big)\n  = \\sum_{= k}^n \\binom{n}{}\\, F_X(x)^{}\\,[1 - F_X(x)]^{n - }\n  \\tag{13.4}\n\\end{equation}\\]\nprobability mass function \\(k\\)th order statistic \\(X_{(k)}\\) integer values  \\(x\\R\\) :\n\\[\\begin{align}\n  p_{X_{(k)}}(x)\n  &= F_{X_{(k)}}(x) - F_{X_{(k)}}(x^-)\\notag \\\\\n  &= \\sum_{= k}^n\n         \\binom{n}{}\n         \\left\\{\n           F_X(x)^{}\\,[1 - F_X(x)]^{\\,n - }\n           -\n           F_X(x^-)^{}\\,[1 - F_X(x^-)]^{\\,n - }\n         \\right\\}\n   \\tag{13.5}\n\\end{align}\\]\n\\(n > k > 0\\)  \\(n\\)  \\(k\\) integers.Example 13.8  (Rolling fair die: general order statistic) Continuing Example 13.7, standard six-sided die rolled \\(10\\) times.\ngeneral \\(k\\)-th order statistic \\(X_{(k)}\\) \n\\[\n  F_{X_{(k)}}(x)\n  = \\Pr\\big(X_{(k)} \\le x\\big)\n  = \\sum_{= k}^{10} \\binom{10}{}\\,\\left(\\frac{x}{6}\\right)^{}\\,\\left(1 - \\frac{x}{6}\\right)^{10 - },\n\\]\n\\(x\\\\{1,\\dots,6\\}\\).Since, dice, outcomes rolling consecutive integers, \\(x^- = x - 1\\), corresponding probability mass function found using discrete difference\n\\[\\begin{align*}\n  \\Pr\\big(X_{(k)} =  x\\big)\n  &=\n  \\Pr\\big(X_{(k)}\\le x\\big) - \\Pr\\big(X_{(k)}\\le x - 1\\big)\\\\\n  &= \\sum_{= k}^{10} \\binom{10}{}\n     \\left\\{\\left(\\frac{x}{6}\\right)^{}\\,\\left(1 - \\frac{x}{6}\\right)^{10 - } - \\right.\\\\\n  &\\phantom{={}\\sum_{= k}^{10} \\binom{10}{}\\Huge\\{}\n     \\left.\\left(\\frac{x - 1}{6}\\right)^{}\\,\\left(1 - \\frac{x - 1}{6}\\right)^{10 - }\\right\\}\n\\end{align*}\\]\n, instance, seventh order statistic (.e., \\(k = 7\\)) \n\\[\n  \\Pr\\big(X_{(7)} = x\\big)\n  =\n  \\sum_{= 7}^{10} \\binom{10}{}\n     \\left\\{\\left(\\frac{x}{6}\\right)^{}\\,\\left(1 - \\frac{x}{6}\\right)^{10 - }\n     -\n            \\left(\\frac{x - 1}{6}\\right)^{}\\,\\left(1 - \\frac{x - 1}{6}\\right)^{10 - }\\right\\}\n\\]\n\\(x\\\\{ 1, 2, 3, 4, 5, 6\\}\\).\ncan computed R, plotted (dots Fig. 13.5, right panel):situation can simulated plotted also (bars Fig. 13.5, right panel):example, means , set \\(10\\) rolls, probability seventh smallest roll two \n\\[\n  \\Pr( X_{(7)} = 2) \\approx 0.019 = 1.9\\%.\n\\]\nFIGURE 13.5: Left: distribution first order statistic (minimum roll), sets \\(10\\) rolls. Right: distribution \\(7\\)th order statistic, sets \\(10\\) rolls. cases, bars show values \\(1\\,000\\) simulated sets \\(10\\) rolls, solid dots theoretical values.\n","code":"\n### THEORETICAL\nx <- 1:6\nCDF <- c(0,\n         1 - ((6 - x)/6)^10)\nround(CDF, 4)\n#> [1] 0.0000 0.8385 0.9827 0.9990 1.0000 1.0000\n#> [7] 1.0000\n\nPMF <- diff(CDF)\nround(PMF, 4)\n#> [1] 0.8385 0.1442 0.0164 0.0010 0.0000 0.0000\n### SIMULATION\nset.seed(62004) # For reproducibility\n\n# Replicate many sets of 10 rolls of a die\nrolls_In_Set <- 10  # Sets f 10 rolls\nnum_Sims <- 1000    # Number of simulations\n\nrolls <- sample( x = 1:6,\n                 size = rolls_In_Set * num_Sims,\n                 replace = TRUE)\n                 \n# Arrange each set of 10 as a row \nrolls <- matrix(data = rolls,\n                nrow = num_Sims)\n\n# Find the minimum in each row (i.e., each set of 10 rolls)\nmin_Roll <- apply(rolls,\n                  MARGIN = 1,\n                  FUN = \"min\")\nmin_Roll <- factor(min_Roll, \n                   levels = c(1:6))\n### SIMULATED \n# Simulation, using  rolls  from the last Example\nOrder_Stats_7 <- apply( rolls,\n                        MARGIN = 1, # i.e., across rows\n                        FUN = function(x){ sort(x)[7]} )\n\nOrder_Stats_7 <- factor(Order_Stats_7, \n                        levels = c(1:6))\ntable(Order_Stats_7)/num_Sims\n#> Order_Stats_7\n#>     1     2     3     4     5     6 \n#> 0.000 0.009 0.158 0.378 0.383 0.072\n### SIMULATED \n# Simulation, using  rolls  from the last Example\nOrder_Stats_7 <- apply( rolls,\n                        MARGIN = 1, # i.e., across rows\n                        FUN = function(x){ sort(x)[7]} )\n\nOrder_Stats_7 <- factor(Order_Stats_7, \n                        levels = c(1:6))\ntable(Order_Stats_7)/num_Sims\n#> Order_Stats_7\n#>     1     2     3     4     5     6 \n#> 0.000 0.009 0.158 0.378 0.383 0.072\n\n### THEORETICAL\nx_Values <- 1:6\ni <- 7:10\nDF <- array( dim = 6)\n\nfor (x in seq_along(x_Values)){\n   DF[x] <- sum( choose(10, i) *\n                 (x/6)^i * ( (1 - (x/6)))^(10 - i) )\n}\n\nPMF <- diff( c(0, DF) )\nround(PMF, 3)\n#> [1] 0.000 0.019 0.152 0.387 0.371 0.070"},{"path":"OrderStatisticsChapter.html","id":"QuantilesOrderStats","chapter":"13 Order statistcs","heading":"13.6 Quantiles and order statistics","text":"far, order statistics defined.\nsimilar, different, concept quantilesFor random variable \\(X\\) distribution function \\(F_X(x)\\), \\(p\\)-th quantile \\(q_p\\) \n\\[\n   q_p\n   = F^{-1}(p)\n   = \\inf \\{x \\\\mathbb{R} : F(x) \\ge p \\} \\quad \\text{$0 < p < 1$}.\n\\]\nQuantiles fixed values determined distribution, whereas order statistics random variables derived finite sample.\nSample quantiles typically estimated data using order statistics.\nexample, sample median \\(X_{(\\lceil n/2 \\rceil)}\\) (average two adjacent order statistics  \\(n\\) even).General order statistics R found first sorting sample values using sort(), selecting relevant values result using indexing; example, sort(x-Values)[3] selects third sorted value sample.R can also find quantiles, using quantile() function.\nquantile(x, probs) computes order statistics specified probabilities (called quantiles) defined prob.\nGenerally, exact quantiles found, approximated.\nR offers nine different methods approximating quantiles exactly defined (Hyndman Fan 1996).Example 13.9  (Quantiles R) ","code":"\nset.seed(9901) # For reproducibility\n\nx_Sample <- runif(50, \n                  min = 0, \n                  max = 1)\n# Place sample observations in order\nx_Sort <- sort(x_Sample)\n\n# Examine first few values:\nhead( round( x_Sort, 4) )\n#> [1] 0.0041 0.0309 0.0310 0.0324 0.0411 0.0430\n\n# Compute 25th quantiles (first quartile)\n# Using each of the none different methods in R:\nfor (method in (1:9)){\n  cat(\"Method \", method, \": \",\n      round( quantile(x_Sort, 0.25, type = method), 5),\n      \"\\n\",\n      sep = \"\")\n}\n#> Method 1: 0.17359\n#> Method 2: 0.17359\n#> Method 3: 0.17055\n#> Method 4: 0.17207\n#> Method 5: 0.17359\n#> Method 6: 0.17283\n#> Method 7: 0.17715\n#> Method 8: 0.17334\n#> Method 9: 0.1734"},{"path":"OrderStatisticsChapter.html","id":"OrderStatisticsExercises","chapter":"13 Order statistcs","heading":"13.7 Exercises","text":"Selected answers appear Sect. E.13.Exercise 13.1  Prove probably density function order statistics case continuous uniform distribution, shown Eq. (13.1), valid density function.\n(Hint: observe similarity Eq. (13.1) beta density function Sect. 8.6.)Exercise 13.2  Prove expected value variance order statistics case continuous uniform distribution \n\\[\n  \\operatorname{E}[X] = k/(n + 1)\\qquad\\text{}\\qquad \\operatorname{var}[X] = \\frac{k (k + 1)}{(n + 1)(n + 2)},\n\\]\nSect. 13.4.\n(Hint: observe similarity Eq. (13.1) beta density function Sect. 8.6.)Exercise 13.3  Find distribution order statistics sample size \\(n\\) drawn continuous uniform distribution defined \\([-1, 1]\\).PLOTS??Exercise 13.4  Find distribution order statistics samples size \\(n\\) drawn continuous distribution\n\\[\n  f_Y(y) = y/2\\quad\\text{$0 < y < 2$}\n\\]\nzero elsewhere.PLOTSExercise 13.5  Suppose sample  \\(n\\) independent values taken exponential distribution \\(\\text{Exp}(\\lambda)\\) (REF).Determine distribution function minimum value, \\(X_{(1)}\\).Hence determine density function minimum value, \\(X_{(1)}\\).probability minimum value sample size \\(n\\) larger \\(\\lambda/2\\)?Compute expected value minimum value sample.PLOTSExercise 13.6  Suppose sample  \\(n\\) independent values taken exponential distribution \\(\\text{Exp}(\\lambda)\\) (REF).Determine distribution function maximum value, \\(X_{(n)}\\).Hence determine density function maximum value, \\(X_{(n)}\\).probability maximum value sample size \\(n\\) larger \\(\\lambda/2\\)?Compute expected value maximum value sample.PLOTSExercise 13.7  new computer server room four air-conditioning units installed.\nlifetime unit follows exponential, mean lifetime \\(2000\\,\\text{h}\\).\ntwo units fail, service technician called.Describe distribution showing wait time technician need called.Exercise 13.8  Consider Equation (13.5), gives \\(k\\)th order statistic discrete random variable.Show expression given probability mass function minimum value given using \\(k = 1\\) Equation (13.5).Use Equation (13.5) find formula probability mass function maximum value.Exercise 13.9  probability mass function \\(k\\)th order statistic can found different way shown Sect. 13.5.\nFirst, define\n\\[\\begin{align*}\n  &= F_X(x^-) = \\Pr(X < x);\\\\\n  b & = p_X(x) = \\Pr(X = x); \\text{}\\\\\n  c &= 1 - - b = \\Pr(X > x),\n\\end{align*}\\]\nuse \\((R, S, T)\\) denote number observations less , equal , greater  \\(x\\), respectively.Show \\((R,S,T) \\sim \\mathrm{Multinomial}\\big(n; ,b,c\\big)\\).Consider discrete random variable \\(X\\), \n\\[\np_X(x) =\n\\begin{cases}\n   0.2 & \\text{$x = 1$};\\\\\n   0.5 & \\text{$x = 2$};\\\\\n   0.3 & \\text{$x = 3$};\\\\\n   0   & \\text{elsewhere.}\n\\end{cases}\n  \\]\nShow expressions Equation (13.4) equivalent case \\(n = 3\\) finding second order statistic \\(X_{(2)}\\).Exercise 13.10  Suppose choose number random continuous uniform distribution  \\(0\\)  \\(1\\).\n, keep choosing numbers \\(\\text{Unif}(0, 1)\\) distribution obtain number larger one started .\nrecord number times select number success (.e., larger number teh initially-selected number).precisely:Let \\(X\\sim\\text{Unif}(0, 1)\\), initial number.draw \\(Y_1, Y_2, \\dots\\sim\\text{Unif}(0, 1)\\), ..d., get value \\(Y_n > X\\).score \\(N\\), number trials happens.expected value  \\(N\\), number values draw exceed initially-selected number?","code":""},{"path":"BayesianIntro.html","id":"BayesianIntro","chapter":"14 Introduction to Bayesian statistics","heading":"14 Introduction to Bayesian statistics","text":"completion chapter able :explain idea Bayesian statistics.compute conditional, marginal, prior posterior distributions context Bayesian statistics.compute conjugate distributions identify hyperparameters Bayesian situations.","code":""},{"path":"BayesianIntro.html","id":"RandomParameters","chapter":"14 Introduction to Bayesian statistics","heading":"14.1 Random parameters","text":"far, parameters used describe model populations fixed values.\napplications, though, parameters may assumed distributions.\nidea best introduced using examples.Example 14.1  (Exam scores) score achieved students exam approximately normal mean \\(\\theta\\).\nscore achieved specific student \\(X\\) can modelled normal distribution, using mean \\(\\theta\\).\nmean \\(\\theta\\) constant, varies (due specific questions appear exam, amount study completed individual students, etc.), follows approximately normal distribution (mean 60 standard deviation 10).Example 14.2  (Vehicle crashes) number motor vehicle crashes per day occur town vary according Poisson distribution mean \\(\\theta\\).\nnumber crashes specific day \\(X\\) can modelled distribution, mean \\(\\theta\\) 10 per day fine, 20 per day wet.\n10% days wet.Example 14.3  (Goal kicking) probability rugby league goal-kicker lands goal \\(\\theta\\).\nnumber kicks landed \\(X\\) can modelled using binomial distribution probability \\(\\theta\\), probability successful kick \\(\\theta\\) may vary (depending conditions, kick taken , taking kick, ), \\(\\theta \\sim \\text{Beta}(4, 4)\\).examples, distribution \\(f_{X}(x; \\theta)\\) describes data given value \\(\\Theta\\).\nHowever, value \\(\\Theta\\) distribution, \\(f_\\Theta(\\theta)\\), actual distribution \\(f_{X}(x; \\theta)\\) depends value \\(\\Theta\\) realised.\nHence, distribution \\(f_X(x; \\theta)\\) effectively conditional distribution \\(f_{x|\\Theta}(x \\mid \\Theta)\\), parameter \\(\\Theta\\) effectively random variable.marginal distribution \\(X\\) (.e., \\(f_X(x)\\)) can found Def. 4.6 \\(X\\) \\(\\Theta\\) continuous:\\[\\begin{equation}\n   f_X(x)\n   = \\int f_{X \\mid \\Theta}(x, \\theta)f_\\Theta(\\theta)\\,d\\theta.\n   \\tag{14.1}\n\\end{equation}\\]\n\\(X\\) \\(\\Theta\\) discrete, Def. 4.5 yields\\[\\begin{equation}\n   p_X(x)\n   = \\sum_\\theta p_{X \\mid \\Theta}(x, \\theta)p_\\Theta(\\theta).\n   \\tag{14.2}\n\\end{equation}\\]\nmixed cases can dealt similarly:\\[\\begin{equation}\n   f_X(x)\n   = \\sum_\\theta f_{X \\mid \\Theta}(x, \\theta)p_\\Theta(\\theta)\n   \\tag{14.3}\n\\end{equation}\\]\n\\(X\\) continuous \\(\\Theta\\) discrete, \\[\\begin{equation}\n   p_X(x)\n   = \\int p_{X \\mid \\Theta}(x, \\theta)f_\\Theta(\\theta)\\,d\\theta\n   \\tag{14.4}\n\\end{equation}\\]\n\\(X\\) discrete \\(\\Theta\\) continuous.special cases marginal distributions standard distribution closed form.\nparameters describe distribution parameter \\(\\Theta\\) called hyperparameters.Example 14.4  (Exam scores) (Follows Example 14.1.)\nscore \\(X\\) achieved student exam normal mean \\(\\Theta\\) standard deviation \\(\\sigma = 5\\); , given value \\(\\Theta\\), \\(X \\sim N(\\Theta, 5^2)\\).\nmean \\(\\Theta\\) varies student student normal distribution, two hyperparameters: mean 60 standard deviation 10; , \\(\\Theta \\sim N(60, 10^2)\\).\n(14.1), marginal PDF \\(X\\) \n\\[\\begin{align}\n  f_X(x)\n  &= \\int f_{X \\mid \\Theta}(x, \\theta)f_\\Theta(\\theta)\\,d\\theta\\notag\\\\\n  &= \\int_{-\\infty}^\\infty \\frac{1}{5\\sqrt{2\\pi}}\n     \\exp\\left[ -\\frac{1}{2}\\left(\\frac{x - \\theta}{5}\\right)^2 \\right]\n     \\frac{1}{10\\sqrt{2\\pi}}\n  \\exp\\left[ -\\frac{1}{2}\\left(\\frac{\\theta - 60}{10}\\right)^2 \\right]\\,d\\theta\\notag\\\\\n  &= \\frac{1}{11.18\\sqrt{2\\pi}}\n     \\exp\\left[-\\frac{1}{2}\\left(\\frac{x - 60}{11.18}\\right)^2\\right]\n     \\tag{14.5}\n\\end{align}\\]\nconsiderable algebra involving completing square.\n, \\(X\\sim N(60, 11.18^2)\\) (see Fig. 14.1, left panel; note \\(11.18 = \\sqrt{5^2 + 10^2}\\).)describes distribution \\(X\\), taking account various values \\(\\Theta\\) possible (described \\(f_\\Theta(\\theta)\\).)Example 14.5  (Vehicle crashes) (Follows Example 14.2.)\nLet \\(X\\) number motor vehicle crashes per day.\n\\(X \\sim \\text{Pois}(\\Theta)\\) given value \\(\\Theta\\).\nHowever, mean parameter \\(\\Theta\\) distribution \\(\\Pr(\\Theta = 10) = 0.9\\) \\(\\Pr(\\Theta = 20) = 0.1\\), \n\\[\n   p_\\Theta(\\theta) = 0.1^{(\\theta - 10)/10} 0.9^{(20 - \\theta)/10}\\quad\\text{$\\theta = 10, 20$}.\n\\]\n(14.2), marginal pf \\(X\\) \n\\[\\begin{align}\n  p_X(x)\n  &= \\sum_\\theta p_{X \\mid \\Theta}(x, \\theta)p_\\Theta(\\theta)\\notag\\\\\n  &= 0.9 \\frac{e^{-10}10^x}{x!} + 0.1 \\frac{e^{-20}{20^x}}{x!}\\quad\\text{$x = 0, 1, 2, \\dots$}.\n  \\tag{14.6}\n\\end{align}\\]\nSee Fig. 14.1 (centre panel).Example 14.6  (Goal kicking) (Follows Example 14.3.)\nSuppose goal-kicker takes 10 kicks goal match; number successful kicks \\(X\\) can modelled binomial distribution \\(\\text{Bin}(n = 10, p = \\Theta)\\), probability \\(\\Theta\\) \\(\\text{Beta}(4, 4)\\) distribution.\n(14.4), marginal pf \\(X\\) \\[\\begin{align}\n  p_X(x)\n  &= \\int p_{X \\mid \\Theta}(x, \\theta)f_\\Theta(\\theta)\\,d\\theta\\notag\\\\\n  &= \\int_0^1 \\binom{10}{x}\\theta^x(1 - \\theta)^{10 - x}\n     \\frac{\\Gamma(8)}{\\Gamma(4)\\,\\Gamma(4)} \\theta^3(1 - \\theta)^3\\,d\\theta\\notag\\\\\n  &= \\frac{\\Gamma(8)}{\\Gamma(4)\\,\\Gamma(4)}\\binom{10}{x}\\int_0^1 \\theta^{x + 3}(1 - \\theta)^{10 - x + 3}\\,d\\theta\\notag\\\\\n\\end{align}\\]\nintegral can simplified seeing Beta function (see Equation (8.12)) parameters \\(x + 4\\) \\(14 - x\\), \\(\\int_0^1 \\theta^{x + 3}(1 - \\theta)^{10 - x + 3}\\,d\\theta = \\Gamma(x - 4)\\Gamma(14 - x)/\\Gamma(18)\\).\n:\n\\[\n  p_X(x)\n  = \\frac{7!\\, 10!\\, (x + 3)!\\, (13 - x)!}{3!^2\\, 17!\\, x!\\, (10 - x)!}\\quad\\text{$x = 0 , 1 , 2,\\dots, 10$}.\n   \\tag{14.7}\n\\]\nSee Fig. 14.1 (right panel).\nFIGURE 14.1: Marginal distributions\n","code":""},{"path":"BayesianIntro.html","id":"BayesTheoremRevisited","chapter":"14 Introduction to Bayesian statistics","heading":"14.2 Bayes’ theorem revisited","text":"Recall Bayes’ theorem Sect. 2.10.6.\nLet \\(E\\) event, \\(H_1, \\ldots, H_n\\) sequence mutually exclusive exhaustive events partitioning sample space.\n\\[\\begin{equation}\n  \\Pr(H_n \\mid E )\n  = \\frac{\\Pr(H_n) \\Pr(E \\mid H_n) }{\\Pr(E)}\n  = \\frac{\\Pr(H_n) \\Pr(E \\mid H_n) }{\\sum_m \\Pr(H_m ) \\Pr(E \\mid H_m )}\n\\end{equation}\\]\nassuming \\(\\Pr(E) \\neq 0\\).\nBayes’ theorem extends random variables.Suppose \\(X\\) \\(Y\\) discrete random variables, \\(p_Y(y)\\) pf \\(Y\\) \\(p_{X \\mid Y}(x,y)\\) conditional PMF \\(X\\) given \\(Y\\).\nconditional PMF \\(Y\\) given \\(X\\) \\[\\begin{align}\n  p_{Y \\mid X}(y, x)\n  &= \\frac{p_{X, Y}(x, y)}{p_X(x)} \\\\\n  &= \\frac{p_{X \\mid Y}(x, y)p_Y(y)}{\\sum_y p_{X \\mid Y}(x, y)p_Y(y)}.\n  \\tag{14.8}\n\\end{align}\\]usual form, equations combinations continuous discrete random variables produce results:\\[\\begin{align}\n   f_{Y \\mid X}(y, x)\n   &= \\frac{f_{X, Y}(x, y)}{f_X(x)}\n    = \\frac{f_{X \\mid Y}(x, y)f_Y(y)}{\\int_y f_{X \\mid Y}(x, y)f_Y(y)\\,dy}\n    = \\frac{f_{X \\mid Y}(x, y)f_Y(y)}{f_X(x)};\n   \\tag{14.9}\\\\\n  p_{Y \\mid X}(y, x)\n  &= \\frac{f_{X, Y}(x, y)}{p_X(x)}\n   = \\frac{f_{X \\mid Y}(x, y)p_Y(y)}{\\sum_y f_{X \\mid Y}(x, y)p_Y(y)}\n   = \\frac{f_{X \\mid Y}(x, y)p_Y(y)}{p_X(x)};\n  \\tag{14.10}\\\\\n  f_{Y \\mid X}(y, x)\n  &= \\frac{f_{X, Y}(x, y)}{p_X(x)}\n   = \\frac{p_{X \\mid Y}(x, y)f_Y(y)}{\\int_y p_{X \\mid Y}(x, y)f_Y(y)\\,dy}\n   = \\frac{p_{X \\mid Y}(x, y)f_Y(y)}{p_X(x)}.\n  \\tag{14.11}\n\\end{align}\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncases (14.8) (14.11), left-hand side shows distribution function \\(y\\), given observed data \\(X\\).\ndenominator right-hand side function observed data \\(X\\), contains unknowns (effectively normalising constant whose value ensures expression valid probability function)., sometimes expressions (14.8) (14.11) written proportionalities rather equalities omitting denominator (normalising constant); also, write \\(\\Theta\\) parameter interest:\\[\\begin{align*}\n   f_{\\Theta|X}(\\theta \\mid x)      &\\propto f_{X|\\Theta}(x\\mid \\theta)      \\times f_\\Theta(\\theta)\\\\\n   \\text{Posterior distn} &\\propto \\text{Sampling distn} \\times \\text{Prior distn}.\n\\end{align*}\\]\n:prior distribution: distribution parameter \\(\\Theta\\) assumed data \\(X\\) observed, perhaps based past experience.sampling distribution likelihood function: describes distribution data assumed come.posterior distribution: describes distribution parameter \\(\\Theta\\), data \\(X\\) observed.distribution \\(\\Theta\\) assumed prior seeing data (.e., \\(f_\\Theta(\\theta)\\)), adjusted knowledge data \\(X\\) produce conditional (posterior) distribution \\(f_{\\Theta|X}(\\theta\\mid x)\\).context:\\(\\Theta\\) parameter interest.original, unconditional distribution \\(f_\\Theta(\\theta)\\) models prior beliefs parameter \\(\\theta\\).\ncalled prior distribution (modelling beliefs parameter distribution prior seeing data).observe data \\(X\\).Using extra available information, description \\(\\theta\\) updated.\nconditional distribution \\(f_{\\Theta \\mid X}(\\theta, x)\\), called posterior distribution context.Example 14.7  (Exam scores) (Follows Examples 14.1 14.4.)\nconditional PDF mean score \\(\\Theta\\), given observed exam score \\(X = x\\), follows directly (14.9) substituting (14.5):\\[\\begin{align}\n  f_{\\Theta \\mid X}(\\theta, x)\n  &= \\frac{f_{X|\\Theta}(x, \\theta)f_\\Theta(\\theta)}{f_X(x)}\\notag\\\\\n  &= \\frac{\\frac{1}{5\\sqrt{2\\pi}}\n  \\exp\\left[ -\\frac{1}{2}\\left(\\frac{x - \\theta}{5}\\right)^2 \\right]\n  \\frac{1}{10\\sqrt{2\\pi}}\n  \\exp\\left[ -\\frac{1}{2}\\left(\\frac{\\theta - 60}{10}\\right)^2 \\right]}\n  {\\frac{1}{11.18\\sqrt{2\\pi}}\n  \\exp\\left[ -\\frac{1}{2}\\left(\\frac{x - 60}{11.18}\\right)^2 \\right]}\\notag\\\\\n  &=\\frac{1}{4.472\\sqrt{2\\pi}}\n  \\exp\\left[ -\\frac{1}{2}\\left(\\frac{\\theta - (0.8x + 12)}{4.472}\\right)^2 \\right]\n   \\tag{14.12}\n\\end{align}\\]\nalgebraic manipulation; .e., \\(\\Theta | (X = x)\\sim N(0.8x + 12, 4.472^2)\\).distribution derived mean score, given observed value \\(x\\).\nThus :initial (prior) distribution mean score (Example 14.4): \\(\\Theta \\sim N(60, 10^2)\\).seeing exam score \\(x\\), adjusted (posterior) distribution mean score \\(\\Theta\\mid X\\sim N(0.8x + 12, 4.472^2)\\).Consider implications:observed exam score \\(x = 60\\) (.e., equal prior belief mean), change prior mean: \\(0.8\\times 60 + 12 = 60\\).\n, mean prior distribution 60, data don’t provide evidence change , mean posterior distribution also 60.\nNote, however, variance smaller, confidence value parameter increased.observed exam score less \\(60\\) (e.g., \\(x = 50\\)), original expectation adjusted original assumption mean 60, since \\(0.8\\times 50 + 12 = 52\\).observed exam score greater \\(60\\) (e.g., \\(x = 70\\)), original expectation adjusted original assumption mean 60, since \\(0.8\\times 70 + 12 = 68\\).See Fig. 14.2.\nwords, prior belief mean score adjusted light observed evidence produce posterior distribution.\nextra observations adds confidence estimate (.e., reduced variance).\nFIGURE 14.2: Exam scores: prior distribution mean \\(\\Theta\\) (left panel) posterior distribution \\(x = 50\\) centre panel) \\(x = 70\\) (right panel). dashed grey lines two right plots shows prior distribution.\nExample 14.8  (Vehicle crashes) (Follows Example 14.1 14.5.)\n(14.6) (14.9), conditional pf mean number accidents per day \\(\\Theta\\) given \\(X\\) \n\\[\\begin{align}\n  p_{\\Theta \\mid X}(\\theta, x)\n  &= \\frac{f_{X \\mid \\Theta}(x, \\theta)f_\\Theta(\\theta)}{p_X(x)}\\notag\\\\\n  &= \\begin{cases}\n        \\displaystyle \\frac{0.9\\frac{e^{-10}10^x}{x!}} {0.9 \\frac{e^{-10}10^x}{x!} + 0.1 \\frac{e^{-20}{20^x}}{x!}} & \\text{$\\theta = 10$}\\\\[6pt]\n        \\displaystyle \\frac{0.1\\frac{e^{-20}20^x}{x!}}\n     {0.9 \\frac{e^{-10}10^x}{x!}+ 0.1 \\frac{e^{-20}{20^x}}{x!}} & \\text{$\\theta = 20$}.\n  \\end{cases}\\notag\\\\\n  &= \\begin{cases}\n        \\displaystyle \\frac{0.9 e^{-10}10^x} {0.9 e^{-10}10^x + 0.1 e^{-20}{20^x}} & \\text{$\\theta = 10$}\\\\[6pt]\n        \\displaystyle \\frac{0.1 e^{-20}20^x}\n     {0.9 e^{-10}10^x + 0.1 e^{-20}{20^x}} & \\text{$\\theta = 20$}.\n   \\tag{14.13}\n  \\end{cases}\n\\end{align}\\]prior belief \\(\\Theta\\) \n\\[\n  p_\\Theta(\\theta) =\n  \\begin{cases}\n    0.9 & \\text{$\\theta = 10$ (.e., fine)};\\\\\n    0.1 & \\text{$\\theta = 20$ (.e, wet)}.\n  \\end{cases}\n\\]\nobserving number crashes, prior belief adjusted accordingly produce posterior distribution (Eq. (14.13)); .e.:observe \\(x = 8\\) crashes, prior belief adjusted \n\\[\np_{\\Theta\\mid X}(\\theta \\mid X) =\n\\begin{cases}\n   0.9987 & \\text{$\\theta = 10$ (.e., fine)};\\\\\n   0.0013 & \\text{$\\theta = 20$ (.e., wet)}.\n\\end{cases}\n\\]observe \\(x = 8\\) crashes, prior belief adjusted \n\\[\np_{\\Theta\\mid X}(\\theta \\mid X) =\n\\begin{cases}\n   0.9987 & \\text{$\\theta = 10$ (.e., fine)};\\\\\n   0.0013 & \\text{$\\theta = 20$ (.e., wet)}.\n\\end{cases}\n\\]observe \\(x = 17\\) crashes, prior belief adjusted \n\\[\np_{\\Theta|X}(m \\mid \\theta) =\n\\begin{cases}\n   0.602 & \\text{$\\theta = 10$ (.e., fine)};\\\\\n   0.398 & \\text{$\\theta = 20$ (.e., wet)}.\n\\end{cases}\n\\]\nSee Fig. 14.3.observe \\(x = 17\\) crashes, prior belief adjusted \n\\[\np_{\\Theta|X}(m \\mid \\theta) =\n\\begin{cases}\n   0.602 & \\text{$\\theta = 10$ (.e., fine)};\\\\\n   0.398 & \\text{$\\theta = 20$ (.e., wet)}.\n\\end{cases}\n\\]\nSee Fig. 14.3.\nFIGURE 14.3: Vehicle crashes: prior distribution \\(\\Theta\\) (left panel) posterior distribution \\(x = 8\\) crashes (centre panel) \\(x = 17\\) crashes (right panel). grey crosses two right plots shows prior distribution.\nExample 14.9  (Goal kicking) (Follows Example 14.3 14.6.)\n(14.4), conditional PDF \\(\\Theta\\) given \\(X\\) \n\\[\n  f_{\\Theta \\mid X}(\\theta, x)\n  = \\frac{p_{X \\mid \\Theta}(x, \\theta) f_\\Theta(\\theta)}{p_X(x)}\n\\]\n\\(X \\mid \\theta \\sim \\text{Bin}(n = 10, p = \\theta)\\) \n\\[\n  p(X \\mid\\Theta) = \\binom{10}{x}\\theta^x (1 - \\theta)^{10 - x},\n\\]\n\\(p_X(x)\\) given (14.7), \\(\\Theta \\sim \\text{Beta}(4, 4)\\).\nPutting pieces together:\n\\[\\begin{align}\n  f_{\\Theta \\mid X}(\\theta, x)\n  &= \\frac{ \\binom{10}{x}\\theta^x (1 - \\theta)^{10 - x}\n     \\frac{\\Gamma(8)}{\\Gamma(4)\\Gamma(4)} \\theta^3 (1 - \\theta)^3}\n          {\\frac{7!\\, 10!\\, (x + 3)!\\, (13 - x)!}{3!^2\\, 17!\\, x!\\, (10 - x)!} } \\\\\n  &= \\frac{17!\\theta^{x + 3} (1 - \\theta)^{13 - x}}{(x + 3)! (13 - x)!}\n   \\tag{14.14}\n\\end{align}\\]\ncorresponds posterior distribution \\(\\Theta\\) \\(\\text{Beta}(4 + x, 14 - x)\\).\ncontrast, prior distribution \\(\\Theta\\) \\(\\text{Beta}(4, 4)\\).observing 10 kicks, prior belief adjusted accordingly:number kicks made \\(5/10 = 0.5\\), prior belief adjusted \\(\\text{Beta}(9, 9)\\), mean remains \\(9/(9 + 9) = 0.5\\);number kicks made \\(8/10 = 0.8\\), prior belief adjusted upwards; \\(\\text{Beta}(12, 6)\\) mean becomes \\(12/(12 + 6) = 0.67\\);number kicks made \\(1/10 = 0.1\\), prior belief adjusted downwards: \\(\\text{Beta}(5, 13)\\) mean becomes \\(5/(5 + 13) = 0.28\\).See Fig. 14.4.\nFIGURE 14.4: Goal kicking: prior distribution \\(\\theta\\) (left panel) posterior distribution \\(x = 8\\) (.e., lands 8 kicks 10; centre panel) \\(x = 1\\) (.e., lands 1 kick 10; right panel). grey vertical lines correspond distribution means. dashed grey lines two right plots show prior distribution.\nApplying meaning conditional distribution parameters, (14.12), (14.13) (14.14), historically controversial statistics.\nHowever, idea modifying parameter distribution based new information proven constructive, basis important branch statistics called Bayesian statistics.algebra involved deriving posterior distributions , except special cases (Table 14.1), intractable numerical methods necessary (including simulation techniques).\nprior posterior distributions type distribution (e.g., normal distribution), called conjugate distributions.\nprior posterior distribution combinations 14.1) conjugate distributions.\nTABLE 14.1: Conjugate distributions random parameters: sampling distribution, \\(f(X\\mid\\Theta)\\); prior distribution \\(f(\\Theta)\\); posterior distribution \\(f(\\Theta\\mid X\\))\n","code":""},{"path":"BayesianIntro.html","id":"statistical-computing","chapter":"14 Introduction to Bayesian statistics","heading":"14.3 Statistical computing","text":"","code":""},{"path":"BayesianIntro.html","id":"ExercisesBayesian","chapter":"14 Introduction to Bayesian statistics","heading":"14.4 Exercises","text":"Selected answers appear Sect. E.14.Exercise 14.1  studies human reproduction (Paul 2005), number menstrual cycles pregnancy denoted \\(Y\\), \\(Y\\) often modelled using geometric distribution: \\(Y\\sim \\text{Geom}(\\theta)\\), \\(\\theta\\) pre-cycle conception probability.\n\\(\\theta\\) usually considered constant couple, can vary couples beta distribution: \\(\\theta\\sim \\text{Beta}(m, n)\\).Determine posterior probability \\(f(\\theta\\mid y)\\), identify distribution terms known distributions (including parameters).\n(normalising constant needed.)Determine prior probability \\(\\theta\\) \\(m = 1.2\\) \\(n = 2\\), hence expected number cycles till pregnancy.Determine posterior probability \\(\\theta\\) \\(m = 1.2\\) \\(n = 2\\), couple takes three cycles fall pregnant.","code":""},{"path":"SymbolsUsed.html","id":"SymbolsUsed","chapter":"A Symbols used","heading":"A Symbols used","text":"","code":""},{"path":"SymbolsUsed.html","id":"Symbols","chapter":"A Symbols used","heading":"A.1 Symbols and abbreviations used","text":"Range space rv \\(X\\): \\(\\mathcal{R}_X\\)","code":""},{"path":"UsefulSeries.html","id":"UsefulSeries","chapter":"B Some useful series","heading":"B Some useful series","text":"convenience reference, results given involving series occur frequently probability statistical theory.","code":""},{"path":"UsefulSeries.html","id":"finite-series","chapter":"B Some useful series","heading":"B.1 Finite series","text":"Sum natural numbers:\\[\\begin{equation}\n1 + 2 + 3 + \\ldots + n\n= \\frac{n(n + 1)}{2}.\n\\tag{B.1}\n\\end{equation}\\]Sum natural numbers:\\[\\begin{equation}\n1 + 2 + 3 + \\ldots + n\n= \\frac{n(n + 1)}{2}.\n\\tag{B.1}\n\\end{equation}\\]Sum squares natural numbers:\\[\\begin{equation}\n1^2 + 2^2 + 3^2 + \\ldots + n^2\n= \\frac{n}{6}(n + 1)(2n + 1).\n\\tag{B.2}\n\\end{equation}\\]Sum squares natural numbers:\\[\\begin{equation}\n1^2 + 2^2 + 3^2 + \\ldots + n^2\n= \\frac{n}{6}(n + 1)(2n + 1).\n\\tag{B.2}\n\\end{equation}\\]Geometric series:\\[\\begin{equation}\n+ ar + ar^2 + \\ldots + ar^{n - 1}\n= \\frac{(1 - r^n)}{1 - r}\n\\tag{B.3}.\n\\end{equation}\\]\nSee next section case \\(n\\\\infty\\).Geometric series:\\[\\begin{equation}\n+ ar + ar^2 + \\ldots + ar^{n - 1}\n= \\frac{(1 - r^n)}{1 - r}\n\\tag{B.3}.\n\\end{equation}\\]\nSee next section case \\(n\\\\infty\\).Binomial expansion:\\[\\begin{equation}\n(+ b)^n = b^n + \\binom{n}{1}ab^{n - 1} + \\ldots + \\binom{n}{r}^r b^{n - r} + \\ldots ^n.\n\\tag{B.4}\n\\end{equation}\\]Binomial expansion:\\[\\begin{equation}\n(+ b)^n = b^n + \\binom{n}{1}ab^{n - 1} + \\ldots + \\binom{n}{r}^r b^{n - r} + \\ldots ^n.\n\\tag{B.4}\n\\end{equation}\\]","code":""},{"path":"UsefulSeries.html","id":"InfiniteSeriesLimits","chapter":"B Some useful series","heading":"B.2 Infinite series and limits","text":"Geometric series:\\[\\begin{equation}\n   \\sum_{n = 0}^\\infty r^n\n= + ar + ar^2 + \\ldots\n\\rightarrow \\frac{}{1 - r} \\text{ $n \\rightarrow \\infty$} \\quad\\text{$|r|<1$.}\n\\tag{B.5}\n\\end{equation}\\]Geometric series:\\[\\begin{equation}\n   \\sum_{n = 0}^\\infty r^n\n= + ar + ar^2 + \\ldots\n\\rightarrow \\frac{}{1 - r} \\text{ $n \\rightarrow \\infty$} \\quad\\text{$|r|<1$.}\n\\tag{B.5}\n\\end{equation}\\]Geometric-like series:\\[\\begin{equation}\n   \\sum_{n = 0}^\\infty n r^n\n= ar + 2ar^2 + 3ar^3 + \\ldots\n\\rightarrow \\frac{ar}{(1 - r)^2} \\quad\\text{$|r|<1$.}\n\\tag{B.6}\n\\end{equation}\\]\n\\[\\begin{equation}\n   \\sum_{n=0}^\\infty n^2 r^n\n= ar + 4ar^2 + 9ar^3 \\ldots\n\\rightarrow \\frac{ar(r + 1)}{(1 - r)^3} \\quad\\text{$|r|<1$.}\n\\tag{B.7}\n\\end{equation}\\]Geometric-like series:\\[\\begin{equation}\n   \\sum_{n = 0}^\\infty n r^n\n= ar + 2ar^2 + 3ar^3 + \\ldots\n\\rightarrow \\frac{ar}{(1 - r)^2} \\quad\\text{$|r|<1$.}\n\\tag{B.6}\n\\end{equation}\\]\n\\[\\begin{equation}\n   \\sum_{n=0}^\\infty n^2 r^n\n= ar + 4ar^2 + 9ar^3 \\ldots\n\\rightarrow \\frac{ar(r + 1)}{(1 - r)^3} \\quad\\text{$|r|<1$.}\n\\tag{B.7}\n\\end{equation}\\]Exponential function:\\[\\begin{align}\ne^z\n&= \\sum_{= 1}^n \\frac{z^}{!}\n  = 1 + z + \\frac{z^2}{2!} + \\frac{z^3}{3!} + \\ldots \\quad\\text{values $z$}.\n\\tag{B.8}\\\\\ne^{-z}\n&= \\sum_{= 1}^n \\frac{(-z)^}{!}\n  = 1 - z + \\frac{z^2}{2!} - \\frac{z^3}{3!} + \\ldots \\quad\\text{values $z$}.\n\\tag{B.9} \\\\\ne\n&= \\lim_{z\\\\infty} \\left(1 + \\frac{1}{z}\\right)^z\n\\tag{B.10}\\\\\n1/e\n&= \\lim_{z\\\\infty} \\left(1 - \\frac{1}{z}\\right)^z\n\\tag{B.11}\n\\end{align}\\]Exponential function:\\[\\begin{align}\ne^z\n&= \\sum_{= 1}^n \\frac{z^}{!}\n  = 1 + z + \\frac{z^2}{2!} + \\frac{z^3}{3!} + \\ldots \\quad\\text{values $z$}.\n\\tag{B.8}\\\\\ne^{-z}\n&= \\sum_{= 1}^n \\frac{(-z)^}{!}\n  = 1 - z + \\frac{z^2}{2!} - \\frac{z^3}{3!} + \\ldots \\quad\\text{values $z$}.\n\\tag{B.9} \\\\\ne\n&= \\lim_{z\\\\infty} \\left(1 + \\frac{1}{z}\\right)^z\n\\tag{B.10}\\\\\n1/e\n&= \\lim_{z\\\\infty} \\left(1 - \\frac{1}{z}\\right)^z\n\\tag{B.11}\n\\end{align}\\]Logarithmic series:\\[\\begin{align}\n\\log(1 + z)\n&= z - \\frac{z^2}{2} + \\frac{z^3}{3} - \\ldots \\quad\\text{$|z|<1$}.\n\\tag{B.12}\\\\\n\\log(1 - z)\n&= -z - \\frac{z^2}{2} - \\frac{z^3}{3} - \\ldots \\quad\\text{$|z|<1$}.\n\\tag{B.13}\n\\end{align}\\]Logarithmic series:\\[\\begin{align}\n\\log(1 + z)\n&= z - \\frac{z^2}{2} + \\frac{z^3}{3} - \\ldots \\quad\\text{$|z|<1$}.\n\\tag{B.12}\\\\\n\\log(1 - z)\n&= -z - \\frac{z^2}{2} - \\frac{z^3}{3} - \\ldots \\quad\\text{$|z|<1$}.\n\\tag{B.13}\n\\end{align}\\]Others:\\[\\begin{align}\n(1 - z)^{-1}\n&= 1 + z + z^2 + z^3 + \\ldots \\quad\\text{$|z|<1$}.\n\\tag{B.14}\\\\\n(1 - z)^{-r}\n&= 1 + rz + \\frac{r(r + 1)z^2}{2!} + \\frac{r(r + 1)(r + 2)z^3}{3!} + \\ldots \\quad\\text{$|z|<1$}.\n\\tag{B.15}\n\\end{align}\\]Others:\\[\\begin{align}\n(1 - z)^{-1}\n&= 1 + z + z^2 + z^3 + \\ldots \\quad\\text{$|z|<1$}.\n\\tag{B.14}\\\\\n(1 - z)^{-r}\n&= 1 + rz + \\frac{r(r + 1)z^2}{2!} + \\frac{r(r + 1)(r + 2)z^3}{3!} + \\ldots \\quad\\text{$|z|<1$}.\n\\tag{B.15}\n\\end{align}\\]Note expressions RHS converge LHS values \\(z\\).","code":""},{"path":"ShortRIntro.html","id":"ShortRIntro","chapter":"C Short R introduction","heading":"C Short R introduction","text":"","code":""},{"path":"ShortRIntro.html","id":"general-ideas","chapter":"C Short R introduction","heading":"C.1 General ideas","text":"Whn start R, presented prompt asking enter instruction:show prompt R code book, enable code show cut--paste directly R evaluated.\n> R prompts enter something .R can used fancy calculator:output begins [1] indicate first item output; sometimes, numerous values appear output (see Sect. C.4 examples).Note spaces important R, R:last better human readability, recommended.R also allows variables used, values can assigned variables using = tradiational <- combination characters.\nVariables start digit.Notice value variable displayed typing variable name.Comments can added using #; text following # ignored R:complete lines instructions cause R wait instruction completed.\ninstance, suppose type:R change prompt > +, indicating need add information complete command.\ncommand complete, R evaluates instruction:","code":">\n(-5 + 7) * 2\n#> [1] 4\n2 * pi * 3\n#> [1] 18.84956\nexp(-1)\n#> [1] 0.3678794\nsqrt( 9 ) * sin(3 * pi / 2)\n#> [1] -3\n1 +                2\n#> [1] 3\n1          +  2\n#> [1] 3\n1+2\n#> [1] 3\n1 + 2\n#> [1] 3\nRadius = 2\nArea = pi * Radius^2\nArea\n#> [1] 12.56637\n\nside1 <- 2\nside2 <- 4\nhypot <- sqrt( side1^2 + side2^2 )\nhypot\n#> [1] 4.472136\nhourly_Wage <- 20  # Hourly wage, in $ per hr\nhours_Worked <- 16 # Number of hours worked\n\n# This all assumes no overtime rates!\n\nweekly_Income <- hourly_Wage * hours_Worked # Income in dollar for the week\nweekly_Income\n#> [1] 320sqrt( 4  # Notce: no closing of parentheses\nsqrt(4\n)\n#> [1] 2"},{"path":"ShortRIntro.html","id":"FunctionsInR","chapter":"C Short R introduction","heading":"C.2 Using functions in R","text":"R thousands functions perform specific tasks.\nalready seen two used :sqrt() takes square root given value.exp() find value \\(\\exp(x) = e^x\\) given value \\(x\\).log() finds logarithm function.Functions, general, can take one input.\nConsider function log().\ncan typeIf look help log() function (typing ?log), see usage described like :, function can take two inputs: one called x, one called base.\nhelp information starting default value base, told otherwise, assumed exp(1) (\\(\\exp(1)= e^1 = 2.71828\\dots\\)).\nwords, default logarithm natural logarithm.Since command (.e., log(100)) gave one input log() function, value second input (.e., base) uses default value exp(1).\n, \\(\\log_e 100 = \\ln 100 = 4.605\\dots\\).specify different base, can set value base explicitly:inputs assumed R given order shown help; x = 100 base = 10.\nclearer, though, name value second input:value first input can named also:inputs named, can appear order:functions R many inputs.\nfunctions collected R packages provide additional functionality.","code":"\nlog(100)\n#> [1] 4.60517Usage\nlog(x, base = exp(1))\nlog(100, 10)\n#> [1] 2\nlog(100, base = 10)\n#> [1] 2\nlog(x = 100, base = 10)\n#> [1] 2\nlog(base = 10, x = 100)\n#> [1] 2"},{"path":"ShortRIntro.html","id":"OtherUsefulRGeneral","chapter":"C Short R introduction","heading":"C.3 General functions","text":"seq() produces sequence integers:\nseq(1, 4) produces list: \\(1\\), \\(2\\), \\(3\\), \\(4\\).\nseq(0, 10, = 2) produces list going two time: \\(0\\), \\(2\\), \\(4\\), \\(6\\), \\(8\\), \\(10\\).\nseq(0, 10, length = 3) produces list length three: \\(0\\), \\(5\\), \\(10\\).\ncolon can also used special cases: 3 : 7 produces: \\(3\\), \\(4\\), \\(5\\), \\(6\\), \\(7\\).\nseq(1, 4) produces list: \\(1\\), \\(2\\), \\(3\\), \\(4\\).seq(0, 10, = 2) produces list going two time: \\(0\\), \\(2\\), \\(4\\), \\(6\\), \\(8\\), \\(10\\).seq(0, 10, length = 3) produces list length three: \\(0\\), \\(5\\), \\(10\\).colon can also used special cases: 3 : 7 produces: \\(3\\), \\(4\\), \\(5\\), \\(6\\), \\(7\\).c() used concatenate (join together) series values:\nc(\"fred\", \"martha\") creates vector two text elements.\nc(1, 8, 3.14, -2)create vector four numerical elements.\nc(\"fred\", \"martha\") creates vector two text elements.c(1, 8, 3.14, -2)create vector four numerical elements.cat() often used print information.matrix() used produces matrices (\\(2\\)-dimensions):\nmatrix(data = c(1, 2, 3, 4, 5, 6), byrow = TRUE, ncol = 3) produces \\(2\\times 3\\) array (.e., ncol = 3 means number columns  \\(3\\)):\n\\[\n\\left[\n\\begin{array}{ccc}\n1 & 2 & 3\\\\\n4 & 5 & 6\\\\\n\\end{array}\n\\right].\n\\]\nmatrix(data = c(1, 2, 3, 4, 5, 6), byrow = TRUE, ncol = 3) produces \\(2\\times 3\\) array (.e., ncol = 3 means number columns  \\(3\\)):\n\\[\n\\left[\n\\begin{array}{ccc}\n1 & 2 & 3\\\\\n4 & 5 & 6\\\\\n\\end{array}\n\\right].\n\\]matrix(data = c(1, 2, 3, 4, 5, 6), bycol = TRUE, nrow = 2 produces \\(2\\times 3\\) array (.e., nrow = 2 means number rows  \\(2\\)):\n\\[\n  \\left[\n  \\begin{array}{ccc}\n1 & 3 & 5\\\\\n2 & 4 & 6\\\\\n  \\end{array}\n  \\right].\n\\]array() produces matrix-like structure, can many dimensions (just two, like matrix())\narray( dim = c(2, 4)) produces empty \\(2\\times 4\\) array.\narray( data = c(1, 2, 3, 4, 5, 6, 7, 8), dim = c(2, 2, 2)) produces \\(2\\times 2\\times 2\\) array:\narray( dim = c(2, 4)) produces empty \\(2\\times 4\\) array.array( data = c(1, 2, 3, 4, 5, 6, 7, 8), dim = c(2, 2, 2)) produces \\(2\\times 2\\times 2\\) array:t() transposes matrix array:Logical comparison R possible (notice use == rather =):Specific elements vector can accessed using square brackets:","code":"\nmatrix(data = c(1, 2, 3, 4, 5, 6), \n       byrow = TRUE, \n       ncol = 3)\n#>      [,1] [,2] [,3]\n#> [1,]    1    2    3\n#> [2,]    4    5    6\narray( data = c(1, 2, 3, 4, 5, 6, 7, 8), dim = c(2, 2, 2))\n#> , , 1\n#> \n#>      [,1] [,2]\n#> [1,]    1    3\n#> [2,]    2    4\n#> \n#> , , 2\n#> \n#>      [,1] [,2]\n#> [1,]    5    7\n#> [2,]    6    8\nA <- matrix( c(1, -2, -20, 3), ncol = 2)\nA\n#>      [,1] [,2]\n#> [1,]    1  -20\n#> [2,]   -2    3\nt(A)\n#>      [,1] [,2]\n#> [1,]    1   -2\n#> [2,]  -20    3\nDays_Of_Week <- c(\"Mon\", \"Tues\", \"Wed\", \"Thurs\", \"Fri\", \"Sat\", \"Sun\")\nHours_Sleep <- c(8, 8, 8, 8, 6, 6, 10)\n\nHours_Sleep == 8\n#> [1]  TRUE  TRUE  TRUE  TRUE FALSE FALSE FALSE\nHours_Sleep > 8\n#> [1] FALSE FALSE FALSE FALSE FALSE FALSE  TRUE\nHours_Sleep >= 8\n#> [1]  TRUE  TRUE  TRUE  TRUE FALSE FALSE  TRUE\n\nHours_Sleep == 6 | Hours_Sleep == 10 # | means \"OR\"\n#> [1] FALSE FALSE FALSE FALSE  TRUE  TRUE  TRUE\n\nHours_Sleep > 6 & Hours_Sleep < 10 # & means \"AND\"\n#> [1]  TRUE  TRUE  TRUE  TRUE FALSE FALSE FALSE\n\nwhich(Hours_Sleep == 8)\n#> [1] 1 2 3 4\nwhich(Hours_Sleep > 8)\n#> [1] 7\nwhich(Hours_Sleep >= 8)\n#> [1] 1 2 3 4 7\n\nwhich(Hours_Sleep == 6 | Hours_Sleep == 10 ) # | means \"OR\"\n#> [1] 5 6 7\n\nwhich(Hours_Sleep > 6 & Hours_Sleep < 10 ) # & means \"AND\"\n#> [1] 1 2 3 4\n\nWeekend <- Days_Of_Week == \"Sat\" | Days_Of_Week == \"Sun\"\n\nWeek_Day <- !Weekend # ! means \"NOT\"\n\nHours_Sleep[Week_Day]\n#> [1] 8 8 8 8 6\n\nHours_Sleep[Weekend]\n#> [1]  6 10\n\n\ncat(\"These are the hours of sleep on weekdays:\", Hours_Sleep[Week_Day], \"\\n\")\n#> These are the hours of sleep on weekdays: 8 8 8 8 6\n   # Notice that  \\n  is used to create a new line.\nHours_Sleep[1]\n#> [1] 8\nHours_Sleep[3:5]\n#> [1] 8 8 6\n\nHours_Sleep[Weekend]\n#> [1]  6 10"},{"path":"ShortRIntro.html","id":"RVector","chapter":"C Short R introduction","heading":"C.4 Vector operations","text":"R vectorised system; , operations work elements vector:","code":"\nx <- 0 : 9\nx\n#>  [1] 0 1 2 3 4 5 6 7 8 9\nx + 3\n#>  [1]  3  4  5  6  7  8  9 10 11 12\n2 * x\n#>  [1]  0  2  4  6  8 10 12 14 16 18\n\nsqrt(x) # The square root function\n#>  [1] 0.000000 1.000000 1.414214 1.732051 2.000000\n#>  [6] 2.236068 2.449490 2.645751 2.828427 3.000000\ncos(x)  # The cosine function\n#>  [1]  1.0000000  0.5403023 -0.4161468 -0.9899925\n#>  [5] -0.6536436  0.2836622  0.9601703  0.7539023\n#>  [9] -0.1455000 -0.9111303\nexp( -x ) # The exponential function\n#>  [1] 1.0000000000 0.3678794412 0.1353352832\n#>  [4] 0.0497870684 0.0183156389 0.0067379470\n#>  [7] 0.0024787522 0.0009118820 0.0003354626\n#> [10] 0.0001234098"},{"path":"ShortRIntro.html","id":"OtherUsefulRStatistical","chapter":"C Short R introduction","heading":"C.5 Statistical functions","text":"R primarily statistical package environment, basic statistical functions available, includingmean(): Find mean sample values.\nExample: mean( c(1, 2, 3, 4)).median(): Find median sample values.\nExample: median( c(1, 2, 3, 4)).sd(): Finds standard deviation sample values.\nExample: sd( c(1, 2, 3, 4)).var(): Finds variance sample values.\nExample: var( c(1, 2, 3, 4)).lm(): fit linear regression model:","code":"\nx <- c(1, 2, 3, 4, 5)\ny <- c(12, 10, 9, 7, 4)\n\nout <- lm(y ~ x) # Read: 'y as a function of x'\ncoef(out)\n#> (Intercept)           x \n#>        14.1        -1.9\nsummary(out)\n#> \n#> Call:\n#> lm(formula = y ~ x)\n#> \n#> Residuals:\n#>    1    2    3    4    5 \n#> -0.2 -0.3  0.6  0.5 -0.6 \n#> \n#> Coefficients:\n#>             Estimate Std. Error t value Pr(>|t|)\n#> (Intercept)  14.1000     0.6351  22.202  0.00020\n#> x            -1.9000     0.1915  -9.922  0.00218\n#>                \n#> (Intercept) ***\n#> x           ** \n#> ---\n#> Signif. codes:  \n#> 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 0.6055 on 3 degrees of freedom\n#> Multiple R-squared:  0.9704, Adjusted R-squared:  0.9606 \n#> F-statistic: 98.45 on 1 and 3 DF,  p-value: 0.002178"},{"path":"ShortRIntro.html","id":"RProbCounting","chapter":"C Short R introduction","heading":"C.6 Probability and counting functions","text":"number combinations n elements taken k time found using choose(n, k).\\(n!\\) given factorial(n).number permutations n elements taken k time found using choose(n, k) * factorial(k).list combinations n elements, m time, given combn(x, m).\\(\\Gamma(x)\\) given gamma(x).","code":""},{"path":"ShortRIntro.html","id":"OtherUsefulRPlotting","chapter":"C Short R introduction","heading":"C.7 Plotting functions","text":"Three systems exists plotting R, believe .\n, discuss base system.plot() basic function plotting, many options:plot(x, y) plot(y ~ x) produce scatterplot, y vertical axis x horizontal axis.plot(..., type = \"l\") plots lines rather points.plot(..., lwd = \"3\") plots line width thrice thick.plot(..., xlab = \"text\", ylab = \"Info\") adds label x y axes respectively.plot(..., main = \"Title\") adds main title plot.plot(..., col = \"green\") plots green rather default (black).plot() starts new canvas every time called.\nlines() points() add lines points respectively existing plot.\nlegend() adds legend existing plot.\nFIGURE C.1: example plot: probability function two exponential distributions\n","code":"\nx <- seq(0, 4, length = 100)\ny1 <- dexp(x, rate = 2) # Probability function for an exponential distn\n\nplot(y1 ~ x,\n     type = \"l\", # Use lines, not points\n     lwd = 2, # Make lines a bit thicker\n     xlab = \"Values of x\",\n     ylab = \"Probability function\",\n     main = \"Probability function for\\nexponential distribution\",\n      # NOTE: using  \\n  adds a line break, and # is used for comments\n     col = \"red\", \n     las = 1, # Makes the axis labels all horizontal\n     xlim = c(0, 4.5), # Changes the displayed limits of the x-axis\n     ylim = c(0, 3) ) # Changes the displayed limits on the y-axis\n\ny2 <- dexp(x, rate = 3)\nlines( y2 ~ x, # ADD a line to existing point\n       lwd = 2,\n       col = \"blue\")\n\nlegend(\"topright\", # The location\n       lwd = 2,\n       col = c(\"red\", \"blue\"),\n       legend = c(\"Rate = 2\",\n                  \"Rate = 3\")\n       )"},{"path":"UseRDistributions.html","id":"UseRDistributions","chapter":"D Using R with distributions","heading":"D Using R with distributions","text":"R, standard distributions four associated functions:Computing probability function: begin p (pnorm() normal distribution).Computing distribution function: begin d(dnorm()).Computing quantile function: begin q (qnorm()).Generating random numbers: begin r (rnorm()).example, fours function called, normal distribution (denoted norm R):dnorm()pnorm()qnorm()rnorm()","code":""},{"path":"UseRDistributions.html","id":"univariate-distributions","chapter":"D Using R with distributions","heading":"D.1 Univariate distributions","text":"Common parameters:x: Values evaluate probability functionq: Quantilesp: Probabilitiesn: number random observations generateMost distributions parameters also; check help (e.g., ?punif).","code":""},{"path":"UseRDistributions.html","id":"multivariate-distributions","chapter":"D Using R with distributions","heading":"D.2 Multivariate distributions","text":"multinomial distribution:rmultinom()dmultinom()multivariate normal distribution:dmnorm()pmnorm()rmnorm()","code":""},{"path":"selected-solutions.html","id":"selected-solutions","chapter":"E Selected solutions","heading":"E Selected solutions","text":"Appendix contains answers end--chapter questions.","code":""},{"path":"selected-solutions.html","id":"AnswersChapSetTheory","chapter":"E Selected solutions","heading":"E.1 Answers for Chap. 1","text":"Answer Exercise 1.3.\\(\\mathbb{C} = \\{ + bi \\mid (\\\\mathbb{R})\\cup (b \\\\mathbb{R}) \\cup (^2 = -1)\\}\\).\\(\\mathbb{} = \\{ + bi \\mid (= 0) \\cup (b \\\\mathbb{R}) \\cup (^2 = -1)\\}\\).Answer Exercise 1.4.\\(S = \\{ (,b,c) \\mid (\\(-\\infty, 0)\\cap (0, \\infty)), (-\\infty < b < \\infty), (-\\infty < c < \\infty)\\}\\), can written \\(X \\\\{(, b, c) | (, b, c) \\\\mathbb{R}, \\ne 0\\}\\) \\(\\mathbb{R}\\) represents real numbers.solutions quadratic equation given \n\\[\nx = \\frac{-b \\pm \\sqrt{b^2 - 4ac} }{2a}.\n\\]\ntwo equal roots, \\(b^2 - 4ac = 0\\), event \\(R\\) defined \\(R = \\{ (,b,c) \\mid (, b, c)\\\\mathbb{R}, \\ne 0, b^2 - 4ac = 0\\}\\).real roots, \\(b^2 - 4ac < 0\\), event \\(Z\\) defined \\(Z = \\{ (,b,c) \\mid (, b, c)\\\\mathbb{R}, \\ne 0, b^2 - 4ac < 0\\}\\).Answer Exercise 1.5.\\(\\{x \\mid x^2 > 1\\}\\), \\(\\{x \\mid (x < -1) \\cup (x > 1)\\}\\).\\(\\{x \\mid x^2 > 2\\}\\), \\(\\{x \\mid (x < -\\sqrt{2}) \\cup (x > \\sqrt{2} \\}\\).\\(\\varnothing\\).Answer Exercise 1.6.\\(= \\{ x \\mid x\\S \\cap P\\}\\).\\(B = \\{ x \\mid x\\\\bar{S} \\cap P\\}\\).\\(C = \\{ x \\mid x\\\\bar{S} \\cap \\bar{P}\\}\\).Answer Exercise 1.7.Answer Exercise 1.8.Answer Exercise 1.9.\n\\(T = \\{ x\\\\mathbb{R} \\mid x \\ne 0\\}\\).Answer Exercise 1.10.\n\\(D = \\{ x\\\\mathbb{R} \\mid x \\ne \\frac{\\pi}{2} + n\\pi, n \\\\mathbb{Z}\\}\\).Answer Exercise 1.11.\nSet contains real numbers strictly  \\(-2\\)  \\(2\\).\n\\(\\) uncountably infinite set.Answer Exercise 1.12.\nSet \\(B\\) contains integers: \\(B\\\\mathbb{Z}\\).\nCountably infinite cardinality: \\(|B| = |\\mathbb{Z}| = \\aleph_0\\).Answer Exercise 1.13.\n\\[\\begin{align*}\n   \\setminus(\\cap B)\n   &= \\cap (\\cap B)^c\\quad\\text{(set difference)}\\\\\n   &= \\cap (^c \\cup B^c)\\quad\\text{(de Morgan's laws)}\\\\\n   &= (\\cap ^c) \\cup (\\cap B^c)\\quad\\text{(distributive law)}\\\\\n   &= \\cap B^c.\n\\end{align*}\\]Answer Exercise 1.14.\n\\[\\begin{align*}\n   (\\setminus B)^c\n   &= (\\cap B^c)^c\\text{(set difference)}\\\\\n   &= ^c \\cup (B^c)^c\\quad\\text{(Morgan's law)}\\\\\n   &= ^c\\cap B\\quad\\text{(definition complement)}.\n\\end{align*}\\]Answer Exercise 1.15.\n\\([ (\\cup B) \\cap (\\cup B^c) ]\\cap B =\n[ \\cup(B\\cap B^c) ] \\cap B =\n[ \\cup \\varnothing] \\cap B =\n\\cap B\\).Answer Exercise 1.16.\n\\((\\cap B) \\cup (\\cap B^c) = \\cap (B\\cup B^c) = \\cap S = \\).Answer Exercise 1.18.\n\\(C = \\{(x, y) \\mid (x \\S) \\cap (y \\D)\\}\\).","code":"\n# Part 1\nwhereSetA <- which( substr(state.name,\n                           start = 1,\n                           stop = 1)\n                    == \"W\")\nSetA <- state.name[whereSetA]\nSetA\n\n# Part 2\nwhereSetB <- which( substr(state.name,\n                           start = 1,\n                           stop = 5)\n                    == \"North\")\nSetB <- state.name[whereSetB]\nSetB\n\n# Part 3\nLength_State_Names <- nchar(state.name)\nwhereSetC <- which( substr(state.name,\n                          start = Length_State_Names,\n                          stop = Length_State_Names)\n                   == \"a\")\nSetC <- state.name[whereSetC]\nSetC\n\n# Part 4 \nSetD <- union(SetC, SetA)\nSetD\n\n# Part 5\nSetE <- intersect(SetC, SetA)\nSetE\n\n# Part 6\nwhereSetF <- which( substr(state.name,\n                           start = 1,\n                           stop = 1)\n                   != \"W\")\nSetF <- state.name[whereSetF]\n\nSetG <- intersect(SetC, SetF )\nSetG\nhead(state.x77)\n#>            Population Income Illiteracy Life Exp\n#> Alabama          3615   3624        2.1    69.05\n#> Alaska            365   6315        1.5    69.31\n#> Arizona          2212   4530        1.8    70.55\n#> Arkansas         2110   3378        1.9    70.66\n#> California      21198   5114        1.1    71.71\n#> Colorado         2541   4884        0.7    72.06\n#>            Murder HS Grad Frost   Area\n#> Alabama      15.1    41.3    20  50708\n#> Alaska       11.3    66.7   152 566432\n#> Arizona       7.8    58.1    15 113417\n#> Arkansas     10.1    39.9    65  51945\n#> California   10.3    62.6    20 156361\n#> Colorado      6.8    63.9   166 103766\nstate_Names <- rownames(state.x77)\n# Part 1\nSetA <- state_Names[ state.x77[, 4] > 70 ]\nSetB <- state_Names[ state.x77[, 8] < 500000 ]\nSetC <- state_Names[ state.x77[, 3] > 2 ]\nSetD <- state_Names[ state.x77[, 6] < 50 ]\nSetE <- intersect(SetC, SetD)\nSetF <- union( SetC, SetD)"},{"path":"selected-solutions.html","id":"AnswersChapProbability","chapter":"E Selected solutions","heading":"E.2 Answers for Chap. 2","text":"Answer Exercise 2.1.Probably Venn diagram best.\\(\\Pr(\\cup B) = \\Pr() + \\Pr(B) - \\Pr(\\cap B) = 0.66\\).\\(0.13\\).\\(0.89\\).\\(0.11/0.24 \\approx 0.4583...\\)\\(\\Pr() \\times \\Pr(B) = 0.53\\times 0.24 = 0.1272 \\ne \\Pr(\\cap B) = 0.11\\); independent… close.Answer Exercise 2.2.\\((50/100)\\times (49/99)\\times (48/98)\\times (47/97) = C^{50}_4 / ^{100}C_4\\approx 0.0587\\).\\(C^{50}_2\\times C^{50}_2 / C^{100}_4  = 1225/3201 \\approx 0.3826\\).\\(\\Pr(\\text{least 2 odd 1st even}) = \\Pr(\\text{odd, odd, even, either}) + \\Pr(\\text{odd, odd, odd, even)} = (50/100)\\times(49/99)\\times(50/98) + (50/100)\\times (49/99)\\times(48/98)\\times(50/97) \\approx 0.1887\\).\\(\\Pr(\\text{sum odd}) = \\Pr(\\text{exactly 1 odd number drawn, exactly 3 odd numbers drawn}) = 1600/3201 \\approx 0.49984\\).Answer Exercise 2.3.length time (seconds) green lights intersection, say \\(G\\).\\(S = \\{G \\mid 15 \\le G \\le 150\\}\\).—equally likely events defined.probability can approximated—observe lights many times, count often less 90 seconds green\nlights.Answer Exercise 2.4.\\(C^7_5 \\times C^5_4 \\times C^2_1 \\times C^1_1 = 210\\).\\(11^2 + (2\\times 22) = 165\\).Answer Exercise 2.5.Tree diagram: Fig. E.1; table: Table E.1; Venn diagram: Fig. E.2.\nFIGURE E.1: Tree diagram hat-wearing example\n\nTABLE E.1: numbers males females wearing hat middle day Brisbane\n\nFIGURE E.2: hat-wearing data Venn diagram\nAnswer Exercise 2.6.Choose one driver, seven passengers can sit anywhere:\n\\(2!\\times 7! = 10\\,080\\).Choose one driver, seven passengers can sit anywhere:\n\\(3!\\times 7! = 30\\,240\\).Choose one driver, choose sits car sets, five passengers can sit anywhere:\n\\(2!\\times 2! \\times 5! = 480\\).Answer Exercise 2.7.\n\\(8\\times 7\\times 6\\times 5 = 1680\\) ways.Answer Exercise 2.8.\norder important; use permutations.Eight: \\(^{26}P_8 = 62\\,990\\,928\\,000\\);\nnine: \\(^{26}P_9 = 1.133837\\times 10^{12}\\);\nten: \\(^{26}P_8 = 1.927522\\times 10^{13}\\).\nTotal: \\(2.047205\\times 10^{13}\\).\\(^{52}P_8 = 3.034234\\times 10^{13}\\).\\(^{62}P_8 = 1.363259\\times 10^{14}\\).answer (yet).Answer Exercise 2.9.()() (()).\ntwo ways.()()() (())() ()(()) ((())) (()()).\nfive ways.\\(\\displaystyle \\frac{1}{n + 1} \\binom{2n}{n} = \\frac{1}{n + 1}\\frac{(2n)!}{n!n!}  = \\frac{1}{(n + 1)!} \\frac{(2n)!}{n!}\\) shown.Write \\(\\displaystyle \\frac{(2n)!}{n!\\, n!} - \\frac{(2n)!}{(n + 1)! (2n - n - 1)!}\\).\nSimplifying:\n\\[\\begin{align*}\n  \\frac{(2n)!}{n!\\, n!} - \\frac{(2n)!}{(n + 1)! (n - 1)!}\n&= \\frac{(2n)!}{n!\\, n!} - \\left( \\frac{n}{n + 1}\\right) \\frac{(2n)!}{n!\\,n!}\\\\\n&= \\binom{2n}{n}\\left(1 - \\frac{n}{n + 1}\\right) \\\\\n&= \\frac{1}{n + 1} \\binom{2n}{n}\n\\end{align*}\\]first nine Catalan numbers, \\(n = 0, \\dots 8\\), \\(1, 1, 2, 5, 14, 42, 132, 429, 1430\\)Answer Exercise 2.10.\nSee Fig. E.3.\nFIGURE E.3: Relative error Stirling’s approximation\nAnswer Exercise 2.11.\\(\\Pr(\\text{Player throwing first wins})\\) means \\(\\Pr(\\text{First six throw 1 3 5 ...})\\).\n: \\(\\Pr(\\text{First six throw 1}) + \\Pr(\\text{First six throw 3}) + \\cdots\\).\nproduces geometric progression can summed obtained (see App. B).Use Theorem 2.4.\nDefine events \\(= \\text{Player 1 wins}\\), \\(B_1 = \\text{Player 1 throws first}\\), \\(B_2 = \\text{Player 1 throws second}\\).Answer Exercise 2.12.Write \\(y = \\Pr(\\text{answers `yes'})\\)., tree diagram (Step 1: Used drugs/use drugs; Step 2: Card says “Used drugs”/Card says “use drugs”):\\[\\begin{align*}\n   y\n   &= \\Pr(\\text{Never takes drugs say }) + \\Pr(\\text{Takes drugs says })\\\\\n   &= (1 - p) \\times\\left(\\frac{N - m}{N}\\right) + p\\times\\frac{m}{N}.\n\\end{align*}\\]\nSolving unknown \\(p\\):\n\\[\n  p = \\frac{yN - N + m}{2m - N}.\n\\]\n2. \\(m = 0\\), \\(\\Pr(Y) = p\\): mean every card says ‘used illegal drug past twelve months’, proportion used illegal drug just proportion responding ‘Yes’ (anonymity).\n\\(m = 0\\), \\(\\Pr(Y) = 1 - p\\): mean every card says ‘used illegal drug past twelve months’, proportion used illegal drug just proportion responding ‘’ (anonymity).\n\\(m = N/2\\), \\(\\Pr(Y) = 0.5\\); learnt nothing: probability 50–50.\n3. \\(\\displaystyle d = \\frac{yN - N + m}{2m - N}\\), plugging \\(N = 100\\), \\(m = 25\\) \\(\\Pr(Y) = 175/400 = 0.4375\\) gives \\(d = 0.625\\).Answer Exercise 2.13.Use Theorem 2.4 find \\(\\Pr(C)\\) \\(C = \\text{select correct answer}\\), \\(K = \\text{student knows answer}\\).\n, \\(\\Pr(C)  =\\displaystyle {\\frac{mp + q}{m}}\\)Answer Exercise 2.14.\\(\\Pr(W)\\) means ‘probability win’.\n\\(\\Pr(W \\mid D^c)\\) means ‘probability win, given game draw’.\\(\\Pr(W) = 91/208 = 0.4375\\).\n\\(\\Pr(W\\mid D^c) = 91/(208 - 50) = 0.5759494\\).Answer Exercise 2.15.\nWrite \\(d\\) distance; \\(S = \\{d: 0 \\le d \\le \\sqrt{2}\\}\\).\ngrid, let’s use R find values possible:seems 15 possible values distance:\\(0\\), \\(0.25\\), \\(0.50\\), \\(0.75\\) \\(1\\) along grid lines;\\(\\sqrt{2}/4\\), \\(\\sqrt{5}/4\\), \\(\\sqrt{10}/4\\) \\(\\sqrt{17}/4\\) line moved one grid-square right;\\(\\sqrt{13}/4\\) \\(\\sqrt{20}/4\\) line moved two grid-squares right;\\(\\sqrt{25}/4\\) line moved three grid-squares right;.Answer Exercise 2.16.Anywhere  \\(0\\)%  \\(8\\)%.\\(0.06/0.30 = 0.20\\).\\(0.06/0.08 = 0.75\\).Answer Exercise 2.17.\ntotal number children: \\(69\\,279\\).\nDefine \\(N\\) ‘first-nations student’, \\(F\\) ‘female student’,  \\(G\\) ‘attends government school’.\\((2540 + 2734 + 391 + 362) / 69,279 \\approx 0.0870\\).\\(49,067/69,279 \\approx 0.708\\).Females: prob FN: \\(0.107\\); Males: prob FN: \\(0.108\\); close independent.Females: prob FN: \\(0.040\\); Males: prob FN: \\(0.035\\); close independent.Gov: prob FN: \\(0.107\\); NGov: prob FN: \\(0.040\\); independent.Gov: prob FN: \\(0.108\\); NGov: prob FN: \\(0.035\\); independent.Regardless sex, First Nations children likely government school.Answer Exercise 2.18.probability depends happens first card:\n\\[\\begin{align*}\n  \\Pr(\\text{Ace second})\n  &= \\Pr(\\text{Ace, Ace}) + \\Pr(\\text{Non-Ace, Ace})\\\\\n  &= \\left(\\frac{4}{52}\\times \\frac{3}{51}\\right) + \\left(\\frac{48}{52}\\times \\frac{4}{51}\\right) \\\\\n  &= \\frac{204}{52\\times 51} \\approx 0.07843.\n\\end{align*}\\]\ncan use tree diagram, example.careful:\\[\\begin{align*}\n  &\\Pr(\\text{1st card lower rank second card})\\\\\n  &= \\Pr(\\text{2nd card K}) \\times \\Pr(\\text{1st card Q Ace}) + {}\\\\\n  &\\qquad   \\Pr(\\text{2nd card Q}) \\times \\Pr(\\text{1st card J Ace}) +{}\\\\\n  &\\qquad   \\Pr(\\text{2nd card J}) \\times \\Pr(\\text{1st card 10 Ace}) + \\dots + {}\\\\\n  &\\qquad   \\Pr(\\text{2nd card 2}) \\times \\Pr(\\text{1st card Ace}) \\\\\n  &= \\frac{4}{51} \\times \\frac{12\\times 4}{52} + {}\\\\\n  &\\qquad   \\frac{4}{51} \\times \\frac{11\\times 4}{52} +\\\\\n  &\\qquad   \\frac{4}{51} \\times \\frac{10\\times 4}{52} + \\dots +\\\\\n  &\\qquad   \\frac{4}{51} \\times \\frac{1\\times 4}{52} \\\\\n  &= \\frac{4}{51}\\frac{4}{52}\\left[ 12 + 11 + 10 + \\cdots + 1\\right]\\\\\n  &= \\frac{4}{51}\\frac{4}{52} \\frac{13\\times 12}{2} \\approx 0.4705882.\n\\end{align*}\\]\n3. can select \\(52\\) cards begin.\n, four cards higher four lower, total $16 options second card, total \\(52\\times 16 = 832\\) ways can happen.\nnumber ways getting two cards \\(52\\times 51 = 2652\\), probability \\(832/2652 \\approx 0.3137\\).Answer Exercise 2.19.\nanswer (yet).Answer Exercise 2.20.\n\\(x = 0.05\\).Answer Exercise 2.21.\\[\\begin{align*}\n   12\\times {}^7P_k &= 7\\times {}^6P_{k + 1} \\\\\n   12\\times \\frac{7!}{(7 - k)!} &= 7\\times\\frac{6!}{(5 - k)!}\\\\\n   \\frac{12}{(7 - k)!} &= \\frac{1}{(5 - k)!}\\\\\n   \\frac{12}{(7 - k)\\times (6 - k)\\times (5 - k)!} &= \\frac{1}{(5 - k)!}\\\\\n   12 &= (7 - k)(6 - k)\\\\\n   k^2 - 13k + 30 &= 0\\\\\n   (k - 10)(k - 3) &= 0\n\\end{align*}\\]\n\\(k = 10\\) \\(k = 3\\).\n\\(k = 10\\), get silly things like \\(P^7_{10}\\).\nsolution must \\(k = 3\\).Answer Exercise 2.22.\\[\\begin{align*}\n   \\frac{ {}^7 P_{r + 1}}{ {}^{7}C_r}\n   &= \\frac{7!}{(7 - (r + 1))!} \\times \\frac{(7 - r)!\\, r!}{7!}\\\\\n   &= \\frac{7!}{(6 - r)!} \\times \\frac{(7 - r)!\\, r!}{7!}\\\\\n   &= \\frac{(7 - r)!\\, r!}{(6 - r)!}\\\\\n   &= \\frac{(7 - r)\\times (6 - r)!\\, r!}{(6 - r)!}\\\\\n   &= (7 - r) r!\\\\\n   &= 10.\n\\end{align*}\\]\nRewrite: \\(r! = 10/(7 - r)\\).\nSince \\(r!\\) positive integer, \\(r\\) must either \\(r = 2\\) \\(r = 5\\).\nTrying , clearly \\(r = 2\\).Answer Exercise 2.23.Proceed:\n\\[\\begin{align*}\n  \\Pr(\\text{least two birthday})\n  &= 1 - \\Pr(\\text{two birthdays }) \\\\\n  &= 1 - \\Pr(\\text{every birthday different}) \\\\\n  &= 1 - \\left(\\frac{365}{365}\\right) \\times \\left(\\frac{364}{365}\\right) \\times \\left(\\frac{363}{365}\\right) \\times \\cdots \\times \\left(\\frac{365 - n + 1}{365}\\right)\\\\\n  &= 1 - \\left(\\frac{1}{365}\\right)^{n} \\times (365\\times 364 \\times\\cdots (365 - n + 1) )\n\\end{align*}\\]Proceed:\n\\[\\begin{align*}\n  \\Pr(\\text{least two birthday})\n  &= 1 - \\Pr(\\text{two birthdays }) \\\\\n  &= 1 - \\Pr(\\text{every birthday different}) \\\\\n  &= 1 - \\left(\\frac{365}{365}\\right) \\times \\left(\\frac{364}{365}\\right) \\times \\left(\\frac{363}{365}\\right) \\times \\cdots \\times \\left(\\frac{365 - n + 1}{365}\\right)\\\\\n  &= 1 - \\left(\\frac{1}{365}\\right)^{n} \\times (365\\times 364 \\times\\cdots (365 - n + 1) )\n\\end{align*}\\]Graph relationship various values  \\(N\\) ( \\(2\\)  \\(60\\)), using form compute probability.Graph relationship various values  \\(N\\) ( \\(2\\)  \\(60\\)), using form compute probability.answer (yet).answer (yet).Birthdays independent randomly occur year (.e., day equally likely).Birthdays independent randomly occur year (.e., day equally likely).\nFIGURE E.4: Question 1\nAnswer Exercise 2.24.Answer Exercise 2.25.\n\\(\\Pr( ^c \\cap B^c) = 0.5\\) (using, example, two-way table).\n\\(\\)  \\(B\\) independent.Answer Exercise 2.26.\nUse two-way table, Venn diagrams.Answer Exercise 2.27.\n\\(7\\times 3\\times 2 = 42\\).Answer Exercise 2.28.\n\\(10\\times 10\\times 10\\times 26\\times 26\\times 10 = 6\\ 760\\ 000\\).Answer Exercise 2.29.\nOrder important, combinations relevant.\\(\\binom{52}{5} = 2\\,598\\,960\\) ways get five cards  \\(52\\) (without replacement).Pick denomination: \\(13\\) select .\nneed two \\(4\\) cards: \\(13\\times \\binom{4}{2}\\).\nNow three cards drawn \\(12\\) denominations: \\(\\binom{12}{3}\\).\nalso \\(4\\) ways choose suit \\(3\\) cards.\n: number ways \\(13\\times\\binom{4}{2}\\times\\binom{12}{3}\\times 4^3 = 1\\,098\\,240\\).\nprobability therefore \\(1\\,098\\,240/2\\,598\\,960 = 0.422569\\).\nTAKE THREE KIND FOUR KIND!!\\(4\\) suits \\(4\\) picture cards, \\(16\\) picture cards total.\nwant select five picture cards \\(52\\) cards, without replacement:\n\\(^{16}C_5 = 4\\,368\\) ways .\nprobability \\(4\\,368\\ 160/^{52}C_5 = 0.0017\\).Answer Exercise 2.31.\n\\(\\binom{25}{8}/\\binom{26}{6} = (25!\\, 6!\\, 19!)/(8!\\,17!\\,25!) = 171/28 \\approx 6.107\\).Answer Exercise 2.33.\nanswer (yet).Answer Exercise 2.34.\nkey: \\(P\\) must lie semi-circle diameter \\(AB\\).Answer Exercise 2.37.Answer Exercise 2.38.\nappear.Answer Exercise 2.39.Generate random sequence length \\(1000\\) digits \\(1\\), \\(2\\) \\(3\\) represent door hiding car \\(1000\\) nights.Generate another sequence represent contestants first choice \\(1000\\) nights (assumed chosen random).number times numbers two lists random numbers agree represents number times contestant win contestant doesn’t change doors.\nnumbers two columns don’t agree contestant win contestant decides change doors.Recall host selects door knows contains car.Generate random sequence length \\(1000\\) digits \\(1\\), \\(2\\) \\(3\\) represent door hiding car \\(1000\\) nights.Generate another sequence represent contestants first choice \\(1000\\) nights (assumed chosen random).host opens door chosen contestant, contain car.contestant select one unopened doors.number times numbers two lists random numbers agree represents number times contestant win contestant doesn’t change doors.\nnumbers two columns don’t agree contestant win contestant decides change doors.","code":"\n# Define a function to compute Stirling numbers:\nstirling <- function(n){ \n  sqrt(2 * pi *n) * (n/exp(1))^n\n}\n\nn <- 1:10\nActual <- factorial(n) \nApprox <- stirling(n)\nRelError <- (Actual - Approx)/Actual * 100\n\ncbind(Actual, Approx, RelError)\n#>        Actual       Approx  RelError\n#>  [1,]       1 9.221370e-01 7.7862991\n#>  [2,]       2 1.919004e+00 4.0497824\n#>  [3,]       6 5.836210e+00 2.7298401\n#>  [4,]      24 2.350618e+01 2.0576036\n#>  [5,]     120 1.180192e+02 1.6506934\n#>  [6,]     720 7.100782e+02 1.3780299\n#>  [7,]    5040 4.980396e+03 1.1826224\n#>  [8,]   40320 3.990240e+04 1.0357256\n#>  [9,]  362880 3.595369e+05 0.9212762\n#> [10,] 3628800 3.598696e+06 0.8295960\n\nplot(RelError ~ n,\n     ylim = c(0, 8),\n     las = 1, \n     type = \"b\",\n     pch = 19,\n     lwd = 2,\n     xlab = expression(italic(n)),\n     ylab = \"Relative error (%)\",\n     main = \"Relative error of\\nStirling's approximation\")\nx <- y <- seq(0, 1, by = 0.25)\n\ndistances <- outer(x, y, function(x, y){sqrt(x^2 + y^2)})\n\nunique(sort(distances))\n#>  [1] 0.0000000 0.2500000 0.3535534 0.5000000\n#>  [5] 0.5590170 0.7071068 0.7500000 0.7905694\n#>  [9] 0.9013878 1.0000000 1.0307764 1.0606602\n#> [13] 1.1180340 1.2500000 1.4142136\nfor (k in (1:5)){ # Answer must e less than 6\n  cat(\"FOR k = \", k, \":\")\n  cat(\"LHS =\", 12 * factorial(7) / factorial(7 - k), \"; \")\n  cat(\"RHS =\", 7 * factorial(6) / factorial(5 - k), \"\\n\")\n}\n#> FOR k =  1 :LHS = 84 ; RHS = 210 \n#> FOR k =  2 :LHS = 504 ; RHS = 840 \n#> FOR k =  3 :LHS = 2520 ; RHS = 2520 \n#> FOR k =  4 :LHS = 10080 ; RHS = 5040 \n#> FOR k =  5 :LHS = 30240 ; RHS = 5040\nN <- 2:80\nprobs <- array( dim = length(N) )\n\nfor (i in 1:length(N)){\n  \n  probs[i] <- 1 - prod( (365 - (1:N[i]) + 1)/365 )\n\n}\nplot( probs  ~ N,\n      type = \"l\",\n      lwd = 2,\n      ylab = \"Prob. at least two share birthday\",\n      xlab = expression(Group~size~italic(N)),\n      las = 1)\nset.seed(981686)\nnumberSimulations <- 5000\nanyConsecutive <- 0\n\nfor (i in (1:numberSimulations)){\n  # Find the six numbers\n  theNumbers <- sample(1:45,\n                       size = 6,\n                       replace = FALSE)\n  theNumbers <- sort(theNumbers)\n  \n  if (any( diff(theNumbers) == 1 )){\n    anyConsecutive <- anyConsecutive + 1\n  }\n  \n}\ncat(\"Number with consecutive values\", \n    anyConsecutive,\"\\n\")\n#> Number with consecutive values 2691\ncat(\"Proportion with consecutive values\", \n    round(anyConsecutive/numberSimulations, 3), \"\\n\")\n#> Proportion with consecutive values 0.538\ncat(\"Proportion WITHOUT consecutive values\", \n    round(1 - anyConsecutive/numberSimulations, 3), \"\\n\")\n#> Proportion WITHOUT consecutive values 0.462\nset.seed(123)\n\nroll_die <- function(die, n) sample(die, n, replace = TRUE)\n\nn <- 1e6\nA <- c(2, 2, 4, 4, 9, 9)\nB <- c(1, 1, 6, 6, 8, 8)\nC <- c(3, 3, 5, 5, 7, 7)\n\n# Simulate rolls\nA_rolls <- roll_die(A, n)\nB_rolls <- roll_die(B, n)\nC_rolls <- roll_die(C, n)\n\n# Compute win probabilities\np_A_beats_B <- mean(A_rolls > B_rolls)\np_B_beats_C <- mean(B_rolls > C_rolls)\np_C_beats_A <- mean(C_rolls > A_rolls)\n\nc(\n  \"P(A > B)\" = p_A_beats_B,\n  \"P(B > C)\" = p_B_beats_C,\n  \"P(C > A)\" = p_C_beats_A\n)\n#> P(A > B) P(B > C) P(C > A) \n#> 0.555105 0.555608 0.555870\nset.seed(93671)        # For reproducibility\nnum_Reps <- 1000       # Number of simulations\n\n# Initialize counters\nWin_By_Switching <- 0\nWin_By_Staying   <- 0\n\nfor (i in 1:num_Reps) {\n\n  # Step 1: Randomly place the car\n  Car_Door <- sample(1:3, 1)\n\n  # Step 2: CONTESTANT makes an initial choice\n  First_Choice <- sample(1:3, \n                         size = 1)\n\n  # Step 3: HOST then chooses to open a goat door and show contestant.\n  #         Host chooses door *not* picked by contestant, or door *not* hiding car\n  Possible_Reveals <- setdiff(1:3, \n                              c(First_Choice, Car_Door))\n\n  # So Host may now have one or two options of door to open\n  if (length(Possible_Reveals) == 1) {\n    # With one option... just take it\n    Host_Reveal <- Possible_Reveals\n  }\n  if (length(Possible_Reveals) == 2) {\n    # With two options, select one\n    Host_Reveal <- setdiff(1:3, \n                           First_Choice)\n  }\n\n  # Step 4: CONTESTANT may decide to switch to the other unopened door\n  Remaining_Door <- setdiff(1:3, \n                            c(First_Choice, Host_Reveal))\n  Switch_Choice <- Remaining_Door\n\n  # Step 5: Check win conditions\n  if (First_Choice == Car_Door) {\n    Win_By_Staying <- Win_By_Staying + 1\n  } else {\n    if (Switch_Choice == Car_Door) {\n       Win_By_Switching <- Win_By_Switching + 1\n    }\n  }\n}\n\n# Results\nc(Win_By_Staying, Win_By_Switching) / num_Reps\n#> [1] 0.336 0.664"},{"path":"selected-solutions.html","id":"AnswersChapRandomVariables","chapter":"E Selected solutions","heading":"E.3 Answers for Chap. 3","text":"Answer Exercise 3.1.\\(R_X = \\{X \\mid x \\(0, 1, 2) \\}\\); discrete.\\(R_X = \\{X \\mid x \\(1, 2, 3\\dots) \\}\\); discrete.\\(R_X = \\{X \\mid x \\(0, \\infty) \\}\\); continuous.\\(R_X = \\{X \\mid x \\(0, \\infty) \\}\\); continuous.Answer Exercise 3.2.\\(R_X = \\{X \\mid x \\(0, 1, 2, \\dots) \\}\\); discrete.\\(R_X = \\{X \\mid x \\(0, 1, 2, \\dots) \\}\\); discrete.\\(R_X = \\{X \\mid x \\(0, \\infty) \\}\\); continuous.\\(R_X = \\{X \\mid x \\[0, \\infty) \\}\\); mixed.Answer Exercise 3.3.sum probabilities one, none negative.\\(\\displaystyle F_X(x) =\n\\begin{cases}\n   0   & \\text{$x < 10$};\\\\\n   0.3 & \\text{$10 \\le x < 15$};\\\\\n   0.5 & \\text{$15 \\le x < 20$};\\\\\n   1   & \\text{$x \\ge 20$}.\n\\end{cases}\\)\\(\\Pr(X > 13) = 1 - F_X(13) = 0.7\\).\\(\\Pr(X \\le 10 \\mid X\\le 15) = \\Pr(X \\le 10) / \\Pr(X \\le 15) = F_X(10)/F_X(15) = 0.3/0.5 = 0.6\\).Answer Exercise 3.4.probabilities non-negative.\nAlso: \\(\\sum_{x=0}^\\infty p_X(x) = \\frac{1}{2} + \\frac{1}{2}\\left(\\frac{1}{2}\\right) + \\frac{1}{2}\\left(\\frac{1}{2}\\right)^2 + \\frac{1}{2}\\left(\\frac{1}{2}\\right)^3 + \\cdots\\).\nUsing Equation (B.5)), \\(= 1/2\\) \\(r = 1/2\\), sum series \n\\(\\sum_{x=0}^\\infty p_X(x) = 1\\).\n\\(p_X(s)\\) valid PDF.Answer Exercise 3.5.\\(\\alpha = 2/15\\).\\(\\displaystyle F_Z(z) =\n\\begin{cases}\n     0                     &  \\text{$z \\le -1$};\\\\\n     6z/15 - z^2/15 + 7/15 & \\text{$-1 < z < 2$};\\\\\n     1                     & \\text{$z \\ge 2$}.\n\\end{cases}\\)\\(\\Pr(Z < 0) = F_Z(0) = 7/15 \\approx 0.4666\\).\nFIGURE E.5: DF \\(f(z)\\)\nAnswer Exercise 3.6.\n1. \\(F_X(x) = 0\\) \\(y \\le 0\\); \\(F_x(x) = (12 - y^2)y/16\\) \\(0 < y < 2\\); \\(F_X(x) = 1\\) \\(y \\ge 2\\).\n2. \\(11/16\\).Answer Exercise 3.7.\n1. \\(p = 0.5\\).\n1. See .\n1. \\(y < 0\\), \\(F_Y(y) = 0\\); \\(y = 0\\), \\(F_Y(y) = 0.5\\); \\(0 < y < 1\\), \\(F_Y(y) = (1 - y^2 + 2y)/2\\); \\(y \\ge 1\\), \\(F_Y(y) = 1\\).\n1. \\(1/8\\).Answer Exercise 3.8.\n1. \\(c = 0.25\\).\n1. See .\n1. \\(x < 0\\), \\(F_X(x) = 0\\); \\(x = 0\\), \\(F_X(x) = 0.25\\); \\(0 < x \\ge 1\\), \\(F_X(x) = (1 + x^2)/4\\); \\(1 < x \\ge 3\\), \\(F_X(x) = (6x - 1 - x^2)/8\\); \\(x > 3\\), \\(F_X(x) = 1\\).\n1. \\(1/2\\).Answer Exercise 3.9.\n\\(\\alpha = 2\\), \\(f_Y(y) = y^2 - 2\\) \\(y = 1, 2\\).Answer Exercise 3.10.\nSolving mass function give total area one gives \\(b = -2\\).\nproduces negative probability \\(x = -2\\), value  \\(b\\) produces valid probability function.Answer Exercise 3.11.\n\\(= 2/3\\).Answer Exercise 3.12.\n\\(= 2/\\sqrt{2 + \\pi}\\approx 0.8820\\)$.Answer Exercise 3.13.\\(\\displaystyle\np_W(w) = \\begin{cases}\n            0.3 & \\text{$w = 10$};\\\\\n            0.4 & \\text{$w = 20$};\\\\\n            0.2 & \\text{$w = 30$};\\\\\n            0.1 & \\text{$w = 40$};\\\\\n            0   & \\text{elsewhere}.\n         \\end{cases}\\)\\(\\Pr(W < 25) = 0.7\\).Answer Exercise 3.14.\\(\\displaystyle\nf_Y(y) = \\begin{cases}\n            (4/3) - y^2 & \\text{$0 < y < 1$};\\\\\n            0 & \\text{elsewhere}.\n         \\end{cases}\\)\\(\\Pr(Y < 0.5) = 0.625\\).\nFIGURE E.6: PDF\nAnswer Exercise 3.15.Easiest draw, see represents triangular distribution, find area said triangle.\nIntegration can used though.PDF:\n\\[\\begin{align*}\nf_X(x)\n&=\n\\begin{cases}\n   \\frac{2}{8.8\\times 4.4} (x - 0.6) & \\text{$0.6 < x < 5$};\\\\\n   \\frac{2}{8.8\\times 4.4} (9.4 - x) & \\text{$5 < x < 9.4$}\n\\end{cases}\\\\\n&=\n\\begin{cases}\n   \\frac{2}{38.72} (x - 0.6) & \\text{$0.6 < x < 5$};\\\\\n   \\frac{2}{38.72} (9.4 - x) & \\text{$5 < x < 9.4$}.\n\\end{cases}\n\\end{align*}\\]Proceed:\n\\[\\begin{align*}\nF_X(x)\n&=\n\\begin{cases}\n   0                                   & \\text{$x < 0.6$}\\\\\n   \\int_{0.6}^x \\frac{2}{38.72} (t - 0.6) \\, dt               & \\text{$0.6 \\le x < 5$};\\\\\n   \\int_5^x     \\frac{2}{38.72} (9.4 - t)\\, dt + 0.5       & \\text{$5 \\le x < 9.4$}.\\\\\n   1                                   & \\text{$x \\ge 9.4$}\n\\end{cases}\\\\\n&=\n\\begin{cases}\n   0                                   & \\text{$x < 0.6$}\\\\\n   [(0.6 - x)^2 ]/ 38.72               & \\text{$0.6 \\le x < 5$};\\\\\n   1 - [(x - 9.4)^2] / 38.72           & \\text{$5 \\le x < 9.4$}.\\\\\n   1                                   & \\text{$x \\ge 9.4$}\n\\end{cases}\n\\end{align*}\\]See .\\(\\Pr(X > 3) = 1 - \\Pr(X < 3) =  1 - 0.1487603 =  0.8512397\\) (use areas triangles)\\(\\Pr(X > 3 \\mid X > 1) = \\Pr(X > 3) / \\Pr(X > 1) = 0.8512397 / 0.9958678 = 0.8547718\\).One just \\(X\\) exceeds 3; \\(X\\) exceeds 3 already know \\(X\\) exceeded 1.\nFIGURE E.7: PDF DF\nAnswer Exercise 3.16.Using area triangles, ‘height’ \\(2/27 \\approx 0.07407407\\) shown .\n, algebra:\n\\[\n  f_S(s) =\n   \\begin{cases}\n      \\frac{4}{81}s - \\frac{14}{81} & \\text{$3.5 < s < 5$};\\\\\n      -\\frac{4}{1377}s + \\frac{122}{1377} & \\text{$5 < s < 30.5$}.\n   \\end{cases}\n\\]\nAlso,\n\\[\n   F_S(s) =\n   \\begin{cases}\n      0                                    & \\text{$s < 3.5$};\\\\\n      \\frac{(7 - 2s)^2}{162}               & \\text{$3.5 < s < 5$};\\\\\n      \\frac{2}{1377} (s^2 - 61s + 280)     & \\text{$5 < s < 30.5$};\\\\\n      1                                    & \\text{$s > 30.5$}.\n   \\end{cases}\n\\]\n,\n\\(\\Pr(S > 20 \\mid S > 10) = \\Pr( (S > 20) \\cap (S > 10) )/\\Pr(S > 10) = \\Pr(S > 20)/\\Pr(S > 10) = 0.1601311 / 0.6103854\\).\n \\(0.2623\\).\nFIGURE E.8: PDF DF\nAnswer Exercise 3.17.Producers usually need receive least certain amount rainfall.poor graph ; really fix .Six months recorded \\(60\\,\\text{mm}\\); \\(6/81\\).\ntaking half previous ‘40 60’ category, ’d get \\(12/81\\).\nsomewhere around .June, \\(81\\) observations, median \\(41st\\): median rainfall  \\(0\\) \\(20\\,\\text{mm}\\).\nDecember, \\(80\\) observations, median \\(40.5\\)th: median rainfall  \\(0\\) \\(60\\,\\text{mm}\\).Median; skewed right.…\nFIGURE E.9: Exceedance charts\nAnswer Exercise 3.18.Suppose stand Position 1; friend can Positions 2  5, distances \\(1\\), \\(2\\), \\(3\\), \\(4\\).\nSimilar stand Position 5.Suppose stand Position 2; friend can Positions 1, 3  5, distances \\(1\\), \\(1\\), \\(2\\), \\(3\\).\nSimilar stand Position 4.Suppose stand Position 3; friend can Positions 1, 2, 4, 5, distances \\(1\\), \\(2\\), \\(2\\), \\(1\\).counting, PDF number people us, say \\(Y\\), :\n\\[\n   f_Y(y) =\n   \\begin{cases}\n      4/10 & \\text{$y = 0$};\\\\\n      3/10 & \\text{$y = 1$};\\\\\n      2/10 & \\text{$y = 2$};\\\\\n      1/10 & \\text{$y = 3$};\\\\\n         0 & \\text{elsewhere}.\n   \\end{cases}\n\\]\n\\(f_Y(y) = (4 - y)/10\\) \\(y\\\\{0, 1, 2, 3\\}\\).Answer Exercise 3.19.\\(\\int_0^1 (1 - y)^2\\,dy = \\left. (1 - y)^3/3\\right|_0^1 = 1/3\\).\\(\\Pr(|Y - 1/2| > 1/4) = \\Pr(Y > 3/4) + \\Pr(Y < 1/4) = 1 - \\Pr(1/4 < Y < 3/4) = 13/32\\).See Fig. E.10.\nFIGURE E.10: PDF \\(y\\)\nAnswer Exercise 3.20.\ndf :\n\\[\n  F_Y(y) =\n  \\begin{cases}\n    0                    & \\text{$y < 0$};\\\\\n    \\frac{1}{3}(y - 1)^2 & \\text{$1 < y < 2$};\\\\\n    \\frac{1}{3}(2y - 3)  & \\text{$2 < y < 3$};\\\\\n    1                    & \\text{$y \\ge 3$.}\n  \\end{cases}\n\\]\n\\(y = 3\\), expect \\(F_Y(y) = 1\\); true.\n\\(y = 1\\), expect \\(F_Y(y) = 0\\); true.\n \\(y\\), \\(0 \\le F_Y(y) \\le 1\\).Answer Exercise 3.21.\n\\(\\Pr(60 < Y < 70) = 154\\,360\\,000k/3\\approx 51\\,453\\,333k\\) \\(\\Pr(Y > 70) = 54\\,360\\,000k\\).\nUsing model, larger probability dying  \\(70\\).Answer Exercise 3.22.\n1. See \\(X = 1\\) (.e., one pooled test, negative; testing needed) \\(X = n + 1\\) (pooled test positive,  \\(n\\) individual tests needed addition pooled test).\nsample space \\(\\{1, n + 1\\}\\).\n2. \\(X = 1\\) occurs test negative; , \\(\\Pr(X = 1) = (1 - p)^n\\)\n:\n\\[\n  p_X(x) =\n  \\begin{cases}\n     (1 - p)^n     & \\text{$x = 1$ (.e., pooled test negative)};\\\\\n     1 - (1 - p)^n & \\text{$x = n + 1$}\n  \\end{cases}\n\\]\nzero elsewhere.Answer Exercise 3.23.\ncards like:\n\\[\n  2, 3, 4, \\dots, 8, 9, 10, 10 (J), 10 (Q), 10 (K), 10 ().\n\\]\n,  \\(20\\) cards points value ten (four suits five cards ).\n‘distance’ say \\(D\\),  \\(8\\), need \\(\\Pr[ (2, 10)\\ \\text{}\\ (10, 2)]\\), probability.\n\\[\n  \\Pr(D = 8) = 2\\times \\left(\\frac{4 \\times 20}{52 \\times 51}\\right).\n\\]\n‘distance’  \\(7\\), need\n\\[\n  \\Pr[(3, 10)\\ \\text{}\\ (10, 3)] +\n  \\Pr[(2,  9)\\ \\text{}\\ (9,  2)],\n\\]\nprobability\n\\[\n  \\Pr(D = 7)\n  = 2\\times\\left(\\frac{  \\times 20}{52 \\times 51}\\right) +\n    2\\times\\left(\\frac{4 \\times 4}{52\\times 51}\\right).\n\\]\n‘distance’  \\(6\\), need\n\\[\n   \\Pr[(4, 10)\\ \\text{}\\ (10, 4)] +\n   \\Pr[(3,  9)\\ \\text{}\\ (9,  3)] +\n   \\Pr[(2,  8)\\ \\text{}\\ (8,  2)]\n\\]\nprobability\n\\[\n  \\Pr(D = 6)\n  = 2\\times\\left(\\frac{4 \\times 20}{52 \\times 51}\\right) +\n    2\\times\\left(\\frac{4 \\times 4}{52\\times 51}\\right) +\n    2\\times\\left(\\frac{4 \\times 4}{52\\times 51}\\right).\n\\]\ngeneral, \\(D = 1, 2, \\dots 8\\):\n\\[\n  \\Pr(D = d) =\n  2\\times\\left(\\frac{4 \\times 20}{52 \\times 51}\\right) +\n  (8 - d)\\times 2\\times\\left(\\frac{4 \\times  4}{52 \\times 51}\\right)\n\\]case \\(D = 0\\) different.\ncan compute probability  \\(1\\) minus probabilities \\(D = 1\\) \\(D = 8\\), directly.\nsubtraction:proceed directly, see ‘distance’ zero can occur get ‘ten’ another ‘ten’, non-ten card plus non-ten card:\n\\[\n  \\Pr(D = 0) = \\frac{20 \\times 19}{52 \\times 51} + \\frac{32\\times 3}{52\\times 51}.\n\\]\nanswer :See PMF Fig. E.11.\nFIGURE E.11: PDF `distance’ cards\nAnswer Exercise 3.24.Write \\(p(x) = \\log_{10}(1 + x) - \\log_{10}x\\); sum \n\\[\\begin{align*}\n(\\log_{10}(2) - \\log_{10} 1)\n+ (\\log_{10} 3 - \\log_{10} 2)\n+ (\\log_{10} 4 - \\log_{10} 3)\n+ \\dots\\\\\n+ (\\log_{10} 9 - \\log_{10} 8)\n+ (\\log_{10} 10 - \\log_{10} 9)\n\\end{align*}\\]\nthings cancel, leaving \\(\\log_{10}10 = 1\\).df \n\\[\\begin{align*}\nF_D(d)\n&= \\sum_{= 1}^d \\log[ (1 + d)/ \\log(d) ] \\\\\n&= \\sum_{= 1}^d \\log(1 + d) - \\log d\\\\\n&= (\\log 2 - \\log 1) + (\\log 3 - \\log 2) + \\log 4 - \\log 3) + \\cdots\\\\\n&  \\quad {} + (\\log d - \\log(d - 1) ) + (\\log (1 + d) - \\log d)\\\\\n&= \\log (1 + d).\n\\end{align*}\\]\nFIGURE E.12: PDF DF\nAnswer Exercise 3.25.\\(k = 1/\\pi\\).\n\\(F(y) = \\int_0^x \\frac{1}{\\pi\\sqrt{t(1 - t)}}\\, dt = \\frac{1}{\\pi}\\arcsin(2x - 1) + \\frac{1}{2}\\) \\(0 < x < 1\\).\n\\(\\Pr(X > 0.25) = 2/3\\).\nFIGURE E.13: PDF DF\nAnswer Exercise 3.26.PDF \n\\[\n   f_X(x)\n   = \\frac{d}{dx} \\exp(-1/x)\n   = \\frac{\\exp(-1/x)}{x^2}\n\\]\n\\(x > 0\\).\nSee Fig. E.14.\nFIGURE E.14: PDF\nAnswer Exercise 3.27.1 suit: Select 4 cards  13 suit, four suits select.2 suits: two scenarios :\nThree one suit, one another:\nChoose suit, select three cards : \\(\\binom{4}{3}\\binom{13}{3}\\).\nneed another suit (three choices remain) one card ( 13).\nChose two suits, two cards two suits:\n\\(\\binom{4}{2}\\binom{13}{2}\\binom{13}{2}\\)\nThree one suit, one another:\nChoose suit, select three cards : \\(\\binom{4}{3}\\binom{13}{3}\\).\nneed another suit (three choices remain) one card ( 13).Chose two suits, two cards two suits:\n\\(\\binom{4}{2}\\binom{13}{2}\\binom{13}{2}\\)3 suits: Umm…4 suits: Choose one four suits.One way get 3 suits realise total probability must add one…Answer Exercise 3.29.\nidea…ChatGPT (.e., haven’t checked):least four:Exactly four:","code":"\nF1 <- function(x){\n  F <- array( dim = length(x))\n  F[ x < -1] <- 0\n  F[ x > 2] <- 1\n  Interesting <- (x > -1) & (x < 2)\n  F[ Interesting ] <- 6*z[Interesting]/15 - (z[Interesting]^2)/15 + 7/15\n  F\n}\n\nz <- seq(-2, 3, \n         length = 200)\nplot( F1(z) ~ z,\n      las = 1,\n      type = \"l\",\n      xlab = expression(italic(x)),\n      ylab = \"Dist. fn.\",\n      lwd = 2)\nfy <- function(y){\n  PDFy <- array(0, dim = length(y))\n  PDFy <- ifelse( (y > 0) & (y < 1),\n                  (4/3) - y^2,\n                  0)\n  PDFy\n}\ny <- seq(-1, 2,\n         length = 500)\nplot(fy(y) ~ y,\n     lwd = 2,\n     type = \"l\",\n     xlab = expression(italic(y)),\n     ylab = \"PDF\",\n     las = 1)\nf <- function(x){\n  f <- array( dim = length(x))\n  f[x < 0.6] <- 0\n  f[x > 9.4] <- 0\n  xSmall <- (x >= 0.6) & ( x <= 5)\n  xLarge <- (x > 5) & (x <= 9.4)\n  f[xSmall] <- 2 / (8.8 * 4.4) * (x[xSmall] - 0.6)\n  f[xLarge] <- 2 / (8.8 * 4.4) * (9.4 - x[xLarge] )\n  f\n}\n\nF <- function(x){\n  FF <- array( dim = length(x))\n\n  for (i in (1:length(x))){\n    FF[i] <- integrate(f, lower = 0.6, upper = x[i])$value\n  }\n  FF\n}\n\n\npar(mfrow = c(1, 2))\nx <- seq(0, 10,\n         length = 200)\nplot( f(x) ~ x,\n      type = \"l\",\n      lwd = 2,\n      las = 1,\n      main = \"PROB fn\",\n      xlab = expression(italic(x)),\n      ylab = \"Prob. fn.\")\n\n\n\nx <- seq(0, 10,\n         length = 200)\nplot( F(x) ~ x,\n      type = \"l\",\n      lwd = 2,\n      las = 1,\n      main = \"DIST fn\",\n      xlab = expression(italic(x)),\n      ylab = \"Dist fn.\")\n1 - F(3)\n#> [1] 0.8512397\n(1 - F(3)) / (1 - F(1))\n#> [1] 0.8547718\n(1 - Fx(20) ) / ( 1 - Fx(10))\n#> [1] 0.2623443#>       Buckets          Jun  Dec \n#>  [1,] \"Zero\"           \"3\"  \"0\" \n#>  [2,] \"0 < R < 20\"     \"41\" \"17\"\n#>  [3,] \"20 <= R < 40\"   \"19\" \"17\"\n#>  [4,] \"40 <= R < 60\"   \"12\" \"21\"\n#>  [5,] \"60 <= R < 80\"   \"2\"  \"9\" \n#>  [6,] \"80 <= R < 100\"  \"2\"  \"6\" \n#>  [7,] \"100 <= R < 120\" \"0\"  \"3\" \n#>  [8,] \"120 <= R < 140\" \"2\"  \"1\" \n#>  [9,] \"140 <= R < 160\" \"0\"  \"4\" \n#> [10,] \"160 <= R < 180\" \"0\"  \"0\" \n#> [11,] \"180 <= R < 200\" \"0\"  \"1\" \n#> [12,] \"200 <= R < 220\" \"0\"  \"1\"\nd <- 1:8\npd <- (2 * 4 * 20)/(52 * 51) + (8 - d) * (2 * 4 * 4)/(52 * 51)\nrbind(d, pd)\n#>         [,1]    [,2]      [,3]      [,4]\n#> d  1.0000000 2.00000 3.0000000 4.0000000\n#> pd 0.1447964 0.13273 0.1206637 0.1085973\n#>          [,5]       [,6]       [,7]       [,8]\n#> d  5.00000000 6.00000000 7.00000000 8.00000000\n#> pd 0.09653092 0.08446456 0.07239819 0.06033183\n1 - sum(pd)\n#> [1] 0.1794872\n(20 * 19)/(52 * 51) + (32 * 3)/(52 * 51)\n#> [1] 0.1794872#> Warning in sqrt(y * (1 - y)): NaNs produced\n### 1 SUIT\nsuits1 <- 4 * choose(13, 4) / choose(52, 4)\n\n### 2 SUITS\nsuits2 <- choose(4, 3) * choose(13, 3) * choose(3, 1) * choose(13, 1) + \n          choose(4, 2) * choose(13, 2) * choose(13, 2)\nsuits2 <- suits2 / choose(52, 4)\n\n### 4 SUITS:\nsuits4 <- choose(13, 1) * choose(13, 1) * choose(13, 1) * choose(13, 1)\nsuits4 <- suits4  / choose(52, 4)\n\nsuits3 <- 1 - suits1 - suits2 - suits4\n\nround( c(suits1, suits2, suits3, suits4), 3)\n#> [1] 0.011 0.300 0.584 0.105\n# Set the number of simulations\nnum_simulations <- 100000\n\n# Initialize a vector to store the number of rolls required for each simulation\nrolls_required <- numeric(num_simulations)\n\n# Function to simulate rolling a die until the total is 4\nsimulate_rolls <- function() {\n  total <- 0\n  rolls <- 0\n  while (total < 4) {\n    roll <- sample(1:6, 1) # Roll the die\n    total <- total + roll\n    rolls <- rolls + 1\n  }\n  return(rolls)\n}\n\n# Perform simulations\nfor (i in 1:num_simulations) {\n  rolls_required[i] <- simulate_rolls()\n}\n\n# Calculate the PMF\nPMF <- table(rolls_required) / num_simulations\n\n# Print the PMF\nprint(PMF)\n\n# Optionally, plot the PMF\nbarplot(PMF, main=\"Probability Mass Function of Rolls Needed to Sum to 4\",\n        xlab=\"Number of Rolls\", ylab=\"Probability\",\n        col=\"lightblue\", border=\"blue\")\n# Set the number of simulations\nnum_simulations <- 100000\n\n# Initialize a vector to store the number of rolls required for each simulation\nrolls_required <- integer(num_simulations)\n\n# Function to simulate rolling a die until the total is exactly 4\nsimulate_rolls <- function() {\n  total <- 0\n  rolls <- 0\n  while (total < 4) {\n    roll <- sample(1:6, 1) # Roll the die\n    total <- total + roll\n    rolls <- rolls + 1\n    if (total > 4) {\n      return(NA) # Return NA if the total exceeds 4\n    }\n  }\n  return(rolls)\n}\n\n# Perform simulations\nfor (i in 1:num_simulations) {\n  result <- simulate_rolls()\n  if (!is.na(result)) {\n    rolls_required[i] <- result\n  }\n}\n\n# Remove NA values from the results\nrolls_required <- na.omit(rolls_required)\n\n# Remove zero values (impossible cases)\nrolls_required <- rolls_required[rolls_required > 0]\n\n# Calculate the PMF\nPMF <- table(rolls_required) / length(rolls_required)\n\n# Print the PMF\nprint(PMF)\n\n# Optionally, plot the PMF\nbarplot(PMF, main=\"Probability Mass Function of Rolls Needed to Sum to 4\",\n        xlab=\"Number of Rolls\", ylab=\"Probability\",\n        col=\"lightblue\", border=\"blue\")"},{"path":"selected-solutions.html","id":"AnswersChapBivariateExercises","chapter":"E Selected solutions","heading":"E.4 Answers for Chap. 4","text":"Answer Exercise 4.1.corresponds cell \\(X = 1, Y = 2\\): \\(5/24\\approx 0.208333\\).\\(\\Pr(X + Y \\le 1) = \\Pr(X = 0, Y = 0) + \\Pr(X = 0, Y = 1) + \\Pr(X = 1, Y = 0) = 1/2\\).\\(\\Pr(X > Y) = \\Pr(X = 1, Y = 0) = 1/4\\).Write:\n\\[\np_X(x) = \\begin{cases}\n         7/24 & \\text{$x = 0$};\\\\\n         17/24 & \\text{$x = 1$};\\\\\n         0 & \\text{otherwise}.\n         \\end{cases}\n\\]consider column corresponding \\(X = 1\\):\n\\[\np_{Y\\mid X = 1}(y\\mid x = 1) =\n\\begin{cases}\n         (1/4)/(17/24) = 6/17 & \\text{$y = 0$};\\\\\n         (1/4)/(17/24) = 6/17 & \\text{$y = 1$};\\\\\n         (5/24)/(17/24) = 5/17 & \\text{$y = 2$};\\\\\n         0 & \\text{otherwise}.\n\\end{cases}\n\\]Answer Exercise 4.2.\\(0\\).\\(9/15\\).\\(\\Pr(Y = 0) = 1/15\\); \\(\\Pr(Y = 1) = 6/15\\); \\(\\Pr(Y = 2) = 4/15\\); \\(\\Pr(Y = 3) = 3/15\\); \\(\\Pr(Y = 4) = 1/15\\).\\(\\Pr(X = 1) = 4/15\\); \\(\\Pr(X = 2) = 5/15\\); \\(\\Pr(X = 3) = 6/15\\).\\(\\Pr(Y = 1\\mid X = 1) = 2/4\\); \\(\\Pr(Y = 2\\mid X = 1) = 1/4\\); \\(\\Pr(Y = 3\\mid X = 1) = 1/4\\).Answer Exercise 4.3.\n1. \\(\\int_0^2\\!\\!\\!\\int_0^1 x + y^2\\, dy\\, dx = 11/3\\), \\(k = 3/11\\).\n2. \\(\\dfrac{3}{11} \\int_1^2\\!\\!\\!\\int_{1/2}^1 x + y^2\\, dy\\, dx = 37/88\\approx 0.42045\\).\n3. ???\n4. \\(\\dfrac{3}{11} \\int_0^2 x + y^2\\, dy = 2(3x + 4)/11\\) \\(0 < x < 1\\).\n5. \\(\\dfrac{3}{11} \\int_0^1 x + y^2\\, dx = 3(2y^2 + 1)/22\\) \\(0 < y < 2\\).\n6. \\(2(x + y^2) / [2(3x + 4)]\\) \\(0 < x < 1\\), \\(0 < y < 2\\).\n7. \\(3(1 + y^2) / 14\\) \\(0 < y < 2\\).\n8. \\(2(x + y^2) / (2y^2 + 1)\\) \\(0 < x < 1\\), \\(0 < y < 2\\).\n9. \\(2(x + 1)/3\\) \\(0 < x < 1\\).\n10. .Answer Exercise 4.4.\n1. \\(k = 1/7\\).\n2. \\(3/7\\approx 9.42857\\)$.\n3. ???\n4. \\(2(x + 2)/7\\) \\(1 < x < 2\\).\n5. \\((7 - 2y)/14\\) \\(-1 < y < 1\\) \\(1 < x < 2\\), \\(-1 < y < 1\\).\n6. \\((2 + x - y)/[(2(x + 2)]\\) \\(1 < x < 2\\), \\(-1 < y < 1\\).\n7. \\((3 - y)/6\\) \\(-1 < y < 1\\).\n8. \\(2(2 + x -y)/(7 - 2y)\\) \\(1 < x < 2\\).\n9. \\(2(2 + x)/7\\) \\(1 < x < 2\\).\n10. .Answer Exercise 4.8.Need integral one: \\(\\displaystyle \\int_0^1\\!\\!\\!\\int_0^1 kxy\\, dx\\, dy = 1\\), \\(k = 4\\).:\n\\[\n4 \\int_0^{3/8}\\!\\!\\!\\int_0^{5/8} kxy\\, dx\\, dy = 225/4096\\approx 0.05493.\n\\]Answer Exercise 4.10.Construct table () listing four outcomes.get\n\\[\n  p_X(x) =\n  \\begin{cases}\n  1/4 & \\text{$x = 0$};\\\\\n  1/2 & \\text{$x = 1$};\\\\\n  1/4 & \\text{$x = 2$}.\n  \\end{cases}\n\\]given \\(Y = 1\\), probability function non-zero \\(x = 1, 2\\):\n\\[\n  p_{X\\mid Y = 1}(x \\mid Y = 1) =\n  \\begin{cases}\n  1/2 & \\text{$x = 1$};\\\\\n  1/2 & \\text{$x = 2$};\n  \\end{cases}\n\\]independent; instance, \\(Y = 0\\), \\(\\Pr(X) > 0\\) \\(x =   0, 1\\), constrast \\(Y = 1\\).Answer Exercise 4.11.\njoint probability function  \\(X\\)  \\(Y\\) shown Table E.2.\nexample:minimum  2 maximum 1 impossible; makes sense (hence probability zero).minimum  3 maximum 4 can happen two ways:  4 first die  3 ,  3 first die  4 .minimum maximum 2 can happen one way: dice show  2.joint distribution  \\(C\\)  \\(D\\) therefore:, marginal distribution \\(B\\) (minimum) \n\\[\n   f_B(b) =\n   \\begin{cases}\n       11/36 & \\text{$b = 1$};\\\\\n       9/36 & \\text{$b = 2$};\\\\\n       7/36 & \\text{$b = 3$};\\\\\n       5/36 & \\text{$b = 4$};\\\\\n       3/36 & \\text{$b = 5$};\\\\\n       1/36 & \\text{$b = 6$}\n   \\end{cases}\n\\]\neasily confirmed valid PMF.\nTABLE E.2: minimum maximum two rolls die\nAnswer Exercise 4.12.See Fig. E.15.Integrate correctly! \\(\\displaystyle \\int_0^2 \\!\\!\\!\\int_0^{2 - x} x(y + 1)\\,dy \\, dx = 2\\), \\(c = 1/2\\).\\(P(Y < 1 \\mid X > 1) = P(Y < 1 \\cap X > 1)/\\Pr(X > 1)\\).\nFirst, \\(f_X(x) = (1/2) \\int_0^{2 - x} x(y + 1)\\,dy =  (1/4)(x^3 - 6x^2 + 8x)\\) \\(0 < x < 2\\).\n\\(\\Pr(X > 1) = 7/16 = 0.4375\\).\nAlso,\n\\[\n   P(Y < 1 \\cap X > 1)\n   =  (1/2) \\int_1^2 \\!\\!\\!\\int_0^{2 - x} x(y + 1)\\,dy \\, dx\n   = 7/16.\n\\]\n, \\(P(Y < 1 \\mid X > 1) = 1\\), makes sense diagram (\\(X > 1\\), \\(Y\\) must less 1).\\(P(Y < 1 \\mid X > 0.25) = P(Y < 1 \\cap X > 0.25)/\\Pr(X > 0.25)\\).\nUsing results , \\(\\Pr(X > 0.25) = 3871/4096 = 0.945068\\) \\(P(Y < 1 \\cap X > 0.25) = 0.945\\).\n\\(P(Y < 1 \\mid X > 0.25) = P(Y < 1 \\cap X > 0.25)/\\Pr(X > 0.25)\\).First, \\(f_Y(y) = \\frac{1}{2} \\int_0^{2 - y} x(y + 1)\\, dx = \\frac{1}{4}(y + 1)(y - 2)^2\\).\n,\n\\[\n  \\Pr(Y < 1) = \\int_0^1 \\frac{1}{4}(y + 1)(y - 2)^2\\, dy = \\frac{13}{16}\\approx 0.8125.\n\\]\nFIGURE E.15: region \\(X\\) \\(Y\\) positive probability\nAnswer Exercise 4.13.Proceed:\n\\[\\begin{align*}\n1\n&= k\\int_0^1\\!\\!\\!\\int_0^{\\sqrt{y}} (1 - x)y\\, dx\\, dy\\\\\n&= k\\int_0^1\\!\\!\\!\\int_{x^2}^{1} (1 - x)y\\, dy\\, dx\\\\\n&= \\frac{k}{2}\\int_0^1 (1 - x) (1 - x^4) \\, dx\\\\\n&= \\frac{7k}{30},\n\\end{align*}\\]\n\\(k = 30/7\\approx 4.285714\\).careful integration limits (draw diagram!):\n\\[\\begin{align*}\n\\Pr(X > Y)\n&= \\int_0^1 \\!\\int_y^{\\sqrt{y}} f(x, y)\\, dx\\, dy\\\\\n&= \\int_0^1 \\!\\int_x^{x^2} f(x, y)\\, dy\\, dx\\\\\n&= \\frac{15}{7} \\int_0^1 (x - 1) x^2 (x^2 - 1)\\, dx\\\\\n&= 3/28\\approx 0.1071429.\n\\end{align*}\\]First find marginal distribution \\(X\\):\n\\[\n  f_X(x) = \\int_{x^2}^1 f(x, y)\\, dy = \\frac{15}{7} (x - 1)(x^4 - 1),\n\\]\n\n\\[\n\\Pr(X > 0.5) =  \\frac{15}{7} \\int_{1/2}^1 (x - 1)(x^4 - 1)\\, dx = \\frac{183}{896}\\approx 0.2042411.\n\\]","code":""},{"path":"selected-solutions.html","id":"AnswersChapExpectation","chapter":"E Selected solutions","heading":"E.5 Answers for Chap. 5","text":"Answer Exercise 5.1.\\(k = -2\\).See Fig. E.16.\\(\\operatorname{E}(Y) = 5/3\\).\\(\\operatorname{E}(Y^2) = 17/6\\), \\(\\text{var(Y)} = 17/6 - (5/3)^2 = 1/18\\).\\(\\Pr(X > 1.5) = \\int_{1.5}^2 f_Y(y)\\, dy = 3/4\\).\nFIGURE E.16: PDF Y\nAnswer Exercise 5.2.\n1. Plot shown.\n2. \\(1/3\\).\n3. \\(7/18\\).\n4. \\(2/3\\).Answer Exercise 5.5.First: \\(k = 1/4\\).Plot shown.\\(\\operatorname{E}(D) = 7/4 = 1.75\\); \\(\\operatorname{E}(D^2) = 15/4\\) \\(\\operatorname{var}(D) = 11/16 = 0.6875\\).\\(M_D(t) = \\exp(t)/2 + \\exp(2t)/4 + \\exp(3t)/4\\).Mean variance .\\(\\Pr(D < 3) = 3/4\\).Answer Exercise 5.6.\nFirst, \\(c = 144/205\\).Plot shown.\\(\\operatorname{E}(D) = 60/41\\approx 1.46\\dots\\); \\(\\operatorname{var}(D) = 5616/8405\\approx 0.668\\dots\\).\\(M_Z(t) = \\frac{\\exp(t)}{205}(36\\exp(t) + 16\\exp(2t) + 9\\exp(3t) + 144)\\).Mean variance .\\(61/205\\).Answer Exercise 5.7.\\(M_Z'(t) = 0.6\\exp(t)[0.3\\exp(t) + 0.7]\\) \\(\\operatorname{E}(Z) = 0.6\\).\nAlso, \\(M''_Z(t) = 0.18\\exp(2t) + 0.6\\exp(t)[0.3\\exp(t) + 0.7]\\) \\(\\operatorname{E}(Z^2) = 0.78\\), \\(\\operatorname{var}(Z) = 0.42\\) (careful derivatives !)Expand quadratic find: \\(\\Pr(Z = 0) = 0.49\\), \\(\\Pr(Z = 1) = 0.42\\), \\(\\Pr(Z = 2) = 0.09\\).Answer Exercise 5.8.\\(\\operatorname{E}[W] = (1 - p)/p\\); \\(\\operatorname{var}[W] = (1 - p)/p^2\\).\\(p_W(w) = (1 - p)^x\\) \\(x = 1, 2, \\dots\\).Answer Exercise 5.10.\\(17\\).\\(5 + 2 + 0.2 = 7.2\\).\\(14\\).\\((2^2\\times 5) + ((-3)^2\\times 2) + (2\\times -3\\times 0.2) = 36.8\\).Answer Exercise 5.14.\n\\([exp(tb) - \\exp(ta)]/[t (b - )]\\) \\(t\\ne 0\\).Answer Exercise 5.11.\\(M'_G(t) = \\alpha\\beta(1 - \\beta t)^{-\\alpha - 1}\\) \\(\\operatorname{E}(G) = \\alpha\\beta\\).\n\\(M''_G(t) = \\alpha\\beta^2(\\alpha + 1)(1 - \\beta t)^{-\\alpha - 2}\\) \\(\\operatorname{E}(G^2) = \\alpha\\beta^2(\\alpha + 1)\\) \\(\\operatorname{var}(G) = \\alpha\\beta^2\\).Answer Exercise 5.12.Proceed:\n\\[\n  \\mu'_r = \\operatorname{E}(X^r) = \\int_{x = 0}^1 x^r 2(1 - x)\\, dx = 2\\left[ \\left(\\frac{x^{r + 1}}{r + 1} - \\frac{x^{r + 2}}{r + 2}\\right)\\Big|_{0}^{1}\\right]\n      = 2\\left[ \\frac{1}{r + 1} - \\frac{1}{r + 2}\\right].\n\\]Expanding, \\(\\operatorname{E}((X + 3)^2) = \\operatorname{E}(X^2) + 6\\operatorname{E}(X) + 9\\).\nNow, \\(\\operatorname{E}(X) = \\mu'_1 = 1/3\\) , \\(\\operatorname{E}(X^2) = \\mu'_2 = 1/6\\) .\nHence \\(\\operatorname{E}((X + 3)^2) = 67/6\\).\\(\\operatorname{var}(X) = \\operatorname{E}(X^2) - \\operatorname{E}(X)^2 = 1/18\\).Answer Exercise 5.9.\\(13 + 4 = 17\\).\\(5 + 2 = 7\\).\\((2\\times 13) - (3\\times 4) = 14\\).\\((2^2\\times 5) + (-3)^2\\times 2) = 38\\).Answer Exercise 5.15.\n\\(\\left[6\\left( (t - 2)\\exp(t) + t + 2\\right)\\right]/t^3\\) \\(t\\ne 0\\).Answer Exercise 5.16.\\(\\operatorname{E}(Y) = \\int_2^\\infty y\\frac{2}{y^2}\\,dy = 2\\log y\\Big|_2^\\infty\\), converge.\\(\\operatorname{E}(Y)\\) defined, \\(\\operatorname{var}(Y)\\) defined either.See Fig. E.17, left panel.\\(F_Y(y) = \\int_2^y 2/t^2\\, dt = 1 - (2/y)\\); see Fig. E.17, right panel.\\(F_Y(y) = 0.5\\) gives median  \\(4\\).\\(F_Y(y) = 1/4\\) gives \\(Q_1 = 8/3\\); \\(F_Y(y) = 3/4\\) gives \\(Q_3 = 8\\); IQR \\(8 - 8/3 = 16/3\\).\\(\\Pr(Y > 4\\mid Y > 3) = \\Pr(Y > 4)/\\Pr(Y > 3)\\).\n, \\(\\Pr(Y > 4) = 1 - \\Pr(Y < 4) = 1/2\\) using df;\n\\(\\Pr(Y > 3) = 1 - \\Pr(Y < 3) = 2/3\\) using df;\nanswer  \\(3/4\\).\nFIGURE E.17: probability distribution functions distribution mean\nAnswer Exercise 5.17.See Fig. E.18 (left panel).Evaluate \\(F_X(t) = \\displaystyle \\int_{-\\infty}^t \\frac{1}{\\pi(1 + x^2)}\\,dx = \\frac{1}{\\pi}\\arctan(x) + \\frac{1}{2}\\).\nSee Fig. E.18 (right panel).\\(\\displaystyle \\operatorname{E}(X) =  \\int_{-\\infty}^t \\frac{x}{\\pi(1 + x^2)}\\,dx = ...\\), defined.\nFIGURE E.18: Cauchy distribution\nAnswer Exercise 5.19.Begin Definition 5.10 \\(M_X(t)\\) use fact distribution symmetric  \\(0\\) \\(f_X(x) = f_X(-x)\\) using symmetry.\nTransform resulting integral.Answer Exercise 5.20.\n1. real \\(\\) satisfies conditions.\n2. Need \\(= -1\\).Answer Exercise 5.21.solving, find \\(= 1\\) \\(= 1/2\\).\\(= 1\\), \\(\\operatorname{E}[X] = 1/2\\).\n\\(= 1/2\\), \\(\\operatorname{E}[X] = 31/60 > 1/2\\), \\(= 1/2\\).Answer Exercise 5.22.First, PDF needs defined (see Fig. E.19), define \\(W\\) waiting time.\nLet \\(H\\) ‘height’ triangle.\narea triangle \\(A_1\\)  \\(3H/4\\), area triangle \\(A_2\\)  \\(51H/4\\), \\(H = 2/27\\).two lines, \\(L_1\\)  \\(L_2\\) can found (find slope; determine linear equation) :\n\\[\n   f_W(w) =\n   \\begin{cases}\n      4w/81 - 14/81       & \\text{$3.5 < w < 5$};\\\\\n      -4w/1377 + 122/1377 & \\text{$5 \\le w < 30.5$}.\n   \\end{cases}\n\\]\\(\\operatorname{E}(W)\\) can computed usual across two parts PDF: \\(\\operatorname{E}(W) = \\frac{1}{4} + \\frac{51}{4} = 13\\) minutes.\\(\\operatorname{E}(W^2)\\) can computed two parts also: \\(\\operatorname{E}(W^2) = \\frac{163}{144} + \\frac{29\\,699}{144} = 16598/8\\).\nHence \\(\\operatorname{var}(Y) = (1659/8) - 13^2 = 307/8\\approx 38.375\\), standard deviation \\(\\sqrt{38.375} = 6.19\\) minutes.\nFIGURE E.19: Waiting times\nAnswer Exercise 5.23.Using PMF Exercise 3.18:\n\\[\n   \\operatorname{E}(X) =\n   \\left(0\\times\\frac{4}{10}\\right) +\n   \\left(1\\times\\frac{3}{10}\\right) +\n   \\left(2\\times\\frac{2}{10}\\right) +\n   \\left(3\\times\\frac{1}{10}\\right) = 1.\n\\]\nR:Answer Exercise 5.24.\n1. Show substituting.\n2. Proceed:\n\\[\n  \\varphi(t)\n  = \\operatorname{E}[\\exp(itX)]\\\\\n  = \\int_{\\mathbb{R}} \\exp(itx) f(x)\\, dx.\n\\]\nDifferentiating wrt \\(t\\):\n\\[\n   \\varphi(t)' = \\int_{\\mathbb{R}} ix \\exp(itx) f(x)\\, dx.\n\\]\nSetting \\(t = 0\\):\n\\[\n   \\varphi(0)' = \\int_{\\mathbb{R}} xi f(x)\\, dx,\n\\]\n\\(-\\varphi(0) = \\operatorname{E}[X]\\) shown.Answer Exercise 5.25.\\((1 - )^{-1} = 1 + + ^2 + ^3 + \\dots = \\sum_{n=0}^\\infty ^n\\) \\(|| < 1\\).\\((1 - tX)^{-1} = 1 + tX + t^2X^2 + t^3X^3 + \\dots = \\sum_{n=0}^\\infty t^n X^n\\) \\(|tX| < 1\\).\nThus:\n\\[\\begin{align*}\n  \\operatorname{E}\\left[ (1 - tX)^{-1}\\right]\n  &= \\operatorname{E}[1] + \\operatorname{E}[tX] + \\operatorname{E}[t^2X^2] + \\operatorname{E}[t^3X^3] + \\dots\\\\\n  &= \\sum_{n=0}^\\infty \\operatorname{E}\\left[ t^n X^n\\right] \\quad \\text{$|tX| < 1$}.\n\\end{align*}\\]Using definition expected value:\n\\[\\begin{align*}\n  R_Y(t)\n  &= \\operatorname{E}\\left[ (1 - tY)^{-1} \\right]\\\\\n  &= \\int_0^1 \\frac{1}{1 - ty}\\, dy\n= -\\frac{\\log(1 - t)}{t}.\n\\end{align*}\\]Using series expansion \\(\\log(1 - t)\\):\n\\[\n  \\log(1 - t) = -t - \\frac{t^2}{2} - \\frac{t^3}{3} + \\dots\n\\]\n\n\\[\n  -\\frac{\\log(1 - t)}{t}\n  = 1 + \\frac{t}{2} + \\frac{t^2}{3} + \\dots.\n\\]Equating expression found Part 2:\n\\[\\begin{align*}\n  1 + \\frac{t}{2} + \\frac{t^2}{3} + \\dots.\n  &=\n  1 + \\operatorname{E}[tY] + \\operatorname{E}[t^2 Y^2] + \\operatorname{E}[t^3 Y^3] + \\dots\\\\\n  &=\n  1 + t \\operatorname{E}[Y] + t^2\\operatorname{E}[Y^2] + t^3\\operatorname{E}[Y^3] + \\dots\n\\end{align*}\\]\n\n\\[\\begin{align*}\n  t   \\operatorname{E}[Y]   &= t/2         \\Rightarrow \\operatorname{E}[Y] = 1/2;\\\\\n  t^2 \\operatorname{E}[Y^2] &= t^2/3       \\Rightarrow \\operatorname{E}[Y^2] = 1/3;\\\\\n  t^n \\operatorname{E}[Y^n] &= t^n/(n + 1) \\Rightarrow \\operatorname{E}[Y^n] = 1/(n + 1).\\\\\n\\end{align*}\\]Answer Exercise 5.26.\nFirst see area curve must one, \n\\[\n   1 = \\int_{-c}^c k(3x^2 + 4)\\, dx = k(2c^3 + 8c).\n\\]\n, note \\(\\operatorname{E}(W) = 0\\) (PDF symmetric 0), \\(\\operatorname{var}(X) = \\operatorname{E}(X^2)\\), :\n\\[\n  \\operatorname{E}(X^2) = \\int_{-c}^c k x^2 (3x^2 + 4)\\, dx\n  = k c^3\\frac{18c^2 + 40}{15} = \\frac{28}{15},\n\\]\nhence, equating top lines fractions:\n\\[\n  k c^3(9c^2 + 20) = 14.\n\\]\ntwo equations two unknowns.\nEquating obtain, algebra,\n\\[\n  9 c^4 - 8c^2 - 112 = 0.\n\\]\njust quadratic equation  \\(c^2\\); write\n\\[\n  9 X^2 - 8X - 112 = (9X + 28)(X - 4) = 0\n\\]\ntwo solutions \\(X = c^2 = -28/9\\) (real solutions  \\(c\\)), \\(X = c^2 = 4\\), \\(c = 2\\) (PDF must positive), giving \\(k = 1/32\\).Answer Exercise 5.27.\n1. \\(c =  1 - 3k/2\\) \\(c > 0\\) \\(k > 0\\).\n2. \\(c = k = 2/5\\).\n3. possible.Answer Exercise 5.28.\n\\(k = \\infty\\).Answer Exercise 5.29.\n\\(r = 5\\).Answer Exercise 5.30.\n\\(\\operatorname{E}[D] = \\sum_{d=1}^9 \\log_{10}\\left(\\frac{d + 1}{d}\\right) \\times d\\).\nexpanding, collecting like terms, simplifying (e.g., \\(\\log_{10} 1 = 0\\) \\(\\log_{10}10 = 1\\)), find\n\\[\n  \\operatorname{E}[D] = -\\log_{10}2 - \\log_{10}3 - \\cdots - \\log_{10}8 - \\log_{10}9 + 9\n  \\approx 3.440.\n\\]Answer Exercise 5.31.answer (yet).answer (yet).\nFIGURE E.20: von Mises distribution\nAnswer Exercise 5.32.answer (yet).\nFIGURE E.21: inverse Gaussian distributions\nAnswer Exercise 5.33.\\(\\int_c^\\infty c/w^3\\, dy = 1/(2c)\\) \\(c = 1/2\\).\\(\\operatorname{E}[W] = \\int_{1/2}^\\infty w/(2w^3) \\, dy = 1\\).\\(\\operatorname{E}[W^2] = \\int_{1/2}^\\infty w^2/w^3\\, dy\\) converge; variance undefined.Answer Exercise 5.34.\\(k > 0\\)Differentiating: \\(f_X(x) = \\alpha k^\\alpha x^{-\\alpha - 1}\\).\\(\\operatorname{E}[X] = \\alpha k/(\\alpha - 1)\\).\nAlso, \\(\\operatorname{E}[X^2] = \\alpha k^2/(\\alpha - 2)\\), \\(\\operatorname{var}[X] = \\alpha k^2/[(\\alpha - 2)(\\alpha - 1)^2]\\).answer (yet).See .\\(\\Pr(X > 4 \\cap X < 5) = \\Pr(4 < X < 5) = F(5) - F(4) = (3/4)^3 - (3/5)^3 = 0.205875\\).\nAlso, \\(\\Pr(X < 5) = 1 - (3/5)^3 = 0.784\\).\nprob. \\(0.205875/0.784 = 0.2625957\\).answer (yet).\nFIGURE C.1: Pareto distribution\nAnswer Exercise 5.36.\\(\\operatorname{E}(X) = \\sum_{x = 1}^K x. p_X(x) = (1/6) + \\sum_{x = 2}^K 1(x - 1)\\).\n\\(\\operatorname{E}(X^2) = \\frac{1}{K} + \\sum_{x=2}^K \\frac{x}{x - 1}\\) closed form, variance PITA.\nclosed form!See Fig. E.22.Applying definition:\n\\[\n  M_X(t) = \\operatorname{E}(\\exp(tX)) = \\frac{1}{K}\n  + \\left(\n  \\frac{\\exp(2t)}{2\\times 1} +\n  \\frac{\\exp(3t)}{3\\times 2} +\n  \\frac{\\exp(4t)}{4\\times 3} + \\dots +\n  \\frac{\\exp(Kt)}{K\\times (K - 1)}\\right).\n\\]\nFIGURE E.22: Soliton distribution\nAnswer Exercise 5.40.Since \\(M_X(t) = \\lambda/(\\lambda - t)\\), \\(M_X() = \\lambda/(\\lambda - )\\).\n\n\\[\\begin{align*}\n  f_X(x)\n  &= \\int_{-\\infty}^{\\infty} M_X() ] \\exp(-itx)\\, dt \\\\\n  &= \\int_{-\\infty}^{\\infty} \\frac{\\lambda}{\\lambda - } (\\cos(-tx) + \\sin(-tx))\\, dt\\\\\n  &= \\int_{-\\infty}^{\\infty} \\frac{\\lambda(\\lambda + )}{\\lambda^2 + t^2} (\\cos(tx) - \\sin(tx))\\, dt \\\\\n  &= \\int_{-\\infty}^\\lambda \\frac{\\lambda}{\\lambda^2 + t^2} (\\lambda\\cos(tx) + t\\sin(tx))\\, dt.\n\\end{align*}\\]\nFIGURE E.23: integrand\nAnswer Exercise 5.44.\\(\\operatorname{E}[X] = (n + 1) - n (1 - p)^n\\).\\(\\operatorname{E}[X^2] = (1 - p)^n + 25 - 25(1 - p)^n\\), \\(\\operatorname{var}[X] = 16(1 - p)^n[ 1 - (1 - p)^n]\\).\\(p \\0\\) (.e., -one disease), expected number tests one (variation).\n\\(p \\1\\) (.e., everyone disease), expected number tests five (variation).Using \\(\\operatorname{E}[X] > n\\), solve find \\(p > 1 - (1/n)^{1/n}\\).See Fig. E.24.\\(n = 4\\), expected number tests saved \\(4 - (5 - 4(1-p)^4) \\approx 1.6244\\).\n \\(50\\) times (.e., \\(50 \\times 4 = 200\\)) save \\(50\\times 1.6244 \\times 15 = 1218.3\\); $1220 cost savings.\nFIGURE E.24: expected number tests saved pooling\n\\(\\Pr(\\text{Need individual tests})\\)\n\\({}={}\\)\n\\(\\Pr(\\text{Pooled test positive})\\)\n\\({}={}\\)\n\\(1 - \\Pr(\\text{Pooled test negative})\\)\n\\({}={}\\)\n\\(1 - \\Pr(\\text{individuals negative})\\)\n\\({}={}\\)\n\\(1 -(0.9)^ 4 = 0.3439\\).\nPMF  \\(N\\), number tests needed, \n\\[\n  p_N(n) =\n  \\begin{cases}\n     0.6561 & \\text{$n = 1$ (.e., pooled test )};\\\\\n     0.3439 & \\text{$n = 5$ (.e., one pooled test, olus four individual tests)}\n  \\end{cases}\n\\]\n\\(\\operatorname{E}(X) = (0.6561 \\times 1) + (0.3439 \\times 5) = 2.3756\\) \\(\\operatorname{E}(X^2) = (0.6561 \\times 1^2) + (0.3439 \\times 5^2) =  9.2536\\) \\(\\operatorname{var}(X) = 3.610125\\).\npool four people, rather four tests expected conduct \\(2.3756\\) tests, saving \\(4 - 2.3756 = 1.6244\\) tests.\\(200\\) people total, testing individual cost \\(200\\times 15 = \\$3000\\).\n\\(n = 4\\), total \\(50\\) pools created, pool saves \\(1.6244\\) tests, total number tests expected saved \\(50 \\times 1.6244 = 81.22\\); $15 , saving \\(\\$1218.30\\).\nFIGURE E.25: advantage initial pooled testing\nAnswer Exercise 5.46.\\(\\operatorname{E}[X] = 7/2\\).\\(\\operatorname{MAD}[X] = 1.5\\).Answer Exercise 5.49.Plot shown, quadratic symmetric \\(x = 0\\).odd moments zero since distribution symmetric: \\(\\operatorname{E}[X] = 0\\).\n\\(\\operatorname{var}[X] = \\operatorname{E}[X^2] = 3/5\\).Odd moment, distribution obviously symmetric; skewness zero.\\(\\operatorname{E}[X^4] = 3/7\\), kurtosis \\(\\mu_4/\\mu^2_2 = (3/7)/(3/5)^2 = 25/21\\).\nHence, excess kurtosis \\(25/21 - 3 = -38/21\\).values extreme values like normal; values contained within \\(-1 < x < 1\\).Answer Exercise 5.50.Plot shown, straight line \\((0, 0)\\) \\((2, 1)\\).\\(\\operatorname{E}[X^r] = 2^{r + 1}/(r + 2)\\).\\(\\operatorname{E}[X] = 4/3\\); \\(\\operatorname{var}[X] = 2 - (4/3)^2 = 14/9\\).\\(\\operatorname{E}[ (X - \\mu)^3] = -8/135\\), skewness \\(\\mu_3 / \\mu_2^{3/2} = (-8/135)/(14/9)^{3/2} = -2\\sqrt{14}/245\\).\nNegative value: left skewed; bulk probability right side.\\(\\operatorname{E}[(X - \\mu)^4] = 16/135\\), kurtosis \\(\\mu_4/\\mu^2_2 = (16/135)/(14/9)^2 = 12/245\\).\nHence, excess kurtosis \\(12/245 - 3 = -723/245\\)\nvalues extreme values like normal; values contained within \\(0 < x < 2\\).","code":"\nset.seed(777023)\n\ndist <- array(NA, dim = 1000)\nfor (i in (1:1000)){\n  positions <- sample(1:5,\n                      size = 2,\n                      replace = FALSE)\n  dist[i] = abs(diff(positions)) - 1\n}\nmean(dist)\n#> [1] 0.926"},{"path":"selected-solutions.html","id":"AnswersChapterTransformations","chapter":"E Selected solutions","heading":"E.6 Answers for Chap. 6","text":"Answer Exercise 6.1.\\(0 < x < 2\\), transformation one--one.\ninverse transform \\(X = Y^{1/3}\\), \\(0 < y < 8\\).\\(F_Y(y) = \\Pr(Y\\le y)\n= \\Pr(X^3 \\le y) = \\Pr(X\\le y^{1/3}) = F_X( y^{1/3})\n= \\int_{u = 0}^{y^{1/3}} u/2\\,du = u^2/4\\Big|_{u = 0}^{u = y^{1/3}} = y^{2/3}/4.\\)\nDifferentiate find PDF:\n\\(\\frac{d}{dy} y^{2/3}/4 = y^{-1/3}/6\\).\nPDF  \\(Y\\) \n\\[\nf_Y(y) = \\begin{cases}\n            y^{-1/3}/6 & \\text{$0 < y < 8$};\\\\\n            0 & \\text{otherwise}.\n         \\end{cases}\n\\]\\(F_Y(y) = \\Pr(Y\\le y)\n= \\Pr(X^3 \\le y) = \\Pr(X\\le y^{1/3}) = F_X( y^{1/3})\n= \\int_{u = 0}^{y^{1/3}} u/2\\,du = u^2/4\\Big|_{u = 0}^{u = y^{1/3}} = y^{2/3}/4.\\)\nDifferentiate find PDF:\n\\(\\frac{d}{dy} y^{2/3}/4 = y^{-1/3}/6\\).\nPDF  \\(Y\\) \n\\[\nf_Y(y) = \\begin{cases}\n            y^{-1/3}/6 & \\text{$0 < y < 8$};\\\\\n            0 & \\text{otherwise}.\n         \\end{cases}\n\\]Since \\(w(y) = y^{1/3}\\), \\(w'(y) = y^{-2/3}\\).\n:\n\\[\\begin{align*}\nf_Y(y)\n&= f_X(y) |J|\\\\\n&= y^{1/3}/2 \\times \\overbrace{y^{-2/3}/3}^{|J|} \\\\\n&= y^{-1/3}/6.\n\\end{align*}\\]\nPDF  \\(Y\\) .Since \\(w(y) = y^{1/3}\\), \\(w'(y) = y^{-2/3}\\).\n:\n\\[\\begin{align*}\nf_Y(y)\n&= f_X(y) |J|\\\\\n&= y^{1/3}/2 \\times \\overbrace{y^{-2/3}/3}^{|J|} \\\\\n&= y^{-1/3}/6.\n\\end{align*}\\]\nPDF  \\(Y\\) .Answer Exercise 6.2.First:\\(X_1 = 0\\), \\(X_2= 0\\) (prob: \\(0\\)): \\(Y_1 = 0\\); \\(Y_2 = 0\\);\\(X_1 = 0\\), \\(X_2= 1\\) (prob. \\(1/6\\)): \\(Y_1 = 1\\); \\(Y_2 = 1\\);\\(X_1 = 1\\), \\(X_2= 0\\) (prob. \\(2/6\\)): \\(Y_1 = 1\\); \\(Y_2 = 0\\);\\(X_1 = 1\\), \\(X_2= 1\\) (prob. \\(3/6\\)): \\(Y_1 = 2\\); \\(Y_2 = 1\\).Effectively, first line can ignored (since probability zero), \\(Y_1 \\\\{1, 2\\}\\) \\(Y_2\\\\{0, 1\\}\\)., joint pf shown Table E.3.Hence, table:\n\\[\nf_{Y_1}(y_1) =\n\\begin{cases}\n   1/2 & \\text{$y_1 = 1$};\\\\\n   1/2 & \\text{$y_1 = 2$};\\\\\n   0    & \\text{otherwise}.\n\\end{cases}\n\\]\nTABLE E.3: joint distribution \\(Y_1\\) \\(Y_2\\)\nAnswer Exercise 6.3.\\(Y \\sim\\text{Gam}(\\sum\\alpha, \\beta)\\).Answer Exercise 6.4.Transformation 1-1;  \\(Y > 0\\).\n:\n\\[\\begin{align*}\n   F_Y(y)\n   &= \\Pr(Y < y)\\\\\n   &= \\Pr( X^1 < y)\\\\\n   &= \\Pr( -\\sqrt{y} < X < \\sqrt{y} ) \\quad\\text{(draw diagram!)}\\\\\n   &= \\int_{-\\sqrt{y}}^{\\sqrt{y}} \\frac{1}{\\pi(1 + x^2)}\\, dx\\\\\n   &= \\frac{2}{\\pi} \\tan^{-1}(\\sqrt{y}).\n\\end{align*}\\]\n\n\\[\\begin{align*}\n   f_Y(y)\n   &= \\frac{d}{dy} \\frac{2}{\\pi} \\tan^{-1}(\\sqrt{y})\\\\\n   &= \\frac{1}{\\pi(y + 1)\\sqrt{y}}\\quad\\text{$y > 0$}.\n\\end{align*}\\]Answer Exercise 6.5.Differentiating:\n\\[\n  f_X(x) =\n  \\begin{cases}\n    1 & \\text{$-1/2 < x < /1/2$};\\\\\n    0 & \\text{elsewhere}.\n  \\end{cases}\n\\]First: transformation 1:1 (Fig. E.26).\nSee \\(X = \\pm\\sqrt{4 - Y}\\), \\(3.75 < Y < 4\\).\n,\n\\[\\begin{align*}\n  F_Y(y)\n  &= \\Pr(Y < y) \\\\\n  &= 1 - \\Pr( -\\sqrt{4 - y} < X < \\sqrt{4 - y} )\\\\\n  &= 1 - \\int_{-\\sqrt{4 - y}}^{\\sqrt{4 - y}} 1\\, dx\\\\\n  &= 1 - 2\\sqrt{4 - y},\n\\end{align*}\\]\n, differentiating:\n\\[\n  f_Y(y) = \\frac{1}{\\sqrt{4 - y}} \\quad\\text{$3.75 < Y < 4$}\n\\]\nFIGURE E.26: transformation\nAnswer Exercise 6.6.Given \\(f(\\theta) = 4/\\pi\\) \\(0 < \\theta< \\pi/4\\), hence \\(0 < D < (v^2/g)\\).\n:\n\\[\\begin{align*}\n  F_D(d) = \\Pr(D < d)\n  &= \\Pr( v^2/g\\sin2\\theta < d) \\\\\n  &= \\Pr(\\theta < \\frac{1}{2} \\sin^{-1}(d g/v^2 ) \\\\\n  &= \\int_0^{\\sin^{-1}(d g/v^2 )/2} \\frac{4}{\\pi}\\, d\\theta \\\\\n  &= \\frac{2}{\\pi} \\sin^{-1} \\left( \\frac{dg}{v^2} \\right).\n\\end{align*}\\]\n\n\\[\n  f_D(d) = \\frac{2g}{\\pi}\\frac{1}{\\sqrt{v^4 - d^2 g^2}}\\quad\\text{$0 < D < v^2 /g$}.\n\\]Alternatively: see \\(D = v^2\\sin 2\\theta/g \\Rightarrow sin 2\\theta = Dg/v^2\\), \n\\[\n  \\frac{dd}{d\\theta} = \\frac{2v^2}{g}\\cos 2\\theta.\n\\]\n,\n\\[\\begin{align*}\n  f_D(d)\n  &= \\frac{4}{\\pi} \\left| \\frac{g}{2 v^2\\cos 2\\theta} \\right|\\\\\n  &= \\frac{2g}{\\pi v^2 \\cos2\\theta}\\\\\n  &= \\frac{2g}{\\pi \\sqrt{v^4 - d^2 g^2}}\n\\end{align*}\\]\n(e.g.) drawing right-angled triangle re-write \\(\\sin 2\\theta = Dg/v^2\\).\nSee Fig. E.27.\nFIGURE E.27: distance travelled projectile\nAnswer Exercise 6.7.\nProceed:\n\\[\\begin{align*}\n  F_Y(y)\n  &= \\Pr(Y \\le y)\\\\\n  &= \\Pr(X \\ge \\exp(-y/\\alpha)) \\qquad\\text{note change direction!}\\\\\n  &= \\int_{\\exp(-y/\\alpha)}^1 1\\, dx\n\\end{align*}\\]\n\\(f_Y(y) = \\frac{1}{\\alpha} \\exp(-y/\\alpha)\\) \\(y > 0\\), exponential distribution.Answer Exercise 6.8.\\(\\operatorname{E}[W] = -1/3\\); \\(\\operatorname{var}[W] = 17/9\\approx 1.8889\\dots\\).\\(\\Pr(V = 0) = 1/2\\); \\(\\Pr(V = 4) = 1/2\\).Find:\n\\[\n  F_W(w) =\n  \\begin{cases}\n0 & \\text{$w < -2$};\\\\\n1/3 & \\text{$-2 \\le w < 0$};\\\\\n5/6 & \\text{$0 \\le w < 2$};\\\\\n1 & \\text{$w \\ge 2$}.\n  \\end{cases}\n\\]Answer Exercise 6.10.First see \\(Y = \\log X\\).\n:\n\\[\\begin{align*}\n  F_Y(y)\n  &= \\Pr(Y < y) \\\\\n  &= \\Pr( \\exp X < y)\\\\\n  &= \\Pr( X < \\log y)\\\\\n  &= \\Phi\\big((\\log y - \\mu)/\\sigma\\big)\n\\end{align*}\\]\ndefinition \\(\\Phi(\\cdot)\\).Proceed:\n\\[\\begin{align*}\n  f_Y(y)\n  &= \\frac{d}{dy}  \\Phi\\big((\\log y - \\mu)/\\sigma\\big) \\\\\n  &= \\frac{1}{y} \\Phi\\big((\\log y - \\mu)/\\sigma\\big) \\\\\n  &= \\frac{1}{y\\sqrt{2\\pi\\sigma^2}}\n     \\exp\\left[\\left(\n        -\\frac{ (\\log y - \\mu)^2}{\\sigma}\\right)^2\\right]\n\\end{align*}\\]\nsince derivative \\(\\Phi(\\cdot)\\) (df standard normal distribution) \\(\\phi(\\cdot)\\) (PDF standard normal distribution).See Fig. E.28.See :  \\(0.883\\).\nFIGURE E.28: Log-normal distributions\nAnswer Exercise 6.11.\nSee \\(Y\\\\{0, 1, \\sqrt{2}, \\sqrt{3}, 2\\}\\) \n\\[\n   \\Pr(Y = y) = \\binom{4}{y^2} (0.2)^{y^2} (0.8)^{4 - y^2} \\quad \\text{$y = 0, 1, \\sqrt{2}, \\sqrt{3}, 2$}.\n\\]Answer Exercise 6.12.First, see relationships:\n\\[\\begin{align*}\n  X = 1 &\\Y = (X - 3)^2 = 4;\\\\\n  X = 2 &\\Y = (X - 3)^2 = 1;\\\\\n  X = 3 &\\Y = (X - 3)^2 = 0;\\\\\n  X = 4 &\\Y = (X - 3)^2 = 1.\n\\end{align*}\\]\nadding probabilities appropriate:\n\\[\n  p_Y(y) =\n  \\begin{cases}\n  9/30  & \\text{$y = 0$}\\\\\n  20/30 & \\text{$y = 1$}\\\\\n  1/30  & \\text{$y = 4$}\\\\\n  0     & \\text{elsewhere}.\n\\end{cases}\n\\]Answer Exercise 6.16.given beta distribution, \\(\\operatorname{E}(V) = 0.287/(0.287 + 0.926) = 0.2366...\\) \\(\\operatorname{var}(V) = 0.08161874\\).\\(\\operatorname{E}(S) = \\operatorname{E}(4.5 + 11V) = 4.5 + 11\\operatorname{E}(V) = 7.10\\) minutes.\n\\(\\operatorname{var}(S) = 11^2\\times\\operatorname{var}(V) = 9.875\\) minutes2.\\(V\\(4.5, 15.5)\\).\nSee Fig. E.29.corresponds \\(V = 10.5/11 = 0.9545455\\), \\(\\Pr(S > 15) = \\Pr(V > 0.9545455) = 0.01745087\\).\\(V\\), largest 20% correspond \\(V = 0.004080076\\), \\(S = 4.544881\\); quickest \\(20%\\) within \\(4.54\\) minutes.\nFIGURE E.29: Service times\nAnswer Exercise 6.18.Care needed interval: 1:1.\nFIGURE E.30: Question 1\nNote  \\(Y\\) defined \\(0 < Y < 4\\), transformation 1:1 transformation (Fig. 1, left panel) values.\nuse distribution function method, see \\(0 < y < 1\\) corresponds \\(-\\sqrt{Y} < z < \\sqrt{Y}\\), \\(1 \\le y \\le 4\\) corresponds \\(-1 < z <\\sqrt{Y}\\).\n\\(y < 1\\):\n\\[\n  F_Y(y) = \\Pr(Y \\le y) = \\Pr(-\\sqrt{y} < z < \\sqrt{y}\\,) = \\int_{-\\sqrt{y}}^{\\sqrt{y}} \\frac{1}{3}\\, dz = \\frac{2\\sqrt{y}}{3}\\quad \\text{$0 < y < 1$}.\n\\]\n\\(1 < y < 4\\), start writing \\(F_Y(y) = \\Pr(Y \\le y)\\), take care include \\(\\Pr(Y < 1)\\)!\n\\[\n  F_Y(y)\n  =  \\Pr(Y \\le 1) + \\Pr(1 < Y < y)\n  = \\frac{2}{3} + \\Pr(1 < z < \\sqrt{y}\\,)\n  =  \\frac{2}{3} + \\int_{1}^{\\sqrt{y}} \\frac{1}{3}\\, dz = \\frac{1 + \\sqrt{y}}{3}.\n\\]\nHence (noting carefully \\(y = 1\\) PDF):\n\\[\n  F_Y(y) =\n  \\begin{cases}\n0 & \\text{$y \\le 0$};\\\\\n\\frac{2\\sqrt{y}}{3} & \\text{$0 \\le y < 1$};\\\\\n\\frac{1 + \\sqrt{y}}{3} & \\text{$1 \\le y < 4$};\\\\\n1 & \\text{$y \\ge 4$}\n  \\end{cases}\n  \\quad\\text{differentiating:}\\quad\n  f_Y(y) =\n  \\begin{cases}\n\\frac{1}{3\\sqrt{y}} & \\text{$0 < y < 1$};\\\\\n\\frac{1}{6\\sqrt{y}} & \\text{$1 \\le y < 4$};\\\\\n0 & \\text{elsewhere}.\n  \\end{cases}\n\\]values  \\(Y\\), PDF non-negative.\naddition, \\(\\int_Y f_Y(y)\\, dy = 1\\).See Fig. E.30 (right panel).Answer Exercise 6.20.\n\n\\[\n  f_X(x) =\n  \\begin{cases}\n    2x/3      & \\text{$0 \\le x \\le 1$}\\\\\n    (3 - x)/3 & \\text{$1 < x \\le 3$}\n  \\end{cases}\n\\]\nhence\n\\[\n  F_X(x) =\n  \\begin{cases}\n    0                 & \\text{$x < 0$}\\\\\n    x^2/3             & \\text{$0 \\le x \\le 1$}\\\\\n    (6x - x^2 - 3)/6  & \\text{$1 < x \\le 3$}\\\\\n    1                 & \\text{$x > 3$.}\n  \\end{cases}\n\\]\nSince \\(Y = 6 - 2x\\), \\(\\mathcal{R}_X = (0, 3)\\) maps \\(\\mathcal{R}_Y = (0, 6)\\); specifically, \\(0 \\le x \\le 1\\) maps \\(4 \\le y \\le 6\\) \\(1 < x \\le 3\\) maps \\(0 < y \\le 4\\).\n:\n\\[\\begin{align*}\n  F_Y(y)\n  &= \\Pr(Y \\le y)\\\\\n  &= \\Pr\\left( x \\ge \\frac{6 - y}{2}\\right)\\\\\n  &= 1 - \\Pr\\left( x \\le \\frac{6 - y}{2}\\right)\\\\\n  &= 1 - F_X\\left( x \\le \\frac{6 - y}{2}\\right).\n\\end{align*}\\]\n, \\(0 \\le x \\le 1\\) (.e., \\(4\\le y \\le 6\\)):\n\\[\n  F_Y(y) = \\frac{12y - y^2 - 24}{12}\n\\]\n\\(1 < x \\le 3\\) (.e., \\(0 < y \\le 4\\)):\n\\[\n  F_Y(y) = \\frac{y^2}{24}\n\\]\n(CHECK LESS , LESS EQUAL signs!!!!!!!!!!!)\n\\[\n  f_Y(y) =\n  \\begin{cases}\n    y/12 & \\text{$0 < y \\le 4$}\\\\\n    (6 - y)/6 & \\text{$4 \\le y \\le 6$.}\n  \\end{cases}\n\\]Answer Exercise 6.21.\n1:1 function care needed.\n$f_X(x) \\(F_X(x)\\) given .\nSince \\(Z = (X - 2)^2\\), \\(\\mathcal{R}_X = (0, 3)\\) maps \\(\\mathcal{R}_Z = (0, 4)\\); specifically, \\(0 \\le x \\le 1\\) maps \\(1 \\le z \\le 4\\) \\(1 < x \\le 3\\) maps \\(0 < z \\le 1\\).\nFirst consider case \\(0\\le z\\le 1\\):\n\\[\\begin{align*}\n  F_Z(z)\n  &= \\Pr(Z \\le y)\\\\\n  &= \\Pr(2 - \\sqrt{Z} \\le X \\le 2 + \\sqrt{Z})\\\\\n  &= F_X(2 + \\sqrt{z}) - F_X(2 - \\sqrt{z})\\\\\n  &= 2\\sqrt{z} / 3.\n\\end{align*}\\]\n, \\(1\\le z \\le 4\\):\n\\[\\begin{align*}\n  F_Z(z)\n  &= \\Pr(Z \\le z)\\\\\n  &= F_Z(1) + \\Pr(2 - \\sqrt{Z} \\le X\\le 1)\\\\\n  &= \\frac{2}{3} + \\frac{4\\sqrt{z} - z - 3}{3}\\\\\n  &= \\frac{4\\sqrt{z} - z - 1}{3}.\n\\end{align*}\\]\nwrite\n\\[\n  F_Z(z) =\n  \\begin{cases}\n    0                     & \\text{$z < 0$}\\\\\n    2\\sqrt{z}/3           & \\text{$0 < z < 1$}\\\\\n    (4\\sqrt{z} - z - 1)/3 & \\text{$1 < z < 4$}\\\\\n    1                     & \\text{$z > 4$.}\n  \\end{cases}\n\\]\nNote \\(F_Z(4) = 1\\) \\(F_Z(0) = 0\\) required.\nFurthermore, two parts give \\(F_Z(1) = 2/3\\).\n, (CHECK LESS , LESS EQUAL signs!!!!!!!!!!!)\n\\[\n  f_Z(z) =\n  \\begin{cases}\n    1/(3\\sqrt{z}\\,)               & \\text{$0 < z \\le 1$}\\\\\n    \\left(2/\\sqrt{z} - 1\\right)/3 & \\text{$1 \\le z \\le 4$}\\\\\n    0                             & \\text{elsewhere.}\n  \\end{cases}\n\\]\nFIGURE E.31: pdf transformed rvs \\(Y\\)  \\(Z\\).\nAnswer Exercise 6.22.\n\\(f_V(v)\n= f_T(D/v)\\left| \\frac{dT}{dV}\\right|\n= f_T(D/v)       \\frac{D}{v^2}\\);\n\n\\[\n  f_V(v) =\n  \\begin{cases}\n    \\displaystyle\\frac{D(va + v\\mu - D)}{^2\\,v^3} & \\text{$D/(\\mu + \\Delta) < v < D/\\mu$}\\\\[6pt]\n    \\displaystyle\\frac{D(va - v\\mu + D)}{^2\\,v^3} & \\text{$D/\\mu < v < D/(\\mu - )$}\n  \\end{cases}\n\\]\nshown Fig. E.32 (left panel).\nFIGURE E.32: probability density function random variable \\(V\\), run velocity.\nAnswer Exercise 6.23.\n\\(f_P(p)\n= f_V(\\sqrt{pR}) \\frac{1}{2} \\sqrt{\\frac{R}{p}}\\)\n\n\\[\n  f_P(p) =\n  \\frac{\\sqrt{R}}{\\sqrt{2p\\pi\\sigma^2}} \\exp\\left\\{-\\frac{pR}{2\\sigma^2}\\right\\}\n\\]\nshown Fig. E.32 (right panel).","code":"\npar( mfrow = c(2, 2))\n\nx <- seq(0, 8,\n         length = 500)\n\nplot( dlnorm(x, meanlog = log(1), sdlog = 1) ~ x,\n      xlab = expression(italic(y)),\n      ylab = \"PDF\",\n      type = \"l\",\n      main = expression(Log~normal*\":\"~mu==1~and~sigma==1),\n      lwd = 2,\n      las = 1)\nplot( dlnorm(x, meanlog = log(3), sdlog = 1) ~ x,\n      xlab = expression(italic(y)),\n      ylab = \"PDF\",\n      type = \"l\",\n      main = expression(Log~normal*\":\"~mu==3~and~sigma==1),\n      lwd = 2,\n      las = 1)\nplot( dlnorm(x, meanlog = log(1), sdlog = 2) ~ x,\n      xlab = expression(italic(y)),\n      ylab = \"PDF\",\n      main = expression(Log~normal*\":\"~mu==1~and~sigma==2),\n      type = \"l\",\n      lwd = 2,\n      las = 1)\nplot( dlnorm(x, meanlog = log(3), sdlog = 2) ~ x,\n      xlab = expression(italic(y)),\n      ylab = \"PDF\",\n      main = expression(Log~normal*\":\"~mu==3~and~sigma==2),\n      type = \"l\",\n      lwd = 2,\n      las = 1)\n(1 - plnorm(2, meanlog=2, sdlog=2)) / \n  (1 - plnorm(1, meanlog=2, sdlog=2))\n#> [1] 0.8834182#> [1] 0.01745087\n#> [1] 4.50408"},{"path":"selected-solutions.html","id":"AnswersChapDiscreteDistributions","chapter":"E Selected solutions","heading":"E.7 Answers for Chap. 7","text":"Answer Exercise 7.1.\ngiven: \\(p_X(x; n, 1 - p) = \\binom{n}{x} (1 - p)^x p^{n - x}\\).\n, define \\(Y = n - X\\) hence\n\\(f_Y(y) = \\binom{n}{y} p^y (1 - p)^{n - y}\\), \\(f_Y(y) = \\binom{n}{n - x} p^{n - x} (1 - p)^{n}\\).\neasy show \\(\\binom{n}{x} = \\binom{n}{n - x}\\) hence \\(f_X(x)\\)  \\(f_Y(y)\\) equivalent.Answer Exercise 7.2.Care: geometric parameterised  \\(x\\) number failures success (number trails).\nSimilarly negative binomial.Assumes independence people, constant probability.Answer Exercise 7.3.Care: geometric parameterised \\(x\\) number failures success (number trails).\nSimilarly negative binomial.Answer Exercise 7.4.Answer Exercise 7.5.See Fig. E.33.\nFIGURE E.33: number rainfall events summer winter\nAnswer Exercise 7.6.Yep.\\(\\log n_x = \\log N + \\log p + x\\log (1 - p)\\), linear regression model  \\(\\log n_x\\) regressed  \\(x\\), intercept \\(\\beta_0 = \\log N + \\log p\\) slope \\(\\beta_1 = \\log (1 - p)\\).fitted slope, can estimate \\(p\\); , using estimated intercept, can estimate \\(N\\).\nspecifically, estimate  \\(p\\) \\(1 - \\exp( \\hat{\\beta}_1)\\), estimate  \\(N\\) \\(\\exp(\\beta_0 - \\log p)\\).\nfind \\(\\hat{y} = 6.40525 - 1.128753x\\).\n, population size estimate  \\(894\\):\n\\(p = 0.6765636\\).\n\\(N = 894.2445\\).Answer Exercise 7.7\nDefining \\(X\\) ‘number failures \\(4\\)kWh/m2 observed’, since parameterisation used textbook number failures first success., \\(X\\sim \\text{Geom}(p)\\).\\(\\operatorname{E}(X) = (1 - p)/p = 1\\) failures till first success, followed day success: 2.\\(\\operatorname{E}(X) = (1 - p)/p = 3\\) failures, \\(3 + 1 = 4\\).\\(\\operatorname{var}(X) = (1 - p)/p^2 = 12\\).Answer Exercise 7.13.See Fig. E.34; hugely different.See Fig. E.34; hugely different.Similar probabilities: \\(0.85\\) (1999)  \\(0.91\\) (2000).Similar probabilities: \\(0.85\\) (1999)  \\(0.91\\) (2000).Similar: \\(16\\) days (1999) \\(12\\) (2000).Similar: \\(16\\) days (1999) \\(12\\) (2000).Writing \\(X\\) clutch size (\\(n = 237\\)):\n\\(\\operatorname{E}(X) = (1\\times \\frac{9}{237}) + (2\\times \\frac{29}{237}) + (3\\times \\frac{199}{237}) =   2.801688\\), 2.8.\n\\(\\operatorname{E}(X^2) = (1^2\\times \\frac{9}{237}) + (2^2\\times \\frac{29}{237}) + (3^2\\times \\frac{199}{237}) = 8.084388\\).\n, \\(\\operatorname{var}(X) = 8.084388 - (2.801688)^2 = 0.2349324\\), standard deviation 0.4846982, 0.485.\nWriting \\(X\\) clutch size (\\(n = 237\\)):\\(\\operatorname{E}(X) = (1\\times \\frac{9}{237}) + (2\\times \\frac{29}{237}) + (3\\times \\frac{199}{237}) =   2.801688\\), 2.8.\\(\\operatorname{E}(X^2) = (1^2\\times \\frac{9}{237}) + (2^2\\times \\frac{29}{237}) + (3^2\\times \\frac{199}{237}) = 8.084388\\)., \\(\\operatorname{var}(X) = 8.084388 - (2.801688)^2 = 0.2349324\\), standard deviation 0.4846982, 0.485.\nFIGURE E.34: lay date model glaucous-winged gulls, 1999 2000\nAnswer Exercise 7.14.Eq. (7.6), rv \\(X\\) refers number failures \\(r\\)th success observed, \\(X = 0, 1, 2, \\dots\\).\ndefine \\(Y\\) number trials till \\(r\\)th success, hence \\(Y = X + r\\).range space \\(Y\\\\{r, r + 1, r + 2, \\dots\\}\\)\\(p_Y(y; p, r) = \\binom{y - 1}{r - 1}(1 - p)^{y - r} p^{r - 1}\\), \\(y = r, r + 1, r + 2, \\dots\\).\\(\\operatorname{E}(Y) = \\operatorname{E}(X + r) = \\operatorname{E}(X) + r = r/p\\).\\(\\operatorname{var}(Y) = \\operatorname{var}(X + r) = \\operatorname{var}(X) = r(1 - p)/p^2\\).Answer Exercise 7.15.Let \\(X\\) number typos per minute; \\(X\\sim\\text{Pois}(\\lambda = 2.5\\times 5 = 12.5)\\) five-minute test.Part 3: number errors occurring ‘overlap minute’  \\(0, 1, 2\\), 6$.\nproceed:\\(6\\) errors overlap minute:\n\\(\\Pr(\\text{0 errors first 2 mins})\\times{}\\)\n\\(\\Pr(\\text{6 errors overlap min})\\times{}\\)\n\\(\\Pr(\\text{0 errors final 2 mins})\\)\\(5\\) errors overlap minute:\n\\(\\Pr(\\text{1 error first 2 mins})\\times{}\\)\n\\(\\Pr(\\text{5 errors overlap min})\\times{}\\)\n\\(\\Pr(\\text{1 error final 2 mins})\\).Answer Exercise 7.16.\\(0.25\\).dbinom(x = 2, size = 4, prob = 0.25) \\({}=  0.2109375\\).Answer Exercise 7.17.code one simulation part .\nFIGURE E.35: simulation\n\nFIGURE E.36: simulation\n\nFIGURE E.37: simulation\nAnswer Exercise 7.18.\nSuppose \\(X\\sim\\text{Pois}(\\lambda)\\); \\(\\Pr(X) = \\Pr(X + 1)\\) implies\n\\[\\begin{align*}\n   \\frac{\\exp(-\\lambda)\\lambda^x}{x!}\n   &= \\frac{\\exp(-\\lambda)\\lambda^{x + 1}}{(x + 1)!} \\\\\n   \\frac{\\lambda^x}{x!}\n   &= \\frac{\\lambda^{x} \\lambda}{(x + 1) \\times x!}\n\\end{align*}\\]\n\\(\\lambda = x + 1\\) (.e., \\(\\lambda\\) must number).\nexample, \\(x = 4\\) \\(\\lambda = 5\\).\ncan check:Answer Exercise 7.19.\\(\\operatorname{E}(X) = kp\\) binomial (, \\(k\\) sample size).\\(\\operatorname{var}(X) = k p (1 - p) \\times \\left(\\frac{N - k}{N - 1}\\right)\\); first bit variance binomial distribution (recall \\(k\\)$ sample size).\nterm \\((N - k)/(N - 1)\\), FPC factor defined.Answer Exercise 7.20.Since \\(\\operatorname{var}(Y) = \\operatorname{E}(Y^2) - \\operatorname{E}(Y)^2\\), find \\(\\operatorname{E}(Y^2)\\) using (B.2):\\[\\begin{align*}\n  \\operatorname{E}(Y^2)\n  &= \\sum_{= 0}^{b - } ^2\\frac{1}{b - + 1}\\\\\n  &= \\frac{1}{b - + 1}(0^2 + 1^2 + 2^2 + \\dots  +(b - )^2)\\\\\n  &= \\frac{1}{b - + 1}\\frac{(b - )(b - + 1)(2(b - ) + 1)}{6}\\\\\n  &= \\frac{(b - )(2(b - ) + 1)}{6}.\n\\end{align*}\\]\nTherefore\n\\[\\begin{align*}\n   \\operatorname{var}(X)\n    = \\operatorname{var}(Y)\n   &= \\frac{(b - )(2(b - ) + 1)}{6} - \\left(\\frac{b - }2\\right)^2\\\\\n   &= \\frac{(b - )(b - + 2)}{12}.\n\\end{align*}\\]","code":"\nsum( dbinom(10:25, # Part 1\n            size = 25,\n            prob = 0.30) )\n#> [1] 0.189436\nsum( dbinom(0:9,  # Part 2\n            size = 25,\n            prob = 0.30) )\n#> [1] 0.810564\nsum( dbinom(5:10,  # Part 3\n            size = 25,\n            prob = 0.30) )\n#> [1] 0.8117281\ndgeom(x  = 5,  # Part 4: 5 fails before 1st success\n      prob = 0.30)\n#> [1] 0.050421\n\nsum( dgeom(x  = 7:50,  # Part 5: Num. fails! \n           prob = 0.30) )\n#> [1] 0.08235429\n\n# Part 6; This means 5 fails, before 3rd success\ndnbinom(x = 5,\n        prob = 0.30,\n        size = 3)\n#> [1] 0.09529569\nsum( dbinom( 16:81,  # Part 1\n             size = 81,\n             prob = 0.20) ) # 0.5663638\n#> [1] 0.5663638\nsum( dbinom( 12:81,  # Part 2\n             size = 81,\n             prob = 0.20) ) # 0.9082294\n#> [1] 0.9082294\n  # Part 3\ndgeom(x = 2, #i.e., two failures before first success\n      prob = 0.20) # 0.128\n#> [1] 0.128\n\n  # Part 4\ndnbinom(x = 5, # is 5 fails before 5th success\n        prob = 0.20,\n        size = 5) # 0.01321206\n#> [1] 0.01321206\nsum( dbinom(50:81,\n            size = 81,\n            prob = 0.2) )\n#> [1] 3.042983e-16\ndpois( 0,  # Part 1\n       lambda = 3)\n#> [1] 0.04978707\nsum( dpois( 6:50,  # Part 2\n            lambda = 3) )\n#> [1] 0.08391794\ndpois( 2,   # Part 3\n       lambda = 6)\n#> [1] 0.04461754#> [1] 0.1252978\n#> [1] 0.2424239\n#> [1] 0.3401473\n#> [1] 0.5232259\nx <- 1:6\nnx <- c(247, 63, 20, 4, 2, 1)\n\nm1 <- lm( log(nx) ~ x); coef(m1)\n#> (Intercept)           x \n#>    6.405250   -1.128753\nbeta0 <- coef(m1)[1]\nbeta1 <- coef(m1)[2]\n\np <- 1 - exp(beta1); p\n#>         x \n#> 0.6765636\nN <- exp(beta0 - log(p) ); N\n#> (Intercept) \n#>    894.2445\n# Part 4\nsum( dgeom(0:2,  # 0 to 2 failures before the success\n           prob = 0.25))\n#> [1] 0.578125\n  ## Part 2\n1 -  pnbinom(30, mu = 23.0, size = 20.6) # 0.1416996\n#> [1] 0.1416996\n1 - pnbinom(30, mu = 19.5, size = 8.9) # 0.09175251\n#> [1] 0.09175251\n  ## Part 3\nqnbinom(0.15, mu = 23.0, size = 20.6) # 16\n#> [1] 16\nqnbinom(0.15, mu = 19.5, size = 8.9)  # 12\n#> [1] 12\nx <- 0:50\ny1999 <- dnbinom(x, mu = 23.0, size = 20.6)\ny2000 <- dnbinom(x, mu = 19.5, size = 8.9)\n\nplot( y1999 ~ x,\n      pch = 19,\n      las = 1,\n      main = \"Lay date for glaucous-winged gulls\",\n      xlab = \"Lay date\",\n      ylab = \"Prob. function\")\npoints( y2000 ~ x,\n        pch = 1)\nlegend(\"topleft\",\n       pch = c(19, 1),\n       legend = c(\"1999\",\n                  \"2000\"))\ndpois(10, lambda = 12.5) # 0.09564364\n#> [1] 0.09564364\ndpois(6, lambda = 2.5 * 3) * dpois(4, lambda = 2.5 * 2) #  0.02398959\n#> [1] 0.02398959\nprob <- function(ErrorsInOverlap){\n  dpois( (6 - ErrorsInOverlap), lambda = 2 * 2.5) *\n  dpois(ErrorsInOverlap, lambda = 1 * 2.5) *\n  dpois((6 - ErrorsInOverlap), lambda = 2 * 2.5)\n}\nsum( prob( 0:6 ) ) # 0.02120811\n#> [1] 0.02120811\n### Part 1\nset.seed(2268) # For reproducibility\n\nqueueLength <- array(dim = 60)\n\nqueueLength[1] <- rpois(1, lambda = 0.5)\n\nfor (i in 2:60){\n   queueLength[i] <- queueLength[i - 1] + rpois(1, lambda = 0.5)\n   # Print every 10 minutes\n   if ( floor(i/10) == i/10 ) {\n     cat(\"After \", i, \" minutes past 8AM, queue length: \", queueLength[i], \"\\n\")\n   }\n}\n#> After  10  minutes past 8AM, queue length:  6 \n#> After  20  minutes past 8AM, queue length:  14 \n#> After  30  minutes past 8AM, queue length:  19 \n#> After  40  minutes past 8AM, queue length:  24 \n#> After  50  minutes past 8AM, queue length:  28 \n#> After  60  minutes past 8AM, queue length:  34\nplot( queueLength, type = \"l\", las = 1)\n\n\n\n### Part 2\nqueuelength <- array(dim = 60)\n\nqueueLength[1] <- rpois(1, lambda = 0.5)\n\nfor (i in 2:60){\n   NumberIn <- rpois(1, lambda = 0.5)\n\n   # Number being served\n   if ( i < 30 ) {\n     NumberServed <- 0\n   } else {\n     if (i >= 30) {\n        lambda <- 0.75\n     }\n     NumberServed <- rpois(1, lambda = lambda)\n     queueLength[i] <- queueLength[i - 1] + NumberIn - NumberServed\n   }\n\n   if ( queueLength[i] < 0 ) queueLength[i] <- 0\n}\n\nplot( queueLength, type = \"l\", las = 1)\nabline(v = 30,\n       lwd = 2,\n       col = \"grey\")\ntext(30, 5,\n     pos = 3, # To the right,\n     labels = \"Server starts\")\n\n\n\n\n### Part 3.\nqueuelength <- array(dim = 60)\n\nqueueLength[1] <- rpois(1, lambda = 0.5)\n\nfor (i in 2:60){\n   NumberIn <- rpois(1, lambda = 0.5)\n\n   # Number being served\n   if ( i < 30 ) {\n     NumberServed <- 0\n   } else {\n     if ( (i > 30) & (i < 45) ) {\n        lambda <- 0.7\n     }\n     if ( i > 45 ) {\n        lambda <- 1.3\n     }\n     NumberServed <- rpois(1, lambda = lambda)\n   }\n\n   queueLength[i] <- queueLength[i - 1] + NumberIn - NumberServed\n\n   if ( queueLength[i] < 0 ) queueLength[i] <- 0\n}\n\nplot( queueLength, type = \"l\", las = 1)\nabline(v = 30,\n       lwd = 2,\n       col = \"grey\")\ntext(30, 5,\n     pos = 3, # To the right,\n     labels = \"Server 1 starts\")\nabline(v = 45,\n       lwd = 2,\n       col = \"grey\")\ntext(45, 5,\n     pos = 3, # To the right,\n     labels = \"Server 2 starts\")\ndpois(x = 4,     lambda = 5)\n#> [1] 0.1754674\ndpois(x = 4 + 1, lambda = 5)\n#> [1] 0.1754674"},{"path":"selected-solutions.html","id":"AnswersChapContinuousDistributions","chapter":"E Selected solutions","heading":"E.8 Answers for Chap. 8","text":"Answer Exercise 8.1.example:\n\\(\\mu = m/(m + n)\\), get \\(n = m(1 - \\mu)/\\mu\\) \\(m + n = m/\\mu\\).\nAlso, expression variance, substitute \\(m + n = m/\\mu\\) simplify get\n\\[\n   n = \\frac{\\sigma^2 m (m + \\mu)}{\\mu^3}\n\\]\nEquate expression \\(n\\) \\(n = m(1 - \\mu)/\\mu\\) earlier, solve  \\(m\\).\nget \\(m = \\frac{\\mu}{\\sigma^2}\\left( \\mu(1 - \\mu) - \\sigma^2\\right)\\);\n\\(n = \\frac{1 - \\mu}{\\sigma^2}\\left( \\mu(1 - \\mu) - \\sigma^2\\right)\\).Answer Exercise 8.2.\\(\\operatorname{E}(X) = (72 + 30)/2 = 51\\)km.h\\(-1\\).\n\\(\\operatorname{var}(X) = (72 - 30)^2 / 12 = 147\\)(km.h\\(-1\\))\\(2\\) (easily understood, \\(\\sqrt{\\operatorname{var}(X)} = 12.12\\)km.h\\(-1\\)).See Fig. E.38.See ; \\(2/7 \\approx 0.2857\\).See ; \\(7/12 \\approx 0.5833\\).\nFIGURE E.38: Vehicle speeds\nAnswer Exercise 8.3. \\(2.4\\)% vehicles excluded.\\(Y\\) distribution \\(X \\mid (X > 30 \\cap X < 72)\\) \\(X \\sim N(48, 8.8^2)\\).\nPDF \n\\[\n  f_Y(y) =\n  \\begin{cases}\n  0 & \\text{$y < 30$};\\\\\n     \\displaystyle \\frac{1}{8.8k \\sqrt{2\\pi}}\n              \\exp\\left\\{ -\\frac{1}{2}\\left( \\frac{y - 48}{8.8}\\right)^2 \\right\\} & \\text{$30\\le y \\le 72$};\\\\\n  0 & \\text{$y > 72$}.\n  \\end{cases}\n\\]\n\\(k \\approx \\Phi(-2.045455) + (1 - \\Phi(2.727273)) = 0.02359804\\).Answer Exercise 8.4.See Fig. E.39.\\(\\operatorname{E}(X) = \\alpha \\beta \\approx 38.4\\) \\(\\operatorname{var}(X) = \\alpha \\beta^2 \\approx 54.54\\), standard deviation \\(7.38\\).\\(\\Pr(C > 30 \\mid C < 50) = \\Pr(30 < C < 50) / \\Pr(C < 50)\\approx 0.870\\).\nFIGURE E.39: gamma distribution concrete diffusion model\nAnswer Exercise 8.5.See Fig. E.40.\\(0.0228\\).\\(2.17\\) kg/m\\(3\\).\\(2.21\\) kg/m\\(3\\).\nFIGURE E.40: gamma distribution surface chloride concentrations model\nAnswer Exercise 8.6.See Fig. E.41.\\(F_X(x) = \\int_0^x ab t^{- 1} (1 - t^)^{b - 1} \\, dt = 1 - (1 - x^)^b\\).\\(b = 1\\), PDF \\(p_X(x) = x^{- 1}\\).\nAlso,\n\\[\\begin{align*}\n  \\text{Beta}(, 1)\n  &= \\frac{x^{- 1} (1 - x)^1 - 1}{B(, 1)}\\\\\n  &= \\frac{x^{- 1} \\Gamma(+ 1)}{\\Gamma() \\, \\Gamma()} = x^{- 1},\n  \\end{align*}\\]\n.\\(= b = 1\\), PDF \\(p_X(x) = 1\\), continuous uniform distribution.Write \\(Y = 1 - X\\); \\(p_X(x) = ab(1 - y)^{- 1} (y^) ^{b - 1}\\).\nFIGURE E.41: Kumaraswamy distributions\nAnswer Exercise 8.7.\\(\\text{CV} = \\frac{\\sqrt{\\beta^2}}{\\alpha\\beta} = 1/\\sqrt{\\alpha}\\), constant.Answer Exercise 8.8.given beta distribution, \\(\\operatorname{E}(V) = 0.287/(0.287 + 0.926) = 0.2366...\\) \\(\\operatorname{var}(V) = 0.08161874\\).\\(\\operatorname{E}(S) = \\operatorname{E}(4.5 + 11V) = 4.5 + 11\\operatorname{E}(V) = 7.10\\) minutes.\n\\(\\operatorname{var}(S) = 11^2\\times\\operatorname{var}(V) = 9.875\\) minutes2.See Fig. E.42.\nFIGURE E.42: Service times\nAnswer Exercise 8.9.\\(\\operatorname{E}[T] = \\operatorname{E}[0.5 + W] = 0.5 + \\operatorname{E}[W] = 0.5 + 16.5 = 17.0\\).\\(\\operatorname{var}[T] = \\operatorname{var}[0.5 + W] = \\operatorname{var}[W] = 16.5^2\\), std dev \\(16.5\\).\\(\\Pr(T > 1) = \\Pr( [W + 0.5] > 1 ) = \\Pr(W > 0.5) = 0.9702\\).\nFIGURE E.43: Exponential distribution hospitals\nAnswer Exercise 8.12.summer: event lasts \\(1\\) hour, probability eventually lasts three hours?winter: event lasts \\(1\\) hour, probability lasts less two hours?\nFIGURE E.44: Winter summer\nAnswer Exercise 8.14.\\(\\operatorname{E}(X) = \\alpha\\beta = 3.76\\) days.\n\\(\\operatorname{var}(X) = \\alpha\\beta^2 = 12.75\\) days2, std dev \\(3.571\\) days.main difference lower end distribution.\\(35%\\) (gamma) \\(31%\\) (exponential).just \\(10\\) days.\nFIGURE E.45: Gamma exponential\nAnswer Exercise 8.15. \\(25.2\\)%.\\(\\Pr(T > 7 \\mid T > 5) = \\Pr(T > 7)/\\Pr(T > 5)\\)  \\(33.8\\)%. \\(7\\):\\(30\\)pm.Answer Exercise 8.16.\nWrite \\(X\\) percentage clay content soil.two counties:County :\n\\(\\operatorname{E}(X) = m / (m + n) = 0.708\\)  \\(70.8\\)%.\n\\(\\operatorname{var}(X) = 0.01196957\\) standard deviation  \\(10.9\\)%.\n\\(\\operatorname{E}(X) = m / (m + n) = 0.708\\)  \\(70.8\\)%.\\(\\operatorname{var}(X) = 0.01196957\\) standard deviation  \\(10.9\\)%.County B:\n\\(\\operatorname{E}(X) = m / (m + n) = 0.513\\)  \\(51.3\\)%.\n\\(\\operatorname{var}(X) = 0.02939085\\) standard deviation  \\(17.0\\)%.\n\\(\\operatorname{E}(X) = m / (m + n) = 0.513\\)  \\(51.3\\)%.\\(\\operatorname{var}(X) = 0.02939085\\) standard deviation  \\(17.0\\)%. \\(96\\)% County ; \\(53\\)% County B.\nlook reasonable plots.\nFIGURE E.46: Clay content\nAnswer Exercise 8.20.Note \\(Y = X - \\Delta\\),  \\(Y\\) gamma distribution, \\(\\alpha = 2.5\\) \\(\\beta = 0.8\\), \\(\\operatorname{E}(Y) = \\alpha\\beta = 2\\) \\(\\operatorname{var}(Y) = \\alpha\\beta^2 = 1.6\\).\\(\\operatorname{E}(X) = \\operatorname{E}(Y + \\Delta) = 3.2\\) seconds.\n\\(\\operatorname{var}(X) = \\operatorname{var}(Y) = 1.6\\) seconds\\(2\\).\nFIGURE E.47: Headways\nAnswer Exercise 8.23.\nFIGURE E.48: Heights children day-care facilities\nAnswer Exercise 8.24.\nNormal: \\(\\operatorname{var}(X) = \\sigma^2 \\mu^0\\), \\(\\phi = \\sigma^2\\) \\(p = 0\\).Poisson: \\(\\operatorname{var}(X) = \\mu\\), \\(\\phi = 1\\) \\(p = 1\\).Gamma: \\(\\operatorname{var}(X) = \\phi \\mu^2\\), \\(\\phi = ???\\) \\(p = 2\\).","code":"\n1 - punif(60, 30, 72)\n#> [1] 0.2857143\n(1 - punif(65, 30, 72) ) /\n  (1 - punif(60, 30, 72) )\n#> [1] 0.5833333\npar(mfrow = c(1, 2))\n\nx <- seq(10, 100,\n         length = 1000)\nplot( x = x,\n      y = dunif(x, \n                min = 30,\n                max = 72),\n      type = \"l\",\n      las = 1,\n      xlab = \"Vehicle speeds (in km/h)\",\n      ylab = \"Probability density\",\n      main = \"PDF for vehicle speeds\")\n\nx <- seq(10, 100,\n         length = 1000)\nplot( x = x,\n      y = punif(x, \n                min = 30,\n                max = 72),\n      type = \"l\",\n      las = 1,\n      ylim = c(0, 1),\n      xlab = \"Vehicle speeds (in km/h)\",\n      ylab = \"Probability density\",\n      main = \"DF for vehicle speeds\")\n# Proportion excluded:\npnorm(30, 48, 8.8) + # Slower than 30\n   (1- pnorm(72, 48, 8.8)) # Faster than 72\n#> [1] 0.02359804\nalpha <- 27.05\nbeta <- 1.42\n\nx <- seq(0, 80, length = 200)\n\npar(mfrow = c(1, 2))\n\nplot( y = dgamma(x, shape = alpha, scale = beta),\n      x = x,\n      las = 1,\n      xlab = expression(italic(x)),\n      ylab = \"Prob. fn\",\n      type = \"l\",\n      lwd = 2)\nplot( y = pgamma(x, shape = alpha, scale = beta),\n      x = x,\n      xlab = expression(italic(x)),\n      ylab = \"Dist. fn\",\n      type = \"l\",\n      las = 1,\n      lwd = 2)\n\n( pgamma(50, shape = alpha, scale = beta) - \n  pgamma(30, shape = alpha, scale = beta) ) /\npgamma(50, shape = alpha, scale = beta)\n#> [1] 0.8701039\nmn <- 2\nsd <- 0.2\n\nx <- seq(1.2, 2.8, length = 200)\n\npar(mfrow = c(1, 2))\n\nplot( y = dnorm(x, mean = mn, sd = sd),\n      x = x,\n      xlab = expression(italic(x)),\n      ylab = \"Prob. fn\",\n      type = \"l\",\n      lwd = 2)\nplot( y = pnorm(x, mean = mn, sd = sd ),\n      x = x,\n      xlab = expression(italic(x)),\n      ylab = \"Dist. fn\",\n      type = \"l\",\n      lwd = 2)\n\n1 - pnorm(2.4, mean = mn, sd = sd)\n#> [1] 0.02275013\nqnorm(0.8, mean = mn, sd = sd)\n#> [1] 2.168324\nqnorm(0.85, mean = mn, sd = sd)\n#> [1] 2.207287\nv <- seq(0, 1, length = 500)\n\nPDFv <- dbeta(v, shape1 = 0.287, shape2 = 0.926)\n\nplot( PDFv ~ v,\n      type = \"l\",\n      las = 1,\n      xlim = c(0, 1),\n      ylim = c(0, 5),\n      xlab = expression(italic(V)),\n      ylab = \"PDF\",\n      main = \"Distribution of V\",\n      lwd = 2)\nbeta <- 16.5\nx <- seq(0, 80, length = 1000)\n\nyB <- dexp(x,\n           rate = 1/beta)\n\nplot( yB ~  x,\n      las = 1,\n      type = \"l\",\n      ylim = c(0, 0.06),\n      lwd = 2,\n      main = expression(PDF~\"for\"~italic(W)),\n      xlab = expression(italic(w)),\n      ylab = \"Prob. fn.\")\n\n# Part 3\n1 - pexp(0.5, rate = 1/beta) # 0.9701515\n#> [1] 0.9701515\nalpha <- 2; betaS <- 0.04; betaW <- 0.03\nx <- seq(0, 1, length = 1000)\n\nyS <- dgamma( x, shape = alpha, scale = betaS)\nyW <- dgamma( x, shape = alpha, scale = betaW)\n\nplot( range(c(yS, yW)) ~  range(x),\n      type = \"n\", # No plot, just canvas\n      las = 1,\n      xlab = \"Event duration (in ??)\",\n      ylab = \"Prob. fn.\")\nlines(yS ~ x, \n      lty = 1,\n      lwd = 2)\nlines(yW ~ x, \n      lty = 2,\n      lwd = 2)\nlegend( \"topright\",\n        lwd = 2,\n        lty = 1:2,\n        legend = c(\"Summer\", \"Winter\"))\n1 - pgamma(6/24, shape = alpha, scale = betaW)\n#> [1] 0.002243448\n1 - pgamma(6/24, shape = alpha, scale = betaS)\n#> [1] 0.01399579\nalpha <- 1.11; beta <- 3.39\n\nx <- seq(0, 15, length = 500)\nyGamma <- dgamma(x, shape = alpha, scale = beta)\nyExp <- dexp(x, rate = 1/beta)\n\nplot( range (c(yGamma, yExp)) ~ range(x),\n      type = \"n\", # No plot, just canvas\n      las = 1,\n      xlab = \"Duration of symptoms (in days)\",\n      ylab = \"Prob. fn.\")\nlines( yGamma ~ x,\n       lwd = 2,\n       lty = 1)\nlines( yExp ~ x,\n       lwd = 2,\n       lty = 2)\nlegend( \"topright\",\n        lwd = 2,\n        lty = 1:2,\n        legend = c(\"Gamma\", \"Exponential\"))\n\n## Part 3\n1 - pgamma(4, shape = alpha, scale = beta) # 0.3505492\n#> [1] 0.3505492\n1 - pexp(4, rate = 1/beta) # 0.3072969\n#> [1] 0.3072969\n\n## Part 4\nqgamma(0.95, shape = alpha, scale = beta) # 10.86655\n#> [1] 10.86655\nqexp(0.95, rate = 1/beta) # 10.15553\n#> [1] 10.15553\npnorm(5, mean = 6, sd = 1.5)\n#> [1] 0.2524925\n\n(1 - pnorm(7, mean = 6, sd = 1.5) ) /\n  (1 - pnorm(5, mean = 6, sd = 1.5) )\n#> [1] 0.3377793\n\nqnorm(0.85, mean = 6, sd = 1.5)\n#> [1] 7.55465\nx <- seq(0, 1, length = 1000)\nyA <- dbeta(x, shape1 = 11.52, shape2 = 4.75)\nyB <- dbeta(x, shape1 = 3.85, shape2 = 3.65)\n\nplot( range( c(yA, yB)) ~ range(100 * x),\n      type = \"n\", #No plot, just canvas\n      las = 1,\n      xlab = \"Percentage clay\",\n      ylab = \"Prob. fn.\")\nlines( yA ~ I(100 * x),\n       lwd = 2,\n       lty = 1)\nlines( yB ~ I(100 * x),\n       lwd = 2,\n       lty = 2)\nlegend(\"top\",\n       lty = 1:2,\n       lwd = 2,\n       legend = c(\"Location A\", \"Location B\"))\n\n## Part 3\nqbeta( 0.5, shape1 = 11.52, shape2 = 4.75) #  0.7167495\n#> [1] 0.7167495\nqbeta( 0.5, shape1 = 3.85,  shape2 = 3.65) # 0.5145787\n#> [1] 0.5145787\nnumSims <- 1000\nyHeadway <- rgamma( n = numSims,\n                    shape = 2.5,\n                    rate = 0.8)\nxHeadway <- yHeadway + 1.2\nhist(xHeadway,\n     las = 1,\n     lwd = 2,\n     xlim = c(0, max(xHeadway)),\n     main = \"Histogram of headway\",\n     ylab = \"Prob. fn.\",\n     xlab = \"Headway (in sec)\")\n\nxMean <- 3.2; xVar <- 1.6\nxLo <- 3.2 - 2 * sqrt(xVar)\nxHi <- 3.2 + 2 * sqrt(xVar)\ncat(\"Limits\", xLo, \"and\", xHi, \"\\n\")\n#> Limits 0.6701779 and 5.729822\n\n# Part 3\nNum <- sum( (xHeadway < xHi ) & (xHeadway > xLo) )\nEmpiricalProp <- Num / numSims\nEmpiricalProp\n#> [1] 0.766\n\n# Part 4: Tchebyshev: AT LEAST this proportion\nk <- 2\n1 - 1/k^2\n#> [1] 0.75\nhtMean <- function(x) {\n  7.5 * x + 70\n}\nhtSD <- function(x) {\n  0.4575 * x + 1.515\n}\n\nNumSims <- 10000\n\nAges <- c(\n  rep(2, NumSims * 0.32),\n  rep(3, NumSims * 0.33),\n  rep(4, NumSims * 0.25),\n  rep(5, NumSims * 0.10)\n)\n\nHts <- rnorm(NumSims,\n             mean = htMean(Ages),\n             sd = htSD(Ages))\nhist(Hts,\n     las = 1,\n     xlab = \"Heights (in cm)\")\n\n# Taller than 100:\ncat(\"Taller than 100cm:\", sum( Hts > 100) / NumSims * 100, \"%\\n\")\n#> Taller than 100cm: 22.42 %\n# Mean and variance:\ncat(\"Mean:\", mean( Hts ), \"cm; \",\n    \"Std dev: \", sd(Hts), \"cm\\n\")\n#> Mean: 93.52505 cm;  Std dev:  7.936301 cm\n\n# Sort the heights to find where the tallest 20% are:\ncat(\"Tallest 15% are taller than\", \n    sort(Hts)[ NumSims * 0.85], \"cm\\n\")\n#> Tallest 15% are taller than 102.545 cm"},{"path":"selected-solutions.html","id":"AnswersChapMixedDistributions","chapter":"E Selected solutions","heading":"E.9 Answers for Chap. 9","text":"Answer Exercise 9.1.Answer Exercise 9.2.Answer Exercise 9.3.\n1. \n\\[\n  \\Pr(Y < 0) = \\Pr\\big(Z < (0 - 1.5)/1\\big) = \\Phi(-1.5) \\approx 0.0668,\n\\]\nusing \\(Z\\) standard normal variate (Sect. 8.3.2).\n2. use functions \\(\\Phi(\\cdot)\\) \\(\\phi(\\cdot)\\), standardised version  \\(Y\\) must used; , \\(Z = (Y - 1.5)/1 = (Y - 1.5)\\) functions.\n,\n\\[\n  f_X(x) =\n  \\begin{cases}\n    0                & \\text{$X < 0$};\\\\\n    \\Phi(-1.5)       & \\text{$X = 0$};\\\\\n    \\phi(x - 1.5)    & \\text{$X > 0$},\n  \\end{cases}\n\\]\nEquation (9.1),\n\\[\n  f_{X\\mid X > 0}(x)\n  = \\frac{\\phi(x- 1.5)}{1 - \\Phi(-1.5)}\n  \\approx 1.148\\dots \\times \\phi(x)\n\\]\n\\(X > 0\\).\nexpression,\n\\[\\begin{align*}\n  \\operatorname{E}[X]\n   = \\operatorname{E}[X \\mid X>0]\n  &= \\int_0^\\infty ???\\,dx \\\\\n  & \\text{FIX!}\n\\end{align*}\\]\nSimilarly,\n\\[\n  \\operatorname{var}[X]\n  = \\operatorname{var}[X \\mid X>0]\n  = ???\n\\]\n3. See Fig. E.49.\nFIGURE E.49: censored model, using normal distribution continuous component, threshold value \\(X = 0\\).\n","code":""},{"path":"selected-solutions.html","id":"AnswersChapMultivariate","chapter":"E Selected solutions","heading":"E.10 Answers for Chap. 10","text":"","code":""},{"path":"selected-solutions.html","id":"AnswerMultivariateExtensions","chapter":"E Selected solutions","heading":"E.11 Answers for Chap. 11","text":"Answer Exercise 11.1.\n1. \\(\\operatorname{E}(\\overline{X}) = \\operatorname{E}([X_1 + X_2 + \\cdots + X_n]/n) = [\\operatorname{E}(X_1) + \\operatorname{E}(X_2) + \\cdots + \\operatorname{E}(+ X_n)]/n = [n \\mu]/n = \\mu\\).\n2. \\(\\operatorname{var}(\\overline{X}) = \\operatorname{var}([X_1 + X_2 + \\cdots + X_n]/n) = [\\operatorname{var}(X_1) + \\operatorname{var}(X_2) + \\cdots + \\operatorname{var}(X_n)]/n^2 = [n \\sigma^2]/n^2 = \\sigma^2/n\\).Answer Exercise 11.2.one month:\\(1000\\) simulations:Answer Exercise 11.3.\\(\\operatorname{E}[U] = \\mu_x - \\mu_Z\\); \\(\\operatorname{E}[V] = \\mu_x - 2\\mu_Y + \\mu_Z\\).\\(\\operatorname{var}[U] = \\sigma^2_x + \\sigma^2_Z\\); \\(\\operatorname{E}[V] = \\sigma^2_x + 4\\sigma^2_Y + \\sigma^2_Z\\).Care needed!\n\\[\\begin{align*}\n\\text{Cov}[U, V]\n  &= \\operatorname{E}[UV] - \\operatorname{E}[U]\\operatorname{E}[V]\\\\\n  &= \\operatorname{E}[X^2 - 2XY + XZ - XZ + 2YZ - Z^2] - \\\\\n  &\\qquad (\\mu_X^2 + 2\\mu_X\\mu_Y + \\mu_X\\mu_Z - \\mu_X\\mu_Z - 2\\mu_Y\\mu_Z - \\mu^2_z)\\\\\n  &= (\\operatorname{E}[X^2] - \\mu_X^2) - 2(\\operatorname{E}[XY] - \\operatorname{E}[X]\\operatorname{E}[Y]) + 2(\\operatorname{E}[YZ] - \\mu_Y\\mu_Z)  -\\\\\n  &\\qquad (\\operatorname{E}[Z^2] - \\mu_Z^2)\\\\\n  &= \\sigma^2_X - \\sigma^2_Z,\n\\end{align*}\\]\nsince two middle terms become \\(-2\\text{Cov}[X, Y] + 2\\text{Cov}[Y, Z]\\), zero (given).covariance zero \\(\\sigma^2_X = \\sigma^2_Z\\).Answer Exercise 11.4.First, \\(\\Pr(X \\mid Y = 2) = |x - 2|/11\\).\n, marginal distribution \n\\[\n   p(X\\mid Y = 2) =\n   \\begin{cases}\n     2/3 & \\text{$x = 0$};\\\\\n     1/3 & \\text{$x = 1$.}\n   \\end{cases}\n\\]\n\\(\\operatorname{E}(X \\mid Y = 2) = 1/3\\).First, \\(p_X(x) = \\frac{1}{11}( |x - 1| + |x - 2| + |x - 3|)\\) \\(x = 0, 1, 2\\); \\(\\Pr(X\\ge 1) = 5/11\\).\nprobability function non-zero \\(y = 1, 2, 3\\).\n\n\\[\np(Y\\mid x\\ge 1) =\n  \\begin{cases}\n     1/5 & \\text{$y = 1$};\\\\\n     1/5 & \\text{$y = 2$};\\\\\n     3/5 & \\text{$y = 3$}.\n  \\end{cases}\n\\]\n, \\(\\operatorname{E}(Y \\mid X\\ge 1) = 12/5\\).Answer Exercise 11.5.find \\(f_X(x) = 1\\) \\(0 < x < 1\\) \\(f_Y(y) = 1\\) \\(0 < y < 1\\).find \\(f_X(x) = 1\\) \\(0 < x < 1\\) \\(f_Y(y) = 1\\) \\(0 < y < 1\\).need intermediate values:\n\\(\\operatorname{E}(X) = \\operatorname{E}(Y) = 1/2\\);\n\\(\\operatorname{var}(X) = \\operatorname{var}(Y) = 1/12\\);\n\\(\\operatorname{E}(XY) = (9 - \\alpha)/36\\).\n\\(\\text{Cov}(X, Y) = -\\alpha/36\\) \\(\\text{Cor}(X, Y) = -\\alpha/3\\).need intermediate values:\\(\\operatorname{E}(X) = \\operatorname{E}(Y) = 1/2\\);\\(\\operatorname{var}(X) = \\operatorname{var}(Y) = 1/12\\);\\(\\operatorname{E}(XY) = (9 - \\alpha)/36\\).\\(\\text{Cov}(X, Y) = -\\alpha/36\\) \\(\\text{Cor}(X, Y) = -\\alpha/3\\).Independence \\(f_X(x) \\times f_Y(y) = f_{XY}(x, y)\\), occurs \\(\\alpha = 0\\).Independence \\(f_X(x) \\times f_Y(y) = f_{XY}(x, y)\\), occurs \\(\\alpha = 0\\).\\(\\Pr(X < Y) = \\int_0^1\\!\\!\\int_y^1 1 - \\alpha(1 - 2x)(1 - 2y)\\, dx\\, dy = 1/2\\).\\(\\Pr(X < Y) = \\int_0^1\\!\\!\\int_y^1 1 - \\alpha(1 - 2x)(1 - 2y)\\, dx\\, dy = 1/2\\).Answer Exercise 11.9.exponential distribution, MGF \\(M_X(t) = (1 - \\beta t)^{-1}\\).\nNow, MGF sum product MGFs (see Theorem 5.6), MGF \\(W = Z_1 + Z_2 + \\cdots + Z_n\\) \n\\[\n   M_W(t) = \\prod_{= 1}^n (1 - \\beta t)^{-1} =  (1 - \\beta t)^{-n},\n\\]\nMGF gamma distribution parameters \\(n\\)  \\(\\beta\\), shown.Answer Exercise 11.11.Adding:\n\\[\n  p_X(x) =\n  \\begin{cases}\n0.40 & \\text{$x = 0$};\\\\\n0.45 & \\text{$x = 1$};\\\\\n0.15 & \\text{$x = 3$}.\n  \\end{cases}\n\\]Adding:\n\\[\n  p_X(x) =\n  \\begin{cases}\n0.40 & \\text{$x = 0$};\\\\\n0.45 & \\text{$x = 1$};\\\\\n0.15 & \\text{$x = 3$}.\n  \\end{cases}\n\\]\\(\\Pr(X \\ne Y) = 1 - \\Pr(X = Y) = 1 - (0.20) = 0.80\\).\\(\\Pr(X \\ne Y) = 1 - \\Pr(X = Y) = 1 - (0.20) = 0.80\\).\\(X < Y\\) includes five \\((x, y)\\) elements sample space:\n\\(\\{ (0, 1), (0, 2), (0, 3); (1, 2), (1, 3)\\}\\).\nsum probabilities  \\(0.65\\).\n, given five, two sum three (.e., \\((0, 3)\\) \\((1, 2)\\)).\nprobability intersection \\(\\Pr( \\{0, 3\\} ) + \\Pr( \\{1, 2\\}) = 0.30\\).\nconditional probability \\(0.30/0.65 = 0.4615385\\),  \\(46\\)%.\\(X < Y\\) includes five \\((x, y)\\) elements sample space:\n\\(\\{ (0, 1), (0, 2), (0, 3); (1, 2), (1, 3)\\}\\).\nsum probabilities  \\(0.65\\)., given five, two sum three (.e., \\((0, 3)\\) \\((1, 2)\\)).\nprobability intersection \\(\\Pr( \\{0, 3\\} ) + \\Pr( \\{1, 2\\}) = 0.30\\).\nconditional probability \\(0.30/0.65 = 0.4615385\\),  \\(46\\)%.: \\(X = 0\\), values  \\(Y\\) non-zero probability \\(Y = 1, 2, 3\\).\nHowever, \\(X = 1\\) (example), values \\(Y\\) non-zero probability \\(Y = 1, 2\\).: \\(X = 0\\), values  \\(Y\\) non-zero probability \\(Y = 1, 2, 3\\).\nHowever, \\(X = 1\\) (example), values \\(Y\\) non-zero probability \\(Y = 1, 2\\).Answer Exercise 11.14.\n1. \\(f(x_1, x_2, \\dots, x_n) = 4^n \\prod_{= 1}^n x_i^3\\).\n2. \\(\\Pr(X_1 < 1/2) = \\int_0^{1/2} 4 x^3 \\, dx =  1/16 = 0.0625\\).\n3. prob.  \\(16^{-n}\\).\n4.  3.Answer Exercise 11.16.table can used show sample space ().\nLooking \\(X = 4\\) deduce \n\\[\n  p_Y(y|X = 4) =\n  \\begin{cases}\n        2/7 = 0.2857 & \\text{$y = 1$};\\\\\n        2/7 & \\text{$y = 2$};\\\\\n        2/7 & \\text{$y = 3$};\\\\\n        1/7 & \\text{$y = 4$};\\\\\n        0 & \\text{elsewhere}.\\\\\n  \\end{cases}\n\\]\nHence deduce \\(\\operatorname{E}[Y | X = 4] = 16/7\\approx 2.2857\\).table shows values \\((x, y)\\) (, table entries \\((\\text{max}, \\text{min})\\)).Answer Exercise 11.18.Answer Exercise 11.19.Since \\(X + Y + Z = 500\\), values \\(X\\) \\(Y\\) known, value  \\(Z\\) one specific value (.e., three values free vary).See Fig. E.50 (left panel).See Fig. E.50 (right panel).Proceed:\n\\[\\begin{align*}\n  1\n  &= \\int_{200}^{300} \\!\\! \\int_{100}^{400 - y} k\\,dx\\,dy +\n     \\int_{100}^{200} \\!\\! \\int_{300 - y}^{400 - y}     k\\,dx\\,dy \\\\\n  &= 30\\ 000,\n  \\end{align*}\\]\n\\(k = 1/30\\,000\\).\nFIGURE E.50: sample space mixture nuts dried fruit\nAnswer Exercise 11.20.Let \\(W\\), \\(\\), \\(P\\) denote walnut, almond, peanut weights (grams).\nSince packet contains \\(250\\,\\text{g}\\):\n\\[\n  W + + P = 250 \\quad \\Rightarrow \\quad P = 250 - W - .\n\\]\n \\(W\\)  \\(\\) specified, \\(P\\) determined.\\(W \\ge 0\\), \\(\\ge 0\\) \\(0 \\le P \\le 100\\).\n, since \\(P = 250 - W - \\),\n\\[\n  0 \\le 250 - W - \\le 100\n  \\quad \\Rightarrow \\quad 150 \\le W + \\le 250.\n\\]\nHence sample space \n\\[\n  \\mathcal{R}_2 = \\{(w,) : w \\ge 0, \\; \\ge 0, \\; 150 \\le w + \\le 250\\}.\n\\]\ntrapezoid \\((W, )\\)-plane vertices\n\\[\n  (150, 0), \\quad (250, 0), \\quad (0, 250), \\quad (0, 150).\n\\]Adding requirements \\(W \\ge 25\\) \\(\\ge 25\\) gives feasible region \n\\[\n  \\mathcal{R}_3 = \\{(w,) : w \\ge 25, \\; \\ge 25, \\; 150 \\le w + \\le 250\\}\n\\]\ncorners \n\\[\n  (125, 25), \\quad (225, 25), \\quad (25, 225), \\quad (25, 125).\n\\]Assuming weights uniformly distributed feasible region just peanuts restriction, area \n\\[\n  \\text{Area}(\\mathcal{R}_2)\n   = \\frac{1}{2}\\cdot 250^2 - \\frac{1}{2}\\cdot 150^2\n   = 20\\,000.\n\\]\n\n\\[\n  f_{W, }(w, ) =\n  \\begin{cases}\n    \\frac{1}{20\\,000} & (w, ) \\\\mathcal{R}_2, \\\\[6pt]\n    0                 & \\text{otherwise}.\n  \\end{cases}\n\\]\n, additional lower bounds, can shift variables: \\(u = w - 25\\) \\(v = - 25\\).\n\\(u, v \\ge 0\\) \\(100 \\le u + v \\le 200\\).\nThus\n\\[\n  \\text{Area}(\\mathcal{R}_3)\n  = \\frac{1}{2}\\cdot 200^2 - \\frac{1}{2}\\cdot 100^2\n  = 15\\,000.\n\\]\n\n\\[\n  f_{W, }(w, ) =\n  \\begin{cases}\n    \\frac{1}{15\\,000} & (w, ) \\\\mathcal{R}_3, \\\\[6pt]\n    0                 & \\text{otherwise}.\n  \\end{cases}\n\\]\nNote cases \\(P = 250 - W - \\), joint distribution \\((W, , P)\\) uniform feasible \\(2\\)-dimensional region\n\\[\n  \\{(w, , p): w + + p = 250, \\; (w, )\\\\mathcal{R}_2 \\text{ } \\mathcal{R}_3\\}.\n\\]Answer Exercise 11.21.Joint density–-mass.\n\\(y\\\\{0, 1\\}\\),\n\\[\nf_{X, Y}(x, y)\n= f_{X\\mid Y}(x\\mid y)\\,p_Y(y)\n=\n\\begin{cases}\n   \\displaystyle \\phi(x; \\mu_1 = 8,\\sigma_1^2 = 1)\\cdot 0.10, & y = 1,\\\\[4pt]\n   \\displaystyle \\phi(x; \\mu_0 = 5,\\sigma_0^2 = 1)\\cdot 0.90, & y = 0,\n\\end{cases}\n  \\]\n\\(\\phi(x; \\mu,\\sigma^2) = \\frac{1}{\\sqrt{2\\pi}\\, \\sigma} \\exp\\!\\left(-\\frac{(x - \\mu)^2}{2\\sigma^2}\\right)\\).Joint density–-mass.\n\\(y\\\\{0, 1\\}\\),\n\\[\nf_{X, Y}(x, y)\n= f_{X\\mid Y}(x\\mid y)\\,p_Y(y)\n=\n\\begin{cases}\n   \\displaystyle \\phi(x; \\mu_1 = 8,\\sigma_1^2 = 1)\\cdot 0.10, & y = 1,\\\\[4pt]\n   \\displaystyle \\phi(x; \\mu_0 = 5,\\sigma_0^2 = 1)\\cdot 0.90, & y = 0,\n\\end{cases}\n  \\]\n\\(\\phi(x; \\mu,\\sigma^2) = \\frac{1}{\\sqrt{2\\pi}\\, \\sigma} \\exp\\!\\left(-\\frac{(x - \\mu)^2}{2\\sigma^2}\\right)\\).Marginal density \\(X\\).\nmixture two normals:\n\\[\nf_X(x)\n= \\sum_{y\\\\{0, 1\\}} f_{X\\mid Y}(x\\mid y)\\,p_Y(y)\n= 0.10\\,\\phi(x; 8, 1) + 0.90\\,\\phi(x; 5, 1).\n  \\]Marginal density \\(X\\).\nmixture two normals:\n\\[\nf_X(x)\n= \\sum_{y\\\\{0, 1\\}} f_{X\\mid Y}(x\\mid y)\\,p_Y(y)\n= 0.10\\,\\phi(x; 8, 1) + 0.90\\,\\phi(x; 5, 1).\n  \\]Posterior \\(\\Pr(Y = 1\\mid X = 7)\\).\nBayes’ rule densities,\n\\[\n\\Pr(Y = 1\\mid X = 7)\n= \\frac{f_{X\\mid Y}(7\\mid 1)\\,p_Y(1)}{f_X(7)}\n= \\frac{0.10\\,\\phi(7; 8, 1)}{0.10\\,\\phi(7; 8, 1) + 0.90\\,\\phi(7; 5, 1)}.\n  \\]\nNumerically,\n\\[\n\\phi(7; 8, 1) = 0.2419707245,\\quad\n\\phi(7; 5, 1) = 0.0539909665,\n  \\]\n\n\\[\nf_X(7)\n= 0.10 (0.2419707245) + 0.90 (0.0539909665)\n= 0.0727889423,\n  \\]\n\n\\[\n\\Pr(Y = 1\\mid X = 7)\n= \\frac{0.0241970725}{0.0727889423}\n\\approx 0.3324.\n  \\]Posterior \\(\\Pr(Y = 1\\mid X = 7)\\).\nBayes’ rule densities,\n\\[\n\\Pr(Y = 1\\mid X = 7)\n= \\frac{f_{X\\mid Y}(7\\mid 1)\\,p_Y(1)}{f_X(7)}\n= \\frac{0.10\\,\\phi(7; 8, 1)}{0.10\\,\\phi(7; 8, 1) + 0.90\\,\\phi(7; 5, 1)}.\n  \\]\nNumerically,\n\\[\n\\phi(7; 8, 1) = 0.2419707245,\\quad\n\\phi(7; 5, 1) = 0.0539909665,\n  \\]\n\n\\[\nf_X(7)\n= 0.10 (0.2419707245) + 0.90 (0.0539909665)\n= 0.0727889423,\n  \\]\n\n\\[\n\\Pr(Y = 1\\mid X = 7)\n= \\frac{0.0241970725}{0.0727889423}\n\\approx 0.3324.\n  \\]Given biomarker value \\(7\\) mmol.L\\(-1\\), estimated probability patient disease 33% model (\\(10\\)% prevalence specified conditional normal distributions).Given biomarker value \\(7\\) mmol.L\\(-1\\), estimated probability patient disease 33% model (\\(10\\)% prevalence specified conditional normal distributions).Answer Exercise 11.22.Joint density–-mass.\nLet \\(p_Y(1) = 0.5\\), \\(p_Y(2) = 0.3\\), \\(p_Y(3) = 0.2\\), \n\\[\n  X\\mid Y = y \\sim \\mathrm{Exp}(\\lambda_y), \\quad\n  \\lambda_1 = 1,\\ \\lambda_2 = 0.5,\\ \\lambda_3 = 0.25.\n\\]\n\\(x\\ge 0\\),\n\\[\n  f_{X, Y}(x,y)\n  = p_Y(y)\\, f_{X\\mid Y}(x\\mid y)\n  = p_Y(y)\\,\\lambda_y e^{-\\lambda_y x}.\n\\]Joint density–-mass.\nLet \\(p_Y(1) = 0.5\\), \\(p_Y(2) = 0.3\\), \\(p_Y(3) = 0.2\\), \n\\[\n  X\\mid Y = y \\sim \\mathrm{Exp}(\\lambda_y), \\quad\n  \\lambda_1 = 1,\\ \\lambda_2 = 0.5,\\ \\lambda_3 = 0.25.\n\\]\n\\(x\\ge 0\\),\n\\[\n  f_{X, Y}(x,y)\n  = p_Y(y)\\, f_{X\\mid Y}(x\\mid y)\n  = p_Y(y)\\,\\lambda_y e^{-\\lambda_y x}.\n\\]Marginal density \\(X\\).\nMixture exponentials:\n\\[\n  f_X(x)\n  = \\sum_{y = 1}^3 p_Y(y)\\,\\lambda_y e^{-\\lambda_y x}\n  = 0.5\\,e^{-x} + 0.15\\,e^{-0.5x} + 0.05\\,e^{-0.25x}, \\quad x\\ge 0.\n\\]Marginal density \\(X\\).\nMixture exponentials:\n\\[\n  f_X(x)\n  = \\sum_{y = 1}^3 p_Y(y)\\,\\lambda_y e^{-\\lambda_y x}\n  = 0.5\\,e^{-x} + 0.15\\,e^{-0.5x} + 0.05\\,e^{-0.25x}, \\quad x\\ge 0.\n\\]\\(\\Pr(Y = 1\\mid X\\le 2)\\).\nUse Bayes’ rule events:\n\\[\n  \\Pr(Y = 1\\mid X\\le 2)\n  = \\frac{\\Pr(Y = 1, X\\le 2)}{\\Pr(X\\le 2)}\n  = \\frac{p_Y(1)\\,\\Pr(X\\le 2\\mid Y = 1)}{\\sum_{y = 1}^3 p_Y(y)\\,\\Pr(X\\le 2\\mid Y = y)}.\n\\]\nexponential, \\(\\Pr(X\\le t\\mid Y = y) = 1 - e^{-\\lambda_y t}\\).\nThus,\n\\[\n  \\Pr(Y = 1, X\\le 2) = 0.5\\,(1-e^{-2}) = 0.4323323584,\n\\]\n\\[\n  \\Pr(X\\le 2)\n  = 0.5(1 - e^{-2}) + 0.3(1 - e^{-1}) + 0.2(1 - e^{-0.5})\n  = 0.7006623941.\n\\]\nTherefore,\n\\[\n  \\Pr(Y = 1\\mid X\\le 2)\n  = \\frac{0.4323323584}{0.7006623941}\n  \\approx 0.6170.\n\\]\\(\\Pr(Y = 1\\mid X\\le 2)\\).\nUse Bayes’ rule events:\n\\[\n  \\Pr(Y = 1\\mid X\\le 2)\n  = \\frac{\\Pr(Y = 1, X\\le 2)}{\\Pr(X\\le 2)}\n  = \\frac{p_Y(1)\\,\\Pr(X\\le 2\\mid Y = 1)}{\\sum_{y = 1}^3 p_Y(y)\\,\\Pr(X\\le 2\\mid Y = y)}.\n\\]\nexponential, \\(\\Pr(X\\le t\\mid Y = y) = 1 - e^{-\\lambda_y t}\\).\nThus,\n\\[\n  \\Pr(Y = 1, X\\le 2) = 0.5\\,(1-e^{-2}) = 0.4323323584,\n\\]\n\\[\n  \\Pr(X\\le 2)\n  = 0.5(1 - e^{-2}) + 0.3(1 - e^{-1}) + 0.2(1 - e^{-0.5})\n  = 0.7006623941.\n\\]\nTherefore,\n\\[\n  \\Pr(Y = 1\\mid X\\le 2)\n  = \\frac{0.4323323584}{0.7006623941}\n  \\approx 0.6170.\n\\]Interpretation.\nGiven component failed within \\(200\\) hours, likely mode mechanical; posterior probability \\(62\\)%, higher prior \\(50\\)% mechanical mode shortest expected lifetime (largest rate).Interpretation.\nGiven component failed within \\(200\\) hours, likely mode mechanical; posterior probability \\(62\\)%, higher prior \\(50\\)% mechanical mode shortest expected lifetime (largest rate).Answer Exercise ??.\\(= 1/3\\).find:\n\\[\n  F_Y(y) =\n  \\begin{cases}\n0.  & \\text{$t < 0$};\\\\\n1/3 & \\text{$t = 0$};\\\\\n(1 - t + 3t^2 - t^3)/3 & \\text{$0 < t < 2$};\\\\\n1 & \\text{$t \\ge 2$}.\n  \\end{cases}\n\\]","code":"\nset.seed(932649)\n\nNumRainEvents <- rpois(1,\n                       lambda = 0.78)\nMonthlyRain <- 0\n\nif ( NumRainEvents > 0) {\n     EventAmounts <- rgamma(NumRainEvents,\n                           shape = 0.5,\n                           scale = 6 )\n     MonthlyRain <- sum( EventAmounts )\n}\nset.seed(932649)\n\nMonthlyRain <- array(0,  \n                     dim = 1000)\n\nfor (i in 1:1000){\n NumRainEvents <- rpois(1,\n                       lambda = 0.78)\n MonthlyRain[i] <- 0\n\n  if ( NumRainEvents > 0) {\n       EventAmounts <- rgamma(NumRainEvents,\n                             shape = 0.5,\n                             scale = 6 )\n       MonthlyRain[i] <- sum( EventAmounts )\n  }\n}\n\n# Exact zeros:\nsum(MonthlyRain == 0) / 1000\n#> [1] 0.484\nmean(MonthlyRain)\n#> [1] 2.196798\nmean(MonthlyRain[MonthlyRain > 0])\n#> [1] 4.25736#> [1] 2.301363\nannualRainfallList <- array( NA, dim = 1000)\nfor (i in 1:1000){\n   rainAmounts <- array( 0, dim = 365) # Reset for each simulation\n\n    wetDays <- rbinom(365, size = 1, prob = 0.32) # 1: Wet day\n   locateWetDays <- which( wetDays == 1 )\n\n\n   rainAmounts[locateWetDays] <- rgamma( n = length(locateWetDays),\n                                         shape = 2,\n                                         scale = 20)\n   annualRainfallList[i] <- sum(rainAmounts)\n}\n# hist( annualRainfallList)\nDays <- 1 : 365\nprob <- (1 + cos( 2 * pi * Days/365) ) / 2.2\n\nannualRainfallList2 <- array( NA, dim = 1000)\nfor (i in 1:1000){\n\n  wetDays <- rbinom(365, size = 1, prob = prob) # 1: Wet day\n  \n  locateWetDays <- which( wetDays == 1 )\n  \n  rainAmounts2 <- array( 0, dim = 365)\n  \n  rainAmounts2[locateWetDays] <- rgamma( n = length(locateWetDays),\n                                         shape = 2,\n                                         scale = 20)\n   annualRainfallList2[i] <- sum(rainAmounts2)\n}\n#hist( annualRainfallList2)\nDays <- 1 : 365\n\nprobList <- array( dim = 365)\nprobList[1] <- 0.32\n\nannualRainfallList3 <- array( NA, dim = 1000)\n\nfor (i in 1:1000){\n  \n  rainAmounts <- array( dim = 365 )\n\n  for (day in 1:365) {\n    if ( day == 1 ) {\n      probList[1] <- 0.32\n      wetDay <- rbinom(1, \n                       size = 1, \n                       prob = probList[1]) # 1: Wet day\n      rainAmounts[1] <- rgamma( n = 1,\n                                shape = 2,\n                                scale = 20)\n\n    } else {\n    \n      probList[i] <- ifelse( rainAmounts[day - 1] == 0,\n                             0.15,\n                             0.55)\n      wetDay <- rbinom(1, \n                       size = 1, \n                       prob = probList[i]) # 1: Wet day\n\n      if (wetDay) {\n        rainAmounts[day] <- rgamma( n = 1,\n                                    shape = 2,\n                                    scale = 20)\n      } else {\n        rainAmounts[day] <- 0\n      }\n    }\n  }\n  annualRainfallList3[i] <- sum(rainAmounts)\n    \n}\n\n#hist( annualRainfallList3)"},{"path":"selected-solutions.html","id":"AnswersChapSampling","chapter":"E Selected solutions","heading":"E.12 Answers for Chap. 12","text":"Answer Exercise 12.3.\\(\\operatorname{E}(Y) = \\int_0^1 y(3y^2)\\, dy = 3/4\\).\n\\(\\operatorname{E}(Y^2) = 3/5\\), \\(\\operatorname{var}(Y) = 3/80\\).\nCLT, \\(\\overline{Y}\\sim N(3/4, 3/(80n) )\\).\nTherefore\n\\[\n   \\Pr\\left( 3/4 - \\sqrt{3/80} < \\overline{Y} < 3/4 + \\sqrt{3/80} \\right)\n   = \\Pr\\left( 0.56 < \\overline{Y} < 0.94 \\right)\n\\]\nneeded.\nsample size \\(n\\),\\[\n   \\Pr\\left( \\frac{0.56 - 0.75}{0.19/\\sqrt{n}} < Z < \\frac{0.56 - 0.75}{0.19/\\sqrt{n}} \\right)\n   = \\Pr(-\\sqrt{n} < Z < \\sqrt{n} )\n\\]\napproaches one \\(n\\\\infty\\).\nexample, \\(n = 10\\), \\(\\Pr(-\\sqrt{10} < Z < \\sqrt{10} ) = 0.9984\\).Answer Exercise 12.4.Let weight  \\(E\\), \\(E\\sim N(59, 0.7)\\).\nCLT, sample mean \\(\\overline{E} \\sim N(59, 0.7/20)\\).\n\n\\[\n   \\Pr(\\overline{E} > 59.5)\n   = \\Pr(Z > \\frac{59.5 - 59}{\\sqrt{0.7/20}})\n   = \\Pr(z > 2.67)\n   \\approx  0.003792562.\n\\]\n1. seek \\(\\Pr(s^2 > 1)\\).\nSince\n\\[\n   \\frac{(n - 1)s^2}{\\sigma^2}\\sim \\chi^2_{n - 1}\n\\]\n\\(n = 12\\) \\(\\sigma^2 = 0.7\\).\n\n\\[\\begin{align*}\n   \\Pr(s^2 > 1)\n   &= \\Pr\\left( \\frac{11 s^2}{0.7} > \\frac{11\\times 1}{0.7} \\right)\\\\\n   &=\\Pr( \\chi^2_{11} > 15.714)\\\\\n   &\\approx 0.152.\n\\end{align*}\\]Answer Exercise 12.5.Let number broken  \\(B\\), \\(B \\sim \\text{Pois}(0.2)\\).sample mean number (\\(n = 20\\)) broken distribution \\(\\overline{B}\\sim N(0.2, 0.2/20)\\) approx.\n\n\\[\n\\Pr(B\\ge 1)\n= \\Pr\\left( Z > \\frac{1 - 0.2}{\\sqrt{0.2/20}} \\right)\n= \\Pr(Z > 8) = 0.\n\\]contrast, single carton, probability one broken egg \n\\[\n\\Pr(B > 1) = 1 - \\Pr(B = 0) = 1 - 0.8187 = 0.181\n\\]\nusing Poisson distribution.Answer Exercise 12.6.\\(\\operatorname{E}(M) = 3/4\\).\\(\\operatorname{var}(M) = 3/80\\).\\(\\overline{M}\\sim N(3/4, 1/240)\\).\\(0.8788\\).","code":""},{"path":"selected-solutions.html","id":"AnswersOrderStatisticsChapter","chapter":"E Selected solutions","heading":"E.13 Answers for Chap. 13","text":"Answer Exercise 13.1.\nCompare density function Eq. (13.1) beta distribution density function.Answer Exercise 13.2.\nCompare expressions \\(\\operatorname{E}[X]\\) \\(\\operatorname{E}[X^2]\\) beta distribution density function.Answer Exercise 13.3.\n\\(\\displaystyle\n\\frac{n!}{(k - 1)!\\,(n - k)!} \\frac{(x + 1)^{k - 1} (1 - x)^{n - k}}{2^n}\\)\n\\(-1\\le x\\le 1\\).Answer Exercise 13.4.\n\\(\\displaystyle\nf_{(Y)}(y) = \\frac{n!}{(k - 1)! (n - k)!} \\frac{x^{2k - 1}}{2\\, 4^{k - 1}}\\left(1 - \\frac{x^2}{4}\\right)^{n - k}\\)\n\\(0 < x < 2\\).Answer Exercise 13.10.\nFix initial draw: \\(X = x\\).\nGiven value, \\(Y_i \\sim\\text{Unif}(0, 1)\\) independent others, probability \\(Y_i > x\\) \\(1 - x\\).\nfixed \\(x\\), number trials \\(N\\) first \\(Y > x\\) geometric distribution success probability \\(p = 1 - x\\), therefore\n\\[\n  \\operatorname{E}[X] = 1 / (1 - x).\n\\]\nfind overall expected value, integrate possible \\(x \\ [0, 1]\\):\n\\[\n    \\operatorname{E}[N] = \\operatorname{E}[1/(1 - X)] = \\int_0^1 \\frac{1}{1 - x}\\, dx,\n\\]\ndiverges.\nThus, \\(\\operatorname{E}[N] = \\infty\\).","code":""},{"path":"selected-solutions.html","id":"AnswersChapBayesianIntro","chapter":"E Selected solutions","heading":"E.14 Answers for Chap. 14","text":"Answer Exercise 14.1.\\(f(\\theta| y)\\propto f(y\\mid\\theta)\\times f(\\theta)\\), (given):\n\\[\nf(y\\mid \\theta) = (1 - \\theta)^y\\theta\n\\quad\\text{}\\quad\nf(\\theta) = \\frac{\\theta^{m - 1}(1 - \\theta)^{n - 1}}{B(m, n)}.\n\\]\nCombining :\\[\n  f(\\theta| y)\\propto \\theta^{(m + 1) - 1} (1 - \\theta)^{(n + y) - 1}.\n\\]\nbeta distribution parameters \\(m + 1\\) \\(n + y\\).Prior: \\(\\operatorname{E}(\\theta) = m / (m + n) = 1.2/(1.2 + 2) = 0.375\\).\n, \\(\\operatorname{E}(Y) = (1 - p)/p = 1.667\\).Posterior: \\(\\operatorname{E}(\\theta) = (m + 1)/(m + 1 + n + y) = 0.3056\\); slightly reduced.","code":""},{"path":"references.html","id":"references","chapter":"F References","heading":"F References","text":"","code":""}]
