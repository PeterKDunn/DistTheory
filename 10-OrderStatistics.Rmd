
# Order statistcs {#OrderStatisticsChapter}


::: {.objectivesBox .objectives data-latex="{iconmonstr-target-4-240.png}"}
On completion of this chapter, you should be able to:

* determine the sampling distributions of a given order statistics.
* explain how to find the distributions of order statistics.
* use order statistics in practical problems.
:::
:::



## Introduction  {#OrderStatisticsIntro}

\index{Order statistics|(}
In many applications, information about the minimum value, or the maximum value, in a sample is useful.
For example, knowing the maximum water height in a dam is useful for modelling controlled water releases; the mean water height is of little use here.
The minimum and maximum values in a sample are examples of *order statistics*.

More generally, order statistics refer to the values in a sample arranged in increasing order. 
For a given random sample $X_1, X_2, \dots, X_n$, the order statistics are defined and denoted as
$$
  X_{(1)} \le X_{(2)} \le \cdots \le X_{(n)}
$$
$X_{(k)}$ is the $k$th order statistic, the $k$th smallest value in a sample.

Important order statistics in a sample of size\ $n$ include:

* the minimum value: $X_{(1)}$.
* the maximum value: $X_{(n)}$.
* the range: $X_{(n)} - X_{(1)}$.

The median is also an important order statistic:
$$
   \text{median} = 
   \begin{cases}
      \displaystyle\frac{X_{(n/2)} + X_{(n/2 + 1)}}{2} & \text{for $n$ even;}\\
      X_{([n + 1]/2)}                     & \text{for $n$ is odd.}
  \end{cases}
$$
In some circumstances, the median for $n$\ even uses a slightly formula.


:::{.example #OrderedStatisticsMedianOddn name="Median: $n$ odd"}
Consider a sample of five die rolls, say $X_i$:
$$
  x_1 = 5; \quad x_2 = 3; \quad  x_3 = 5; \quad  x_4 = 2; \quad  x_5 = 6.
$$
The order statistics are 
$$
  x_{(1)} = 2; \quad  x_{(2)} = 3; \quad  x_{(3)} = 5; \quad  x_{(4)} = 5; \quad  x_{(5)} = 6.
$$
The minimum value is $X_{(1)} = 2$, and the maximum value is $X_{(5)} = 6$.
The median value is $X_{(3)} = 5$.
:::


:::{.example #OrderedStatisticsMedianEvenn name="Median: $n$ even"}
Consider a sample of six die rolls, say $Y_i$:
$$
  y_1 = 1; \quad y_2 = 4; \quad  y_3 = 6; \quad  y_4 = 2; \quad  y_5 = 5 \quad  y_6 = 1.
$$
The order statistics are 
$$
  y_{(1)} = 1; \quad  y_{(2)} = 1; \quad  y_{(3)} = 2; \quad  y_{(4)} = 4; \quad  y_{(5)} = 5; \quad  y_{(6)} = 6.
$$
The minimum value is $y_{(1)} = 1$, and the maximum value is $y_{(4)} = 6$.
The median value is $\left(X_{(3)} + X_{(4)}\right)/2 = (2 + 4)/2 = 3$, which is not one of the observations in the sample.
:::


As with any statistic, the order statistics have a sampling distribution.
The easiest order statistics for which to find the distribution are the maximum value (the $n$th order statistic; Sect.\ \@ref(OrderStatisticsMaxDistribution)) and the minimum value (first order statistic; Sect.\ \@ref(OrderStatisticsMinDistribution)).


## Sampling distribution of the maximum value {#OrderStatisticsMaxDistribution}

Consider a sample of size\ $n$, say $X_1, X_2, \dots, X_n$, with all $X_i$ drawn independently from a distribution with density function $f_X(x)$ and distribution function $F_X(x)$.
Then define $X_{(n)} = \operatorname{max}\{X_1, X_2, \dots, X_n\}$ as the maximum value in the sample.

The distribution function for $X_{(n)}$ is $F_{X_{(n)}}(x) = \Pr(X_{{(n)}} \le x)$, by definition: the probability that the maximum value is less than some given value\ $x$.
If the value of $X_{(n)}$ is less than or equal to some value\ $x$, then \emph{all} the values in the sample must be less than that value\ $x$.
Since the observations $X_1, X_2, \dots X_n$ are all independent and identically distributed, the probability that *all* observations are less than or equal to some value\ $x$ is
$$
   F_{X_{(n)}}(x) = [F_X(x)]^{n}
$$
for $x \in R_X$.

The density function then is found by differentiating $F_{X_{(n)}}(x)$ with respect to\ $x$ (Sect.\ \@ref(DistributionFunction)), to obtain
$$
   f_{X_{(n)}}(x) = n [F_X(x)]^{n - 1} \times f_X(x)
$$
for $x \in R_X$.


:::{.definition #OrderStatsMaximumDistn name="Distribution of the maximum value"}
Consider a sample of size\ $n$, say $X_1, X_2, \dots, X_n$, with all $X_i$ drawn independently from a continuous distribution with density function $f_X(x)$ and distribution function $F_X(x)$.
Then the distribution function for the maximum value (order statistic $X_{(n)}$) is
$$
   F_{X_{(n)}}(x) = 
       \begin{cases}
      0                   & \text{for $x \le 0$}\\
      [F_X(x)]^{n}        & \text{for $0 < x < 1$}\\
      1                   & \text{for $x \ge 1$.}
    \end{cases}
$$
and the density function is
$$
   f_{X_{(n)}}(x) = 
      n [F_X(x)]^{n - 1} \times f_X(x) 
$$
for $x\in R_X$.
:::


:::{.example #OrderStatsMaxUniform name="Uniform distribution: maximum value"}
For the continuous uniform distribution (Sect.\ \@ref(ContinuousUniform)) defined on $[0, 1]$, the density function is
\begin{equation}
   f_X(x; 0, 1)  = 1 \quad\text{for $0\le x\le 1$},
   (\#eq:ContinuousUniformPMForder)
\end{equation}
and the distribution function is
$$
  F_X(x; 0, 1) = 
  \begin{cases}
    0                & \text{for $x < 0$};\\
    \displaystyle x  & \text{for $0 \le x \le 1$};\\ 
    1                & \text{for $x > 1$}.
  \end{cases}
$$
Hence, the density function for the maximum value in a sample of size\ $n$ is, from Def.\ \@ref(def:OrderStatsMaximumDistn),
$$
  f_{X_{(n)}}(x) 
  = n\, x^{n - 1} \times 1
  = n \, x^{n - 1},
$$
for $0 < x < 1$.
In addition, the distribution function is, from Def.\ \@ref(def:OrderStatsMaximumDistn),
$$
  F_{X_{(n)}}(x) 
  = 
  \begin{cases}
    0       & \text{for $x < 0$}\\
    x^n     & \text{for $0 \le x \le 1$}\\
    1       & \text{for $x > 1$.}
  \end{cases}
$$

Using the density function of the maximum value as given above, the expected value and the variance of the maximum value can be found from their definitions; for example,
\begin{align*}
  \operatorname{E}[X_{(n)}]   &= \int_0^1 x \cdot n x^{n - 1}\, dx   = \frac{n}{n + 1};\\
  \operatorname{E}[X^2_{(n)}] &= \int_0^1 x^2 \cdot n x^{n - 1}\, dx = \frac{n}{n + 2};\\
  \operatorname{var}[X_{(n)}] &= \frac{n}{n + 2} - \left( \frac{n}{n + 1} \right)^2.
\end{align*}

For the case where $n = 10$, then:
$$
  f_{X_{(n)}}(x) 
  = 10 \, x^{9},
$$
for $0 < x < 1$.
In addition, the distribution function is, from Def.\ \@ref(def:OrderStatsMaximumDistn),
$$
  F_{X_{(n)}}(x) 
  = 
  \begin{cases}
    0               & \text{for $x < 0$}\\
    x^{10}          & \text{for $0 \le x \le 1$}\\
    1               & \text{for $x > 1$.}
  \end{cases}
$$
Fig.\ \@ref(fig:OrderStatsMax) shows the probability density function (left panel) and the distribution function (right panel).
For example, the probability that the largest value in a sample of size\ $10$ is smaller than\ $0.8$ is
$$
  F_{(10)}(0.8) = 0.8^{10} = 0.1073742\dots
$$

The expected value and variance of the maximum value are:
\begin{align*}
  \operatorname{E}[X_{(n)}]   &= \frac{10}{11} \approx 0.90909;\\
  \operatorname{var}[X_{(n)}] &= \frac{10}{12} - \left( \frac{10}{11} \right)^2\approx 0.006887.
\end{align*}
These results can be confirmed using a quick computer simulation:

```{r}
num_Sims <- 5000
sample_Size <- 10

x_Max <- array( dim = num_Sims)
for (i in 1:num_Sims){
  x <- runif(sample_Size,
             min = 0,
             max = 1)
  x_Max[i] <- max(x) 
}
cat("Mean of the maximum value", mean(x_Max), "\n")
cat("Variance of the maximum value", var(x_Max), "\n")
```

:::


```{r OrderStatsMax, echo=FALSE, fig.align="center", fig.cap="The distribution of the maximum value for the uniform distribution on $[0, 1]$ in a sample of size\\ $10$. Left: the probability density function of the maximum value $X_{(10)}$. Right: the distribution function of the maximum value $X_{(10)}$. The grey, dotted lines show $F_{(10)}(0.8)$.", fig.width=8, fig.height=4, out.width="90%"}

par(mfrow = c(1, 2))

x <- seq(0, 1,
         length = 1000)

f_Max <- function(x, n){ n * x ^ (n - 1)}
F_Max <- function(x, n){ x ^ n }


plot( f_Max(x, n = 10) ~ x, 
     type = "l",
     lwd = 3,
     col = plotColour1, 
     las = 1,
     axes = FALSE,
     main = expression( atop(Probability~density~"function",
                             of~italic(X)[(10)]) ),
     xlab = expression(italic(X)[(10)]),
     ylab = "Density")
axis(side = 1)
box()

plot( F_Max(x, n = 10) ~ x, 
     type = "l",
     lwd = 3,
     las = 1,
     col = plotColour1, 
     las = 1,
     main = expression( atop(Distribution~"function",
                             of~italic(X)[(10)]) ),
     xlab = expression(italic(X)[(10)]),
     ylab = "Distribution fn")

lines( x = c(0, 0.8),
       y = c(0.1073742, 0.1073742),
       lwd = 2,
       col = "grey", 
       lty = 2)
lines( x = c(0.8, 0.8),
       y = c(0, 0.1073742),
       lwd = 2,
       col = "grey", 
       lty = 2)
```



## Sampling distribution of the minimum value {#OrderStatisticsMinDistribution}

Consider again a sample of size\ $n$, say $X_1, X_2, \dots, X_n$, with all $X_i$ drawn independently from a distribution with density function $f_X(x)$ and distribution function $F_X(x)$.
Then define $X_{(1)} = \operatorname{min}\{X_1, X_2, \dots, X_n\}$ as the minimum value in the sample.

The distribution function for $X_{(1)}$ is $F_{X_{(1)}}(x) = \Pr(X_{{(1)}} \le x)$.
If the value of $X_{(1)}$ is less than or equal to some value\ $x$, then there must be *at least* one value less than that value\ $x$.
Alternatively, this means that we cannot have *all* the sample values larger than the value\ $x$ (as then there must be *none* that are less than\ $x$).

The probability that *all* values are larger than\ $x$ is $[1 - F_X(x)]^n$ (since all\ $n$ values have identical, independent distributions).
Hence, the probability that *all* values are *not* larger than\ $x$ is
$$
  1 - [1 - F_X(x)]^n
$$
for $x \in R_X$.
The density function is found by differentiating $F_{X_{(1)}}(x)$ with respect to\ $x$ (Sect.\ \@ref(DistributionFunction)), to obtain
$$
  f_{X_{(1)}}(x) = n [1 - F_X(x)]^{n-1} f_X(x)
$$
for $x \in R_X$.


:::{.definition #OrderStatsMinimumDistn name="Distribution of the minimum value"}
Consider a sample of size\ $n$, say $X_1, X_2, \dots, X_n$, with all $X_i$ drawn independently from a continuous distribution with density function $f_X(x)$ and distribution function $F_X(x)$.
Then the distribution function for the minimum value (order statistic $X_{(1)}$) is
$$
   F_{(X_1)}(x) = 1 - [1 - F_X(x)]^{n}
$$
and the density function is
$$
   f_{(X_1)}(x) = n [1 - F_X(x)]^{n - 1} \times f_X(x)
$$
for $x\in R_X$.
:::


:::{.example #OrderStatsMinUniform name="Uniform distribution: minimum value"}
For the continuous uniform distribution (Sect.\ \@ref(ContinuousUniform)) defined on $[0, 1]$, the density function and distribution function are giuven in Example\ \@ref(exm:OrderStatsMaxUniform).
Using these, the density function for the minimum value in a sample of size\ $n$ is
$$
  f_{X_{(1)}}(x) 
  = n(1 - x)^{n - 1} \times 1
  = n (1- x)^{n - 1},
$$
for $0 < x < 1$.
In addition, the distribution function is
$$
  F_{X_{(1)}}(x) 
  = 
  \begin{cases}
    0             & \text{for $x < 0$}\\
    1 - (1 - x)^n & \text{for $0 \le x \le 1$}\\
    1             & \text{for $x > 1$.}
  \end{cases}
$$

For the case where $n = 10$, 
$$
  f_{X_{(1)}}(x) 
  = 10 (1- x)^9,
$$
for $0 < x < 1$, and the distribution function is
$$
  F_{X_{(1)}}(x) 
  = 
  \begin{cases} 
    0                 & \text{for $x < 0$}\\
    1 - (1 - x)^{10}  & \text{for $0 \le x\le 1$}\\
    1                 & \text{for $x > 0$.}
  \end{cases}
$$
Figure\ \@ref(fig:OrderStatsMin) shows the probability density function (left panel) and the distribution function (right panel).
For example, the probability that the smallest value in a sample of size\ $10$ is smaller than\ $0.2$ is
$$
  F_{(1)}(0.2) = 1 - (1 - 0.2)^{10} = 0.8926\dots
$$
This can be confirmed using a small simulation:

```{r}
num_Sims <- 5000
sample_Size <- 10

x_Min <- array( dim = num_Sims)
for (i in 1:num_Sims){
  x <- runif(sample_Size,
             min = 0,
             max = 1)
  x_Min[i] <- max(x) 
}

cat("Proportion of smallest values less than 0.2: ", 
    round(sum(x_Min) / num_Sims, 3), "\n")
```


Using the density function of the minimum value, the expected value and the variance of the minimum value can be found too; for example,
$$
  \operatorname{E}[X_{(10)}] = \int_0^1 x \cdot 10(1 - x)^9\, dx = 1/11 = 0.0909\dots
$$
:::

```{r OrderStatsMin, echo=FALSE, fig.align="center", fig.cap="The distribution of the minimum value for the uniform distribution on $[0, 1]$ in a sample of size\\ $10$. Left: the probability density function of the minimum value $X_{(1)}$. Right: the distribution function of the minimum value $X_{(1)}$. The grey, dotted lines show $F_{X_{(1)}}(0.2)$.", fig.width=8, fig.height=4, out.width="90%"}

par(mfrow = c(1, 2))

x <- seq(0, 1,
         length = 1000)

f_Min <- function(x, n){ n * (1 - x) ^ (n - 1)}
F_Min <- function(x, n){ 1 - (1 - x) ^ n }


plot( f_Min(x, n = 10) ~ x, 
     type = "l",
     lwd = 3,
     col = plotColour1, 
     las = 1,
     axes = FALSE,
     main = expression( atop(Probability~density~"function",
                             of~italic(X)[(1)]) ),
     xlab = expression(italic(X)[(1)]),
     ylab = "Density")
axis(side = 1)
box()

plot( F_Min(x, n = 10) ~ x, 
     type = "l",
     lwd = 3,
     las = 1,
     col = plotColour1, 
     las = 1,
     main = expression( atop(Distribution~"function",
                             of~italic(X)[(1)]) ),
     xlab = expression(italic(X)[(1)]),
     ylab = "Distribution fn")

lines( x = c(0, 0.2),
       y = c(F_Min(0.2, n = 10), F_Min(0.2, n = 10) ),
       lwd = 2,
       col = "grey",
       lty = 2)
lines( x = c(0.2, 0.2),
       y = c(0, F_Min(0.2, n = 10) ),
       lwd = 2,
       col = "grey",
       lty = 2)

```




## Sampling distribution of the $k$th order statistic {#OrderStatisticsDistribution}

The ideas developed above for the distribution of the minimum and maximum value in a sample can be applied to other order statistics also.
Consider a sample from a continuous distribution with density function\ $f_X(x)$ and distribution function\ $F_X(x)$.
The $k$th order statistic $X_{(k)}$ has the distribution function $F_{X_(k)}(x) = \Pr(X_{(k)}\le x)$. 
By definition, this is the probability that the $k$th smallest value in the sample is less than (or equal to) some value\ $x$.

Writing $X_{(k)} \le x$ means that *at least*\ $k$ of the\ $n$ observations are smaller than the value\ $x$; that is, $k, k + 1, \dots n$ of the\ $n$ observations are smaller than\ $x$.

First, consider the case where exactly\ $k$ observations are smaller than\ $x$.
This means there must be $k$\ observations with a value less than or equal to $x$; the probability of this is
$$
  [F_X(x)]^k
$$
In addition, the remaining $n - k$ observations must have a value *larger* than $x$; the probability of this is
$$
  [1 - F_{X}(x)]^{n -k}
$$
The\ $k$ observations that are smaller than\ $x$ could be located in many different places among the\ $n$ values; these\ $k$ observations could be in $\binom{n}{k}$ different arrangements among the sample of size\ $n$.

Combining, the probability that exactly\ $k$ observations are smaller than\ $x$ is
$$
  \binom{n}{k} \cdot [F_X(x)]^k \cdot [1 - F_{X}(x)]^{n - k}.
$$

However, as noted above, there may be *more* than\ $k$ observations smaller than\ $x$ also.
Following the above ideas, the probability that exactly\ $k + 1$ observations are smaller than\ $x$ is
$$
  \binom{n}{k + 1} \cdot [F_X(x)]^{k+1} \cdot [1 - F_{X}(x)]^{n - (k+1)}
$$
and so on, up to all\ $n$ observations being less than\ $x$.
Summing these probabilities gives
$$
  \Pr(X_{(k)} \le x) = \sum_{i = k}^n \binom{n}{i} \cdot [F_X(x)]^i \cdot [1 - F_{X}(x)]^{n - i}.
$$

From this distribution function, the probability density function can be obtained as
$$
  f_{X_{(k)}}(x) 
  = \frac{d}{dx} F_{X_{(k)}}(x) 
  = \frac{n!}{(k - 1)!\,(n - r)!}f_{X}(x) [F_{X}(x)]^{k - 1} [1 - F_{X}(x)]^{n - r},
$$
for $n > k \ge 1$ and\ $n$ and\ $k$ are integers.


:::{.definition #OrderStatsDistn name="Distribution of an order statistic: continuous rv"}
Consider a sample of size\ $n$, say $X_1, X_2, \dots, X_n$, with all $X_i$ drawn independently from a continuous distribution with density function $f_X(x)$ and distribution function $F_X(x)$.
Then the distribution function for the $k$th order statistic $X_{(k)}$ is
\begin{equation}
  F_{X_{(k)}}(x) = \Pr(X_{(k)}\le x) 
  = 
  \sum_{i = k}^n \binom{n}{k}\times [F_{X}(x)]^i \times [1 - F_{X}(x)]^{n - i}
   (\#eq:OrderStatisticDF)
\end{equation}
and the density function of the $k$th order statistic $X_{(k)}$ 
is:
\begin{equation}
   f_{X_{(k)}}(x) = \frac{n!}{(k - 1)! (n - k)!} [F_X(x)]^{k-1} [1 - F_X(x)]^{n-k} f(x),
   (\#eq:OrderStatisticPDF)
\end{equation}
where $n > k > 0$ and\ $n$ and\ $k$ are integers.
:::


::: {.example #UniformkOrderStats name="Uniform distribution: other order statistics"}
Consider the second and sixth order statistics from sample of size $n = 10$ from a Unif$(0, 1)$ distribution (where $f_X(x)$ and $F_X(x)$ are given in Example\ \@ref(exm:OrderStatsMaxUniform)); using Equation\ \@ref(eq:OrderStatisticPDF):
\begin{align*}
   f_{X_{(2)}}(x) 
   &= \frac{10!}{1!\, 8!} x (1 - x)^8
    = 90 x (1 - x)^8\quad\text{and}\\
   f_{X_{(6)}}(x) 
   &= \frac{10!}{5!\, 4!} x^5 (1 - x)^4
    = 1260 x^5 (1 - x)^4.
\end{align*}

Consider simulating $5\,000$ samples of size $n = 10$ from a Unif$(0, 1)$ distribution:

```{r OrderStatsSimOnly}
n <- 10   # Take samples of size 10

samples2 <- replicate(5000,               # 1000 repetitions
                      sort(runif(n))[2])  # 2nd order statistic
samples6 <- replicate(5000,               # 1000 repetitions
                      sort(runif(n))[6])  # 6th order statistic
```

The distribution of $X_{(2)}$ (i.e., `samples2`) and $X_{(6)}$ (i.e., `samples6`) are shown in Fig.\  \@ref(fig:OrderStats1).
:::


```{r OrderStats1, echo=FALSE, fig.align="center", fig.cap="Simulating the distribution of two order statistics for the uniform distribution on $[0, 1]$ with a sample of size $10$. Left: the distribution of $X_{(2)}$. Right: the distribution of  $X_{(6)}$. The solid lines show the theoretical distributions.", fig.width=8, fig.height=4, out.width="90%"}
set.seed(123)  # For reproducibility

<<OrderStatsSimOnly>>

os2 <- function(x){ 90 * x * (1 - x)^8 }
os6 <- function(x){ 1260 * x^5 * (1 - x)^4 }

x_Plot <- seq(0, 1,
              length = 500)
breaks <- seq(0, 1,
              by = 0.05)

par(mfrow = c(1, 2))

MASS::truehist(samples2, 
     col = plotColour1, 
     breaks = breaks,
     xlim = c(0, 1),
     las = 1,
     main = expression(Distribution~of~italic(X)[(2)]),
     xlab = expression(italic(X)[(2)]),
     ylab = "Density")
lines( os2(x_Plot) ~ x_Plot,
       lwd = 2)

MASS::truehist(samples6, 
     col = plotColour1, 
     breaks = breaks,
     xlim = c(0, 1),
     las = 1,
     main = expression(atop(Distribution~of~italic(X)[(6)]),
                            "("*the~maximum~value*")"),
     xlab = expression(italic(X)[(6)]),
     ylab = "Density")
lines( os6(x_Plot) ~ x_Plot,
       lwd = 2)
```


:::{.example #ServerRoom name="Air conditioner servicing"}
A new server room has four air-conditioning units installed. 
The lifetime of each unit (in hours) follows an exponential distribution with mean $\lambda = 2000\hs$. 
The units operate independently.
A service technician is called when *two* of the units fail. 
We seek the distribution of the time until the call to the technician is made.

Let $T_1, T_2, T_3, T_4$ denote the lifetimes of the four units. 
Then:
$$
  T_i \sim \text{Exponential}(\lambda)\quad\text{for $i = 1, 2, 3, 4$},
$$
where $\lambda = 1/2000$, and the $T_i$ are independent.
That is,
$$
  f_{T_i}(t)= \frac{1}{\lambda}\exp(-t/\lambda)
  \quad\text{and}\quad
  F_{T_i}(t)= \exp(-t/\lambda)
$$
for $0 < x < \infty$.

Define the order statistics as $T_{(1)} < T_{(2)} < T_{(3)} < T_{(4)}$. 
The technician is called at time \( T_{(2)} \), the *second failure*.
Since we have $n = 4$ and $k = 2$, the distribution of $T_{(2)}$ is (from REF)
$$
   f_{T_{(2)}}(t) = 
   6 [\exp(-t/\lambda)]^1 [1 - \exp(-t/\lambda)] ^2 \times \frac{1}{\lambda}\exp(-t/\lambda),
$$
which is very complicated.

However, the exponential distribution has the *memoryless property* (REF), so a different approach is much simpler.
The time to the *first* failure $T_{(1)}$ has the probability density function
$$
   T_{(1)} \sim \text{Exponential}(4\lambda)
$$
since *any* of the $4$\ units can fail first.
The time *between* the first and second failure also has an exponential distribution, due to the memoryless property, this means that
$$
  T_{(2)} - T_{(1)} \sim \text{Exponential}(3\lambda)
$$
because only $3$\ units remain after the first failure.

Thus, the total time until the second failure is:
$$
  T_{(2)} = E_1 + E_2
$$
where
$$
  E_1 \sim \text{Exp}(4\lambda) \quad\text{and}\quad E_2 \sim \text{Exp}(3\lambda).
$$

This distribution is not exponential, but the *sum of two independent exponentials with different rates*.

The expected value is
$$
\operatorname{E}[T_{(2)}] = \frac{1}{4\lambda} + \frac{1}{3\lambda} = \frac{1}{\lambda} \left( \frac{1}{4} + \frac{1}{3} \right) = 2000 \cdot \frac{7}{12} = 1166.6\dots\hs.
$$

On average, a technician will be called after about\ $1166.67\hs$.
:::



The PDF of $T_{(2)}$ can be found using **convolution**:
$$
  f_{T_{(2)}}(t) = \int_0^t f_{E_1}(x) f_{E_2}(t - x)\, dx
$$
where $f_{E_1}(x) = 4\lambda e^{-4\lambda x}$, and $f_{E_2}(x) = 3\lambda e^{-3\lambda x}$.
This approach can be extended to *any* $k$th failure out of\ $n$ components.


## Discrete random variables {#OrderStatisticsDiscrete}

Working with order statistics is easier with continuous random variables than for discrete random variables, because ties are not possible for continuous random variables (since $\Pr(X = x) = 0$ for some value\ $x$ when $X$ is a continuous random variable).
As a result, the formulas for discrete random variables are more complicated than for continuous random variables, since they need to accommodate the possibility of ties in the sample.

Let $X_1, X_2, \dots, X_n$ be i.i.d.\ from a discrete distribution with probability mass function
$$
   p_X(x) = \Pr(X = x),
$$
and distribution function
$$
   F_X(x) = \Pr(X \le x).
$$
In the discrete case, defining
$$
   F_X(x^-) = \lim_{t \uparrow x} F_X(t) = \Pr(X < x).
$$
proves helpful in what follows (see Fig.\ \@ref(fig:OrderStatsDiscreteDefine)).


```{r OrderStatsDiscreteDefine, echo=FALSE, fig.align="center", fig.cap="Explaining the meaning of $F_X(a^-)$.", fig.width=8, fig.height=4, out.width="90%"}


par(mar = c(5, 5, 4, 1) + 0.1) # Left side: more space
plot( x = c(-2.25, 1.25),
      y = c(-1.25, 1.25),
      type = "n",
      axes = FALSE,
      xlab = expression(Values~of~italic(X)),
      ylab = "Distribution function\n"
      )
axis(side = 2,
     at = c(1, 0),
     las = 1,
     label = c( expression(italic(F)[italic(X)](italic(a))),
                expression(italic(F)[italic(X)](italic(a^"\u2013"))) ) )
axis(side = 1,
     at = 0,
     label = expression(italic(a)) )
box()

abline(v = 0,
       lwd = 1,
       col = "grey",
       lty = 3)

# Left side
lines( x = c( -1, 0),
       y = c(0, 0),
       lwd = 3)
lines( x = c( -2, -1),
       y = c(-1, -1),
       lwd = 3)

lines( x = c( -2, -2.25),
       y = c(-1, -1),
       lwd = 3,
       lty = 2)

points(x = 0,
       y = 0,
       cex = 1.25,
       pch = 1)  # Open circle
points(x = -1,
       y = 0,
       cex = 1.25,
       pch = 19)  # Closed circle

# Right side
lines( x = c( 0, 1),
       y = c(1, 1),
       lwd = 3)
lines( x = c( 1, 1.25),
       y = c(1, 1),
       lwd = 3,
       lty = 2)

points(x = 0,
       y = 1,
       cex = 1.25,
       pch = 19)  # Closed circle
points(x = -1,
       y = -1,
       cex = 1.25,
       pch = 1)  # Open circle

# Explanatory info
arrows( x0 = -0.5,
        x1 = -0.05,
        y0 = 0.9,
        y1 = 1,
        angle = 15,
        length = 0.15)
text(x = -0.5,
     y = 0.9,
     label = expression(italic(F)[italic(X)](italic(a))),
     pos = 2)

arrows( x0 = 0.5,
        x1 = 0.05,
        y0 = 0.1,
        y1 = 0,
        angle = 15,
        length = 0.15)
text(x = 0.5,
     y = 0.1,
     label = expression(italic(F)[italic(X)](italic(a^"\u2013"))), #\u2013 is en-dash; \u2014 is em-dash
     pos = 4)

```


We begin with the first order statistic (the minimum value) to establish the general ideas of working with discrete random variables.
The first order statistic is $X_{(1)} = \min\{ X_1, X_2, \dots, X_n \}$; then, following similar logic to the continuous case,
\begin{align*}
F_{X_{(1)}}(x) 
  &= \Pr(X_{(1)} \le x) \\
  &= 1 - \Pr(X_{(1)} > x) \\
  &= 1 - \Pr(X_1 > x, X_2 > x, \dots, X_n > x) \\
  &= 1 - \big[ 1 - F_X(x) \big]^n.
\end{align*}
The probability mass function is the probability that all observations are at least\ $x$, but not all are greater than\ $x$:
\begin{align*}
  p_{X_{(1)}}(x) 
    &= \Pr(X_{(1)} \ge x) - \Pr(X_{(1)} \ge x+1) \\
    &= \big[ 1 - F_X(x^-) \big]^n - \big[ 1 - F_X(x) \big]^n.
\end{align*}


More generally, for the $k$th order statistic\ $X_{(k)}$ for a discrete random variable\ $X$,  let $X_1, X_2, \dots, X_n$ be i.i.d.\ from a discrete distribution with
\begin{align*}
  p_X(x)   &= \Pr(X = x);\\
  F_X(x)   &= \Pr(X \le x); \text{and}\\
  F_X(x^-) &=\Pr(X < x).
\end{align*}

First, define the *number* of observations less than or equal to\ $x$ as
$$
  Y = \sum_{j = 1}^n I\{X_j\le x\} \sim \mathrm{Binom}\!\left(n,\,F_X(x)\right).
$$
Then
\begin{align*}
  F_{X_{(k)}}(x)
  &= \Pr\bigl(X_{(k)} \le x\bigr)
   = \Pr(Y \ge k) \\
  &= \sum_{i = k}^n \binom{n}{i}\,F_X(x)^{i}\,[1 - F_X(x)]^{\,n - i}.
\end{align*}

For a discrete random variable\ $X$, the probability mass function is how much the distributions function 'jumps' at the values of\ $x$.
So, the probability mass function for $X_{(k)}$ can be found from the distribution function as
\begin{align*}
  p_{X_{(k)}}(x)
    &= \Pr\bigl(X_{(k)}=x\bigr)\\
    &= F_{X_{(k)}}(x) - F_{X_{(k)}}(x^-) \notag\\
    &= \sum_{i=k}^n \binom{n}{i}\,[F_X(x)]^{i}\,[1-F_X(x)]^{\,n-i}
     \;- \notag\\
    &\qquad \qquad
       \sum_{i = k}^n \binom{n}{i}\,[F_X(x^-)]^{i}\,[1 - F_X(x^-)]^{\,n - i}.
\end{align*}


:::{.definition #OrderStatsDistn name="Distribution of an order statistic: discrete rv"}
Consider a sample of size\ $n$, say $X_1, X_2, \dots, X_n$, with all $X_i$ drawn independently from a discrete distribution with probability mass function $f_X(x)$ and distribution function $F_X(x)$.
Then the distribution function for the $k$th order statistic $X_{(k)}$ is
\begin{equation}
  F_{X_{(k)}}(x) 
  = \Pr\bigl(X_{(k)} \le x\bigr)
  = \sum_{i = k}^n \binom{n}{i}\,[F_X(x)]^{i}\,[1 - F_X(x)]^{\,n - i}
  (\#eq:OrderStatisticDFdiscrete)
\end{equation}
and the density function of the $k$th order statistic $X_{(k)}$ is:
\begin{equation}
  p_X(x) 
    = \sum_{i=k}^n \binom{n}{i}\,[F_X(x)]^{i}\,[1-F_X(x)]^{\,n-i}
     \;-
       \sum_{i = k}^n \binom{n}{i}\,[F_X(x^-)]^{i}\,[1 - F_X(x^-)]^{\,n - i}.
   (\#eq:OrderStatisticPMF)
\end{equation}
where $n > k > 0$ and\ $n$ and\ $k$ are integers.
:::


:::{.example #DiscreteOrderStatsDiceMin name="Rolling a fair die"}  
Suppose a fair die is rolled $n = 10$ times, and the number\ $X$ appearing is recorded.
Then $X \sim\text{Unif}(1, 6)$ and so
$$
  p_X(x)
  = 
  \begin{cases}
    1/6 & \text{for $x = 1, 2, \dots 6$}\\
    0   & \text{elsewhere}
  \end{cases}
  \qquad
  \text{and}
  \qquad
  F_X(x)
  = 
  \begin{cases}
    0                   & \text{for $x < 1$}\\
    \lfloor x \rfloor/6 & \text{for $1 \le x \le 6$}\\
    1                   & \text{for $x > 6$.}
  \end{cases}
$$

Then the distribution function for the minimum $X_{(1)}$ is
\begin{align*}
  \Pr\big(X_{(1)} \ge k\big) 
  &= \big[1 - F_X(k - 1)\big]^{10} \\
  &= 
  \begin{cases}
    0 & \text{for $k < 1$}\\
    \displaystyle 1 - \left[1 -\frac{\lfloor{k - 1}\rfloor}{6}\right]^{10} & \text{for $1 \le k\le 6$}\\
    1 & \text{for $k > 6$}
  \end{cases}
\end{align*}
for $1 \le k\le 6$.
If we focus on the distribution function at *integer outcomes* only,
\begin{align*}
  F_{X_{(1)}}(x) 
  &= 
  1 - \Pr\big(X_{(1)} \ge k\big)\\
  &= 
  \left(\frac{6 - x}{6}\right)^{10} \qquad\text{at $x = 1, 2\dots, 6$}.
\end{align*}
Hence the probability mass function (for $k = 1, \dots, 6$) is:
$$
  \Pr\left(X_{(1)} = k\right) 
  = \left(\frac{7 - k}{6}\right)^{10} - \left(\frac{6 - k}{6}\right)^{10}.
$$
These can be computed in **R**, then plotted (the dots in Fig.\ \@ref(fig:OrderStatsDiscreteExample)):


```{r OrderStatsDiscreteExampleSetupTheory}
### Theoretical probabilities
k <- 1:6
CDF <- c(0,
         1 - ((6 - k)/6)^10)
round(CDF, 4)

PMF <- diff(CDF)
round(PMF, 4)
```

The situation can be simulated and plotted also (the bars in Fig.\ \@ref(fig:OrderStatsDiscreteExample)): 

```{r OrderStatsDiscreteExampleSetupSim}
set.seed(62004) # For reproducibility

# Replicate many sets of 10 rolls of a die
rolls_In_Set <- 10  # Sets f 10 rolls
num_Sims <- 5000    # Number of simulations

rolls <- sample( x = 1:6,
                 size = rolls_In_Set * num_Sims,
                 replace = TRUE)
                 
# Arrange each set of 10 as a row 
rolls <- matrix(data = rolls,
                nrow = num_Sims)

# Find the minimum in each row (i.e., each set of 10 rolls)
min_Roll <- apply(rolls,
                  MARGIN = 1,
                  FUN = "min")
min_Roll <- factor(min_Roll, 
                   levels = c(1:6))
```
:::


:::{.example #DiscreteOrderStatsDiceAny name="Rolling a fair die"}  
Continuing the previous example, the general $r$-th order statistic $X_{(r)}$ is 
$$
  F_{X_{(r)}}(x)
  = \Pr\big(X_{(r)} \le k\big)
  = \sum_{i = r}^{10} \binom{10}{i}\,\left(\frac{k}{6}\right)^{i}\,\left(1 - \frac{k}{6}\right)^{10 - i},
$$
for $k\in\{1,\dots,6\}$.
The corresponding probability mass function is the discrete difference
\begin{align*}
  \Pr\big(X_{(r)} =  k\big) 
  &=
  \Pr\big(X_{(r)}\le k\big) - \Pr\big(X_{(r)}\le k - 1\big)\\
  &= \sum_{i = r}^{10} \binom{10}{i}\,\left(\frac{k}{6}\right)^{i}\,\left(1 - \frac{k}{6}\right)^{10 - i} -\\
  &\phantom{={}}
     \sum_{i = (r - 1)}^{10} \binom{10}{i}\,\left(\frac{k}{6}\right)^{i}\,\left(1 - \frac{k}{6}\right)^{10 - i}\\
  &=  \binom{10}{r}\,\left(\frac{k}{6}\right)^{r}\,\left(1 - \frac{k}{6}\right)^{10 - r}.
\end{align*}
So, for instance, the seventh order statistic (i.e., where $r = 7$) is
$$
  \Pr\big(X_{(7)} = k\big)
  =
  \binom{10}{7}\left(\frac{k}{6}\right)^{7}\left(1 - \frac{k}{6}\right)^{10 - 7}
  =
  \binom{10}{7}\left(\frac{k}{6}\right)^{7}\left(\frac{6 - k}{6}\right)^{3}.
$$
CHECK; SIMULATE!!!!!!!!!!!
:::





```{r OrderStatsDiscreteExample, echo=FALSE, fig.align="center", fig.cap="The distribution of the first order statistic (the minimum roll), in sets of $10$\\ rolls. The bars show the values from $5000$ simulated sets of $10$\\ rolls ; the solid dots are the theoretical values.", fig.width=8, fig.height=4, out.width="90%"}

<<OrderStatsDiscreteExampleSetupTheory>>
<<OrderStatsDiscreteExampleSetupSim>>

### Graph
out <- barplot(table(min_Roll)/num_Sims,
               xlim = c(0, 7),
               ylim = c(0, 0.9),
               main = "The distribution of the minimum\n roll, in sets of 10 rolls",
               xlab = "Minimum roll in 10 rolls", 
               ylab = "Proportion",
               col = plotColour1,
               las = 1)

points( y = PMF, 
        x = out,
        cex = 1.25,
        pch = 19)

```


## Joint distribution

$$
 f_{X_{(1)}, X_{(2)}, \dots, X_{(n)}}(x_1, x_2, \dots, x_n)  
 =
 \begin{cases}
    n !\, f_{X_{(1)}} \times f_{X_{(2)}}\times \cdots \times f_{X_{(n)}} & \text{for $x_{(1)} < x_{(2)} < \cdots < x_{(n)}$}\\
    0 & \text{elsewhere}
 \end{cases}
$$


## Finishing up...

Order statistics are fundamental in probability and statistics. 
They underlie robust estimation, reliability analysis, and form the basis of many intuitive statistics.

\index{Order statistics|)}


## Exercises  {#OrderStatisticsExercises}

:::{.exercise #OrderStatisticsUniform}
Prove that the probably density function for the order statistics in the case of the uniform distribution, shown in Eq.\ \@ref(eq:ContinuousUniformPMForder), is a valid density function.
(Hint: observe the similarity between Eq.\ \@ref(eq:ContinuousUniformPMForder) and the beta density function in Sect.\ \@ref(BetaDistribution).)
:::


:::{.exercise #OrderStatisticsUniformExpectations}
Prove that the expected value and variance of the order statistics in the case of the uniform distribution are
$$
  \operatorname{E}[X] = k/(n + 1)\qquad\text{and}\qquad \operatorname{var}[X] = \frac{k (k + 1)}{(n + 1)(n + 2)},
$$
as in Sect.\ \@ref(OrderStatisticsDistribution).
(Hint: observe the similarity between Eq.\ \@ref(eq:ContinuousUniformPMForder) and the beta density function in Sect.\ \@ref(BetaDistribution).)
:::



:::{.exercise #OrderStatisticsPDF}
Find the distribution of the order statistics for the uniform distribution defined on $[-1, 1]$.
:::


:::{.exercise #OrderStatisticsPDF2}
Find the distribution of the order statistics for the distribution
$$
  f_Y(y) = y/2\quad\text{for $0 < y < 2$}
$$
and zero elsewhere.
:::


:::{.exercise #OrderStatisticsExponential}
Suppose a sample of\ $n$ independent values is taken from the exponential distribution $\text{Exp}(\lambda)$ (REF).

1. Determine the distribution function for the minimum value, $X_{(1)}$.
2. Hence determine the density function for the minimum value, $X_{(1)}$.
3. What is the probability that the minimum value in a sample of size\ $n$ is larger than $\lambda/2$?
4. Compute the expected value of the minimum value in the sample.
:::


:::{.exercise #OrderStatisticsExponential2}
Suppose a sample of\ $n$ independent values is taken from the exponential distribution $\text{Exp}(\lambda)$ (REF).

1. Determine the distribution function for the maximum value, $X_{(n)}$.
2. Hence determine the density function for the maximum value, $X_{(n)}$.
3. What is the probability that the maximum value in a sample of size\ $n$ is larger than $\lambda/2$?
4. Compute the expected value of the maximum value in the sample.
:::

:::{.exercise}
A new computer server room has four air-conditioning units installed.
The lifetime of each unit follows an exponential, with a mean lifetime of $2000\hs$.
When two units fail, a service technician is called.

1. Describe the distribution showing the wait time until a technician need to be called. 
:::



:::{.exercise #OrderStatsDiscreteSpecialCases}
Consider Equation\ \@ref(eq:OrderStatsDiscrete), which gives the $k$th order statistic for a discrete random variable.

1. Show that the expression given for the probability mass function for the *minimum* value is given by using $k = 1$ in Equation\ \@ref(eq:OrderStatsDiscrete).
1. Use Equation\ \@ref(eq:OrderStatsDiscrete) to find a formula for the probability mass function for the *maximum* value.
:::



:::{.exercise #OrderStatsDiscreteMultinomial}
The probability mass function for the $k$th order statistic can be found in a different way than shown in Sect.\ \@ref(OrderStatisticsDiscrete).
First, define 
\begin{align*}
  a &= F_X(x^-) = \Pr(X < x);\\
  b & = p_X(x) = \Pr(X = x); \text{and}\\
  c &= 1 - a - b = \Pr(X > x),
\end{align*}
and use $(R, S, T)$ to denote the *number* of observations that are less than, equal to, and greater than\ $x$, respectively.

1. Show that $(R,S,T) \sim \mathrm{Multinomial}\big(n; a,b,c\big)$.
2. Consider the discrete random variable\ $X$, where
  $$
    p_X(x) =
    \begin{cases}
      0.2 & \text{for $x = 1$};\\
      0.5 & \text{for $x = 2$};\\
      0.3 & \text{for $x = 3$};\\
      0   & \text{elsewhere.}
    \end{cases}
  $$
     Show that the above expressions and Equation\ \@ref(eq:OrderStatsDiscrete) are equivalent for the case $n = 3$ and finding the second order statistic $X_{(2)}$.
:::