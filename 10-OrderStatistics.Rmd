
# Order statistcs {#OrderStatisticsChapter}


::: {.objectivesBox .objectives data-latex="{iconmonstr-target-4-240.png}"}
On completion of this chapter, you should be able to:

* determine the sampling distributions of a given order statistics.
* explain how to find the distributions of order statistics.
* use order statistics in practical problems.
:::
:::



## Introduction  {#OrderStatisticsIntro}

\index{Order statistics|(}
In many applications, information about the minimum value, or the maximum value, in a sample is useful.
For example, knowing the maximum water height in a dam is useful for modelling controlled water releases; the mean water height is of little use here.
The minimum and maximum values in a sample are examples of *order statistics*.

More generally, order statistics refer to the values in a sample arranged in increasing order. 
For a given random sample $X_1, X_2, \dots, X_n$, the order statistics are defined and denoted as
$$
  X_{(1)} \le X_{(2)} \le \cdots \le X_{(n)}
$$
$X_{(k)}$ is the $k$th order statistic, the $k$th smallest value in a sample.

Important order statistics in a sample of size\ $n$ include:

* the minimum value: $X_{(1)}$.
* the maximum value: $X_{(n)}$.
* the range: $X_{(n)} - X_{(1)}$.

The median is also an important order statistic:
$$
   \text{median} = 
   \begin{cases}
      \displaystyle\frac{X_{(n/2)} + X_{(n/2 + 1)}}{2} & \text{for $n$ even;}\\
      X_{([n + 1]/2)}                     & \text{for $n$ is odd.}
  \end{cases}
$$
In some circumstances, the median for $n$\ even uses a slightly formula.


:::{.example #OrderedStatisticsMedianOddn name="Median: $n$ odd"}
Consider a sample of five die rolls, say $X_i$:
$$
  x_1 = 5; \quad x_2 = 3; \quad  x_3 = 5; \quad  x_4 = 2; \quad  x_5 = 6.
$$
The order statistics are 
$$
  x_{(1)} = 2; \quad  x_{(2)} = 3; \quad  x_{(3)} = 5; \quad  x_{(4)} = 5; \quad  x_{(5)} = 6.
$$
The minimum value is $X_{(1)} = 2$, and the maximum value is $X_{(5)} = 6$.
The median value is $X_{(3)} = 5$.
:::


:::{.example #OrderedStatisticsMedianEvenn name="Median: $n$ even"}
Consider a sample of six die rolls, say $Y_i$:
$$
  y_1 = 1; \quad y_2 = 4; \quad  y_3 = 6; \quad  y_4 = 2; \quad  y_5 = 5 \quad  y_6 = 1.
$$
The order statistics are 
$$
  y_{(1)} = 1; \quad  y_{(2)} = 1; \quad  y_{(3)} = 2; \quad  y_{(4)} = 4; \quad  y_{(5)} = 5; \quad  y_{(6)} = 6.
$$
The minimum value is $y_{(1)} = 1$, and the maximum value is $y_{(4)} = 6$.
The median value is $\left(X_{(3)} + X_{(4)}\right)/2 = (2 + 4)/2 = 3$, which is not one of the observations in the sample.
:::


As with any statistic, the order statistics have a sampling distribution.
The easiest order statistics for which to find the distribution are the maximum value (the $n$th order statistic; Sect.\ \@ref(OrderStatisticsMaxDistribution)) and the minimum value (first order statistic; Sect.\ \@ref(OrderStatisticsMinDistribution)).


## Sampling distribution of the maximum value {#OrderStatisticsMaxDistribution}

Consider a sample of size\ $n$, say $X_1, X_2, \dots, X_n$, with all $X_i$ drawn independently from a distribution with density function $f_X(x)$ and distribution function $F_X(x)$.
Then define $X_{(n)} = \operatorname{max}\{X_1, X_2, \dots, X_n\}$ as the maximum value in the sample.

The distribution function for $X_{(n)}$ is $F_{X_{(n)}}(x) = \Pr(X_{{(n)}} \le x)$, by definition: the probability that the maximum value is less than some given value\ $x$.
If the value of $X_{(n)}$ is less than or equal to some value\ $x$, then \emph{all} the values in the sample must be less than that value\ $x$.
Since the observations $X_1, X_2, \dots X_n$ are all independent and identically distributed, the probability that *all* observations are less than or equal to some value\ $x$ is
$$
   F_{X_{(n)}}(x) = [F_X(x)]^{n}
$$
for $x \in R_X$.

The density function then is found by differentiating $F_{X_{(n)}}(x)$ with respect to\ $x$ (Sect.\ \@ref(DistributionFunction)), to obtain
$$
   f_{X_{(n)}}(x) = n [F_X(x)]^{n - 1} \times f_X(x)
$$
for $x \in R_X$.


:::{.definition #OrderStatsMaximumDistn name="Distribution of the maximum value"}
Consider a sample of size\ $n$, say $X_1, X_2, \dots, X_n$, with all $X_i$ drawn independently from a distribution with density function $f_X(x)$ and distribution function $F_X(x)$.
Then the distribution function for the maximum value (order statistic $X_{(n)}$) is
$$
   F_{X_{(n)}}(x) = 
       \begin{cases}
      0                   & \text{for $x \le 0$}\\
      [F_X(x)]^{n}        & \text{for $0 < x < 1$}\\
      1                   & \text{for $x \ge 1$.}
    \end{cases}
$$
and the density function is
$$
   f_{X_{(n)}}(x) = 
      n [F_X(x)]^{n - 1} \times f_X(x) 
$$
for $x\in R_X$.
:::


:::{.example #OrderStatsMaxUniform name="Uniform distribution: maximum value"}
For the continuous uniform distribution (Sect.\ \@ref(ContinuousUniform)) defined on $[0, 1]$, the density function is
\begin{equation}
   f_X(x; 0, 1)  = 1 \quad\text{for $0\le x\le 1$},
   (\#eq:ContinuousUniformPMForder)
\end{equation}
and the distribution function is
$$
  F_X(x; 0, 1) = 
  \begin{cases}
    0                & \text{for $x < 0$};\\
    \displaystyle x  & \text{for $0 \le x \le 1$};\\ 
    1                & \text{for $x > 1$}.
  \end{cases}
$$
Hence, the density function for the maximum value in a sample of size\ $n$ is, from Def.\ \@ref(def:OrderStatsMaximumDistn),
$$
  f_{X_{(n)}}(x) 
  = n\, x^{n - 1} \times 1
  = n \, x^{n - 1},
$$
for $0 < x < 1$.
In addition, the distribution function is, from Def.\ \@ref(def:OrderStatsMaximumDistn),
$$
  F_{X_{(n)}}(x) 
  = 
  \begin{cases}
    0       & \text{for $x < 0$}\\
    x^n     & \text{for $0 \le x \le 1$}\\
    1       & \text{for $x > 1$.}
  \end{cases}
$$

Using the density function of the maximum value as given above, the expected value and the variance of the maximum value can be found from their definitions; for example,
\begin{align*}
  \operatorname{E}[X_{(n)}]   &= \int_0^1 x \cdot n x^{n - 1}\, dx   = \frac{n}{n + 1};\\
  \operatorname{E}[X^2_{(n)}] &= \int_0^1 x^2 \cdot n x^{n - 1}\, dx = \frac{n}{n + 2};\\
  \operatorname{var}[X_{(n)}] &= \frac{n}{n + 2} - \left( \frac{n}{n + 1} \right)^2.
\end{align*}

For the case where $n = 10$, then:
$$
  f_{X_{(n)}}(x) 
  = 10 \, x^{9},
$$
for $0 < x < 1$.
In addition, the distribution function is, from Def.\ \@ref(def:OrderStatsMaximumDistn),
$$
  F_{X_{(n)}}(x) 
  = 
  \begin{cases}
    0               & \text{for $x < 0$}\\
    x^{10}          & \text{for $0 \le x \le 1$}\\
    1               & \text{for $x > 1$.}
  \end{cases}
$$
Fig.\ \@ref(fig:OrderStatsMax) shows the probability density function (left panel) and the distribution function (right panel).
For example, the probability that the largest value in a sample of size\ $10$ is smaller than\ $0.8$ is
$$
  F_{(10)}(0.8) = 0.8^{10} = 0.1073742\dots
$$

The expected value and variance of the maximum value are:
\begin{align*}
  \operatorname{E}[X_{(n)}]   &= \frac{10}{11} \approx 0.90909;\\
  \operatorname{var}[X_{(n)}] &= \frac{10}{12} - \left( \frac{10}{11} \right)^2\approx 0.006887.
\end{align*}
These results can be confirmed using a quick computer simulation:

```{r}
num_Sims <- 5000
sample_Size <- 10

x_Max <- array( dim = num_Sims)
for (i in 1:num_Sims){
  x <- runif(sample_Size,
             min = 0,
             max = 1)
  x_Max[i] <- max(x) 
}
cat("Mean of the maximum value", mean(x_Max), "\n")
cat("Variance of the maximum value", var(x_Max), "\n")
```

:::


```{r OrderStatsMax, echo=FALSE, fig.align="center", fig.cap="The distribution of the maximum value for the uniform distribution on $[0, 1]$ in a sample of size\\ $10$. Left: the probability density function of the maximum value $X_{(10)}$. Right: the distribution function of the maximum value $X_{(10)}$. The grey, dotted lines show $F_{(10)}(0.8)$.", fig.width=8, fig.height=4, out.width="90%"}

par(mfrow = c(1, 2))

x <- seq(0, 1,
         length = 1000)

f_Max <- function(x, n){ n * x ^ (n - 1)}
F_Max <- function(x, n){ x ^ n }


plot( f_Max(x, n = 10) ~ x, 
     type = "l",
     lwd = 3,
     col = plotColour1, 
     las = 1,
     axes = FALSE,
     main = expression( atop(Probability~density~"function",
                             of~italic(X)[(10)]) ),
     xlab = expression(italic(X)[(10)]),
     ylab = "Density")
axis(side = 1)
box()

plot( F_Max(x, n = 10) ~ x, 
     type = "l",
     lwd = 3,
     las = 1,
     col = plotColour1, 
     las = 1,
     main = expression( atop(Distribution~"function",
                             of~italic(X)[(10)]) ),
     xlab = expression(italic(X)[(10)]),
     ylab = "Distribution fn")

lines( x = c(0, 0.8),
       y = c(0.1073742, 0.1073742),
       lwd = 2,
       col = "grey", 
       lty = 2)
lines( x = c(0.8, 0.8),
       y = c(0, 0.1073742),
       lwd = 2,
       col = "grey", 
       lty = 2)
```



## Sampling distribution of the minimum value {#OrderStatisticsMinDistribution}

Consider again a sample of size\ $n$, say $X_1, X_2, \dots, X_n$, with all $X_i$ drawn independently from a distribution with density function $f_X(x)$ and distribution function $F_X(x)$.
Then define $X_{(1)} = \operatorname{min}\{X_1, X_2, \dots, X_n\}$ as the minimum value in the sample.

The distribution function for $X_{(1)}$ is $F_{X_{(1)}}(x) = \Pr(X_{{(1)}} \le x)$.
If the value of $X_{(1)}$ is less than or equal to some value\ $x$, then there must be *at least* one value less than that value\ $x$.
Alternatively, this means that we cannot have *all* the sample values larger than the value\ $x$ (as then there must be *none* that are less than\ $x$).

The probability that *all* values are larger than\ $x$ is $(1 - F_X(x))^n$ (since all\ $n$ values have identical, independent distributions).
Hence, the probability that *all* values are *not* larger than\ $x$ is
$$
  1 - (1 - F_X(x))^n
$$
for $x \in R_X$.
The density function is found by differentiating $F_{X_{(1)}}(x)$ with respect to\ $x$ (Sect.\ \@ref(DistributionFunction)), to obtain
$$
  f_{X_{(1)}}(x) = n [1 - F_X(x)]^{n-1} f_X(x)
$$
for $x \in R_X$.


:::{.definition #OrderStatsMinimumDistn name="Distribution of the minimum value"}
Consider a sample of size\ $n$, say $X_1, X_2, \dots, X_n$, with all $X_i$ drawn independently from a distribution with density function $f_X(x)$ and distribution function $F_X(x)$.
Then the distribution function for the minimum value (order statistic $X_{(1)}$) is
$$
   F_{(X_1)}(x) = 1 - [1 - F_X(x)]^{n}
$$
and the density function is
$$
   f_{(X_1)}(x) = n [1 - F_X(x)]^{n - 1} \times f_X(x)
$$
for $x\in R_X$.
:::


:::{.example #OrderStatsMinUniform name="Uniform distribution: minimum value"}
For the continuous uniform distribution (Sect.\ \@ref(ContinuousUniform)) defined on $[0, 1]$, the density function and distribution function are giuven in Example\ \@ref(exm:OrderStatsMaxUniform).
Using these, the density function for the minimum value in a sample of size\ $n$ is
$$
  f_{X_{(1)}}(x) 
  = n(1 - x)^{n - 1} \times 1
  = n (1- x)^{n - 1},
$$
for $0 < x < 1$.
In addition, the distribution function is
$$
  F_{X_{(1)}}(x) 
  = 
  \begin{cases}
    0             & \text{for $x < 0$}\\
    1 - (1 - x)^n & \text{for $0 \le x \le 1$}\\
    1             & \text{for $x > 1$.}
  \end{cases}
$$

For the case where $n = 10$, 
$$
  f_{X_{(1)}}(x) 
  = 10 (1- x)^9,
$$
for $0 < x < 1$, and the distribution function is
$$
  F_{X_{(1)}}(x) 
  = 
  \begin{cases} 
    0                 & \text{for $x < 0$}\\
    1 - (1 - x)^{10}  & \text{for $0 \le x\le 1$}\\
    1                 & \text{for $x > 0$.}
  \end{cases}
$$
Figure\ \@ref(fig:OrderStatsMin) shows the probability density function (left panel) and the distribution function (right panel).
For example, the probability that the smallest value in a sample of size\ $10$ is smaller than\ $0.2$ is
$$
  F_{(1)}(0.2) = 1 - (1 - 0.2)^{10} = 0.8926\dots
$$
This can be confirmed using a small simulation:

```{r}
num_Sims <- 5000
sample_Size <- 10

x_Min <- array( dim = num_Sims)
for (i in 1:num_Sims){
  x <- runif(sample_Size,
             min = 0,
             max = 1)
  x_Min[i] <- max(x) 
}

cat("Proportion of smallest valuess less than 0.2: ", 
    round(sum(x_Min) / num_Sims, 3), "\n")
```


Using the density function of the minimum value, the expected value and the variance of the minimum value can be found too; for example,
$$
  \operatorname{E}[X_{(10)}] = \int_0^1 x \cdot 10(1 - x)^9\, dx = 1/11 = 0.0909\dots
$$
:::

```{r OrderStatsMin, echo=FALSE, fig.align="center", fig.cap="The distribution of the minimum value for the uniform distribution on $[0, 1]$ in a sample of size\\ $10$. Left: the probability density function of the minimum value $X_{(1)}$. Right: the distribution function of the minimum value $X_{(1)}$. The grey, dotted lines show $F_{X_{(1)}}(0.2)$.", fig.width=8, fig.height=4, out.width="90%"}

par(mfrow = c(1, 2))

x <- seq(0, 1,
         length = 1000)

f_Min <- function(x, n){ n * (1 - x) ^ (n - 1)}
F_Min <- function(x, n){ 1 - (1 - x) ^ n }


plot( f_Min(x, n = 10) ~ x, 
     type = "l",
     lwd = 3,
     col = plotColour1, 
     las = 1,
     axes = FALSE,
     main = expression( atop(Probability~density~"function",
                             of~italic(X)[(1)]) ),
     xlab = expression(italic(X)[(1)]),
     ylab = "Density")
axis(side = 1)
box()

plot( F_Min(x, n = 10) ~ x, 
     type = "l",
     lwd = 3,
     las = 1,
     col = plotColour1, 
     las = 1,
     main = expression( atop(Distribution~"function",
                             of~italic(X)[(1)]) ),
     xlab = expression(italic(X)[(1)]),
     ylab = "Distribution fn")

lines( x = c(0, 0.2),
       y = c(F_Min(0.2, n = 10), F_Min(0.2, n = 10) ),
       lwd = 2,
       col = "grey",
       lty = 2)
lines( x = c(0.2, 0.2),
       y = c(0, F_Min(0.2, n = 10) ),
       lwd = 2,
       col = "grey",
       lty = 2)

```




## Sampling distribution of the $k$th order statistic {#OrderStatisticsDistribution}

The ideas developed above for the distribution of the minimum and maximum value in a sample can be applied to other order statistics also.
Consider a sample from a continuous distribution with density function\ $f_X(x)$ and distribution function\ $F_X(x)$.
The $k$th order statistic $X_{(k)}$ has the distribution function $F_{X_(k)}(x) = \Pr(X_{(k)}\le x)$. 
By definition, this is the probability that the $k$th smallest value in the sample is less than (or equal to) some value\ $x$.

---

Writing $X_{(k)} \le x$ means that *at least*\ $k$ of the\ $n$ observations are smaller than the value\ $x$.
In addition, the probability than an observations has a value less than\ $x$ is $p = F_X(x)$.
The distribution of the $k$th order statistic is therefore like a binomial problem (Sect.\ \@ref(BinomialDistribution)): the $k$th order statistic (REWORD) is the binomial probability  of observing $k$ values in the samples with a value smaller than\ $x$ (with probability\ $p$), and $n - k$ with a value larger than\ $x$.
That is, at least $1$ values is less than $x$ is the binomial probability
$$
     p_X(x = 1; n, p) = \binom{n}{x} p^1 q^{n - 1}.
$$
That is, at least $2$ values is less than $x$ is the binomial probability
$$
     p_X(x = 2; n, p) = \binom{n}{2} p^2 q^{n - 2}.
$$
In general, at least $k$ values is less than $x$ is the binomial probability
$$
     p_X(x = k; n, p) = \binom{n}{k} p^k q^{n - k}.
$$
So we need to add these probabilities from\ $1$ to\ $k$:
\begin{equation}
  F_{X_{(k)}}(x) = \Pr(X_{(k)}\le x) 
  = 
  \sum_{i = k}^n \binom{n}{k}\times [F_{X_{(k)}}(x)]^k \times [1 - F_{X_{(k)}}(x)]^{n -k}
\end{equation}




----

NEEDS FIXING!!

For some given value\ $x$ to be the $k$th smallest, there must be $k$\ observations with a value less than or equal to $x$; the probability of this is
$$
  [F_{(k)}(x)]^k
$$
In addition, the remaining $n - k$ observations with a value *larger* than $x$; the probability of this is
$$
  [1 - F_{X_{(k)}}(x)]^{n -k}
$$
Finally, there are $\binom{n}{k}$ ways for the first $k$ observations to be ordered that are equivalent; we also to consider the $\binom{n}{k-1}$ ways... $\binom{n}{1}$ AND SO ON.
(For instance, consider a sample of size $n = 6$, where we seek the distribution of the $3$rd order statistic (i.e., $k = 3$).
There are $\binom{6}{3}$ ways to choose\ $k = 3$ of the\ $n = 6$ observation to be smaller than\ $x$; in addition, there are $\binom{6}{2}$ ways to choose\ $k = 2$ of the\ $n = 6$ observation to be smaller than\ $x$; and finally, there are $\binom{6}{1}$ ways to choose\ $k = 1$ of the\ $n = 6$ observation to be smaller than\ $x$.

Combining these elements,
$$
  F_{X_{(k)}}(x) = \Pr(X_{(k)}\le x) 
  = 
  \sum_{i = k}^n \binom{n}{k}\times [F_{X_{(k)}}(x)]^k \times [1 - F_{X_{(k)}}(x)]^{n -k}
$$

From the distribution function, the probability density functin can be obtained as
$$
  f_{X_{(k)}}(x) = \frac{d}{dx} F_{X_{(k)}} 
  = 
  \frac{n!}{(k - 1)!\,(n - r)!}f_{X_{(k)}}[F_{X_{(k)}}]^{k - 1} [1 - F_{X_{(k)}}]^{n - r},
$$
for $n > k \ge 1$ and\ $n$ and\ $k$ are integers.


:::{.definition #OrderStatsDistn name="Distribution of an order statistic"}
Consider a sample of size\ $n$, say $X_1, X_2, \dots, X_n$, with all $X_i$ drawn independently from a distribution with density function $f_X(x)$ and distribution function $F_X(x)$.
Then the distribution function for the $k$th order statistic $X_{(k)}$) is
\begin{equation}
  F_{X_{(k)}}(x) = \Pr(X_{(k)}\le x) 
  = 
  \sum_{i = k}^n \binom{n}{k}\times [F_{X_{(k)}}(x)]^k \times [1 - F_{X_{(k)}}(x)]^{n -k}
   (\#eq:OrderStatisticDF)
\end{equation}
and the density function of the $k$th order statistic $X_{(k)}$ 
is:
\begin{equation}
   f_{X_{(k)}}(x) = \frac{n!}{(k-1)! (n-k)!} [F(x)]^{k-1} [1 - F(x)]^{n-k} f(x),
   (\#eq:OrderStatisticPDF)
\end{equation}
where $n > k > 0$ and\ $n$ and\ $k$ are integers.
:::


::: {.example}
Consider simulating $1\,000$ samples of size $n = 10$ from a Uniform$(0, 1)$ distribution.
The distribution of $X_{(2)}$ and $X_{(6)}$ are shown in Fig.\  \@ref(fig:OrderStats1).
:::


```{r OrderStats1, echo=TRUE, fig.align="center", fig.cap="Simulating the distribution of two order statistics for the uniform distribution on $[0, 1]$ with a sample of size $10$. Left: the distribution of $X_{(2)}$. Right: the distribution of  $X_{(6)}$.", fig.width=8, fig.height=4, out.width="90%"}
set.seed(123)  # For reproducibility

n <- 10   # Take samples of size 10
samples2 <- replicate(1000,               # 1000 repetitions
                      sort(runif(n))[2])  # 2nd order statistic
samples6 <- replicate(1000,               # 1000 repetitions
                      sort(runif(n))[6])  # 6th order statistic

par(mfrow = c(1, 2))

hist(samples2, 
     col = plotColour1, 
     xlim = c(0, 1),
     las = 1,
     main = expression(Distribution~of~italic(X)[(2)]),
     xlab = expression(italic(X)[(2)]),
     ylab = "Density")
hist(samples6, 
     col = plotColour1, 
     xlim = c(0, 1),
     las = 1,
     main = expression(atop(Distribution~of~italic(X)[(6)]),
                            "("*the~maximum~value*")"),
     xlab = expression(italic(X)[(6)]),
     ylab = "Density")
```


:::{.example #OrderStatisticsUniform name="Order statistics (uniform distribution)"}
For the continuous uniform distribution (Sect.\ \@ref(ContinuousUniform)) defined on $[0, 1]$, the density function is
\begin{equation}
   f_X(x; 0, 1)  = 1 \quad\text{for $0\le x\le 1$},
   (\#eq:ContinuousUniformPMForder)
\end{equation}
and the distribution function is
$$
  F_X(x; 0, 1) = 
  \begin{cases}
    0                & \text{for $x < 0$};\\
    \displaystyle x  & \text{for $0 \le x \le 1$};\\ 
    1                & \text{for $x > 1$}.
  \end{cases}
$$
Hence, Eq. \@ref(eq:OrderStatisticDF) becomes
\begin{equation}
   f_{X_{(k)}}(x) 
   = \frac{n!}{(k - 1)!\, (n - k)!} x^{k-1} (1 - x)^{n-k},
   (\#eq:OrderStatisticUniformPDF)
\end{equation}
for the $k$th order statistic $X_{(k)}$ in a sample of size\ $n$.
Since this is a density function, the total area must be one (Exercise\ \@ref(exr:OrderStatisticsUniform)).
The distributions are shown in Fig.\ \@ref(fig:OrderStatsUnif) for $k = 2$ and $k = 6$.
The simulated order statistics (Fig.\ \@ref(fig:OrderStats1)) compare favourable with these theoretical distributions.

The mean value of the $k$th order statistic is (Exercise\ \@ref(exr:OrderStatisticsUniformExpectations)):
$$
  \operatorname{E}[X] 
  = \frac{n!}{(k - 1)!\, (n - k)!} \int_0^1 x^k (1 - x)^{n-k}\,dx
  = \frac{k}{n + 1}.
$$
Similarly (Exercise\ \@ref(exr:OrderStatisticsUniformExpectations)), 
$$
  \operatorname{var}[X] = \frac{k(n + 1 - k)}{(n + 1)^2\, (n + 2)}.
$$

For example, in a sample of size $n = 10$, the second order statistic (i.e., $k = 2$) is
$$
   f_{X_{(k)}}(x) 
   = \frac{10!}{1!\, 8!} x^{1} (1 - x)^{8}
   = 90 x (1 - x)^{8}
$$
for $0 < x < 1$, where 
$$
  \operatorname{E}[X] 
  = \frac{2}{11}
  \approx 0.1818\dots.
$$
and
$$
  \operatorname{var}[X] 
  = \frac{2\times (10 + 1 - 2)}{11^2 \times 12}
  = \frac{18}{1452}
  \approx 0.01239\dots.
$$
In **R**

```{r}
# 2nd order statistic
mean(samples2)   # Simulated mean of X_{(2)}
var(samples2)    # Simulated varianceof X_{(2)}
```

Likewise, in a sample of size $n = 10$, the sixth order statistic (i.e., $k = 6$) is
$$
   f_{X_{(k)}}(x) 
   = \frac{10!}{9!\, 0!} x^{9} (1 - x)^{0}
   = 10 x^{9}
$$
for $0 < x < 1$, and
$$
  \operatorname{E}[X] 
  = \frac{6}{11}
  \approx 0.5454\dots.
$$
and
$$
  \operatorname{var}[X] 
  = \frac{6\times (10 + 1 - 6)}{11^2 \times 12}
  = \frac{15}{726}
  \approx 0.02066\dots.
$$
In **R**:

```{r}
# 6th order statistic (i.e., maximum)
mean(samples6)   # Simulated mean of X_{(6)}
var(samples6)    # Simulated variance of X_{(6)}
```

:::


```{r OrderStatsUnif, echo=TRUE, fig.align="center", fig.cap="The distribution of two order statistics for the uniform distribution on $[0, 1]$. Left: the density function of $X_{(2)}$. Right: the density function of  $X_{(6)}$.", fig.width=8, fig.height=4, out.width="90%"}
### Uniform order stats for 2nd and 10th order stats
f_Order <- function(x, n, k){
  factorial(n)/( factorial(k - 1) * factorial(n - k)) *
    x^(k - 1) * 
    (1 - x)^(n - k)
}

par(mfrow = c(1, 2))

x <- seq(0, 1,
         length = 100)

plot( f_Order(x, n = 10, k = 2) ~ x,
      lwd = 3,
      xlim = c(0, 1),
      col = plotColour1,
      axes = FALSE,
      xlab = expression(italic(x)),
      ylab = "Density",
      main = expression( atop(Distribution~of~"2nd",
                              order~statistic)),
      type = "l")
axis(side = 1)
box()

plot( f_Order(x, n = 10, k = 6) ~ x,
      lwd = 3,
      xlim = c(0, 1),
      col = plotColour1,
      axes = FALSE,
      xlab = expression(italic(x)),
      ylab = "Density",
      main = expression( atop(Distribution~of~"6th",
                              order~statistic)),
      type = "l")
axis(side = 1)
box()
```



:::{.example #ServerRoom name="Air conditioner servicing"}
A new server room has four air-conditioning units installed. 
The lifetime of each unit (in hours) follows an exponential distribution with mean $\lambda = 2000\hs$. 
The units operate independently.
A service technician is called when *two* of the units fail. 
We seek the distribution of the time until the call to the technician is made.

Let $T_1, T_2, T_3, T_4$ denote the lifetimes of the four units. 
Then:
$$
  T_i \sim \text{Exponential}(\lambda)\quad\text{for $i = 1, 2, 3, 4$},
$$
where $\lambda = 1/2000$, and the $T_i$ are independent.
That is,
$$
  f_{T_i}(t)= \frac{1}{\lambda}\exp(-t/\lambda)
  \quad\text{and}\quad
  F_{T_i}(t)= \exp(-t/\lambda)
$$
for $0 < x < \infty$.

Define the order statistics as $T_{(1)} < T_{(2)} < T_{(3)} < T_{(4)}$. 
The technician is called at time \( T_{(2)} \), the *second failure*.
Since we have $n = 4$ and $k = 2$, the distribution of $T_{(2)}$ is (from REF)
$$
   f_{T_{(2)}}(t) = 
   6 [\exp(-t/\lambda)]^1 [1 - \exp(-t/\lambda)] ^2 \times \frac{1}{\lambda}\exp(-t/\lambda),
$$
which is very complicated.

However, the exponential distribution has the *memoryless property* (REF), so a different approach is much simpler.
The time to the *first* failure $T_{(1)}$ has the probability density function
$$
   T_{(1)} \sim \text{Exponential}(4\lambda)
$$
since *any* of the $4$\ units can fail first.
The time *between* the first and second failure also has an exponential distribution, due to the memoryless property, this means that
$$
  T_{(2)} - T_{(1)} \sim \text{Exponential}(3\lambda)
$$
because only $3$\ units remain after the first failure.

Thus, the total time until the second failure is:
$$
  T_{(2)} = E_1 + E_2
$$
where
$$
  E_1 \sim \text{Exp}(4\lambda) \quad\text{and}\quad E_2 \sim \text{Exp}(3\lambda).
$$

This distribution is not exponential, but the *sum of two independent exponentials with different rates*.

The expected value is
$$
\operatorname{E}[T_{(2)}] = \frac{1}{4\lambda} + \frac{1}{3\lambda} = \frac{1}{\lambda} \left( \frac{1}{4} + \frac{1}{3} \right) = 2000 \cdot \frac{7}{12} = 1166.6\dots\hs.
$$

On average, a technician will be called after about\ $1166.67\hs$.
:::



The PDF of $T_{(2)}$ can be found using **convolution**:
$$
  f_{T_{(2)}}(t) = \int_0^t f_{E_1}(x) f_{E_2}(t - x)\, dx
$$
where $f_{E_1}(x) = 4\lambda e^{-4\lambda x}$, and $f_{E_2}(x) = 3\lambda e^{-3\lambda x}$.
This approach can be extended to *any* $k$th failure out of\ $n$ components.


## Discrete random variables

Bit trickier, esp for general order stat.
Perhaps just give min and max?

Let $F(x^{-}) = \Pr(X<x)$.
 
Min:
\[
\Pr(X_{(1)} = x) = \left[1 - F(x^-)\right]^n - \left[1 - F(x)\right]^n
\]

Max; 
\[
\Pr(X_{(n)} = x) = F(x)^n - F(x^-)^n
\]

kth order stats

\[
\Pr(X_{(k)} = x) = \sum_{j = k}^{n} \binom{n}{j} \binom{j}{1} 
\left[F(x^-)\right]^{n - j} \cdot p(x) \cdot \left[1 - F(x)\right]^{j - 1}
\]

\[
\Pr(X_{(k)} = x) = \binom{n}{k} p(x) \left[F(x^-)\right]^{k - 1} \left[1 - F(x)\right]^{n - k}
\]


## Joint distribution

$$
 f_{X_{(1)}, X_{(2)}, \dots, X_{(n)}}(x_1, x_2, \dots, x_n)  
 =
 \begin{cases}
    n !\, f_{X_{(1)}} \times f_{X_{(2)}}\times \cdots \times f_{X_{(n)}} & \text{for $x_{(1)} < x_{(2)} < \cdots < x_{(n)}$}\\
    0 & \text{elsewhere}
 \end{cases}
$$


## Finishing up...

Order statistics are fundamental in probability and statistics. 
They underlie robust estimation, reliability analysis, and form the basis of many intuitive statistics.

\index{Order statistics|)}


## Exercises  {#OrderStatisticsExercises}

:::{.exercise #OrderStatisticsUniform}
Prove that the probably density function for the order statistics in the case of the uniform distribution, shown in Eq.\ \@ref(eq:ContinuousUniformPMForder), is a valid density function.
(Hint: observe the similarity between Eq.\ \@ref(eq:ContinuousUniformPMForder) and the beta density function in Sect.\ \@ref(BetaDistribution).)
:::


:::{.exercise #OrderStatisticsUniformExpectations}
Prove that the expected value and variance of the order statistics in the case of the uniform distribution are
$$
  \operatorname{E}[X] = k/(n + 1)\qquad\text{and}\qquad \operatorname{var}[X] = \frac{k (k + 1)}{(n + 1)(n + 2)},
$$
as in Sect.\ \@ref(OrderStatisticsDistribution).
(Hint: observe the similarity between Eq.\ \@ref(eq:ContinuousUniformPMForder) and the beta density function in Sect.\ \@ref(BetaDistribution).)
:::



:::{.exercise #OrderStatisticsPDF}
Find the distribution of the order statistics for the uniform distribution defined on $[-1, 1]$.
:::


:::{.exercise #OrderStatisticsPDF2}
Find the distribution of the order statistics for the distribution
$$
  f_Y(y) = y/2\quad\text{for $0 < y < 2$}
$$
and zero elsewhere.
:::


:::{.exercise #OrderStatisticsExponential}
Suppose a sample of\ $n$ independent values is taken from the exponential distribution $\text{Exp}(\lambda)$ (REF).

1. Determine the distribution function for the minimum value, $X_{(1)}$.
2. Hence determine the density function for the minimum value, $X_{(1)}$.
3. What is the probability that the minimum value in a sample of size\ $n$ is larger than $\lambda/2$?
4. Compute the expected value of the minimum value in the sample.
:::


:::{.exercise #OrderStatisticsExponential2}
Suppose a sample of\ $n$ independent values is taken from the exponential distribution $\text{Exp}(\lambda)$ (REF).

1. Determine the distribution function for the maximum value, $X_{(n)}$.
2. Hence determine the density function for the maximum value, $X_{(n)}$.
3. What is the probability that the maximum value in a sample of size\ $n$ is larger than $\lambda/2$?
4. Compute the expected value of the maximum value in the sample.
:::

:::{.exercise}
A new computer server room has four air-conditioning units installed.
The lifetime of each unit follows an exponential, with a mean lifetime of $2000\hs$.
When two units fail, a service technician is called.

1. Describe the distribution showing the wait time until a technician need to be called. 
:::
