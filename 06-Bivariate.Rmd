# Bivariate distributions {#Bivariate}

::: {.objectivesBox .objectives data-latex="{iconmonstr-target-4-240.png}"}
Upon completion of this module students should be able to:

* understand the concept of bivariate random variables
* understand the concept of the joint probability function and the distribution function of two random variables
* find the marginal and conditional probability functions of random variables in both discrete and continuous cases
* understand and apply the concept of independence of two random variables
* compute the expectation and variance of linear combinations of random variables
* interpret and compute the covariance and the coefficient of correlation between two random variables
* compute the conditional mean and conditional variance of a random variable for some given value of another random variable
* be familiar with the multinomial and bivariate normal distributions
* know the computation of conditional mean and variance for a bivariate normal distribution
:::


## Introduction

Not all random processes are sufficiently simple to have the outcome denoted by a single number $x$. 
In many situations we are interested in observing two or more numerical characteristics simultaneously.

This section only discusses the two-variable, or bivariate, case.

Let $\cal E$ be a random process and $S$ a sample space associated with $\cal E$.

:::{.definition #RandomVector name="Random vector"}
Let $X = X(s)$ and $Y = Y(s)$ be two functions, each assigning a real number to each sample point $s \in S$. 
Then $(X,Y)$ is called a *two-dimensional random variable*, or a *random vector*.
:::

The range space of $(X, Y)$, $R_{X \times Y}$, will be a subset of the Euclidean plane. 
Each outcome $X(s)$, $Y(s)$ may be represented as a point $(x, y)$ in the plane. 
As in the 1-dimensional case, it is necessary to distinguish between discrete and continuous random variables.

:::{.definition #DiscreteBV name="Bivariate probability function"}
$(X,Y)$ is a 2-dimensional *discrete* random variable if the range space $R_{X \times Y}$ is finite or countably infinite. 
That is, if values of $(X, Y)$ may be represented as $(x_i, y_i)$, $i = 1,2,\ldots, j=1,2,\ldots$.

$(X,Y)$ is a 2-dimensional *continuous* random variable if the range space $R_{X \times Y}$ is a non-denumerable set of the Euclidean plane; for example, $R_{X \times Y} = \{(x,y): a \leq x \leq b, c \leq y \leq d\}$.
:::

:::{.definition #BivariateProbFunction name="Bivariate distribution function"}
Let $(X,Y)$ be a 2-dimensional discrete random variable. 
With each $(x_i, y_j)$ we associate a number $p_{X,Y}(x_i, y_j)$ representing $\Pr(X = x_i, Y = y_j)$ and satisfying
\begin{align*}
   p_{X, Y}(x_i, y_j) &\geq 0, \text{ for all } (x_i,y_j)  \\
   \sum_{j = 1}^{\infty} \sum_{i = 1}^{\infty} p_{X,Y}(x_i,y_j)
   &=1.
\end{align*}
Then the function $p_{X,Y}$, defined for all $(x_i,y_j) \in R$ is called the *probability function* of $(X,Y)$. 
Also, 
\[
   \{x_i, y_j, p_{X,Y}(x_i, y_j); i, j = 1, 2, \ldots\} 
\]
is called the *probability distribution* of $(X,Y)$.

Let $(X,Y)$ be a continuous random variable assuming values in a 2-dimensional set $R$.
The *joint probability density function*, $f_{X,Y}$, is a function satisfying
\begin{align}
   f_{X, Y}(x, y) 
   &\geq 0, \text{ for all } (x,y) \in R, \\
   \int \!\! \int_{R} f_{X,Y}(x,y) \, dx \, dy &= 1.
\end{align}
:::

Note that the second of these indicates that the volume under the surface $f_{X,Y}(x,y)$ is one. 
Also, for $\Delta x, \Delta y$
sufficiently small,
\begin{equation}
   f_{X, Y}(x, y) \, \Delta x \Delta y \approx \Pr(x \leq X \leq x + \Delta x, y \leq Y \leq y+\Delta y).
\end{equation}
Probabilities of events can be determined by the probability function or the probability density function as follows.

:::{.definition #BVDistributionFn name="Bivariate distribution function"}
For any event $A$, *the probability of $A$* is given by
\begin{align}
   \Pr(A) 
   &= \sum_{(x,y) \in A} p(x,y),\quad \quad \quad \quad \quad \text{ for $(X, Y)$ discrete}\\
   \Pr(A)
   &= \int \!\! \int_{(x,y) \in A}f(x,y) \, dx \, dy \quad \text{for $(X, Y)$ continuous}
\end{align}
:::

As in the univariate case, the (cumulative) distribution function can be used  to represent a sum of probabilities or a volume under a surface. 
It is denoted by $F_{X, Y}(x, y)$ and defined as follows.

:::{.example #BVDistributionFN name="Bivariate distribution function"}
The *bivariate distribution function* is
\begin{align}
   F(x, y)
   &= \Pr(X \leq x, \, Y \leq y), \text{ for $(X,Y)$ discrete}\\
   F(x, y)
   &= \int_{-\infty}^y \int_{-\infty}^x f(u,v) \, du \, dv, \text{for $(X,Y)$ continuous}
\end{align}
:::

As in the univariate case, a bivariate distribution can be expressed in various ways:

* by enumerating the range space and corresponding probabilities;
* by a formula; or
* by a table.

The following examples illustrate these concepts.

:::{.example #BVDiscrete name="Bivariate discrete"}
Consider an experiment where, simultaneously, *two* coins are tossed, and *one* die is rolled. 
Let $X_1$ be the number of heads that show on the two coins, and $X_2$ the number on the top face of the die. 
Then $(X_1, X_2)$ is a discrete, bivariate random variable.

Note the possible values of $X_1$ are $R_{X_1}=\{0, 1, 2\}$ and the possible values of $X_2$ are $R_{X_2}=\{1, 2, 3, 4, 5, 6\}$. 
So the sample space for the random vector $(X_1, X_2)$ is
\begin{align*}
   S
   &=\left\{ (0, 1), (0, 2), \dots, (0, 6);  \right. \\
   & \phantom{(} (1, 1), (1,2), \dots, (1,6); \\
   & \left.\phantom{(} (2, 1), (2,2), \dots, (2,6)\right\}
\end{align*}
:::


:::{.example #BVDiscrete2 name="Bivariate discrete"}
Consider the following discrete distribution where probabilities $\Pr(X = x, Y = y)$ are shown as a graph in Fig. \@ref(fig:Bivar1graph) and as a Table in Table \@ref(tab:Bivar1). 
Find $\Pr(X + Y \geq 2)$.


```{r, setup, echo=FALSE, message=FALSE, warning=FALSE}
### Needed to add the rgl plot to bookdown
library(rgl)
knitr::knit_hooks$set(webgl = hook_webgl)
```

```{r Bivar1graph, echo=FALSE, warning=FALSE, message=FALSE, webgl=TRUE, fig.cap="The bivariate probability function", fig.align="center"}
library(rgl)

# This is to output a rgl plot in a rmarkdown document.
# setupKnitr()

pmf1 <- data.frame(
  x = c(0, 0, 0, 1, 1, 1, 2, 2, 2),
  y = c(0, 1, 2, 0, 1, 2, 0, 1, 2),
  f = c(1, 4, 4, 6, 12, 0, 9, 0, 0) / 36
)

# Add a new column with color

# Plot
plot3d( 
  x = pmf1$x,
  y = pmf1$y, 
  z = pmf1$f, 
  xlim = c(-0.5, 2.5),
  ylim = c(-0.5, 2.5),
  zlim = c(0, 0.4),
  col = plotColour, 
  type = "s", # Plot the points
  radius = 0.05,
  xlab = "X", 
  ylab = "Y", 
  zlab = "pmf")
plot3d( 
  add = TRUE,
  x = pmf1$x,
  y = pmf1$y, 
  z = pmf1$f, 
  col = plotColour, 
  type = "h") # Plot the line segments from z = 0 

# To display in an R Markdown document:
# rglwidget()

# To save to a file:

#htmlwidgets::saveWidget(rglwidget(width = 520, height = 520), 
#                        file = "Widgets/BivarPMF1.html",
#                        libdir = "libs",
#                        selfcontained = FALSE
#                        )
```


```{r Bivar1, echo=FALSE}
Bivar1Table <- array( dim = c(3, 3))

Bivar1Table[1, ] <- c(1, 4, 12) / 36
Bivar1Table[2, ] <- c(4, 12, 0) / 36
Bivar1Table[3, ] <- c(9, 0, 0) / 36

rownames(Bivar1Table) <- c("y = 0",
                           "y = 1",
                           "y = 2")
colnames(Bivar1Table) <- c("x = 0",
                           "x = 1",
                           "x = 2")

knitr::kable(Bivar1Table,
             caption = "The bivariate probability function")
```



Note that the probabilities in the table add to one.

Treating $X + Y \geq 2$ as an event $A$, we have,
\begin{align*}
   \Pr(X + Y \geq 2)
   &=\Pr(X = 2, Y = 0 \text{ or } X = 1, Y = 1 \text{ or } X = 0, Y = 2)\\
   &= \Pr(X = 2, Y = 0) \, + \, \Pr(X = 1, Y = 1) \, + \, \Pr(X = 0, Y = 2)\\
   &= \frac{1}{9} \ + \ \frac{1}{4} \ + \ \frac{1}{3}\\
   &= \frac{25}{36}
\end{align*}
:::


:::{.example #BVDiscreteUnif name="Bivariate uniform distribution"}
Consider the following continuous bivariate distribution with joint pdf
\[
   f_{X, Y}(x, y) = 1, \quad 0 \leq x \leq 1, \ 0 \leq y \leq 1,
\]
and find $\Pr(0 \leq x \leq \frac{1}{2}, \,  0 \leq y \leq \frac{1}{2})$. 
This is sometimes called the *bivariate uniform distribution* (Fig. \@ref(fig:BVDiscrete3)). 
Note that the *volume* under the surface is one.

Now the probability of the event above is the volume above the square with vertices $(0, 0), (0, \frac{1}{2}), (\frac{1}{2}, 0), (\frac{1}{2},\frac{1}{2})$ and is thus $\frac{1}{4}$.




:::{.example #BVDiscrete3 name="Bivariate discrete"}
Consider again the experiment in Examples~\ref{EG:rvs:bivariate:discreteDC}. 
As an example, the joint df at $(1, 2)$ would be computed as follows:
\begin{align*}
   F_{X_1, X_2}(1, 2)
   &= \displaystyle \sum_{x_1\le1} \sum_{x_2\le2} p_{X_1, X_2}(x_1, x_2)\\
   &= p_{X_1, X_2}(0, 1) + p_{X_1, X_2}(0, 2)+
   p_{X_1, X_2}(1, 1) + p_{X_1, X_2}(1, 2) \\
   &= 1/24 + 1/24 + 1/12 + 1/12 = 6/24.
\end{align*}
The complete joint df is give below. 
It is reasonably complicated even for this simple example.
:::

\begin{center}
\footnotesize
\begin{tabular}{ccccc}
& \multicolumn{4}{c}{$X_1$}
\\
\cline{2-5} $X_2$ & $x_1<0$ & $0\le x_1<1$ & $1\le x_1
<2$ & $x_1\ge 2$
\\
\hline
$x_2< 1$        & $0$ & $0$ & $0$ & $0$   \\
$1 \le x_2 < 2$ & $0$ & $1/24$ & $3/24$ & $4/24$   \\
$2 \le x_2 < 3$ & $0$ & $2/24$ & $6/24$ & $8/24$   \\
$3 \le x_2 < 4$ & $0$ & $3/24$ & $9/24$ & $12/24$  \\
$4 \le x_2 < 5$ & $0$ & $4/24$ & $12/24$ & $16/24$ \\
$5 \le x_2 < 6$ & $0$ & $5/24$ & $15/24$ & $20/24$ \\
$x_2 \ge 6$     & $0$ & $6/24$ & $18/24$ & $24/24$ \\
\end{tabular}
\end{center}
:::


:::{.example #BVDiscreteTwoDice name="Two dice"}
Consider the bivariate discrete distribution which results when two dice are thrown. 

Let $X$ be the number of $5$'s and $Y$ the number of 6's that result. 
Now range spaces of $X$ and $Y$ are $R_X = \{0, 1 ,2 \}$, $R_Y = \{0, 1, 2\}$ and the range space for the experiment is the Cartesian product of $R_X$ and $R_Y$, with the interpretation that some of the resulting points may have
probability zero. 
The probabilities in Table \@ref(tab:2dice) are $\Pr(X = x, Y = y)$ for the $(x, y)$ pairs in the range space.

\begin{table}
\begin{center}
\begin{tabular}{cc|ccc}
\multicolumn{2}{c}{}&\multicolumn{3}{c}{$x$}\\
&&0&1&2 \\ \cline{2-5}
&$0$&$(2/3)^2$&$2(1/6)(2/3)$&$(1/6)^2$\\
$y$&$1$&$2(1/6)(2/3)$&$2(1/6)(1/6)$&$0$\\
&$2$&$(1/6)^2$&0&0\\
\end{tabular}
\caption{Joint probability distribution for Example \@ref(exm:BVDiscreteTwoDice)}\label{TB:2dice}
\end{center}
\end{table}

The probabilities in the table are found by considering that we really have two repetitions of a simple experiment with 3 possible outcomes, $\{5, 6,\overline{5 \text{ or }6}\}$, with probabilities $\frac{1}{6}, \frac{1}{6}, \frac{2}{3}$, the same on each repetition.
Of course the event $X = 2, Y = 1$ cannot occur in two trials, so has probability zero.
:::

Example \@ref(exm:BVDiscreteTwoDice) is a special case of the *multinomial distribution* (a generalisation of the binomial distribution) which will be described later.

:::{.example #BVDiscreteTossRoll name="Tossing and rolling"}
Consider Example~\ref{EG:rvs:bivariate:discreteDC}. 
Since the toss of the coin and the roll of the die are independent, the probabilities are computed as follows:
\begin{align*}
   \Pr(X_1 = 0, X_2 = 1) 
   &= \Pr(X_1 = 0) \times \Pr(X_2 = 1) = \frac{1}{4}\times\frac{1}{6} = \frac{1}{24}\\
   \Pr(X_1 = 1, X_2 = 1) 
   &= \Pr(X_1 = 1) \times \Pr(X_2 = 2) = \frac{1}{2}\times\frac{1}{6} = \frac{1}{12}\\
\end{align*}
and so on. 
The complete joint pf can be displayed in a graph (often tricky), a function, or a table. 
Here, the joint pf could be given (but is not obvious) as the function
\[
   p_{X_1, X_2}(x_1, x_2) =
      \begin{cases}
         \left(\frac{1}{12}\right) 0.5^{|x_1 -1|} & \text{for $(x_1, x_2)\in S$ defined earlier}\\
         0 & \text{elsewhere}
      \end{cases}
\]
In tabular form (probably clearer in this example), we would have the joint pf as given in Table \@ref(tab:rvs:joint:eg).

\begin{table}
\begin{center}
\begin{tabular}{cccccccc}
& \multicolumn{6}{c}{$X_2$}
\\
\cline{2-7}  $X_1$ & $1$ & $2$ & $3$ & $4$ & $5$ & $6$ &
Total
\\
\hline
$0$ & $1/24$ & $1/24$ & $1/24$ & $1/24$ & $1/24$ & $1/24$ & $1/4$\\
$1$ & $1/12$ & $1/12$ & $1/12$ & $1/12$ & $1/12$ & $1/12$ & $1/2$\\
$2$ & $1/24$ & $1/24$ & $1/24$ & $1/24$ & $1/24$ & $1/24$ & $1/4$
\\
\hline  Total & $1/6$ & $1/6$ & $1/6$ & $1/6$ & $1/6$ &
$1/6$ & $1$
\end{tabular}
\caption{The joint pf for
Example~\ref{EG:rvs:bivariate:discreteDCpmf}.}
\label{TB:rvs:joint:eg}
\end{center}
\end{table}
:::


:::{.example #BVDiscreteBank name="Banks"}
A bank operates both a drive-up and a walk-up window. 
On a randomly selected day, let $X_1$ be the proportion of time  the drive-up facility is in use (at least one customer is being served or waiting to be served), and $X_2$ is the proportion of time  the walk-up
window is in use. 

Then the set of possible values for $X_1$ and $X_2$ is the rectangle $R = \{(x_1, x_2)\mid 0 \le x_1 \le 1, 0 \le
x_2 \le 1\}$.
From experience, the joint pdf of $(X_1, X_2)$ is given by
\[
   f_{X_1, X_2}(x_1, x_2) =
      \begin{cases}
        c(x_1 + x_2^2) & \text{for $0\le x_1\le 1$; $0\le x_2\le 1$}\\
        0 & \text{elsewhere}
    \end{cases}
\]

* Determine a value for $c$.
* Compute the probability *neither* facility is busy more than half the time.

Obviously, $f_{X_1, X_2}(x_1, x_2) \ge 0$ for all $x_1$ and $x_2$ from the definition provided $c > 0$. 
Secondly, we need
\[
   \int_{-\infty}^{\infty}\!\!\!\int_{-\infty}^{\infty} f_{X_1, X_2}(x_1, x_2)\, dx_1\,dx_2 = 1.
\]
Hence,
\begin{align*}
    \int_{-\infty}^{\infty}\!\int_{-\infty}^{\infty} f_{X_1, X_2}(x_1, x_2)\, dx_1\,dx_2
    &= \int_{0}^{1}\!\!\!\int_{0}^{1} f_{X_1, X_2}(x_1, x_2)\, dx_1\,dx_2 \\
    &= \int_{0}^{1}\left\{\int_{0}^{1} f_{X_1, X_2}(x_1, x_2)\, dx_1\right\} dx_2 \\
    &= c \int_{x_2=0}^{1}\left\{\int_{x_1=0}^{1} (x_1 + x_2^2)\, dx_1\right\} dx_2\\
    &= c \int_{x_2=0}^{1} (1/2 + x_2^2) \, dx_2\\
    &= c (1/2 + 1/3) = 5c/6,
\end{align*}
and so $c = 6/5$.

The question is asking to find $\Pr( 0\le X_1\le 0.5, 0\le X_2\le 0.5)$; call this event $A$.
Then,
\begin{align*}
   \Pr(A)
   &= \int_{0}^{0.5}\,\,\, \int_{0}^{0.5} f_{X_1, X_2}(x_1, x_2)\, dx_1\, dx_2 \\
   &= \frac{6}{5} \int_{0}^{0.5}\left\{\int_{0}^{0.5} x_1 + x_2^2\, dx_1\right\} dx_2 \\
   &= \frac{6}{5} \int_{0}^{0.5} (1/8 + x_2^2/2) \, dx_2 \\
   &= 1/10.
\end{align*}
:::

:::{.example #BVCont name="Bivariate continuous"}
From Example~\ref{EG:rvs:bivar:cont},
\begin{align*}
   F_{X_1, X_2}(x_1, x_2)
   &= \frac{6}{5} \int_0^{x_1} \int_0^{x_2} (t_1 + t_2^2)\, dt_2 dt_1 \\
   &= \frac{6}{5} \int_0^{x_1} (t_1 t_2 + t_2^3/3)\Big|_{t_2=0}^{t_2=x_2} \, dt_1 \\
   &= \frac{6}{5} \int_0^{x_1} (t_1 x_2 + x_2^3/3)\, dt_1 \\
   &= \frac{6}{5} \left( \frac{x_1 x_2}{2} + \frac{x_1 x_2^3}{3}\right)
\end{align*}
for $0 < x_1 < 1$ and $0 < x_2 < 1$. 
So
\[
   F_{X_1, X_2}(x_1, x_2)
   = \begin{cases}
      0 & \text{if $x_1<0$ or $x_2<0$}\\
      \frac{6}{5} \left( x_1 x_2/2 + x_1 x_2^3/3\right) & \text{if $0 \le x_1 \le 1$ and $0 \le x_2 \le 1$}\\
      1 & \text{if $x_1 > 1$ and $x_2 > 1$}
     \end{cases}
\]
:::



### Marginal distributions {#MarginalDistributions}

With each two-dimensional random variable $(X,Y)$ we associate two one-dimensional random variables, namely $X$ and $Y$. 
We now find the probability distributions of each of $X$ and $Y$ separately.

In the case of a discrete random vector $(X, Y)$, the event $X = x_i$ should be thought of as the union of the mutually exclusive events
\[
   \{X = x_i, Y = y_1\}, \{\ X = x_i, Y = y_2\}, \{X = x_i, Y = y_3\}, \dots
\]
Thus,
\begin{align*}
   \Pr(X = x_i)
   &= \Pr(X = x_i, Y = y_1) + \Pr(X = x_i, Y = y_2) + \dots \\
   &= \sum_jp_{X, Y}(x_i, y_j)
\end{align*}
Hence, when $(X, Y)$ is a discrete random vector we have:

:::{.definition #BVDiscreteMarginal name="Bivariate discrete marginal distributions"}
Given $(X, Y)$ with joint probability function $p(x, y)$, the *marginal probability functions* of $X$ and $Y$ are, respectively
\begin{equation}
   \Pr(X = x)
   = \sum_{y}p_{X, Y}(x, y)
   \quad\text{and}\quad
   \Pr(Y = y) 
   = \sum_{x}p_{X, Y}(x, y).
\end{equation}
:::

Analogously, when the random vector, $(X,Y)$, is continuous:

:::{.definition #BVConteMarginal name="Bivariate continuous marginal distributions"}
If $(X, Y)$ has joint pdf $f(x, y)$, the *marginal pdfs* of $X$ and $Y$, denoted by $f_X(x)$, $f_Y(y)$ respectively, are
\begin{equation}
   f_X(x)
   = \int_{-\infty}^{\infty}f(x,y) \, dy\quad\text{and}\quad
   f_Y(y)
   = \int_{-\infty}^{\infty}f(x,y) \, dx.
\end{equation}
:::


:::{.example #BVDiscreteMarginal name="Bivariate discrete marginal distributions"}
The joint pdf of $X$ and $Y$ is
\[
   f(x, y) = 
   \left\{
   \begin{array}{ll}
       k(3x^2 + xy), & 0 \leq x \leq 1, \, 0 \leq y \leq 2\\
       0 & \text{ elsewhere.} 
   \end{array} 
   \right.
\] 
Find 

1. $k$; 
2. the marginal pdfs of $X$ and $Y$; and
3. $\Pr(Y < X)$.

For this to be a pdf, (5.2) must be satisfied. 
Now
\begin{align*}
   k \int_0^2 \int_0^1(3x^2+xy) \, dx \, dy&= k \int_0^2 \left[x^3 \, + \, \frac{x^2y}{2}\right]_0^1 \, dy\\
   &=k \int_0^2(1+\frac{y}{2}) \, dy\\
   &= k \left[y+\frac{1}{4}y^2\right]_0^2\\
   &=3k
\end{align*}
so $k$ must be $\frac{1}{3}$.

$\displaystyle f_X(x)=\int_0^2\left(x^2+\frac{xy}{3}\right) dy = \left[x^2y+\frac{xy^2}{6}\right]_{y=0}^2$.
So $\displaystyle f_X(x)=2x^2+\frac{2x}{3}, \, 0 \leq x \leq 1$.
Also $\displaystyle f_Y(y)=\int_0^1\left(x^2+\frac{xy}{3}\right)dx=\left[\frac{1}{3}x^3 + \frac{1}{6}x^2y\right]_{x=0}^1.$

So $f_Y(y) = \frac{1}{6}(2+y)$, $0 \leq y \leq 2$.

If $A = \{(x, y):0 \leq x \leq 1,\ 0 \leq y \leq 2\}$ then
\begin{align*}
   \Pr(Y < X)
   &= \int \!\!\int_{{(x,y) \in A}\atop y<x} f(x,y) \, dx \, dy \\
   &= \frac{1}{3}\int_0^1 \int_y^1(3x^2+xy) \, dx \, dy\\
   &= \frac{1}{3} \int_0^1\left[x^3+\frac{1}{2}x^2y\right]_y^1 dy\\
   &= \frac{1}{3}\int_0^1(1+\frac{1}{2}y-\frac{3}{2}y^3) \, dy\\
   &= \frac{7}{24}
\end{align*}
:::


:::{.example #BVDiscreteMarginal name="Bivariate discrete marginal distributions"}
Recall again Example~\ref{EG:3:2dice}, where we will now find the marginal distributions of $X$ and $Y$.
The probabilities in the first row, for instance, are summed and appear as the first term in the final column and this is the probability that $Y = 0$. 
Similarly, for the other rows.

\begin{center}
\begin{tabular}{cc|ccc|c}
\multicolumn{2}{c}{}&\multicolumn{3}{c}{x}&\multicolumn{1}{c}{}\\
&&0&1&2&$\Pr(Y = y)$ \\ \cline{2-6}
&0&4/9&2/9&1/36&25/36\\
y&1&2/9&1/18&0&10/36\\
&2&1/36&0&0&1/36 \\ \cline{2-6}
&$\Pr(X = x)$&25/36&10/36&1/36&1\\
\end{tabular}
\end{center}

Recalling that $X$ is the number of $5$'s resulting from two dice being thrown, it is clear that the distribution of $X$ is bin(2,\, $\frac{1}{6}$), and the probabilities given in the last row of the table agree with this. 
That is, $\Pr(X = x) = {2 \choose x}\left(\frac{1}{6}\right)^x \left(\frac{5}{6}\right)^{2- x}$,
$x = 0, 1, 2$. 
Of course, the distribution of $Y$ is the same.
:::


:::{.definition #BVDiscreteMarginal name="Bivariate discrete marginal distributions"}
Consider again the random process in Examples \@ref(exm:rvs:bivariate:discreteDC) and \@ref(exm:rvs:bivariate:discreteDCpmf). From Table \@ref(tab:rvs:joint:eg), the marginal distribution for $X_1$ is found simply by summing over the values for $X_2$ in the table. 
When $x_1 = 0$,
\[
   p_{X_1}(0) = \sum_{x_2} p_{X_1, X_2}(0,x_2) = 1/24 + 1/24 + 1/24 +\dots = 6/24.
\]
Likewise,
\begin{align*}
   p_{X_1}(1) 
   &= \sum_{x_2} p_{X_1, X_2}(1,x_2) = 6/12\\
   p_{X_1}(2) 
   &= \sum_{x_2} p_{X_1, X_2}(2,x_2) = 6/24.
\end{align*}
So the marginal distribution of $X_1$ is
\[
   p_{X_1}(x_1) = \begin{cases}
            1/4 & \text{if $x_1 = 0$}\\
            1/2 & \text{if $x_1 = 1$}\\
            1/4 & \text{if $x_1 = 2$}\\
            0 & \text{otherwise.}\\
           \end{cases}
\]
This is equivalent to adding the row probabilities in Table~\ref{TB:rvs:joint:eg}. 
In this example, the marginal distribution is easily found from the total column of Table~\ref{TB:rvs:joint:eg}.
:::


### Conditional distributions {#ConditionalDistributions}

Consider $(X, Y)$ with joint probability function as in Example~\ref{EG:discbiv}, with marginal distributions of $X$ and $Y$ as shown in Table~\ref{TB:3:marg}.

\begin{figure}
\begin{center}
\begin{tabular}{cc|ccc|c}
\multicolumn{2}{c}{}&\multicolumn{3}{c}{x}&\multicolumn{1}{c}{}\\
&&0&1&2&$\Pr(Y = y)$ \\ \cline{2-6}
&0&1/36&1/6&1/4&4/9\\
y&1&1/9&1/3&0&4/9\\
&2&1/9&0&0&1/9\\ \cline{2-6}
&$\Pr(X = x)$&1/4&1/2&1/4&1\\
\end{tabular}
\caption{Joint distribution for Example~\ref{EG:discbiv}}\label{TB:3:marg}
\end{center}
\end{figure}

Suppose we want to evaluate the conditional probability $\Pr(X = 1 \mid Y=1)$. 
We use the fact that $\Pr(A \mid B) = \Pr(A \cap B)/\Pr(B)$. 
So
\[
   \Pr(X = 1 \mid Y = 1) 
   = \frac{\Pr(X = 1, Y = 1)}{\Pr(Y=1)} 
   = \frac{1/3}{4/9}
   = \frac{3}{4}.
\]
So, for each $x\in R_X$ we could find $\Pr(X = x, Y = 1)$ and this is then the conditional distribution of $X$ given that $Y=1$.


:::{.definition #BVDiscreteMarginal name="Bivariate discrete marginal distributions"}
For a discrete random vector $(X, Y)$ with probability function $p_{X, Y}(x, y)$ the *conditional probability distribution* of $X$ given $Y = y$ is defined by
\begin{align}
   p_{X \mid Y = y}(x \mid y)
   &= \Pr(X = x \mid Y = y)\\
   &= \frac{\Pr(X = x, Y = y)}{\Pr(Y = y)}\\
   &= \frac{p_{X,Y}(x,y)}{p_Y(y)}
\end{align}
for $x \in R_X$ and provided $p_Y(y) > 0$.
:::


Similarly, in the continuous case we have:

:::{.definition #BVDiscreteMarginal name="Bivariate discrete marginal distributions"}
If $(X, Y)$ is a continuous 2-dimensional random variable with joint pdf $f_{X, Y}(x, y)$ and respective marginal pdfs $f_X(x)$, $f_Y(y)$, then the *conditional probability distribution* of $X$ given $Y = y$ is defined by
\begin{equation}
   f_{X \mid Y = y}(x \mid y)
   = \frac{f_{X, Y}(x, y)}{f_Y(y)}
\end{equation}
for $x \in R_X$ and provided $f_Y(y) > 0$.
:::


Note that the above conditional pdfs satisfy the requirements for a univariate pdf; that is, $f_{X \mid Y}(x \mid y) \ge 0$ for all $x$ and $\int_0^\infty f_{X\mid Y}(x\mid y)\,dx=1$.

:::{.example #BVDiscreteMarginal name="Bivariate discrete marginal distributions"}
It was shown in Example~\ref{EG:5.4} that if the joint pdf of $X$ and $Y$ was
\[
   f_{X,Y}(x,y) = 
   \left\{
   \begin{array}{ll}
      \frac{1}{3}(3x^2 + xy), & 0 < x <1, \, 0 < y < 2\\
      0 & \text{ elsewhere}, 
      \end{array} 
      \right.
\] 
then the marginal pdfs of $X$ and $Y$ were
\[ 
   f_X(x) = 2x^2 + \frac{2}{3}x, \quad0,x,1,
\]
and that
\[
   f_Y(y) = \frac{1}{6}(2 + y), \quad 0 < y < 2).
\]
Hence, the conditional distribution of $X \mid Y = y$ is
\[
   f_{X\mid Y = y}(x \mid y) 
   = \frac{(3x^2 + xy)/3}{(2 + y)/6}
   = \frac{2x(3x + y)}{2 + y},  0 < x < 1,
\]
and the conditional distribution of $Y \mid X = x$ is 
\[ 
   f_{Y \mid X = x}(y \mid x) 
   = \frac{3x + y}{2(3x + 1)},\quad  0 < y < 2.
\]
It is easy to verify that both these conditional density functions are in fact density functions.
:::



#### Interpretation of a conditional pdf

To interpret for example, $f_{X \mid Y = y}(x \mid y)$, consider slicing through the surface $f_{X, Y}(x, y)$ with the plane $y = c$ say, for $c$ a constant (see Fig~\ref{FG:5.3}). 
The intersection of the plane with the surface, will be proportional to a 1-dimensional pdf. 
This is $f_{X, Y}(x, c)$, which will not, in general, be a density function since the area under this curve will be $f_Y(c)$. 
Dividing by the constant $f_Y(c)$ ensures the area under $\displaystyle\frac{f_{X,Y}(x,c)}{f_Y(c)}$ is one. 
This is a 1-dimensional pdf, namely that of $X$ given $Y = c$; that is $f_{X \mid Y = c}(x\mid c)$.


:::{.example #BVDiscreteMarginal name="Bivariate discrete marginal distributions"}
Consider again the experiment in Examples~\ref{EG:rvs:bivariate:discreteDC} and~\ref{EG:rvs:bivariate:discreteDCpmf}. 
From Table~\ref{TB:rvs:joint:eg}, the conditional distribution for $X_2$ given $X_1 = 0$ can be found. 
Note we need to first find $p_{X_1}(x_1)$, which was done in Example~\ref{EG:rvs:bivariate:discreteconditional}. 
Then,
\begin{align*}
   p_{X_2\mid X_1 = 0}(x_2\mid 0)
   &= \frac{p_{X_1, X_2}(0, x_2)}{p_{X_1}(0)} \\
   &= \frac{p_{X_1, X_2}(0, x_2)}{1/4},
\end{align*}
from which we can deduce
\[
   p_{X_2 \mid X_1 = 0}(x_2 \mid 0) =
   \begin{cases}
      \frac{1/24}{1/4} = 1/6 & \text{for $x_2 = 1$}\\
      \frac{1/24}{1/4} = 1/6 & \text{for $x_2 = 2$}\\
      \frac{1/24}{1/4} = 1/6 & \text{for $x_2 = 3$}\\
      \frac{1/24}{1/4} = 1/6 & \text{for $x_2 = 4$}\\
      \frac{1/24}{1/4} = 1/6 & \text{for $x_2 = 5$}\\
      \frac{1/24}{1/4} = 1/6 & \text{for $x_2 = 6$}\\
   \end{cases}
\]
Note the conditional distribution $p_{X_2\mid X_1=x_1}(x_2\mid x_1)$ is a probability function for $X_2$.
:::




## Independent random variables {#IndependentRVs}

Recall that events $A$ and $B$ are independent if and only if 
\[
   \Pr(A \cap B) = \Pr(A)\Pr(B).
\]
An analogous definition applies for random variables.


:::{.definition #Independent name="Independent randim variables"}
The random variables $X$ and $Y$ with joint df $F_{X, Y}$ and marginal df's $F_X$ and $F_Y$ are *independent* if and only if
\begin{equation}
   F_{X, Y}(x, y) = F_X(x) \times F_Y(y)
\end{equation}
for all $x$ and $y$.

If $X$ and $Y$ are not independent they are said to be *dependent*.
:::

The following theorem is often used to establish independence or dependence of random variables. 
The proof is omitted.

:::{.theorem}
The discrete random variables $X$ and $Y$ with joint probability function $p_{X, Y}$ and marginals $p_X$ and $p_Y$ are *independent* if, and only if,
\begin{equation}
   p_{X, Y}(x, y) = p_X(x) \times p_Y(y) \text{ for every }(x, y) \in R_{X \times Y}.
\end{equation}
The continuous random variables $(X, Y)$ with joint pdf $f_{X, Y}$ and marginal pdfs $f_X$ and $f_Y$ are *independent* if, and only if,
\begin{equation}
   f_{X, Y}(x, y) = f_X(x)\times f_Y(y)
\end{equation}
for all $x$ and $y$.
:::

To show independence for continuous random variables (and analogously for discrete random variables) we must show $f_{X, Y}(x, y) = f_X(x)\times f_Y(y)$ for *all* pairs $(x, y)$.
If $f_{X, Y}(x, y)\neq f_X(x)\times f_Y(y)$ for any one particular pair of $(x, y)$, then $X$ and $Y$ are dependent.

:::{.example #BVDiscrete name="Bivariate discrete"}
The random variables $X$ and $Y$ have the following joint probability distribution:

~ | x=1 | x=2 | x=3 | x=4
--|---|---|---|------
$y = 1$ | 1/30 | 1/30 | 2/30 | 1/30
$y = 2$ | 2/30 | 2/30 | 4/30 | 2/30
$y = 3$ | 3/30 | 3/30 | 6/30 | 3/30

* Find the marginal distributions.
* Determine whether or not $X$ and $Y$ are independent. 

Summing across the rows we obtain the marginal probability function of $Y$. 
That is, 
\[
   \Pr(Y = 1) = 1/6, \quad
   \Pr(Y = 2) = 1/3, \quad
   \Pr(Y = 3) = 1/2.
\] 
Summing each column, we obtain the marginal probability function of $X$. 
That is, 
\[
   \Pr(X = 1) = 1/5, 
   \Pr(X = 2) = 1/5,
   \Pr(X = 3) = 2/5, 
   \Pr(X = 4) = 1/5.
\] 
Clearly (5.16) is satisfied for all pairs $(x, y)$, so $X$ and $Y$ are independent.
:::


:::{.example #BVDiscrete name="Bivariate discrete"}
Given random variables $X$ and $Y$ with joint pdf
\[
   f(x, y)
   = \left
   \{\begin{array}{ll}
      4xy & \text{for $0<x<1, \, 0<y<1$}\\
      0 & \text{elsewhere}\\ 
   \end{array}
   \right.
\]
show that $X$ and $Y$ are independent.

We need to find the marginal distributions of $X$ and $Y$. 
Now
\[
   f_X(x)
   = \int_0^1 4xy \, dy = 2x, \ x \in (0, 1).
\]
Similarly  $f_Y(y) = 2y, \ y \in (0,1)$.
Thus we have $f_X(x) \, f_Y(y) = f(x,y)$ and $X$ and $Y$ are independent.
:::


:::{.example #BVDiscrete name="Bivariate discrete"}
Consider again the experiment in Examples~\ref{EG:rvs:bivariate:discreteDC} and~\ref{EG:rvs:bivariate:discreteDCpmf}. The marginal distribution of $X_1$ was found in Example~\ref{EG:rvs:bivariate:discreteconditional}. 
The marginal distribution of $X_2$ is (check!)
\[
   p_{X_2}(x_2) =
   \begin{cases}
      \frac{1/24}{1/4} = 1/6 & \text{for $x_2 = 1$}\\
      \frac{1/24}{1/4} = 1/6 & \text{for $x_2 = 2$}\\
      \frac{1/24}{1/4} = 1/6 & \text{for $x_2 = 3$}\\
      \frac{1/24}{1/4} = 1/6 & \text{for $x_2 = 4$}\\
      \frac{1/24}{1/4} = 1/6 & \text{for $x_2 = 5$}\\
      \frac{1/24}{1/4} = 1/6 & \text{for $x_2 = 6$}\\
   \end{cases}
\]
To determine if $X_1$ and $X_2$ are independent, *each* $x_1$ and $x_2$  pair must be considered. 
As an example, we see
\begin{align*}
   p_{X_1}(0) \times p_{X_2}(1) = 1/4 \times 1/6 = 1/24 &= p_{X_1, X_2}(0, 1)\\
   p_{X_1}(0) \times p_{X_2}(2) = 1/4 \times 1/6 = 1/24 &= p_{X_1, X_2}(0, 2)\\
   p_{X_1}(1) \times p_{X_2}(1) = 1/2 \times 1/6 = 1/12 &= p_{X_1, X_2}(1, 1)\\
   p_{X_1}(2) \times p_{X_2}(1) = 1/4 \times 1/6 = 1/24 &= p_{X_1, X_2}(2, 1)
\end{align*}
In fact, this is true for all pairs, and so $X_1$ and $X_2$ are independent random variables. 
Independence is, however, obvious from the description of the experiment (see Example~\ref{EG:rvs:bivariate:discreteDC}). 
This can be most easily seen from Table~\ref{TB:rvs:joint:eg}.
:::



:::{.example #BVDiscrete name="Bivariate discrete"}
Consider the continuous random variables $X_1$ and $X_2$ with joint pdf
\[
   f_{X_1, X_2}(x_1, x_2)  =
   \begin{cases}
      \frac{2}{7}(x_1 + 2x_2) & \text{for $0<x_1<1$, $1<x_2<2$}\\
      0 & \text{elsewhere}
   \end{cases}
\]
The marginal distribution of $X_1$ is
\begin{align*}
   f_{X_1}(x_1)
   &= \int_1^2 \frac{2}{7}(x_1 + 2x_2)\,dx_2\\
   &= \frac{2}{7}(x_1 x_2 + x_2^2)\Big|_{x_2=1}^2 =
   \frac{2}{7}(x_1+3)
\end{align*}
for $0 < x_1 < 1$ (and zero elsewhere). Likewise, the marginal distribution of $X_2$ is
\begin{align*}
   f_{X_2}(x_2)
   &= \int_0^1 \frac{2}{7}(x_1 + 2x_2^2)\,dx_1\\
   &= \frac{2}{7}(x_1^2/2 + 2 x_1 x_2)\Big|_{x_1=0}^1
   = \frac{1}{7}(1+4x_2)
\end{align*}
for $1 < x_2 < 2$ (and zero elsewhere). 
(Note  both the marginal distributions must be valid density functions, and so you should check $\int_0^1 f_{X_1}(x_1)\, dx_1 = 1$ and $\int_1^2 f_{X_2}(x_2)\, dx_2 = 1$.)


Since
\[
   f_{X_1}(x_1) \times f_{X_2}(x_2) = \frac{2}{49}(x_1+3)(1+4x_2) \ne f_{X_1, X_2}(x_1, x_2),
\]
the random variables $X_1$ and $X_2$ are *not independent*.

The conditional distribution of $X_1$ given $X_2 = x_2$ is
\begin{align*}
   f_{X_1 \mid X_2 = x_2}(x_1 \mid x_2)
   &= \frac{ f_{X_1, X_2}(x_1, x_2)}{ f_{X_2}(x_2)} \\
   &= \frac{ (2/7) (x_1 + 2x_2)}{ (1/7)(1+4x_2)}
\end{align*}
for $0 < x_1 < 1$ and any given value of $1 < x_2 < 2$. 
(Since the conditional density must be a valid pdf, you should check $\int_0^1 \frac{ (2/7) (x_1 + 2x_2)}{ (1/7)(1+4x_2)}\, dx_1 = 1$.) 
So, for example,
\[
   f_{X_1 \mid X_2 = 1.5}(x_1\mid 1.5)
   = \frac{ (2/7) (x_1 + 2\times 1.5)}{ (1/7)(1+4\times 1.5)}
   = \frac{2}{7}(x_1 + 3)
\]
for $0 < x_1 < 1$ and is zero elsewhere. 
And,
\[
   f_{X_1\mid X_2=1}(x_1\mid 1)
   = \frac{ (2/7) (x_1 + 2\times 1)}{ (1/7)(1+4\times 1)}
   = \frac{2}{5}(x_1 + 2)
\]
for $0 < x_1 < 1$ and is zero elsewhere. 
Since the distribution of $X_1$ depends on the given value of $X_2$, $X_1$ and $X_2$ are *not* independent.
:::



:::{.example #BVDiscrete name="Bivariate discrete"}
Consider the two continuous random variables $Y_1$ and $Y_2$ with joint pf
\[
   f_{Y_1, Y_2}(y_1, y_2)=
   \begin{cases}
      k(y_1 + y_2) & \text{for $0 < y_1 < y_2 < 1$}\\
      0 & \text{elsewhere}
   \end{cases}
\]

* Find a value for $k$.
* Determine if $Y_1$ and $Y_2$ are independent.

A diagram of the region over which $Y_1$ and $Y_2$ are defined is shown in Fig. \@ref(fig:FG:rvs:bvregion).
To find $k$, proceed as follows (being careful with the integration limits).
\begin{align*}
   k\int_0^1\left\{ \int_0^{y_2} y_1 + y_2\,dy_1\right\}dy_2
   &= k \int_0^1 y_1^2/2 + y_1 y_2\Big|_{y_1=0}^{y_2} dy_2 \\
   &= 3k/2 \int_0^1 y_2^2\, dy_2 \\
   &= k/2,
\end{align*}
and so $k = 2$.
:::

```{r BVRegion, echo=FALSE, fig.align="center", fig.cap="The region over which $f_{Y_1, Y_2}(y_1, y_2)$ is defined"}
plot( x = c(0, 1),
      y = c(0, 1),
      xlab = expression(italic(y)[1]),
      ylab = expression(italic(y)[2]),
      type = "n",
      main = expression(paste("The region where"~italic(f)~"is defined")),
      las = 1)
polygon( x = c(0, 1, 0, 0),
         y = c(0, 1, 1, 0),
         col = plotColour)
lines( x = c(0, 1, 0, 0),
       y = c(1, 1, 0, 1),
       lwd = 2)
```


## Expectations involving bivariate distributions

In a manner analogous to the univariate case, we make the following definition concerning expectation of functions of two random variables.

:::{.definition #BVExpectation name="Expectation for bivariate distributions"}
Let $(X, Y)$ be a 2-dimensional random variable and let $u(X, Y)$ be a function of $X$ and $Y$. 
Then the *expectation* or *expected value* of $\text{E}(u(X, Y))$ is

* for $(X, Y)$ discrete with probability function $p_{X, Y}(x, y)$, $(x, y) \in R$,
   \begin{equation}
      \text{E}[u(X,Y)]
      = \sum\sum_{(x, y)\in R}u(x, y) p_{X, Y}(x, y),
   \end{equation}

* for $(X, Y)$ continuous with pdf $f_{X, Y}(x, y)>0, \, (x,y) \in R$
   \begin{equation}
      \text{E}[(u(X,Y)]
      =\int \!\!\int_R u(x, y) f_{X, Y}(x, y) \, dx \, dy.
   \end{equation}
:::

Although we don't give it here, this definition can obviously be extended to the expectation of a function of any number of random variables.

:::{.example #Exprain name="Exponential distributions"}
Consider the joint distribution of $X$ and $Y$ in Example~\ref{EG:rvs:bivariate:discreteDCpmf}. 
Determine $\text{E}(X + Y)$; i.e., the mean of the number of heads plus the number showing on the die.

From the Definition~\ref{DF:5.15} we have $u(X, Y) = X + Y$ and so
\begin{align*}
   \text{E}(X + Y) 
   &= \sum_{x = 0}^2 \sum_{y = 1}^6 (x + y) p_{X, Y}(x, y)\\
   &= 1\times(1/24) + 2\times(1/24) + \dots + 6\times(1/24)\\
   & \qquad + 2\times(1/12) + 3\times(1/12) + \dots + 7\times(1/12)\\
   & \qquad + 3\times(1/24) + 4\times(1/24) + \dots + 8\times(1/24)\\
   &= 21/24 + 27/12 + 33/24\\
   &= 4.5
\end{align*}
Notice the answer is just $\text{E}(X) + \text{E}(Y) = 1 + 3.5 = 4.5$.
This is no coincidence as we see from Theorem~\ref{TM:5.15} below.
:::


:::{.example #Exprain name="Exponential distributions"}
Consider Example~\ref{EG:rvs:bivar:cont}.
Determine $\text{E}(XY)$.

Now $u(X, Y) = XY$ and we have
\begin{align*}
   \text{E}(XY)
   &= \frac65 \int_0^1\int_0^1 xy(x + y^2)\,dx\,dy\\
   &= \frac65\int_0^1 \frac{x^3y}3 + \frac{x^2 y^3}2\biggm|_{x = 0}^{x = 1}\,dy\\
   &= \frac65\int_0^1 \frac y3+\frac{y^3}2\,dy\\
   &= \frac65\left(\frac{y^2}6 + \frac{y^4}8\right)\biggm|_0^1\\
   &= \frac65\left(\frac16 + \frac18\right)\\
   &= \frac7{20}
\end{align*}
For this example, unlike the previous one, we cannot find an alternative simple calculation based on $\text{E}(X)$ and $\text{E}(Y)$ because $\text{E}(XY)\neq\text{E}(X) \text{E}(Y)$ in general.
:::


:::{.theorem}
If $X$ and $Y$ are any random variables and $a$ and $b$ are any constants then 
\[
   \text{E}(aX + bY) = a\text{E}(X) + b\text{E}(Y).
\]
:::

This theorem won't surprise after seeing Theorem~\ref{TM:3.1} but it is a very powerful and useful result. 
The proof given here is for the discrete case but the continuous case is analogous.

:::[.proof}
\begin{align*}
   E(aX + bY)
   &= \sum\sum_{(x, y) \in R}(ax + by) \, p_{X, Y}(x, y), \text{ by definition}\\
  &= \sum_x \sum_y ax p_{X, Y}(x, y) + \sum_x \sum_y by p_{X, Y}(x, y)\\
  &= a\sum_x x\sum_y p_{X, Y}(x, y) + b\sum_y y\sum_x p_{X, Y}(x, y)\\
  &= a\sum_x x \Pr(X = x) + b\sum_y y \Pr(Y = y)\\
  &= a\text{E}(X) + b\text{E}(Y)
\end{align*}
:::


Note that this result is true whether or not $X$ and $Y$ are independent.

Theorem~\ref{TM:5.15} naturally generalises to the expected value of a *linear combination of random variables* as follows.

:::{.theorem}
If $X_1, X_2,\dots, X_n$ are random variables and $a_1, a_2,\ldots a_n$ are any constants then
\[
   \text{E}\left(\sum_{i = 1}^n a_i X_i \right)
   =\sum_{i = 1}^n a_i \, \text{E}(X_i)
\]
:::


:::{.proof}
The proof follows directly from Theorem~\ref{TM:5.15} by induction.
:::


### Moments of a bivariate distribution

The idea of a moment in the univariate case naturally extends to the bivariate case. 
Hence we can define $\mu'_{rs} = \text{E}(X^r Y^s)$ or $\mu_{rs} = \text{E}((X - \mu_X)^r (Y - \mu_Y)^s)$ as raw and central moments for a bivariate distribution.

The most important of these moments is the covariance.

:::{.definition #BVDiscreteMarginal name="Bivariate discrete marginal distributions"}
The *covariance* of $X$ and $Y$ is defined as
\begin{align*}
   \text{Cov}(X, Y) 
   &= \text{E}[(X - \mu_X)(Y - \mu_Y)]\\
   &= \begin{cases}
      \displaystyle
         \sum_{x} \sum_{y} (x - \mu_X)(y - \mu_Y) p_{X, Y}(x, y) & \text{for $X, Y$ discrete}\\[6pt]
         \displaystyle
         \int_{-\infty}^\infty\!\int_{-\infty}^\infty (x - \mu_X)(y - \mu_Y) f_{X, Y}(x, y)\, dx\, dy & \text{for $X, Y$ continuous}
       \end{cases}
\end{align*}
:::

The covariance is a measure of how $X$ and $Y$ vary jointly in the sense that a positive covariance indicates that 'on average' $X$ and $Y$ increase (or decrease) together whereas a negative covariance indicates that `on average' as $X$ increases and $Y$ decreases (and vice versa). 
We say that covariance is a measure of *linear dependence*.

Covariance is best evaluated from the computational formula:

:::{.theorem name="Covariance"}
For any random variables $X$ and $Y$, 
\[
   \text{Cov}(X, Y)
   =
   \text{E}(XY) - \text{E}(X)\text{E}(Y).
\]
:::

:::{.proof}
The proof is a lovely application of Theorems~\ref{TM:5.15} and~\ref{TM:3.1}.
\begin{align*}
   \text{Cov}(X, Y)
   &= \text{E}( (X - \mu_X)(Y-\mu_Y)) \\
   &= \text{E}( XY - \mu_X Y - \mu_Y X+ \mu_X\mu_Y) \\
   &= \text{E}( XY ) - \mu_X\text{E}(Y) - \mu_Y\text{E}(X) +  \mu_X \mu_Y \\
   &= \text{E}( XY ) - \mu_X\mu_Y - \mu_Y\mu_X +  \mu_X \mu_Y \\
   &= \text{E}( XY ) - \mu_X \mu_Y.
\end{align*}
:::


Notice to compute the covariance, $\text{E}(X)$, $\text{E}(Y)$, $\text{E}(X Y)$ need to be computed, and so the joint and marginal distributions of $X$ and $Y$ are needed.

Covariance has units given by the product of the units of $X$ and $Y$. 
For example, if $X$ is in metres and $Y$ is in seconds then $\text{Cov}(XY)$ has the units metre-seconds.

In order to compare the strength of covariation amongst pairs of random variables it is desirable to eliminate the effect of the units. 
Correlation does this by scaling the covariance in terms of the standard deviations.

:::{.definition #Correlation name="Correlation"}
The *correlation coefficient* between the random variables $X$ and $Y$ is denoted by $\text{Corr}(X, Y)$ or $\rho_{X, Y}$ and is defined as
\[
   \rho_{X, Y} 
   = \frac{\text{Cov}(X, Y)}{\sqrt{ \text{Var}(X)\text{Var}(Y)}}
   = \frac{\sigma_{X, Y}}{\sigma_X \sigma_Y}
\]
:::

If there is no confusion over which random variables are involved, we shall simply write $\rho$ rather than $\rho_{XY}$.

It can be shown that $-1 \leq \rho \leq 1$.

:::{.example #Exprain name="Exponential distributions"}
Consider two discrete random variables $X$ and $Y$ with the joint pf given below.

\begin{center}
\begin{tabular}{c|ccc|c}
& \multicolumn{3}{c}{$x$}\\
 $y$ & $0$ & $1$ & $2$ & Total
\\
\hline
$-1$ & $1/8$ & $1/4$ & $1/8$ & $1/2$\\
$1$ & $1/6$ & $1/12$ & $1/4$ & $1/2$
\\
\hline Total & $7/24$ & $1/3$ & $3/8$ & $1$
\end{tabular}
\end{center}

To compute the correlation coefficient, the following steps are required.

* $\text{Corr}(X, Y) = \text{Cov}(X, Y)/\sqrt{ \text{Var}(X)\text{Var}(Y) }$, so $\text{Var}(X)$, $\text{Var}(Y)$ and are needed;
* To find $\text{Var}(X)$ and $\text{Var}(Y)$,  $\text{E}(X)$ and $\text{E}(X^2)$, $\text{E}(Y)$ and $\text{E}(Y^2)$ are needed, so the marginal pfs of $X$ and $Y$ are required.

So first, the marginal pfs are
\[
   p_X(x) = \sum_{y = -1, 1} p_{X, Y}(x, y) =
      \begin{cases}
          7/24 & \text{for $x = 0$}\\
          8/24 & \text{for $x = 1$}\\
          9/24 & \text{for $x = 2$}\\
          0 & \text{otherwise}
      \end{cases}
\]
and
\[
   p_Y(y) = \sum_{x = 0}^2 p_{X, Y}(x, y) =
      \begin{cases}
          1/2 & \text{for $y = -1$}\\
          1/2 & \text{for $y = 1$}\\
          0 & \text{otherwise}
      \end{cases}
\]
So then,
\begin{align*}
   \text{E}(X) &= (7/24 \times 0) + (8/24 \times 1) + (9/24\times 2) = 26/24\\
   \text{E}(X^2) &= (7/24 \times 0^2) + (8/24 \times 1^2) + (9/24\times 2^2) = 44/24\\
   \text{E}(Y) &= (1/2 \times -1) + (1/2 \times 1) = 0\\
   \text{E}(Y^2) &= (1/2 \times (-1)^2) + (1/2 \times 1^2) = 1
\end{align*}
giving $\text{Var}(X) = 44/24 - (26/24)^2 = 0.6597222$ and $\text{Var}(Y) = 1 - 0^2 = 1$. 
Then,
\begin{align*}
   \text{E}(XY) &= \sum_x\sum_y xy\,p_{X,Y}(x,y) \\
    &= (0\times -1 \times 1/8)  + (0\times 1 \times 1/6) + \cdots + (2\times 1 \times 1/4) \\
    &= 1/12.
\end{align*}
Hence,
\[
   \text{Cov}(X,Y) = \text{E}(XY) - \text{E}(X) \text{E}(Y) = 1/12 - (26/24\times 0) = 1/12,
\]
and
\begin{align*}
   \text{Corr}(X,Y)
   &= \frac{ \text{Cov}(X,Y)}{\sqrt{ \text{Var}(X)\text{Var}(Y) } }\\
   &= \frac{1/12}{\sqrt{0.6597222 \times 1}}\\
   &= 0.1025978,
\end{align*}
so the correlation coefficient is about $0.10$, and therefore there is a small positive linear association between $X$ and $Y$.
:::


### Properties of covariance and correlation

* The correlation has no units.
* The covariance has units; if $X_1$ is measured in kilograms and $X_2$ in centimetres, then the units of the covariance are kg-cm.
* If the units of measurements change, the numerical value of the covariance will change, but the numerical value of the correlation will stay the same.
   (For example, if $X_1$ is changed from kilograms to grams, the correlation will not change in value, but the covariance will.)
* The correlation is a number between $-1$ and $1$ (inclusive).
  When the correlation coefficient (or covariance) is negative, a *negative linear relationship* is said to exist
   between the two variables; likewise, when the correlation coefficient (or covariance) is positive, a *positive linear relationship* is said to exist between the two variables.
* When the correlation coefficient (or covariance) is zero, no *linear* dependence is said to exist.


:::{.theorem}
For random variables $X$, $X$ and $Z$, and constants $a$ and $b$

* $\text{Cov}(X, Y) = \text{Cov}(Y, X)$
* $\text{Cov}(aX,bY) = ab\,\text{Cov}(X, Y)$
* $\text{Var}(aX + bY) = a^2\text{Var}(X) + b^2\text{Var}(Y) + ab\,\text{Cov}(X,Y)$
* If $X$ and $Y$ are independent, then $\text{E}(X Y) = \text{E}(X)\text{E}(Y)$ and hence $\text{Cov}(X,Y) = 0$
* $\text{Cov}(X, Y) = 0$ does not imply $X$ and $Y$ are independent, except for the special case of the bivariate normal distribution.
:::

A zero correlation coefficient in an indication of no *linear* dependence only.


:::{.example #Exprain name="Exponential distributions"}
Consider $X_1$ with the pf
\begin{center}
\begin{tabular}{c|ccc}
$x_1$ & $-1$ & $0$ & $1$ \\
\hline $p_{X_1}(x_1)$ & $1/3$ & $1/3$ & $1/3$
\end{tabular}
\end{center}

Define $X_2$ to be explicitly related to $X_1$ such that $X_2 = X_1^2$ (that is, we know there is a relationship between $X_1$ and $X_2$, but it is  not linear). 
The joint pf for $(X_1, X_2)$ is
\begin{center}
\begin{tabular}{c|ccc|c}
& \multicolumn{3}{c}{$x_1$}\\
$x_2$ & $-1$ & $0$ & $1$ & Total
 \\
\hline\
$0$ & $0$ & $1/3$ & $0$ & $1/3$\\
$1$ & $1/3$ & $0$ & $1/3$ & $2/3$
\\
\hline Total & $1/3$ & $1/3$ & $1/3$ & $1$
\end{tabular}
\end{center}
Then
\begin{equation*}
   \text{Cov}(X_1, X_2)
   = \text{E}(X_1 X_2) - \text{E}(X_1)\text{E}(X_2)
   = 0 - 0\times 2/3 = 0
\end{equation*}
so $\text{Corr}(X_1, X_2) = 0$. 
But $X_1$ and $X_2$ *are certainly related* as $X_2$ was explicitly defined as a function of $X_1$.
Since the correlation is a measure of the strength of the *linear* relationship between two random variables, a correlation of zero simply is indication of no *linear* relationship between $X_1$ and $X_2$. 
(As is the case in this example, there may be a different relationship between the variables, but no linear relationship.)
:::


## Conditional expectations {#ConditionalExpectation}

Conditional expectations are simply expectations computed from a conditional distribution.

### Conditional mean

The conditional mean is the expected value computed from a conditional distribution.

:::{.definition #ConditionalExpectation name="Conditional expectation"}
The *conditional expected value* or *conditional mean* of a random variable $X$ for given $Y = y$ is denoted by $\text{E}(X \mid Y = y)$ and is defined as
\begin{align*}
   \text{E}(X \mid Y = y)
   &=
   \begin{cases}
      \sum_{x} x p_{X\mid Y}(x\mid y) & \text{if $p_{X\mid Y}(x\mid y)$ is the conditional pf}\\
      \int_{-\infty}^\infty x f_{X\mid Y}(x\mid y)\, dx & \text{if $f_{X\mid Y}(x\mid y)$ is the conditional pdf}\\
   \end{cases}
\end{align*}
:::

$\text{E}(X \mid Y = y)$ is typically denoted $\mu_{X \mid Y = y}$.

:::{.example #Exprain name="Exponential distributions"}
Consider the two random variables $X$ and $Y$ with joint pdf
\[
   f_{X, Y}(x, y) =
      \begin{cases}
         \frac{3}{5}(x + xy + y^2) & \text{for $0 < x < 1$ and $-1 < y < 1$}\\
         0 & \text{otherwise}
      \end{cases}
\]
To find $f_{Y \mid X = x}(y\mid x)$, we first need $f_X(x)$.
\[
   f_X(x) = \int_{-1}^1 f_{X,Y}(x,y) dy = \frac{3}{15}(6x + 2)
\]
for $0 < x < 1$. 
Then,
\begin{align*}
   f_{Y \mid X = x}(y \mid x)
   &= \frac{ f_{X, Y}(x, y)}{ f_X(x) } \\
   &= \frac{ (3/5)(x + xy + y^2) }{ (3/15)(6x + 2) } \\
   &= \frac{3(x + xy + y^2)}{6x + 2}
\end{align*}
for $-1 < y < 1$ and given $0 < x < 1$. 
The expected value of $Y$ given $X = x$ is then
\begin{align*}
   \text{E}(Y\mid X=x)
   &= \int_{-1}^1 y f_{Y\mid X=x}(y\mid x)\, dy\\
   &= \int_{-1}^1 y \frac{3(x+xy+y^2)}{6x+2}\, dy\\
   &= \frac{3}{6x+2} \int_{-1}^1 y(x+xy+y^2) \, dy\\
   &= \frac{x}{3x+1}.
\end{align*}
This expression indicates that the condiitonal expected value of $Y$ depends on the given value of $X$; for example,
\begin{align*}
   \text{E}(Y\mid X=0) &= 0/1 = 0\\
   \text{E}(Y\mid X=0.5) &= \frac{0.5}{3\times 0.5 + 1} = 0.2\\
   \text{E}(Y\mid X=1) &= 1/4
\end{align*}
Since $\text{E}(Y\mid X=x)$ depends on the value of $X$, $X$ and $Y$ are *not* independent.
:::


### Conditional variance {#ConditionalVariance}

The conditional variance is the variance computed from a conditional distribution.

:::{.definition #ConditionalVariance name="Conditional variance"}
The *conditional variance* of a random variable $X$ for given $Y = y$ is denoted by $\text{Var}(X \mid Y = y)$ and is defined as
\begin{align*}
   \text{Var}(X \mid Y = y)
   &=
   \begin{cases}
      \displaystyle
      \sum_{x} (x-\mu_{X\mid y})^2 p(x\mid y) & \text{if $p(x\mid y)$ is the conditional pf}\\[6pt]
      \displaystyle
      \int_{-\infty}^\infty (x-\mu_{X\mid y})^2 f(x\mid y)\, dx & \text{if $f(x\mid y)$ is the conditional pdf}\\
   \end{cases}
\end{align*}
where $\mu_{X \mid y}$ is the *conditional mean* of $X$ given $Y = y$.
:::

$\text{Var}(X \mid Y = y)$ is typically denoted $\sigma^2_{X \mid Y = y}$.

:::{.example #Exprain name="Exponential distributions"}
Refer to Example~\ref{EG:expect:condmean}. 
The conditional variance of $Y$ given $X = x$ can be found by first computing $\text{E}(Y^2\mid X = x)$.
\begin{align*}
   \text{E}(Y^2\mid X = x)
   &= \int_{-1}^1 y^2 f_{Y\mid X=x}(y\mid x)\,dy \\
   &= \frac{3}{6x+2} \int_{-1}^1 y^2 (x+xy+y^2)\, dy \\
   &= \frac{3}{6x+2} \times \frac{10x+6}{15}\\
   &= \frac{5x+3}{5(3x+1)}.
\end{align*}
So the conditional variance is
\begin{align*}
   \text{Var}(Y\mid X=x)
   &= \text{E}(Y^2\mid X=x) - \left( \text{E}(Y\mid X=x) \right)^2 \\
   &= \frac{5x+3}{5(3x+1)} - \left( \frac{x}{3x+1}\right)^2 \\
   &= \frac{10x^2 + 14x + 3}{5(3x+1)^2}
\end{align*}
for given $0 < x < 1$. 
Hence the variance of $Y$ depends on the value of $X$ that is given; for example,
\begin{align*}
   \text{Var}(Y\mid X=0) &= 3/5 = 0.6\\
   \text{Var}(Y\mid X=0.5) &= \frac{10\times (0.5)^2
   + (14\times0.5) + 3}{5(3\times0.5 + 1)^2} = 0.4\\
   \text{Var}(Y\mid X=1) &= 27/80 = 0.3375
\end{align*}
:::



In general, to compute the conditional variance of $X\mid Y = y$ given a joint probability function, the following steps are required.

* Find the marginal distribution of $Y$.
* Use this to compute the conditional probability function $f_{X \mid Y = y}(x \mid y) = f_{X, Y}(x, y)/f_{X}(x)$.
* Find the conditional mean $\text{E}(X \mid Y = y)$.
* Find the conditional second raw moment $\text{E}(X^2 \mid Y = y)$.
* Finally, compute $\text{Var}(X\mid Y=y) = \text{E}(X^2\mid Y=y) - (\text{E}(X\mid Y=y))^2$.


:::{.example #Exprain name="Exponential distributions"}
Two discrete random variables $U$ and $V$ have the joint pf given below.

\begin{tabular}{c|ccc|c}
& \multicolumn{3}{c}{$u$}\\
$v$ & $10$ & $11$ & $12$ & Total
\\
\hline
$0$ & $1/9$ & $1/18$ & $1/6$ & $1/3$ \\
$1$ & $1/3$ & $1/3$ & $0$ & $2/3$
\\
\hline Total & $4/9$ & $7/18$ & $1/6$ & $1$
\end{tabular}

Find the conditional variance of $V$ given $U = 11$.

Using the steps outlined above:

1.First, find the marginal distribution of $U$.
  From the joint pf table,
   \[
      p_U(u) = \begin{cases}
         4/9 & \text{for $u = 10$}\\
         7/18 & \text{for $u = 11$}\\
         1/6 & \text{for $u = 12$}\\
         0 & \text{otherwise}\\
         \end{cases}
   \]
2. Secondly, compute the conditional probability function
   \begin{align*}
      p_{V\mid U = 11}(v \mid u = 11)
      &= p_{U, V}(u,v)/p_{U}(u = 11) \\
      &= \begin{cases}
             \frac{1/18}{7/18} = 1/7 & \text{if $v=0$}\\
             \frac{1/3}{7/18}  = 6/7 & \text{if $v=1$}
          \end{cases}
   \end{align*}
   using $p_U(u = 11) = 7/18$ from the step 1.
3. Thirdly, find the conditional mean
     \[
      \text{E}(V\mid U=11) = \sum_v v p_{V\mid U=11}(v\mid u) =
         \left(\frac{1}{7}\times 0\right) + \left(\frac{6}{7}\times 1\right)  = 6/7
   \]
4. Fourthly, find the conditional second raw moment
     \[
      \text{E}(V^2\mid U=11) = \sum_v v^2 p_{V\mid U=11}(v\mid u) =
         \left(\frac{1}{7}\times 0^2\right) + \left(\frac{6}{7}\times 1^2\right)  = 6/7
   \]
5. Finally, compute
   \begin{align*}
   \text{Var}(V\mid U=11) &= \text{E}(V\mid U=11) - (\text{E}(V\mid U=11))^2\\
   &= (6/7) - (6/7)^2\\
   &\approx  0.1224
   \end{align*}

:::



## The multivariate extension {#MultivariateExtensions}

Results involving expectations naturally generalise from the bivariate to the multivariate case.

We have already seen the expectation of a linear combination of random variables in Theorem~\ref{TM:5.16}. 
The variance of a linear combination of random variables is given in the following theorem.

:::{.theroem}
If $X_1, X_2, \dots, X_n$ are random variables and $a_1, a_2,\ldots a_n$ are any constants then
\[
   \text{Var}\left(\sum_{i = 1}^n a_iX_i \right)
   =
   \sum^n_{i = 1}a^2_i\text{Var}(X_i) + 2{\sum\sum}_{i<j}a_ia_j\text{Cov}(X_i,X_j)
\]
:::

:::{.proof}
For convenience, put $Y = \sum_{i = 1}^n a_iX_i$.
Then by definition of variance
\begin{align*}
     \text{Var}(Y) & = \text{E}(Y-\text{E}(Y))^2\\
     & = \text{E}[a_1X_1+\dots +a_nX_n-a_1\mu_1- \dots a_n\mu_n]^2\\
     & = \text{E}[a_1(X_1-\mu_1)+\dots +a_n(X_n -\mu_n)]^2\\
     & = \text{E}\left[ \sum_ia^2_i(X_i-\mu_i)^2+2\sum\sum_{i<j}a_ia_j(X_i-
\mu_i)X_j-\mu_j)\right]\\
     & = \sum_ia^2_i\text{E}(X_i-\mu_i)^2 +2{\sum\sum}_{i<j}a_ia_j\text{E}(X_i-
\mu_i)X_j-\mu_j)\quad\text{using Theorem~\ref{TM:5.16}}\\
     & = \sum_ia^2_i\sigma^2_i
+2{\sum\sum}_{i<j}a_ia_j\text{Cov}(X_i,\,X_j).
     \end{align*}
:::

In statistical theory, an important special case of Theorem~\ref{TM:varlincomb} occurs when the $X_i$ are independently and identically distributed. 
That is, each of $X_1, X_2, \dots, X_n$ has got the same distribution and are independent of each other. 
(We see the relevance of this in Chap. \@ref(SamplingDistributions).)
Because of its importance this special case is called a corollary of Theorems~\ref{TM:5.16} and~\ref{TM:varlincomb}.

:::{.corollary}
If $X_1, X_2, \dots, X_n$ are independently distributed random variables, each with mean $\mu$ and variance $\sigma^2$, and $a_1, a_2,\ldots a_n$ are any constants, then
\begin{align*}
   \text{E}\left(\sum_{i=1}^n a_iX_i \right)
   &=\mu\sum_{i=1}^n a_i\\
   \text{Var}\left(\sum_{i=1}^n a_iX_i \right)
   &=\sigma^2\sum^n_{i=1}a^2_i
\end{align*}
:::

:::{.proof}
Exercise!
:::


### Vector formulation {#Vectors}

Linear combinations of random variables are most elegantly dealt with using the methods and notation of vectors and matrices.

In the bivariate case we define
\begin{align}
   \mathbf{X} 
   &= \left[ \begin{array}{c} X_1 \\ X_2 \end{array} \right]
  \label{EQN:matrix1} \\
  \text{E}(\mathbf{X}) & =  \text{E} \left( \left[ \begin{array}{c} X_1 \\ X_2
  \end{array} \right] \right) =  \left[ \begin{array}{c} \mu_1 \\
  \mu_2 \end{array} \right] =
  \mathbf{\mu}\label{EQN:matrix2} \\
  \text{Var}(\mathbf{X}) & = \text{Var}\left( \left[ \begin{array}{c} X_1 \\ X_2
  \end{array} \right] \right)
  = \left[ \begin{array}{cc} \sigma^2_1 & \sigma_{12} \\
                              \sigma_{21}& \sigma^2_2 \end{array} \right]
  = \mathbf{\Sigma}\label{EQN:matrix3}
\end{align}

The matrix $\mathbf{\Sigma}$ is called the *variance-covariance* matrix. 
Notice that this matrix is square and symmetric since $\sigma_{12} = \sigma_{21}$.

The linear combination $Y = a_1 X_1 + a_2 X_2$ can be expressed 
\begin{equation}
   Y 
   = a_1 X_1 + a_2 X_2 
   = [a_1, a_2] 
   \left[ 
   \begin{array}{c} 
   X_1 \\ X_2 
   \end{array} 
   \right] 
   =\mathbf{a}'\mathbf{X}
\end{equation}
where the (column) vector $\mathbf{a}=\left[ \begin{array}{c} a_1 \\ a_2 \end{array} \right]$.

With the standard rules of matrix multiplication, Theorems~\ref{TM:5.16} and~\ref{TM:varlincomb} applied to~\ref{EQN:bivsum} then give respectively (check the details for yourself)
\begin{equation}
   \text{E}(Y)
   = \text{E}(\mathbf{a}'\mathbf{X})
   = [a_1,a_2]
   \left[ 
   \begin{array}{c} 
      \mu_1 \\ 
      \mu_2 
    \end{array} 
    \right]
   = \mathbf{a}'\mathbf{\mu}
\end{equation}
and
\begin{align}
  \text{Var}(Y)
  &= \text{Var}(\mathbf{a}'\mathbf{X})\nonumber\\
  &= [a_1,a_2]
  \left[ \begin{array}{cc} 
     \sigma^2_1 & \sigma_{12} \\
     \sigma_{21}& \sigma^2_2 
  \end{array} \right]
  \left[ \begin{array}{c} 
     a_1 \\ 
     a_2
  \end{array} \right]\nonumber\\
  &= \mathbf{a}'\mathbf{\Sigma}\mathbf{a}.
\end{align}

The vector formulation of these results apply directly in the multivariate case as described below.

Write
\begin{align*}
  \mathbf{X} 
  & =(X_1, X_2, \ldots, X_n)' \\
  \text{E}(\mathbf{X}) 
  & =(\mu_1, \ldots, \mu_n)' = \mathbf{\mu}' \\
  \text{Var}(\mathbf{X}) 
  & = \mathbf{\Sigma} \\
   \mathbf{a}' & = \left[a_1,a_2,\ldots, a_n \right] \\
\end{align*}

Now Theorems~\ref{TM:5.16} and~\ref{TM:varlincomb} re-expressed in vector form become:


:::{.theorem}
If $\mathbf{X}$ is a random vector of length $n$ with mean $\mathbf{\mu}$ and variance $\mathbf{\Sigma}$ and $\mathbf{a}$ is any constant vector of length $n$ then
\[
   \text{E}(\mathbf{a}'\mathbf{X})
   =
   \mathbf{a}'\mathbf{\mu}
\]
and
\[
   \text{Var}(\mathbf{a}'\mathbf{X})=\mathbf{a}'\mathbf{\Sigma}\mathbf{a}
\]
:::

:::{.proof}
Exercise!
:::

These elegant statements concerning linear combinations are a feature of vector formulations that extend to many statistical results in the theory of statistics.

One obvious advantage of this formulation is the implementation in vector-based computer programming used by packages such as R.

As a further example we finish this module by providing without proof (although the proof is relatively straightforward) one further result, this time involving two linear combinations.

:::{.theorem}
If $\mathbf{X}$ is a random vector of length $n$ with mean $\mathbf{\mu}$ and variance $\mathbf{\Sigma}$ and $\mathbf{a}$ and $\mathbf{b}$ are any constant vectors, each of length $n$, then
\[
   \text{Cov}(\mathbf{a}'\mathbf{X},\mathbf{b}'\mathbf{X}) 
   = 
   \mathbf{a}'\mathbf{\Sigma}\mathbf{b}.
\]
:::



:::{.example #Exprain name="Exponential distributions"}
Suppose the random variables $X_1, X_2, X_3$ have respective means 1, 2, and 3, respective variances 4, 5, and 6, and covariances $\text{Cov}(X_1, X_2) = -1$, $\text{Cov}(X_1, X_3) = 1$ and $\text{Cov}(X_2, X_3) = 0$.
Consider the random variables $Y_1 = 3X_1 + 2X_2 - X_3$ and $Y_2 = X_3 - X_1$. 
Determine $\text{E}(Y_1)$, $\text{E}(Y_2)$, $\text{Var}(Y_1)$, $\text{Var}(Y_2)$ and $\text{Cov}(Y_1,Y_2)$

A vector formulation of this problem allows us to use Theorems~\ref{TM:veccomb} and~\ref{TM:veccov} directly. 
Putting $\mathbf{a}' = (3, 2, -1)'$ and $\mathbf{b}' = (-1, 0, 1)'$ we have
\[
   Y_1 = \mathbf{a}'\mathbf{X}
   \quad\text{and}\quad 
   Y_2 = \mathbf{b}'\mathbf{X}
\]
where $\mathbf{X}'=(X_1, X_2, X_3)'$.

We also define $\mathbf{\mu}'=(1,2,3,)'$ and $\mathbf{\Sigma}=\left[\begin{array}{ccc} 4&-1&1\\-1&5&0\\
1&0&6\end{array}\right]$ as the mean and variance-covariance matrix respectively of $\mathbf{X}$.

Then
\[
   \text{E}(Y_1)
   =\mathbf{a}'\mathbf{\mu}
   =(3, 2, -1)'
   \left[\begin{array}{c} 1\\2\\3\end{array}\right] 
   =4
\]
and
\[
   \text{Var}(Y_1)
   =\mathbf{a}'\mathbf{\Sigma}\mathbf{a}
   = (3, 2, -1)'
   \left[ \begin{array}{ccc} 4&-1&1\\-1&5&0\\
   1&0&6\end{array} \right]\left[\begin{array}{c} 
   3\\2\\-1
   \end{array}\right] 
   = 44
\]
Similarly $\text{E}(Y_2) = 2$ and $\text{Var}(Y_2) = 8$.

Finally we have
\[
   \text{Cov}(Y_1, Y_2)
   = \mathbf{a}'\mathbf{\Sigma}\mathbf{b}
   = (3, 2, -1)'
   \left[ \begin{array}{ccc} 4&-1&1\\-1&5&0\\
      1&0&6\end{array} \right]\left[\begin{array}{c}
      -1\\0\\1
  \end{array}\right] 
  = -12
\]
:::




## Multinomial distribution {#MultinomialDistribution}

The *multinomial distribution* is a generalization of the binomial distribution and is an example of a discrete multivariate distribution.


:::{.definition #MultinomialDistribution name="Multinomial distribution"}
Consider an experiment with the the sample space partitioned as  $S = \{B_1, B_2, \ldots, B_k\}$. 
Let $p_i = \Pr(B_i), \ i = 1, 2,\ldots k$ where $\sum_{i = 1}^k p_i = 1$. 
Suppose there are $n$ repetitions of the experiment in which $p_i$ is constant. 
Let the random variable $X_i$ be the number of times (in the $n$ repetitions) that the event $B_i$ occurs. 
In this situation, the random vector $(X_1, X_2, \dots, X_k)$ is said to have a *multinomial distribution* with probability function
\begin{equation}
   \Pr(X_1 = x_1, X_2 = x_2, \ldots, X_k = x_k)
   = \frac{n!}{x_1! x_2! \ldots x_k!} 
      p_1^{x_1} p_2^{x_2} \ldots p_k^{x_k},
\end{equation}
where $R_X = \{(x_1, \ldots x_k) : x_i = 0,1,\ldots,n, \, i = 1, 2, \ldots k, \, \sum_{i = 1}^k x_i = n\}$.
:::

The part of (\ref{EQN:multinomial}) involving factorials arises as the number of ways of arranging $n$ objects, $x_1$ of which are of the first kind, $x_2$ of which are of the second kind, etc. 
The above distribution is really $(k - 1)$-variate since $x_k = n-\sum_{i = 1}^{k - 1}x_i$.
In particular if $k = 2$, the multinomial distribution reduces to the binomial distribution which is a univariate distribution.

If we consider $X_i$, it is the number of times (out of $n$) that the event $B_i$, which has probability $p_i$, occurs. 
So the random variable $X_i$ clearly has a binomial distribution with parameters $n$, $p_i$.
We see then that the marginal probability distribution of one of the components of a multinomial distribution is a binomial distribution.

Notice that the distribution in Example~\ref{EG:3:2dice} is an example of a *trinomial distribution*. 
The probabilities shown in Table~\ref{TB:2dice} can be expressed algebraically as
\[
   \Pr(X = x, Y = y)
   = \frac{2!}{x!y!(2 - x - y)!}
      \left(\frac{1}{6}\right)^x\left(\frac{1}{6}\right)^y\left(\frac{2}{3}\right)^ {2 - x - y}
\]
for $x, y= 0 , 1 , 2; x + y \leq 2$.

The following are the basic properties of the multinomial distribution.


:::{.theroem}
Suppose $(X_1, X_2, \ldots, X_k)$ has the multinomial distribution given in Definition \@ref(def:MultinomialDistribution). 
Then for $i = 1, 2, \ldots, k$:

* $\text{E}(X_i) = np_i$
* $\text{Var}(X_i) = n p_i(1 - p_i)$
* $\text{Cov}(X_i, X_j) = -n p_i p_j$ for $i \neq j$
:::


:::{.proof}
We will use $x$ for $x_1$ and $y$ for $x_2$ in 3.\ for convenience.

1 and 2 follow from the fact that $X_i \sim \text{bin}(n, p_i)$.
Consider only the case $k=3$, and note that
\[
   \sum_{(x, y) \in R} 
   \frac{n!}{x! y! (n - x - y)!} p_1^x p_2^y (1 - p_1 - p_2)^{n - x -y} = 1.
\]
Then, putting $p_3 = 1 - p_1 - p_2$,
\begin{align*}
  E(XY)
  &= \sum_{(x, y)}xy \Pr(X = x, Y = y)\\
  &= \sum_{(x, y)}\frac{n!}{(x - 1)!(y - 1)!(n - x - y)!} p_1^x  p_2^y p_3^{n - x - y}\\
  &= n(n - 1) p_1 p_2\underbrace{\sum_{(x,y)}\frac{(n - 2)!}{(x - 1)!(y - 1)!(n - x - y)!} p_1^{x - 1} p_2^{y - 1}p_3^{n - x - y}}_{ = 1}
\end{align*}
So $\text{Cov}(X, Y) = n^2 p_1 p_2 - n p_1 p_2 - (n p_1)(n p_2) = -n p_1 p_2$.
:::



:::{.example #Multinomial name="Multinomial distribution"}
Suppose that the four basic blood groups O, A, B and AB are known to occur in the following proportions $9:8:2:1$. Given a random sample of $8$ individuals, what is the probability that there will be $3$ each of types O and A and $1$ each of types B and AB?

The probabilities are $p_1 = 0.45$, $p_2 = 0.4$, $p_3 = 0.1$, $p_4 = 0.05$, and
\begin{align*}
   \Pr(X_O = 3, X_A = 3, X_B = 1, X_{AB} = 1)
   &= \frac{8!}{3!3!1!1!}(0.45)^3 (0.4)^3 (0.1)(.05)\\
   &= 0.033
\end{align*}
The above problem could be simulated in **R** using

```{r echo=TRUE}
n <- 1000
Sim <- sample(c('O', 'A', 'B', 'AB'),
              prob = c(0.45, 0.4, 0.1, 0.05),
              replace = TRUE,
              size = n)

```
:::


## The bivariate normal distribution {#BVNormalDistribution}

:::{.definition #BVNormalDistribution name="The bivariate normal distribution"}
If a pair of random variables $X$ and $Y$ have the joint pdf
\begin{equation}
   f_{X, Y}(x, y) =
   \frac{1}{2\pi\sigma_X\sigma_Y\sqrt{1 - \rho^2}}\exp(-Q/2)
   \label{EQN:speccontinuous:bivarnormalpdf}
\end{equation}
where
\[
   Q = \frac{1}{1-\rho^2}\left[
                      \left(\frac{x-\mu_X}{\sigma_X}\right)^2 -
                2\rho\left( \frac{x-\mu_X}{\sigma_X}\right)\left(\frac{y-\mu_Y}{\sigma_Y}\right)
                +     \left(\frac{y-\mu_Y}{\sigma_Y}\right)^2 \right],
\]
then $X$ and $Y$ have a *bivariate normal distribution*.
We write 
\[ 
   (X, Y) \sim N_2(\mu_X, \mu_Y, \sigma^2_X, \sigma^2_Y, \rho ).
\]
:::

For notational convenience with the *bivariate* normal distribution we often use $X_1$, $X_2$ instead of $X$, $Y$.

A typical graph of the bivariate normal surface above the $x$--$y$ plane is shown below.


It can be shown that $\int^\infty_{-\infty}\!\int^\infty_{-\infty}f_{X,Y}(x,y)\,dx\,dy = 1$.

Some important facts about the bivariate normal distribution are contained in the theorem below.

:::{.theorem}
For $(X, Y)$ with pdf given in (\ref{EQN:speccontinuous:bivarnormalpdf}),

* the marginal distributions of $X$ and of $Y$ are $N(\mu_X, \sigma^2_X)$ and $N(\mu_Y, \sigma^2_Y)$ respectively
* the parameter $\rho$ appearing in (\ref{EQN:speccontinuous:bivarnormalpdf}) is the correlation coefficient between $X$ and $Y$
* the conditional distributions of $X$ given $Y = y$, and of $Y$ given $X = x$, are respectively
  \[ 
     N\left[ \mu_X + \rho\frac{\sigma_X}{\sigma_Y}(y-\mu_Y),\sigma\ ^2_X(1-\rho^2)\right],\,
     N\left[\mu_Y+\rho\frac{\sigma_Y}{\sigma_X}(x- \mu_X),\quad \sigma^2_Y(1-\rho^2)\right]. 
\]
:::


:::{.proof}
Recall that the marginal pdf of $X$ is $f_X(x) = \int^\infty_{-\infty} f_{X, Y}(x, y)\,dy$. 
In the integral, put $u = (x - \mu_X)/\sigma_X, v = (y - \mu_Y)/\sigma_Y,\, dy = \sigma_Y\,dv$ and complete the
square (in the exponent) on $v$.
\begin{align*}
     g(x) 
     &= \frac{1}{2\pi\sigma_X\sqrt{1-\rho^2}\sigma_Y}\int^\infty_{- \infty}\exp\left\{ -\frac{1}{2(1-\rho^2)}\left[ u^2-2\rho uv+v^2\right]\right\} \sigma_Y\,dv\\[2mm]
     &= \frac{1}{2\pi \sigma_X\sqrt{1-\rho^2}}\int^\infty_{-\infty} \exp\left\{ -\frac{1}{2(1-\rho^2)}\left[ (v-\rho u)^2+ u^2 - \rho^2u^2\right]\right\}\,dv\\[2mm]
     &= \frac{e^{-u^2/2}}{\sqrt{2\pi} \sigma_X} \ \underbrace{\int^\infty_{-\infty} \frac{1}{\sqrt{2\pi (1-\rho^2)}} \exp\left\{ -\frac{1}{2(1-\rho^2)}(v-\rho u)^2\right\}\,dv}_{=1}
\end{align*}
Replacing $u$ by $(x - \mu_X )/\sigma_X$, we see from the pdf that $X \sim N(\mu_X, \sigma^2_X)$. 
Similarly for the marginal pdf of $Y$, $f_Y(y)$.

To show that $\rho$ in (\ref{EQN:speccontinuous:bivarnormalpdf}) is actually the correlation coefficient of $X$ and $Y$, recall that
\begin{align*}
     \rho_{X,Y} 
     &= \text{Cov}(X,Y)/\sigma_X\sigma_Y=\text{E}[(X-\mu_X)(Y - \mu_Y)]/\sigma_X\sigma_Y   \\[2mm]
     & = \int^\infty_{-\infty}\!\int^\infty_{-\infty} \frac{(x-\mu_X)}{\sigma_X}\frac{(y- \mu_Y)}{\sigma_Y}f(x,y)\,dx\,dy\\[2mm]
     &= \int^\infty_{-\infty}\!\int^\infty_{-\infty} uv\frac{1}{2\pi\sqrt{1- \rho^2} \sigma_X\sigma_Y}\exp\left\{ -\frac {1}{2(1-\rho^2)}[u^2-2\rho uv+v^2]\right\} \sigma_X\sigma_Y\,du\,dv.
\end{align*}

The exponent is
\[
   -\frac{[(u-\rho v)^2+v^2-\rho^2v^2]}{2(1-\rho^2)}
   = - \frac 12 \left\{\frac{(u-\rho v)^2}{(1-\rho^2)}+v^2\right\}.
\]
Then:
\begin{align*} 
   \rho_{X,Y}
   &=\int^\infty_{-\infty}\frac{ve^{-v^2/2}}{\sqrt{2\pi}}\underbrace{\int^\infty_{-\infty} \frac{u}{\sqrt{2\pi
  (1-\rho^2)}}\exp\{ -(u-\rho v)^2/2(1- \rho^2)\}\,du}_{\displaystyle{=\text{E}(U)\text{ where } u \sim N(\rho
  v,1-\rho^2)\atop =\rho v}}\,dv \\[2mm]
   &= \rho \int^\infty_{-\infty} \frac{v^2}{\sqrt{2\pi}}e^{-v^2/2}\,dv\\[2mm]
   &= \rho\quad \text{since the integral is $\text{E}(V^2)$ where $V \sim N(0,1)$.}
\end{align*}

In finding the conditional pdf of $X$ given $Y = y$, we use 
\[ 
   f_{X \mid Y = y}(x) = f_{X, Y}(x, y)/f_Y(y). 
\]
Then in this ratio, the constant is
\[ 
  \frac{\sqrt{2\pi} \sigma_Y}{2\pi \sigma_X\sigma_Y \sqrt{1-\rho^2}}
  =\frac{1}{\sqrt{2\pi}\sigma_X\sqrt{1-\rho^2}} 
\]
The exponent is
\begin{align*}
       & \frac{\exp\left\{ -\left[ \displaystyle{\frac{(x-\mu_X)^2}{\sigma^2_X}} -
  \displaystyle{\frac{2\rho(x-\mu_X)(y-\mu_Y)}{\sigma_X\sigma_Y}} + \displaystyle{\frac{(y-
  \mu_Y)^2}{\sigma^2_Y}}\right] / 2(1-\rho^2) \right\}  }{\exp\left[ -(y-
  \mu_Y)^2 / 2\sigma^2_Y\right]}\\[2mm]
       &= \exp\left\{ - \frac{1}{2(1-\rho^2)} \left[ \frac{(x-
  \mu_X)^2}{\sigma^2_X} - \frac{2\rho (x-\mu_X)(y-\mu_Y)}{\sigma_X\sigma_Y} +
  \frac{(y-\mu_Y)^2}{\sigma^2_Y} (1-1+\rho^2)\right] \right\}\\[2mm]
       &= \exp\left\{ - \frac{1}{2\sigma^2_X(1-\rho^2)} \left[ (x-
  \mu_X)^2-2\rho \frac{\sigma_X}{\sigma_Y} (x-\mu_X)(y-\mu_Y) + \frac{
  \rho^2\sigma^2_X}{\sigma^2_Y}(y-\mu_Y)^2\right]\right\}\\[2mm]
       &= \exp \left\{ - \frac{1}{2(1-\rho^2)\sigma^2_X} \left[ x-
  \mu_X-\rho\frac{\sigma_X}{\sigma_Y}(y-\mu_Y)\right]^2\right\}.
\end{align*}
So the conditional distribution of $X$ given $Y=y$ is
\[ 
   N\left( \mu_X+\rho\frac{\sigma_X}{\sigma_Y}(y-\mu_Y), \
   \sigma^2_X(1-\rho^2)\right).
\]
Recall the interpretation of the conditional distribution of $X$ given $Y = y$ (Section~\ref{SS:conddist}) and note the shape of this density in Figure~\ref{FG:binorm}.
:::


A couple of comments are worth noting about Theorem~\ref{TM:binorm}.

* From (a) and (c) we have $\text{E}(X) = \mu_X$ and $\text{E}(X \mid Y = y) = \mu_X + \rho \sigma_X (y - \mu_Y)/\sigma_Y$ (and similarly for $Y$). 
  Notice that $\text{E}(X \mid Y = y)$ is a linear function of $y$; ie if $(X, Y)$ is bivariate normal, the regression line of $Y$ on $X$ (and $X$ on $Y$) is linear.
*  An important result follows from (b). If $X$ and $Y$ are uncorrelated (i.e., if $\rho = 0$) then $f_{X, Y}(x, y) = f_X(x) f_Y(y)$ and thus $X$ and $Y$ are independent. 
   That is, if two normally distributed random variables are uncorrelated, they are also independent.



:::{.example #Exprain name="Exponential distributions"}
Marsh~\cite{BIB:Marsh:exploring} gives data from 200 married men and their wives from the OPCS study of heights and weights of adults in Great Britain in 1980.
Histograms of the husbands' and wives' heights are given in Figure~\ref{FG:speccontinuous:bvnormh}; the marginal distributions are approximately normal.
The scatterplot of the heights is shown in Figure~\ref{FG:speccontinuous:bvnorms}.

From the histograms, there is reason to suspect that a bivariate normal distribution would be appropriate.

Using $H$ to refer to heights of husbands and $W$ to the heights of wives, the sample statistics are:

\begin{tabular}{lcc}
Statistic & Husbands & Wives
\\
\hline
Sample mean: & 1732 & 1602 \\
Sample std dev:& 68.8 & 62.4 \\[3pt]
Correlation: & \multicolumn{2}{c}{+0.364}
\end{tabular}

Note that $\rho$ is positive; this implies taller men marry taller women on average.

Using this sample information, the bivariate normal distribution can be computed.
This 3-dimensional density function can be difficult to plot on a two-dimensional page; but see Figure~\ref{FG:speccontinuous:bvnorm2}.

The pdf for the bivariate normal distribution for the heights of the husbands and wives could be written down in the form of Equation~(\ref{EQN:speccontinuous:bivarnormalpdf}) for the values of $\mu_H$, $\mu_W$, $\sigma^2_H$, $\sigma^2_W$ and $\rho$ above; but this is tedious.

Given the information, what is the probability that a randomly chosen man in the UK in 1980 who is $173$ centimetres tall had married a woman taller than himself?

The information implies that $H = 1730$ is given (remembering the data are given in millimetres).
So we need the *conditional distribution* of $W \mid H = 1730$.
Using the results above, this conditional distribution will have mean
\begin{align*}
   b
   &= \mu_W + \rho\frac{\sigma_W}{\sigma_H}(y_H - \mu_H) \\
   &= 1602 + 0.364\frac{62.4}{68.8}(1730 - 1732) \\
   &= 1601.34
\end{align*}
and variance
\begin{align*}
   \sigma_2^2(1-\rho^2)
   &= 62.4^2(1-0.364^2) \\
   &= 3377.85.
\end{align*}
In summary, $W \mid  (H = 1730) \sim \text{N}(1601.34, 3377.85)$.
Note that this conditional distribution has a univariate normal distribution, and so probabilities such as $W > 1730$ are easily determined.
Then,
\begin{align*}
   \Pr(W > 1730 \mid H = 1730)
   &= \Pr\left( Z > \frac{1730 - 1601.34}{\sqrt{3377.85}}\right) \\
   &= \Pr( Z > 2.2137)\\
   &= 0.013
\end{align*}
i.e., approximately 1\% of males 173cm tall had married women taller than themselves in the UK in 1980.
:::



## Random parameters {#RandomParameters}

So far we've assumed the parameters describing a model or population are fixed numbers.
In some applications it might be more realistic or convenient to assume parameters themselves have distributions.

This idea is best introduced with some examples.

:::{.example #Exprain name="Exam scores"}
The score achieved by a student on an exam is approximately normal with mean $\mu$ where $\mu$ is dependent on the amount of study completed and has an approximately normal distribution amongst the population of students.
:::



:::{.example #Exprain name="Vehicle accidents"}
Notifiable motor vehicle accidents occur in a town according to a Poisson distribution at the mean rate of 10 per day when it's fine and 20 per day when it's wet.
It's wet on 10% of days.
:::

:::{.example #Exprain name="Goal kicking"}
Let $\theta$ be the probability that Jake, a local rugby league star, kicks a goal.
The distribution of $\theta$ depends on where the kick is taken from amongst other factors, but can be well approximated by a beta distribution.
:::



In each of these examples, there's a distribution, $f_X(x;\Theta)$ say, which depends on a parameter $\Theta$ where $\Theta$ itself has a distribution, $f_\Theta(\theta)$ say.
The distribution $f_X(x;\theta)$ therefore is effectively a *conditional* distribution and $\Theta$ is effectively a random variable.
Consequently the marginal distribution of $X$ can be found from (\ref{DF:marginalcont}) and (\ref{DF:condcont}) provided $X$ and $\Theta$ are continuous; ie
\begin{equation}
   f_X(x)
   = \int f_{X|\Theta}(x,\theta)f_\Theta(\theta)\,d\theta\label{EQ:marg1}
\end{equation}
For $X$ and $\Theta$ discrete, (\ref{DF:marginaldisc}) and (\ref{DF:conddisc}) yield
\begin{equation}
   p_X(x) 
   = \sum_\theta p_{X|\Theta}(x,\theta)p_\Theta(\theta)\label{EQ:marg2}.
\end{equation}
The mixed cases can similarly be dealt with; viz
\begin{equation}
   f_X(x) 
   = \sum_\theta f_{X|\Theta}(x,\theta)p_\Theta(\theta)\label{EQ:marg3}
\end{equation}
for $X$ continuous and $\Theta$ discrete, and
\begin{equation}
   p_X(x)
   = \int p_{X|\Theta}(x,\theta)f_\Theta(\theta)\,d\theta\label{EQ:marg4}
\end{equation}
for $X$ discrete and $\Theta$ continuous.

Only in special cases will these marginal distributions be of a standard or closed form.

:::{.example #Exprain name="Exam scores"}
The score $X$ achieved by a student on an exam is normal with mean $\Theta$ and standard deviation 5 where $\Theta$ is normal with mean 60 and standard deviation 10. 
From (\ref{EQ:marg1}), the marginal pdf of $X$ is
\begin{align}
  f_X(x)
  &=\int_{-\infty}^\infty \frac{1}{5\sqrt{2\pi}}
     \exp\left[-\frac12\left(\frac{x-\theta}{5}\right)^2\right]
     \frac{1}{10\sqrt{2\pi}}
  \exp\left[-\frac12\left(\frac{\theta-60}{10}\right)^2\right]\,d\theta\notag\\
  &= \frac{1}{11.18\sqrt{2\pi}}
     \exp\left[-\frac12\left(\frac{x-60}{11.18}\right)^2\right]\label{EQ:exam2}
\end{align}
after considerable algebra involving completing the square.
We find $X\sim N(60, 11.18^2)$. 
(Note that the $11.18 = \sqrt(5^2 + 10^2)$.)
:::



:::{.example #Exprain name="Vehicle accidents"}
Let $X$ be the number of notifiable motor vehicle accidents in a day.
Then $X \sim \text{Pois}(M)$ where parameter $M$ has distribution defined by $\Pr(M = 10) = 0.9$ and $\Pr(M = 20) = 0.1$.
From (\ref{EQ:marg2}), the marginal pf of $X$ is
\begin{equation}
  p_X(x)
  = 0.9 \frac{e^{-10}10^x}{x!}+ 0.1 \frac{e^{-20}{20^x}}{x!}\quad\hbox{for $x=0,1,2,\dots$}
\end{equation}
:::



:::{.example #Exprain name="Gaol kicking"}
Suppose Jake has 10 kicks at goal in a match and the number of successful kicks $X$ can be be modelled by a binomial distribution with parameter $\Theta$, where $\Theta$ has a Beta(4,4) distribution. 
From (\ref{EQ:marg4}), the marginal pf of $X$ is
\begin{align}
  p_X(x)
  &= \int_0^1 {10\choose x}\theta^x(1-\theta)^{10-x}
     {\Gamma(8)\over\Gamma(4)\Gamma(4)}\theta^3(1-\theta)^3\,d\theta\notag\\
  &= {\Gamma(8)\over\Gamma(4)\Gamma(4)}{10\choose x}\int_0^1\theta^{x+3}(1-\theta)^{n-x+3}\,d\theta\notag\\
  &= {7!10!(x+3)!(13-x)!\over3!^217!x!(10-x)!}\quad\hbox{for $x=0,1,2,\dots,10$}\label{EQ:goal2}
\end{align}
:::



### Bayes' theorem revisited {#BayesTheoremRevisited}

Recall Bayes' theorem in (\S\ref{SS:Bayes}). 
In particular, let $E$ be an event, with $H_1, \ldots, H_n$ a sequence of mutually exclusive and exhaustive events partitioning the sample space. 
Then
\begin{equation}
  \Pr(H_n | E ) 
  = \frac{\Pr(H_n) \Pr(E|H_n) }{\Pr(E)}
  = \frac{\Pr(H_n) \Pr(E|H_n) }{\sum_m \Pr(H_m ) \Pr(E|H_m )}
\end{equation}
assuming that $\Pr(E) \neq 0$.

Bayes' theorem extends directly to random variables.

Suppose that $X$ and $Y$ are discrete random variable's such that $p_Y(y)$ is the pf for $Y$ and $p_{X|Y}(x,y)$ is the conditional pf for $X$ given $Y$.
Then the conditional pf of $Y$ given $X$ is
\begin{align}
  p_{Y | X}(y, x)
  &= \frac{p_{X, Y}(x, y)}{p_X(x)} \\
  &= \frac{p_{X | Y}(x, y)p_Y(y)}{\sum_y p_{X|Y}(x,y)p_Y(y)}\label{EQ:Bayesdis}
\end{align}
Analogously, for the various combinations of continuous and discrete random variables,

\begin{equation}
   f_{Y|X}(y,x)
   = \frac{f_{X,Y}(x,y)}{f_X(x)}
   = \frac{f_{X|Y}(x,y)f_Y(y)}{\int_y f_{X|Y}(x,y)f_Y(y)\,dy},\label{EQ:Bayescts}
\end{equation}

\begin{equation}
  p_{Y|X}(y,x)
  = \frac{f_{X,Y}(x,y)}{p_X(x)}
  = \frac{f_{X|Y}(x,y)p_Y(y)}{\sum_y f_{X|Y}(x,y)p_Y(y)},\label{EQ:Bayesdiscts}
\end{equation}
and
\begin{equation}
  f_{Y|X}(y,x)
  = \frac{f_{X,Y}(x,y)}{p_X(x)}
  = \frac{p_{X|Y}(x,y)f_Y(y)}{\int_y p_{X|Y}(x,y)f_Y(y)\,dy}\label{EQ:Bayesctsdis}
\end{equation}
The concept in Bayes' theorem of determining the probability of $B$ given $A$ from that of $A$ given $B$ finds important and interesting applications in random parameters.

Let's look at some examples using (\ref{EQ:Bayesdis}), (\ref{EQ:Bayescts}), (\ref{EQ:Bayesdiscts}) or (\ref{EQ:Bayesctsdis}) in which $Y$ is now interpreted as a random parameter.


:::{.example #ExamScores3 name="Exam scores"}
The conditional pdf of the mean $\Theta$ given $X = x$ follows directly from (\ref{EQ:Bayescts}) on substituting (\ref{EQ:exam2}):
\begin{align}
  f_{\Theta | X}(\theta, x)
  &= \frac{f_{X|\Theta}(x,\theta)f_\Theta(\theta)}{f_X(x)}\notag\\
  &= \frac{\frac{1}{5\sqrt{2\pi}}
  \exp\left[-\frac12\left(\frac{x-\theta}{5}\right)^2\right]
  \frac{1}{10\sqrt{2\pi}}
  \exp\left[-\frac12\left(\frac{\theta-60}{10}\right)^2\right]}
  {\frac{1}{11.18\sqrt{2\pi}}
  \exp\left[-\frac12\left(\frac{x-60}{11.18}\right)^2\right]}\notag\\
  &=\frac{1}{4.472\sqrt{2\pi}}
  \exp\left[-\frac12\left(\frac{\theta-(0.8x+12)}{4.472}\right)^2\right]\label{EQ:exam3}
\end{align}
after some algebraic manipulation; i.e. ,$\Theta | (X = x)\sim N(0.8x + 12, 4.472^2)$
:::


:::{.example #Accidents3 name="Vehicle accidents"}
From (\ref{EQ:Bayescts}) and (\ref{EQ:accidents2}), the conditional pf of $M$ given $X$ is 
\begin{align}
  p_{M|X}(\mu,x)
  &= \frac{f_{X|M}(x,\mu)f_M(\mu)}{p_X(x)}\notag\\
  &= \begin{cases}\frac{0.9\frac{e^{-\mu}\mu^x}{x!}}
  {0.9 \frac{e^{-10}10^x}{x!}+ 0.1 \frac{e^{-20}{20^x}}{x!}}&\hbox{for $\mu=10$}\\
  \frac{0.1\frac{e^{-\mu}\mu^x}{x!}}
  {0.9 \frac{e^{-10}10^x}{x!}+ 0.1 \frac{e^{-20}{20^x}}{x!}}&\hbox{for $\mu=20$}\label{EQ:accidents3}
  \end{cases}
\end{align}
:::


:::{.example #GoalKicking3 name="Goal kicking"}
From (\ref{EQ:marg4}), the conditional pdf of $\Theta$ given $X$ is
\begin{align}
  f_{\Theta|X}(\theta,x)
  &= \frac{p_{X|\Theta}(x,\theta)f_\Theta(\theta)}{p_X(x)}\notag\\
  &= \frac{{10\choose x}\theta^x(1-\theta)^{10-x}
  {\Gamma(8)\over\Gamma(4)\Gamma(4)}\theta^3(1-\theta)^3}
  {{7!10!(x+3)!(13-x)!\over3!^217!x!(10-x)!}}\label{EQ:goal3}
\end{align}
:::

Giving meaning to conditional distribution of parameters such as those described by (\ref{EQ:exam3}), (\ref{EQ:accidents3}) and (\ref{EQ:goal3}) has historically been a source of controversy in the discipline of statistics.
However the idea of being able to modify the distribution of a parameter based on information, which is essentially what is happening, is very worthwhile, and is the basis of a very important contemporary branch of statistics
known as *Bayesian statistics*.

If, for example, we think of the distribution of means in the exam scores example as a model of our beliefs describing the population of students, then we can think of the conditional distribution of means given the actual exam scores as an updated model of our beliefs concerning the distribution of means. 
In this context it makes sense to talk of the original or unconditional distribution $f_\Theta(\theta)$ as the *prior*
distribution and the resultant conditional distribution $f_{\Theta|X}(\theta,x)$ as the *posterior* distribution.

This notion of updating knowledge of parameter values based on new information is the basis of Bayesian statistics. 
It is pertinent to note that the algebra involved in deriving posterior distributions is, except in special cases, practically intractable. 
Consequently numerical, especially some clever simulation techniques, are used extensively in Bayesian statistics.




