# Bivariate and multivariate distributions {#ChapMultivariate}

::: {.objectivesBox .objectives data-latex="{iconmonstr-target-4-240.png}"}
On completion of this chapter you should be able to:

* apply the concept of bivariate random variables.
* compute joint probability functions and the distribution function of two random variables.
* find the marginal and conditional probability functions of random variables in both discrete and continuous cases.
* apply the concept of independence of two random variables.
* compute the expectation and variance of linear combinations of random variables.
* interpret and compute the covariance and the coefficient of correlation between two random variables.
* compute the conditional mean and conditional variance of a random variable for some given value of another random variable.
* use the multinomial and bivariate normal distributions.
:::



 

```{r, setup, echo=FALSE, message=FALSE, warning=FALSE}
### Needed to add the rgl plot to bookdown for bivariate plots in html
library(rgl)
#knitr::knit_hooks$set(webgl = hook_webgl)
```


## Introduction {#BivariateIntroduction}

Not all random processes are sufficiently simple to have the outcome denoted by a single variable\ $X$. 
Many situations require observing two or more numerical characteristics simultaneously.
This chapter begins discussing the two-variable (bivariate) case (Sect.\ \@ref(BivariateDistributions)), and then the multi-variable (more than two variable) case (Sect.\ \@ref(Multivariate)).


## Bivariate random variables and their distributions {#BivariateRVs}

The joint probability function is a function that simultaneously describes ho the two random variables vary.
Hence, the range space of $(X, Y)$, written $\mathcal{R}_{X \times Y}$, is a subset of the Euclidean plane. 
Each outcome\ $X(s)$, $Y(s)$ may be represented as a point $(x, y)$ in the plane. 
As in the one-dimensional case, distinguishing between discrete and continuous random variables is necessary.


:::{.definition #RandomVector name="Random vector"}
Let $X = X(s)$ and $Y = Y(s)$ be two functions, each assigning a real number to each sample point $s \in S$. 
Then $(X, Y)$ is called a *two-dimensional random variable*, or a *random vector*.
:::


:::{.example #BVDiscreteEG name="Bivariate discrete"}
Consider a random process where, simultaneously, *two* coins are tossed, and *one* die is rolled. 
Let\ $X$ be the number of heads that show on the two coins, and $Y$\ be the number of rolls needed to roll a `r knitr::include_graphics("Dice/die6.png", dpi=2000)`.

$X$ is discrete with $\mathcal{R}_X = \{0, 1, 2\}$.
$Y$ is discrete with a countably infinite range space $\mathcal{R}_Y = \{ 1, 2, 3, \dots\}$.

The range space is $\mathcal{R}_{X\times Y} = \{ (x, y): 0 \le x \le 2, y = 1, 2, 3, \dots\}$.
:::


As with the univariate case, the description and language for the probability function are different, depending on whether the random variables\ $X$ and\ $Y$ are discrete or continuous case (though the ideas remain similar).
While not common, the case where one variable, say\ $X$, is continuous and the other, say\ $Y$, is discrete also occurs.
We defer this case until Sect.\ \@ref(MixedJointPF).


:::{.definition #BivariateProbFunction name="Discrete bivariate distribution function"}
Let $(X, Y)$ be a $2$-dimensional random variable where\ $X$ and\ $Y$ are both discrete random variables. 
With each $(x_i, y_j)$ we associate a number $p_{X, Y}(x_i, y_j)$ representing $\Pr(X = x_i, Y = y_j)$ and satisfying
\begin{align}
   p_{X, Y}(x_i, y_j) &\geq 0, \text{ for all } (x_i, y_j)  \\
   \sum_{j = 1}^{\infty} \sum_{i = 1}^{\infty} p_{X, Y}(x_i, y_j)
   &= 1.
   (\#eq:BivariateProbFunctionDiscrete)
\end{align}
Then the function $p_{X, Y}(x, y)$, defined for all $(x_i, y_j) \in R$ is called the *probability function* of $(X, Y)$. 
Also,
$$
   \{x_i, y_j, p_{X,Y}(x_i, y_j); i, j = 1, 2, \ldots\} 
$$
is called the *probability distribution* of $(X, Y)$.
:::


:::{.definition #BivariateProbFunctionCont name="Continuous bivariate distribution function"}
Let $(X, Y)$ be a $2$-dimensional random variable where\ $X$ and\ $Y$ are both continuous random variables.
The *joint probability density function*, $f_{X, Y}$, is a function satisfying
\begin{align}
   f_{X, Y}(x, y) &\geq 0, \text{ for all } (x, y) \in R, \\
   \int \!\! \int_{R} f_{X, Y}(x, y) \, dx \, dy &= 1.
\end{align}
:::


The second of these indicates that the *volume* under the surface $f_{X, Y}(x, y)$ is one. 
Also, for $\Delta x, \Delta y$ sufficiently small,
\begin{equation}
   f_{X, Y}(x, y) \, \Delta x \Delta y \approx \Pr(x \leq X \leq x + \Delta x, y \leq Y \leq y + \Delta y).
\end{equation}
Probabilities of events can be determined by the probability function or the probability density function as follows.


:::{.definition #BVProbabilities name="Bivariate distribution probabilities"}
For any event\ $A$, *the probability of\ $A$* is given by
\begin{align*}
   \Pr(A) 
   &= \sum_{(x, y) \in A} p(x, y),                       &&\text{for $(X, Y)$ discrete;}\\
   \Pr(A)
   &= \int \!\! \int_{(x, y) \in A}f(x, y) \, dx \, dy   &&\text{for $(X, Y)$ continuous.}
\end{align*}
:::


As in the univariate case, a bivariate distribution can be given in various ways:

* by enumerating the range space and corresponding probabilities;
* by a formula; or
* a graph; or
* by a table.


:::{.example #BVDiscrete2 name="Bivariate discrete"}
Consider the following discrete distribution where probabilities $\Pr(X = x, Y = y)$ are shown as a graph
`r if (knitr::is_latex_output()) {
   '(Fig.\\ \\@ref(fig:Bivar1graphlatex))'
} else {
   '(Fig.\\ \\@ref(fig:Bivar1graph))'
}`
and a table (Table\ \@ref(tab:Bivar1)).

To find $\Pr(X + Y = 2)$:
\begin{align*}
   \Pr(X + Y = 2)
   &=\Pr\big(\{X = 2, Y = 0\} \text{ or } 
             \{X = 1, Y = 1\} \text{ or } 
             \{X = 0, Y = 2\}\big)\\
   &= \Pr(X = 2, Y = 0) \, + 
      \, \Pr(X = 1, Y = 1) \, + 
      \, \Pr(X = 0, Y = 2)\\
   &= \frac{9}{42} \ + \ \frac{12}{42} \ + \ \frac{12}{42} 
   = \frac{33}{42}.
\end{align*}
:::


```{r echo=FALSE}

# Add a new column with color
pmf1 <- data.frame(
  x = c(0, 0, 0, 1, 1, 1, 2, 2, 2),
  y = c(0, 1, 2, 0, 1, 2, 0, 1, 2),
  f = c(1, 4, 4, 6, 12, 0, 9, 0, 0) / 42
)
```

```{r Bivar1graph, echo=FALSE, warning=FALSE, message=FALSE, fig.cap="A bivariate discrete probability function.", fig.align="center", out.width='80%',fig.width=6}

# Plot
if (knitr::is_html_output()) {
  setupKnitr(autoprint = TRUE) #NEW: Seems needed as of 02 May 2024: https://github.com/rstudio/rmarkdown-cookbook/issues/370

  rgl::plot3d( # Plot the points
    x = pmf1$x,
    y = pmf1$y, 
    z = pmf1$f, 
    xlim = c(-0.5, 2.5),
    ylim = c(-0.5, 2.5),
    zlim = c(0, 0.4),
    cex = 0.75,
    col = "black", 
    type = "s", # Plot the points
    radius = 0.05,
    xlab = "X", 
    ylab = "Y", 
    zlab = "pmf",
    bty = "u", # User defined, so these only work when bty = "u":
      col.axis = "black",
      col.panel = "white",
      col.grid = grey(0.9),
      lwd.panel = 1,
      lwd.grid = 1,
    lty = 2
)
#  rgl::plot3d( # Plots the sticks from z=0 to the points
#    add = TRUE,
#    x = pmf1$x,
#    y = pmf1$y, 
#    z = pmf1$f, 
#    col = plotColour, 
#    type = "h") # Plot the line segments from z = 0 
}
```



```{r Bivar1graphlatex, echo=FALSE, warning=FALSE, message=FALSE, fig.cap="A bivariate discrete probability function.", fig.align="center", out.width='50%',fig.width=15}

# Plot
if (knitr::is_latex_output()) {
  plot3D::scatter3D(x = pmf1$x,
                    y = pmf1$y,  
                    z = pmf1$f,
                    cex.lab = 0.75,
                    xlim = c(-0.5, 2.5),
                    ylim = c(-0.5, 2.5),
                    zlim = c(0, 0.4),
                    col = plotColour,
                    type = "h", # Plot the points and sticks
                    #radius = 0.05,
                    d = 10, # Reduce perspective effect
                    pch = 19,
                    phi = 20,
                    theta = 20,
                    ticktype = "detailed",
                    xlab = "X", 
                    ylab = "Y", 
                    zlab = "pmf",
                    bty = "u", # User defined, so these only work when bty = "u":
                    col.axis = "black",
                    col.panel = "white",
                    col.grid = grey(0.9),
                    lwd.panel = 1,
                    lwd.grid = 1,
                    lty = 2
  ) 
  rgl::plot3d( # Plots the sticks from z = 0 to the points
    add = TRUE,
    x = pmf1$x,
    y = pmf1$y, 
    z = pmf1$f, 
    col = plotColour, 
    type = "h") # Plot the line segments from z = 0 
}
```


```{r Bivar1, echo=FALSE}
Bivar1Table <- array( dim = c(3, 3))

Bivar1Table[1, ] <- c("$1/42$", 
                      "$4/42$",
                      "$12/42$")
Bivar1Table[2, ] <- c("$4/42$",
                      "$12/42$",
                      "$0$")
Bivar1Table[3, ] <- c("$9/42$",
                      "$0$",
                      "$0$")

rownames(Bivar1Table) <- c( "$y = 0$",
                            "$y = 1$",
                            "$y = 2$")
colnames(Bivar1Table) <- c( "$x = 0$",
                            "$x = 1$",
                            "$x = 2$")


Bivar1Table.caption <- "A bivariate discrete probability function"
if( knitr::is_latex_output() ) {
  knitr::kable(Bivar1Table,
               format = "latex",
               booktabs = TRUE,
               align = "r",
               longtable = FALSE,                
               escape = FALSE,
               caption = Bivar1Table.caption) %>%
    kable_styling(font_size = 10) %>%
    row_spec(0, bold = TRUE) 
}
if( knitr::is_html_output() ) {
  knitr::kable(Bivar1Table,
               escape = TRUE,
               format = "html",
               booktabs = TRUE,
               align = "r",
               longtable = FALSE,                
               caption = Bivar1Table.caption) %>%
    row_spec(0, bold = TRUE) 
}
```



:::{.example #BVContinuousUnif name="Bivariate uniform distribution"}
Consider the following continuous bivariate distribution with joint pdf
$$
   f_{X, Y}(x, y) = 1, \quad \text{for $0 \leq x \leq 1$ and $0 \leq y \leq 1$}.
$$
This is sometimes called the *bivariate continuous uniform distribution*\index{Bivariate continuous uniform distribution}
`r if (knitr::is_latex_output()) {
   '(Fig.\\ \\@ref(fig:BVCont3)).'
} else {
   '(see below).'
}`
The *volume* under the surface is one.


To find $\Pr(0 \leq x \leq \frac{1}{2}, 0 \leq y \leq \frac{1}{2})$, find the volume above the square with vertices $(0, 0), (0, 1/2), (1/2, 0), (1/2, 1/2)$.
Hence the probability is\ $1/4$.
:::


```{r, BVCont3, echo=FALSE, fig.align='center', fig.width=7, fig.height=4.5, out.width='60%',fig.cap="The bivariate continuous uniform distribution."}

M <- plot3D::mesh( x = seq(-0.25, 1.25,
                           by = 0.05),
                   y = seq(-0.25, 1.25, 
                           by = 0.05) )
zeroPDF <- M$x < 0 | M$x > 1 | M$y < 0 | M$y > 1

z <-  array( 1,
dim = dim(M$x) )
z[zeroPDF] <- 0


if (knitr::is_html_output()) {
  rgl::persp3d( x = seq(-0.25, 1.25,
                           by = 0.05),
                y = seq(-0.25, 1.25, 
                           by = 0.05),
                z = z,
                col = "grey",
xlab = expression(italic(x)),
ylab = expression(italic(y)),
zlab = expression(italic(f)) )


} else {
  plot3D::surf3D(x = M$x, 
  y = M$y,
  z = z,
  d = 2, 
  facets = FALSE,
  col = NULL,
  colkey = FALSE, 
  phi = 40, 
  theta = 30, 
  main = "Bivariate continuous uniform")
}

```


:::{.example #BVDiscrete3 name="Bivariate discrete"}
Consider a random process where *two* coins are tossed, and *one* die is rolled simultaneously (Example\ \@ref(exm:BVDiscreteEG)).
Let\ $X$ be the number of heads that show on the two coins, and\ $Y$ the number on the die.

Since the toss of the coin and the roll of the die are independent, the probabilities are computed as follows:
\begin{align*}
   \Pr(X = 0, Y = 1) 
   &= \Pr(X = 0) \times \Pr(Y = 1) = \frac{1}{4}\times\frac{1}{6} = \frac{1}{24};\\
   \Pr(X = 1, Y = 1) 
   &= \Pr(X_1 = 1) \times \Pr(Y = 2) = \frac{1}{2}\times\frac{1}{6} = \frac{1}{12};
\end{align*}
and so on. 
The complete joint pf can be displayed in a graph (often tricky), a function, or a table (Table\ \@ref(tab:JointPF)). 
Here, the joint pf could be given as the function
$$
   p_{X, Y}(x, y) =
      \begin{cases}
         \left(\frac{1}{12}\right) 0.5^{|x - 1|} & \text{for $(x, y)\in S$ defined earlier};\\
         0                                         & \text{elsewhere.}
      \end{cases}
$$
:::



```{r JointPF, echo=FALSE}
JointPF <- array( dim = c(4, 8))

JointPF[1, ] <- c("$X_1 = 0$",  "$1/24$", "$1/24$", "$1/24$", "$1/24$", "$1/24$", "$1/24$", "$1/4$")
JointPF[2, ] <- c("$X_1 = 1$", "$1/12$", "$1/12$", "$1/12$", "$1/12$", "$1/12$", "$1/12$", "$1/2$")
JointPF[3, ] <- c("$X_1 = 2$", "$1/24$", "$1/24$", "$1/24$", "$1/24$", "$1/24$", "$1/24$", "$1/4$")
JointPF[4, ] <- c("Total", "$1/6$", "$1/6$", "$1/6$", "$1/6$", "$1/6$", "$1/6$", "$1$")


JointPF.caption <- ifelse(knitr::is_latex_output(),
                          "The joint pf for Example \\ref{exm:BVDiscrete3}",
                          "The joint pf for Example \\@ref(exm:BVDiscrete3)")

if( knitr::is_latex_output() ) {
  knitr::kable(JointPF,
               format = "latex",
               booktabs = TRUE,
               align = "r",
               longtable = FALSE,                
               escape = FALSE,
               col.names = c("",
                             "$X_2 = 1$",
                             "$X_2 = 2$",
                             "$X_2 = 3$",
                             "$X_2 = 4$",
                             "$X_2 = 5$",
                             "$X_2 = 6$",
                             "Total"),
               caption = JointPF.caption) %>%
    kable_styling(font_size = 10) %>%
    row_spec(0, bold = TRUE)   %>%
    column_spec(1, bold = TRUE)
}
if( knitr::is_html_output() ) {
  knitr::kable(JointPF,
               escape = TRUE,
               col.names = c("",
                             "$X_2 = 1$",
                             "$X_2 = 2$" ,
                             "$X_2 = 3$" ,
                             "$X_2 = 4$" ,
                             "$X_2 = 5$",
                             "$X_2 = 6$",
                             "Total"),
               format = "html",
               booktabs = TRUE,
               align = "r",
               longtable = FALSE,                
               caption = JointPF.caption) %>%
    row_spec(0, bold = TRUE)  %>%
    column_spec(1, bold = TRUE) 
}
```




:::{.example #BVDiscreteTwoDice name="Two dice"}
Consider the bivariate discrete distribution which results when two dice are thrown.

Let\ $X$ be the number of times a `r knitr::include_graphics("Dice/die5.png", dpi=2000)` appears, and\ $Y$ the number of times a `r knitr::include_graphics("Dice/die6.png", dpi=2000)` appears. 
The range spaces of\ $X$ and\ $Y$ are $\mathcal{R}_X = \{0, 1 ,2 \}$, $\mathcal{R}_Y = \{0, 1, 2\}$ and the range space for the random process is the Cartesian product of\ $\mathcal{R}_X$ and\ $\mathcal{R}_Y$, understanding that some of the resulting points may have probability zero. 
The probabilities in Table\ \@ref(tab:BVTwoDice) are $\Pr(X = x, Y = y)$ for the $(x, y)$ pairs in the range space.

The probabilities are found by realising we really have two repetitions of a simple random process with three possible outcomes, $\{5, 6, (\text{$5$ or $6$})^c \}$, with probabilities $\frac{1}{6}, \frac{1}{6}, \frac{2}{3}$, the same for each repetition.
(Recall: $\overline{\text{$5$ or $6$}}$ means 'not 5 or 6'; see Def.\ \@ref(def:IntersectionUnionSubset).)
Of course the event $X = 2, Y = 1$ cannot occur in two trials, so has probability zero.
:::


```{r, BVTwoDice, echo=FALSE}
TwoDice <- array( dim = c(3, 4))

TwoDice[1, ] <- c("$y = 0$", "$(2/3)^2$", "$2(1/6)(2/3)$", "$(1/6)^2$")
TwoDice[2, ] <- c("$y = 1$", "$2(1/6)(2/3)$", "$2(1/6)(1/6)$", "$0$")
TwoDice[3, ] <- c("$y = 2$", "$(1/6)^2$", "$0$", "$0$")

TwoDice.caption <- ifelse(knitr::is_latex_output(),
                          "Joint probability distribution for Example \\ref{exm:BVDiscreteTwoDice}.",
                          "Joint probability distribution for Example \\@ref(exm:BVDiscreteTwoDice).")

if( knitr::is_latex_output() ) {
  knitr::kable(TwoDice,
               format = "latex",
               booktabs = TRUE,
               align = "r",
               longtable = FALSE,                
               escape = FALSE,
               col.names = c("",
                             "$x = 1$",
                             "$x = 2$" ,
                             "$x = 3$"),
               caption = TwoDice.caption) %>%
    kable_styling(font_size = 10) %>%
    row_spec(0, bold = TRUE)   %>%
    column_spec(1, bold = TRUE)
}
if( knitr::is_html_output() ) {
  knitr::kable(TwoDice,
               escape = TRUE,
               col.names = c("",
                             "$x = 1$",
                             "$x = 2$" ,
                             "$x = 3$"),
               format = "html",
               booktabs = TRUE,
               align = "r",
               longtable = FALSE,                
               caption = TwoDice.caption) %>%
    row_spec(0, bold = TRUE)  %>%
    column_spec(1, bold = TRUE) 
}
```





Example\ \@ref(exm:BVDiscreteTwoDice) is a special case of the *multinomial distribution*\index{Multinomial distribution} (a generalisation of the binomial distribution), described later (Sect.\ \@ref(MultinomialDistribution)).



:::{.example #BVDiscreteBank name="Banks"}
A bank operates both an ATM and a teller. 
On a randomly selected day, let\ $X_1$ be the proportion of time the ATM is in use (at least one customer is being served or waiting to be served), and\ $X_2$ is the proportion of time the teller is busy.

The set of possible values for\ $X_1$ and\ $X_2$ is the rectangle $R = \{(x_1, x_2)\mid 0 \le x_1 \le 1, 0 \le x_2 \le 1\}$.
From experience, the joint pdf of $(X_1, X_2)$ is
$$
   f_{X_1, X_2}(x_1, x_2) =
      \begin{cases}
        c(x_1 + x_2^2) & \text{for $0\le x_1\le 1$; $0\le x_2\le 1$};\\
        0 & \text{elsewhere.}
    \end{cases}
$$

To determine a value for\ $c$, first see that if  $f_{X_1, X_2}(x_1, x_2) \ge 0$ for all\ $x_1$ and\ $x_2$, then $c > 0$; and
$$
   \int_{-\infty}^{\infty}\!\int_{-\infty}^{\infty} f_{X_1, X_2}(x_1, x_2)\, dx_1\,dx_2 = 1.
$$
Hence,
\begin{align*}
    \int_{-\infty}^{\infty}\!\int_{-\infty}^{\infty} f_{X_1, X_2}(x_1, x_2)\, dx_1\,dx_2
    &= \int_{0}^{1}\!\!\!\int_{0}^{1} f_{X_1, X_2}(x_1, x_2)\, dx_1\,dx_2 \\
    &= c \int_{x_2 = 0}^{1}\left\{\int_{x_1=0}^{1} (x_1 + x_2^2)\, dx_1\right\} dx_2\\
    &= c (1/2 + 1/3) = 5c/6,
\end{align*}
and so $c = 6/5$.

Consider the probability *neither* facility is busy more than half the time.
Mathematically, the question is asking to find $\Pr( 0\le X_1\le 0.5, 0\le X_2\le 0.5)$; call this event\ $A$.
Then,
\begin{align*}
   \Pr(A)
   &= \int_{0}^{0.5}\,\,\, \int_{0}^{0.5} f_{X_1, X_2}(x_1, x_2)\, dx_1\, dx_2 \\
   &= \frac{6}{5} \int_{0}^{0.5}\left\{\int_{0}^{0.5} x_1 + x_2^2\, dx_1\right\} dx_2 \\
   &= \frac{6}{5} \int_{0}^{0.5} (1/8 + x_2^2/2) \, dx_2 = 1/10.
\end{align*}
:::


## Bivariate random variables: joint distribution function {#BivariateDistributionFunction}

The (cumulative) distribution function represents a sum of probabilities, or a volume under a surface, is denoted by $F_{X, Y}(x, y)$, and defined as follows.


:::{.example #BVDistributionFN name="Bivariate distribution function"}
The *bivariate distribution function* is
\begin{align}
   F(x, y)
   &= \Pr(X \leq x, \, Y \leq y),                           & \text{for $(X,Y)$ discrete;}
   (\#eq:BivarXYdiscrete)\\
   F(x, y)
   &= \int_{-\infty}^y \int_{-\infty}^x f(u,v) \, du \, dv, & \text{for $(X,Y)$ continuous.}
   (\#eq:BivarXYcontinuous)
\end{align}
:::

:::{.example #BVDiscrete3DF name="Bivariate discrete"}
Consider the random process in Example\ \@ref(exm:BVDiscrete3), where *two* coins are tossed, and *one* die is rolled (simultaneously).
The probability function is given in Table\ \@ref(tab:JointPF).

The complete joint distribution function is given in Table\ \@ref(tab:Bivar2TableDF), and complicated even for this simple case.
As an example, the joint df at $(1, 2)$ would be computed as follows:
\begin{align*}
   F_{X_1, X_2}(1, 2)
   &= \displaystyle \sum_{x_1\le1} \,  \sum_{x_2\le 2} p_{X_1, X_2}(x_1, x_2)\\
   &= p_{X_1, X_2}(0, 1) + p_{X_1, X_2}(0, 2) +  p_{X_1, X_2}(1, 1) + p_{X_1, X_2}(1, 2) \\
   &= 1/24 + 1/24 + 1/12 + 1/12 = 6/24.
\end{align*}
:::


```{r Bivar2TableDF, echo=FALSE}
Bivar2Table <- array( "0",
                      dim = c(4, 8))

Bivar2Table[2, 2:8 ] <- c( "0",  "$1/24$", "$2/24$", "$3/24$", "$4/24$", "$5/24$", "$6/24$")
Bivar2Table[3, 2:8 ] <- c( "0",  "$3/24$", "$6/24$", "$9/24$", "$12/24$", "$15/24$", "$18/24$")
Bivar2Table[4, 2:8 ] <- c( "0",  "$4/24$", "$8/24$", "$12/24$", "$16/24$", "$20/24$", "$24/24$")

# Bivar2Table[, 1] <- c("$x \\text{less than} 0$",
#                       "$x \\text{less than} 0$",
#                       "$x \\text{less than} 1$",
#                       "$x \\text{less than} 2$")
Bivar2Table[, 1] <- c("$x\\lt 0$",
                      "$x\\le 0$", #$x$ less than, equal to  $0$",
                      "$x\\le 1$",
                      "$x\\le 2$")


Bivar2Table.caption <- "The bivariate probability function for $X$ and $Y$."
if( knitr::is_latex_output() ) {
  knitr::kable(Bivar2Table,
               format = "latex",
               booktabs = TRUE,
               align = "r",
               longtable = FALSE,                
               escape = FALSE,
               col.names = c("",
                             "$y < 1$",
                             "$y \\le 1$" ,
                             "$y \\le 2$" ,
                             "$y \\le 3$" ,
                             "$y \\le 4$" ,
                             "$y \\le 5$" ,
                             "$y \\le 6$" ),
               caption = Bivar2Table.caption) %>%
    kable_styling(font_size = 10) %>%
    row_spec(0, bold = TRUE) %>%
    column_spec(1, bold = TRUE) 
}
if( knitr::is_html_output() ) {
  knitr::kable(Bivar2Table,
               escape = TRUE,
               col.names = c("",
                             "$y \\lt 1$",
                             "$y \\le 1$" ,
                             "$y \\le 2$" ,
                             "$y \\le 3$" ,
                             "$y \\le 4$" ,
                             "$y \\le 5$" ,
                             "$y \\le 6$" ),
               format = "html",
               booktabs = TRUE,
               align = "r",
               longtable = FALSE,                
               caption = Bivar2Table.caption) %>%
    row_spec(0, bold = TRUE)  %>%
    column_spec(1, bold = TRUE) 
}
```



:::{.example #BVCont name="Bivariate continuous"}
From Example\ \@ref(exm:BVDiscreteBank),
\begin{align*}
   F_{X_1, X_2}(x_1, x_2)
   &= \frac{6}{5} \int_0^{x_1} \int_0^{x_2} (t_1 + t_2^2)\, dt_2 dt_1 \\
   &= \frac{6}{5} \int_0^{x_1} (t_1 t_2 + t_2^3/3)\Big|_{t_2 = 0}^{t_2 = x_2} \, dt_1 \\
   &= \frac{6}{5} \int_0^{x_1} (t_1 x_2 + x_2^3/3)\, dt_1 \\
   &= \frac{6}{5} \left( \frac{x_1 x_2}{2} + \frac{x_1 x_2^3}{3}\right)
\end{align*}
for $0 < x_1 < 1$ and $0 < x_2 < 1$. 
So
$$
   F_{X_1, X_2}(x_1, x_2)
   = \begin{cases}
      0 & \text{if $x_1<0$ or $x_2<0$};\\
      \frac{6}{5} \left( x_1 x_2/2 + x_1 x_2^3/3\right) & \text{if $0 \le x_1 \le 1$ and $0 \le x_2 \le 1$};\\
      1 & \text{if $x_1 > 1$ and $x_2 > 1$}.
     \end{cases}
$$
:::



## Bivariate random variables: marginal distributions {#MarginalDistributions}

With each two-dimensional random variable $(X, Y)$ two one-dimensional random variables, namely\ $X$ and\ $Y$, can be described. 
We can find the probability distributions of each of\ $X$ and\ $Y$ separately.

In the case of a discrete random vector $(X, Y)$, the event $X = x_i$ is the union of the mutually exclusive events
$$
   \{X = x_i, Y = y_1\}, \{\ X = x_i, Y = y_2\}, \{X = x_i, Y = y_3\}, \dots
$$
Thus,
\begin{align*}
   \Pr(X = x_i)
   &= \Pr(X = x_i, Y = y_1) + \Pr(X = x_i, Y = y_2) + \dots \\
   &= \sum_jp_{X, Y}(x_i, y_j),
\end{align*}
where the notation means to sum over all values given under the summation sign.
Hence, the marginal distributions can be defined when $(X, Y)$ is a discrete random vector.


:::{.definition #BVDiscreteMarginal name="Bivariate discrete marginal distributions"}
Given $(X, Y)$ with joint discrete probability function $p(x, y)$, the *marginal probability functions* of\ $X$ and\ $Y$ are, respectively
\begin{equation}
   \Pr(X = x) = \sum_{y}p_{X, Y}(x, y)
   \quad\text{and}\quad
   \Pr(Y = y) = \sum_{x}p_{X, Y}(x, y).
   (\#eq:BivariateMarginal)
\end{equation}
:::


An analogous definition exists when the random vector $(X,Y)$ is continuous.


:::{.definition #BVContMarginal name="Bivariate continuous marginal distributions"}
If $(X, Y)$ has joint continuous pdf $f(x, y)$, the *marginal pdfs* of\ $X$ and\ $Y$, denoted by $f_X(x)$, $f_Y(y)$ respectively, are
$$
   f_X(x) = \int_{-\infty}^{\infty}f(x,y) \, dy 
   \quad\text{and}\quad
   f_Y(y) = \int_{-\infty}^{\infty}f(x,y) \, dx.
$$
:::


:::{.example #BVContinuousMarginal name="Bivariate continuous marginal distributions"}
The joint pdf of\ $X$ and\ $Y$ is
$$
   f(x, y) = 
   \left\{
   \begin{array}{ll}
       \frac{1}{3} (3x^2 + xy), & 0 \leq x \leq 1, \, 0 \leq y \leq 2;\\
       0 & \text{ elsewhere.} 
   \end{array} 
   \right.
$$
The marginal PDF for\ $X$ is
\begin{align*}
   f_X(x)
   = \int_0^2\left(x^2 + \frac{xy}{3}\right) dy 
   &= \left.x^2y + \frac{xy^2}{6}\right|_{y = 0}^2\\
   &= 2x^2 + \frac{2x}{3}\quad\text{for $0 \leq x \leq 1$}.
\end{align*}
Also,
$$
   f_Y(y)
   = \int_0^1\left(x^2 + \frac{xy}{3}\right)dx
   = \left.\frac{1}{3}x^3 + \frac{1}{6}x^2y\right|_{x = 0}^1.
$$
So $\displaystyle f_Y(y) = \frac{1}{6}(2 + y)$, for $0 \leq y \leq 2$.

Consider computing $\Pr(Y < X)$; see Fig. \@ref(fig:regionXY); then
\begin{align*}
   \Pr(Y < X)
   &= \int \!\!\int_{\substack{(x, y) \in A\\ y < x}} f(x,y) \, dx \, dy \\
   &= \frac{1}{3}\int_0^1 \int_y^1(3x^2 + xy) \, dx \, dy\\
   &= \frac{1}{3} \int_0^1\left. x^3 + \frac{1}{2}x^2y\right|_y^1 dy\\
   &= \frac{1}{3} \int_0^1(1 + \frac{1}{2}y - \frac{3}{2}y^3) \, dy = \frac{7}{24}.
\end{align*}
:::

```{r regionXY, echo=FALSE, fig.cap="The region where $Y < X$.", fig.align="center", fig.width=6, out.width='60%'}
x <- seq(0, 1, 
         length = 100)
y <- x

plot(y ~ x,
     xlim = c(0, 1),
     ylim = c(0, 2),
     las = 1,
     main = expression(The~region~where~italic(Y) < italic(X)),
     xlab = expression(italic(x)),
     ylab = expression(italic(y)),
     type = "l",
     lwd = 2
     )
polygon( x = c(0, 1, 1, 0),
         y = c(0, 0, 1, 0),
         col = "grey")
polygon( x = c(0, 1, 1, 0),
         y = c(0, 0, 2, 2),
         col = NA, # Unfilled
         border = "black")

```


:::{.example #BVDiscreteMarginal2 name="Bivariate discrete marginal distributions"}
Recall Example\ \@ref(exm:BVDiscreteTwoDice), where two dice are rolled.
We can find the marginal distributions of\ $X$ and\ $Y$ (Table\ \@ref(tab:Marginals)).
The probabilities in the first row (where $Y = 0$), for instance, are summed and appear as the first term in the final column; this is the marginal distribution for $Y = 0$. 
Similarly for the other rows.

Recalling that\ $X$ is the number of times a `r knitr::include_graphics("Dice/die1.png", dpi=2000)` is rolled when two dice are thrown, the distribution of\ $X$ should be $\text{Bin}(2,  1/6$); the probabilities given in the last row of the table agree with this. 
That is, $\Pr(X = x) = \binom{2}{x}\left(\frac{1}{6}\right)^x \left(\frac{5}{6}\right)^{2 - x}$ for $x = 0, 1, 2$. 
Of course, the distribution of\ $Y$ is the same.
:::


```{r Marginals, echo=FALSE}
Marginals <- array( dim = c(4, 5))

Marginals[1, ] <- c("$y = 0$", "$4/9$", "$2/9$", "$1/36$", "$25/36$")
Marginals[2, ] <- c("$y = 1$", "$2/9$", "$1/18$", "$0$", "$10/36$")
Marginals[3, ] <- c("$y = 2$", "$1/36$", "$0$", "$0$", "$1/36$")
Marginals[4, ] <- c("$\\Pr(X = x)$", "$25/36$", "$10/36$", "$1/36$", "$1$")


Marginal.caption <- "The marginal distributions."
if( knitr::is_latex_output() ) {
  knitr::kable(Marginals,
               format = "latex",
               booktabs = TRUE,
               align = "r",
               longtable = FALSE,
               linesep = c("", "", "\\addlinespace"),
               escape = FALSE,
               col.names = c("",
                             "$x = 0$",
                             "$x = 1$" ,
                             "$x = 2$" ,
                             "$\\Pr(Y = y)$"),
               caption = Marginal.caption) %>%
    kable_styling(font_size = 10) %>%
    row_spec(3, hline_after = TRUE) %>%
    row_spec(0, bold = TRUE)   %>%
    column_spec(1, bold = TRUE)
}
if( knitr::is_html_output() ) {
  knitr::kable(Marginals,
               escape = TRUE,
               col.names = c("",
                             "$x = 0$",
                             "$x = 1$" ,
                             "$x = 2$" ,
                             "$\\Pr(Y = y)$"),
               caption = Marginal.caption,
               format = "html",
               booktabs = TRUE,
               align = "r",
               longtable = FALSE) %>%                
    row_spec(0, bold = TRUE)  %>%
    column_spec(1, bold = TRUE) 
}
```



:::{.example #BVDiscreteMarginal3 name="Bivariate discrete marginal distributions"}
Consider again the random process in Example\ \@ref(exm:BVDiscrete3DF).
From Table\ \@ref(tab:JointPF), the marginal distribution for\ $X_1$ is found simply by summing over the values for\ $X_2$ in the table. 
When $x_1 = 0$,
$$
   p_{X_1}(0) = \sum_{x_2} p_{X_1, X_2}(0,x_2) = 1/24 + 1/24 + 1/24 +\dots = 6/24.
$$
Likewise,
\begin{align*}
   p_{X_1}(1) &= \sum_{x_2} p_{X_1, X_2}(1,x_2) = 6/12;\\
   p_{X_1}(2) &= \sum_{x_2} p_{X_1, X_2}(2,x_2) = 6/24.
\end{align*}
So the marginal distribution of\ $X_1$ is
$$
   p_{X_1}(x_1) = \begin{cases}
            1/4 & \text{if $x_1 = 0$};\\
            1/2 & \text{if $x_1 = 1$};\\
            1/4 & \text{if $x_1 = 2$};\\
            0 & \text{otherwise}.\\
           \end{cases}
$$
This is equivalent to adding the row probabilities in Table\ \@ref(tab:JointPF). 
In this example, the marginal distribution is easily found from the total column of Table\ \@ref(tab:JointPF).
:::


## Bivariate random variables: conditional distributions {#ConditionalDistributions}

Consider $(X, Y)$ with joint probability function as in Example\ \@ref(exm:BVDiscrete2), with marginal distributions of\ $X$ and\ $Y$ as shown in Table\ \@ref(tab:JointPFmarginals).



```{r JointPFmarginals, echo=FALSE}
Conditionals <- array( dim = c(4, 5))

Conditionals[1, ] <- c("$y = 0$", "$1/36$", "$1/6$", "$1/4$", "$4/9$")
Conditionals[2, ] <- c("$y = 1$", "$1/9$", "$1/3$", "$0$", "$4/9$")
Conditionals[3, ] <- c("$y = 2$", "$1/9$", "$0$", "$0$", "$1/9$")
Conditionals[4, ] <- c("$\\Pr(X = x)$", "$1/4$", "$1/2$", "$1/4$", "$1$")


Conditionals.caption <- "A joint distribution with the marginal distributions."
if( knitr::is_latex_output() ) {
  knitr::kable(Conditionals,
               format = "latex",
               booktabs = TRUE,
               align = "r",
               longtable = FALSE,                
               escape = FALSE,
               linesep = c("", "", "\\addlinespace"),
               col.names = c("",
                             "$x = 0$",
                             "$x = 1$" ,
                             "$x = 2$" ,
                             "$\\Pr(Y = y)$"),
               caption = Conditionals.caption) %>%
    kable_styling(font_size = 10) %>%
    row_spec(3, hline_after = TRUE) %>%
    row_spec(0, bold = TRUE)   %>%
    column_spec(1, bold = TRUE)
}
if( knitr::is_html_output() ) {
  knitr::kable(Conditionals,
               escape = TRUE,
               col.names = c("",
                             "$x = 0$",
                             "$x = 1$" ,
                             "$x = 2$" ,
                             "$\\Pr(Y = y)$"),
               caption = Conditionals.caption,
               format = "html",
               booktabs = TRUE,
               align = "r",
               longtable = FALSE) %>%                
    row_spec(0, bold = TRUE)  %>%
    column_spec(1, bold = TRUE) 
}
```


Suppose we want to evaluate the conditional probability $\Pr(X = 1 \mid Y = 1)$. 
We use that $\Pr(A \mid B) = \Pr(A \cap B)/\Pr(B)$. 
So
$$
   \Pr(X = 1 \mid Y = 1) 
   = \frac{\Pr(X = 1, Y = 1)}{\Pr(Y = 1)} 
   = \frac{1/3}{4/9}
   = \frac{3}{4}.
$$
So, for each $x\in \mathcal{R}_X$ we could find $\Pr(X = x, Y = 1)$ and this is then the *conditional* distribution of\ $X$ given that $Y = 1$.


:::{.definition #BVDiscreteConditional name="Bivariate discrete conditional distributions"}
For a discrete random vector $(X, Y)$ with probability function $p_{X, Y}(x, y)$ the *conditional probability distribution* of\ $X$ given $Y = y$ is defined by
\begin{align}
   p_{X \mid Y = y}(x \mid y)
   &= \Pr(X = x \mid Y = y)\\
   &= \frac{\Pr(X = x, Y = y)}{\Pr(Y = y)}\\
   &= \frac{p_{X, Y}(x, y)}{p_Y(y)}
\end{align}
for $x \in \mathcal{R}_X$ and provided $p_Y(y) > 0$.
:::


The continuous case is analogous.


:::{.definition #BVContinuousMarginal name="Bivariate continuous marginal distributions"}
If $(X, Y)$ is a continuous $2$-dimensional random variable with joint pdf $f_{X, Y}(x, y)$ and respective marginal pdfs $f_X(x)$, $f_Y(y)$, then the *conditional probability distribution* of\ $X$ given $Y = y$ is defined by
\begin{equation}
   f_{X \mid Y = y}(x \mid y)
   = \frac{f_{X, Y}(x, y)}{f_Y(y)}
\end{equation}
for $x \in \mathcal{R}_X$ and provided $f_Y(y) > 0$.
:::


The above conditional pdfs satisfy the requirements for a univariate pdf; that is, $f_{X \mid Y}(x \mid y) \ge 0$ for all\ $x$ and $\int_0^\infty f_{X\mid Y}(x\mid y)\,dx = 1$.


:::{.example #BVcontinuousMarginal2 name="Bivariate continuous marginal distributions"}
In Example \@ref(exm:BVContinuousMarginal), the joint pdf of\ $X$ and\ $Y$ considered was
$$
   f_{X,Y}(x,y) = 
   \begin{cases}
      \frac{1}{3}(3x^2 + xy)  & \text{for $0 \leq x \leq 1$ and $0 \leq y \leq 2$};\\
      0                       & \text{elsewhere}. 
   \end{cases} 
$$
The marginal pdfs of\ $X$ and\ $Y$ are
\begin{align*}
   f_X(x) &= 2x^2 + \frac{2}{3}x \quad\text{for $0 \leq x \leq 1$}; \\
   f_Y(y) &= \frac{1}{6}(2 + y) \quad \text{for $0 \leq y \leq 2$}.
\end{align*}
Hence, the conditional distribution of $X \mid Y = y$ is
$$
   f_{X\mid Y = y}(x \mid y) 
   = \frac{(3x^2 + xy)/3}{(2 + y)/6}
   = \frac{2x(3x + y)}{2 + y} \quad\text{for $0 \leq x \leq 1$},
$$
and the conditional distribution of $Y \mid X = x$ is
$$
   f_{Y \mid X = x}(y \mid x) 
   = \frac{3x + y}{2(3x + 1)}\quad\text{for $0 \leq y \leq 2$}.
$$
Both these conditional density functions are valid density functions (verify!).

The marginal distribution for\ $Y$, and two conditional distributions of\ $Y$ (given $X = 0.1$ and $X = 0.9$) are shown in Fig.\ \@ref(fig:MargCondY).
:::



```{r, MargCondY, echo=FALSE, fig.align="center", fig.width=6, fig.height=3.25, out.width='100%',fig.cap="The marginal distribution of $Y$ (left panel), and the conditional distribution of $Y$ for $X = 0$ (centre panel) and $X = 1$ (right panel)."}
par( mfrow = c(1, 3))

y <- seq(-0.5, 2.5,
          length = 100)

# Marginal f(y)
fy <- function(y){
  f <- (2 + y) / 6
  f <- ifelse( y < 0, 0, f)
  f <- ifelse( y > 2, 0, f)
  f
}

plot(fy(y) ~ y,
     type = "l",
     xlab = expression(italic(y)),
     ylab = "Dnsity",
     las = 1,
     lwd = 2,
     ylim = c(0, 1),
     main = expression(atop(Marginal~distribution,
                            of~italic(Y))) )
grid(nx = NULL)


# Conditional f(y | X = x)
fyx <- function(y, x){
  f <- (3 * x + y) / (2 * (3 * x + 1) )
  f <- ifelse( y < 0, 0, f)
  f <- ifelse( y > 2, 0, f)
  f
}

plot(fyx(y, x = 0.1) ~ y,
     type = "l",
     xlab = expression(italic(y)),
     ylab = "Density",
     las = 1,
     lwd = 2,
     ylim = c(0, 1),
     main = expression(atop(Conditional~distribution,
                            of~italic(Y)~"|"~italic(X)==0.1) ) )
grid(nx = NULL)

# Conditional f(x | y = 1)
plot(fyx(y, x = 0.9) ~ y,
     type = "l",
     xlab = expression(italic(y)),
     ylab = "Density",
     las = 1,
     lwd = 2,
     ylim = c(0, 1),
     main = expression(atop(Conditional~distribution,
                            of~italic(Y)~"|"~italic(X)==0.9) ) )
grid(nx = NULL)

```


To interpret the conditional distribution, for example $f_{X \mid Y = y}(x \mid y)$, consider slicing through the surface $f_{X, Y}(x, y)$ with the plane $y = c$ say, for\ $c$ a constant 
`r if (knitr::is_latex_output()) {
   '(Fig.\\ \\@ref(fig:BivarConditionala)).'
} else {
   '(see below).'
}`
The intersection of the plane with the surface, will be proportional to a $1$-dimensional pdf. 
This is $f_{X, Y}(x, c)$, which will not, in general, be a density function since the area under this curve will be $f_Y(c)$. 
Dividing by the constant $f_Y(c)$ ensures the area under $\displaystyle\frac{f_{X,Y}(x,c)}{f_Y(c)}$ is one. 
This is a one-dimensional pdf, of\ $X$ given $Y = c$; that is $f_{X \mid Y = c}(x\mid c)$.



```{r BivarJointa, echo=FALSE, fig.cap="A bivariate distribution.", fig.align="center", out.width='60%', fig.width = 7}

f <- function(x, y){
  (3 *x + y)/(2 * (3*x + 1) )
}

if ( knitr::is_latex_output() ) {
  
  x <- seq(0, 1, 
           length = 30)
  y <- seq(0, 2, 
           length = 30)
  
  z <- zc <- outer(x, y, f)
  
  
  Cut <- which( x < 0.5 )
  zc[, Cut] <- 0
  
  persp(x, y, z, 
        theta = -60, 
        phi = 25, 
        shade = 0.75, 
        col = plotColour1, 
        expand = 0.5, 
        r = 2, 
        ltheta = 25, 
        xlab = "X",
        ylab = "Y",
        zlab = "f(x, y)",
        main = "Joint distribution",
        ticktype = "simple")
  
  
}
if( knitr::is_html_output() ) {
  x <- seq(0, 1, 
           length = 50)
  y <- seq(0, 2, 
           length = 50)
  
  z <- outer(x, y, f)
  
  
  Cut <- which( x < 0.5 )
  z[, Cut] <- 0
  
  
  M <- plot3D::mesh(x = x,
                    y = y)
  
  z <- zc <- outer(x, y, f)
  Cut <- which( M$y < -1 )  
  zc[Cut] <- 0
  
  
  rgl::persp3d(x = M$x, 
               y = M$y, 
               z = z,
               col = "grey",
               colkey = FALSE,
               phi = 40,
               xlab = "X",
               ylab = "Y",
               zlab = "f(x, y)",
               theta = 30,
               main = "Joint distribution")
}
```




```{r BivarConditionala, echo=FALSE, fig.cap="A bivariate distribution, sliced at $Y =  1$, showing the conditional distribution of $X$ when $Y = 0.5$.", fig.align="center", out.width='60%', fig.width = 11}

f <- function(x, y){
  (3 *x + y)/(2 * (3*x + 1) )
}

par( mfrow = c(1,2 ))

if ( knitr::is_latex_output() ) {
  
  x <- seq(0, 1, 
           length = 30)
  y <- seq(0, 2, 
           length = 30)
  
  z <- zc <- outer(x, y, f)
  
  
  Cut <- which( x < 0.5 )
  zc[, Cut] <- 0
  
  persp(x, y, zc, 
        theta = -60, 
        phi = 25, 
        shade = 0.75, 
        col = plotColour1, 
        expand = 0.5, 
        r = 2, 
        ltheta = 25, 
        xlab = "X",
        ylab = "Y",
        zlab = "f(x, y)",
        main = "Conditional distribution\n(Cut at x = 0.5)",
        ticktype = "simple")
}
if( knitr::is_html_output() ) {
  x <- seq(0, 1, 
           length = 50)
  y <- seq(0, 2, 
           length = 50)
  
  z <- outer(x, y, f)
  
  
  
  M <- plot3D::mesh(x = x,
                    y = y)
  
  z <- zc <- outer(x, y, f)
  Cut <- which( x < 0.5 )
  zc[, Cut] <- 0
  
  
  rgl::persp3d(x = M$x, 
               y = M$y, 
               z = zc,
               col = "grey",
               colkey = FALSE,
               phi = 40,
               xlab = "X",
               ylab = "Y",
               zlab = "f(x, y)",
               theta = 30,
               main = "Conditional distribution")
}
```


:::{.example #BVDiscreteConditional name="Bivariate discrete conditional distributions"}
Consider again the random process in Example\ \@ref(exm:BVDiscrete3). 
The conditional distribution for\ $X_2$ given $X_1 = 0$ can be found from Table\ \@ref(tab:JointPFmarginals).
First, $p_{X_1}(x_1)$, is needed, which was found in Example\ \@ref(exm:BVDiscreteMarginal3). 
Then,
\begin{align*}
   p_{X_2\mid X_1 = 0}(x_2\mid X_1 = 0)
   &= \frac{p_{X_1, X_2}(0, x_2)}{p_{X_1}(0)} \\
   &= \frac{p_{X_1, X_2}(0, x_2)}{1/4},
\end{align*}
from which we can deduce
$$
   p_{X_2 \mid X_1 = 0}(x_2 \mid X_1 = 0) =
   \begin{cases}
      \frac{1/24}{1/4} = 1/6 & \text{for $x_2 = 1$};\\
      \frac{1/24}{1/4} = 1/6 & \text{for $x_2 = 2$};\\
      \frac{1/24}{1/4} = 1/6 & \text{for $x_2 = 3$};\\
      \frac{1/24}{1/4} = 1/6 & \text{for $x_2 = 4$};\\
      \frac{1/24}{1/4} = 1/6 & \text{for $x_2 = 5$};\\
      \frac{1/24}{1/4} = 1/6 & \text{for $x_2 = 6$}.\\
   \end{cases}
$$
The conditional distribution $p_{X_2\mid X_1 = x_1}(x_2\mid x_1)$ is a probability function for\ $X_2$ (verify!).
Since\ $X_2$ was the number of the top face of the die, this is exactly as we should expect.
:::


## Bivariate random variables: independent random variables {#IndependentRVs}

Recall that events\ $A$ and\ $B$ are independent if, and only if,
$$
   \Pr(A \cap B) = \Pr(A)\Pr(B).
$$
An analogous definition applies for random variables.


:::{.definition #Independent name="Independent random variables"}
The random variables\ $X$ and\ $Y$ with joint distribution function\ $F_{X, Y}$ and marginal distribution functions\ $F_X$ and\ $F_Y$ are *independent* if, and only if,
\begin{equation}
   F_{X, Y}(x, y) = F_X(x) \times F_Y(y)
\end{equation}
for all\ $x$ and\ $y$.

If\ $X$ and\ $Y$ are not independent they are *dependent*, or *not independent*.
:::


The following theorem is often used to establish independence or dependence of random variables. 
The proof is omitted.


:::{.theorem}
The discrete random variables\ $X$ and\ $Y$ with joint probability function $p_{X, Y}(x, y)$ and marginal distributions $p_X(x)$ and $p_Y(y)$ are *independent* if, and only if,
\begin{equation}
   p_{X, Y}(x, y) = p_X(x) \times p_Y(y) \text{ for every }(x, y) \in \mathcal{R}_{X \times Y}.
      (\#eq:IndependentDiscretervs)
\end{equation}
The continuous random variables $(X, Y)$ with joint pdf $f_{X, Y}$ and marginal pdfs $f_X$ and $f_Y$ are *independent* if, and only if,
\begin{equation}
   f_{X, Y}(x, y) = f_X(x)\times f_Y(y)
\end{equation}
for all\ $x$ and\ $y$.
:::

To show independence for continuous random variables (and analogously for discrete random variables) we must show $f_{X, Y}(x, y) = f_X(x)\times f_Y(y)$ for *all* pairs $(x, y)$.
If $f_{X, Y}(x, y)\neq f_X(x)\times f_Y(y)$, even for any one particular pair of $(x, y)$, then\ $X$ and\ $Y$ are dependent.

:::{.example #BVDiscreteIndependence name="Bivariate discrete: Independence"}
The random variables\ $X$ and\ $Y$ have the joint probability distribution shown in Table\ \@ref(tab:Joint2).
Summing across rows, the marginal probability function of\ $Y$ is:
$$
   p_Y(y) = 
   \begin{cases}
      1/6  & \text{for $y = 1$};\\
      1/3  & \text{for $y = 2$};\\
      1/2  & \text{for $y = 3$}.
   \end{cases}
$$ 
To determine if\ $X$ and\ $Y$ are independent, the marginal probability function of\ $X$ is also needed:
$$
   p_X(x) = 
   \begin{cases}
      1/5  & \text{for $x = 1$};\\
      1/5  & \text{for $x = 2$};\\
      2/5  & \text{for $x = 3$};\\
      1/5  & \text{for $x = 4$}.
   \end{cases}
$$
Clearly, Eq.\ \@ref(eq:IndependentDiscretervs) is satisfied for all pairs $(x, y)$, so\ $X$ and\ $Y$ are independent.
:::




```{r Joint2, echo=FALSE}
Joint2 <- array( dim = c(3, 5))

Joint2[1, ] <- c("$y = 1$", "$1/30$", "$1/30$", "$2/30$", "$1/30$")
Joint2[2, ] <- c("$y = 2$", "$2/30$", "$2/30$", "$4/30$", "$2/30$")
Joint2[3, ] <- c("$y = 3$", "$3/30$", "$3/30$", "$6/30$", "$3/30$")


Joint2.caption <- "A joint probability function."
if( knitr::is_latex_output() ) {
  knitr::kable(Joint2,
               format = "latex",
               booktabs = TRUE,
               align = "r",
               longtable = FALSE,                
               escape = FALSE,
               col.names = c("",
                             "$x = 1$",
                             "$x = 2$" ,
                             "$x = 3$" ,
                             "$x = 4$"),
               caption = Joint2.caption) %>%
    kable_styling(font_size = 10) %>%
    row_spec(0, bold = TRUE)   %>%
    column_spec(1, bold = TRUE)
}
if( knitr::is_html_output() ) {
  knitr::kable(Joint2,
               escape = TRUE,
               col.names = c("",
                             "$x = 1$",
                             "$x = 2$" ,
                             "$x = 3$" ,
                             "$x = 4$"),
               caption = Joint2.caption,
               format = "html",
               booktabs = TRUE,
               align = "r",
               longtable = FALSE) %>%                
    row_spec(0, bold = TRUE)  %>%
    column_spec(1, bold = TRUE) 
}
```




:::{.example #BVContinuousIndependence name="Bivariate continuous: Independence"}
Consider the random variables\ $X$ and\ $Y$ with joint pdf
$$
   f(x, y)
   = \begin{cases}
      4xy & \text{for $0 < x < 1$ and $0 < y < 1 $}\\
      0   & \text{elsewhere}.\\ 
   \end{cases}
$$ 
To show that\ $X$ and\ $Y$ are independent, the marginal distributions of\ $X$ and\ $Y$ are needed. 
Now
$$
   f_X(x)
   = \int_0^1 4xy \, dy = 2x\quad\text{for $0 < x < 1$}.
$$
Similarly  $f_Y(y) = 2y$ for $0 < y < 1$.
Thus we have $f_X(x) \, f_Y(y) = f(x,y)$, so\ $X$ and\ $Y$ are independent.
:::


:::{.example #BVDiscreteIndependence2 name="Bivariate discrete: Independence"}
Consider again the random process in Example\ \@ref(exm:BVDiscrete3). 
The marginal distribution of\ $X_1$ was found in Example\ \@ref(exm:BVDiscreteMarginal3). 
The marginal distribution of\ $X_2$ is (check!)
$$
   p_{X_2}(x_2) =
   \begin{cases}
      \frac{1/24}{1/4} = 1/6 & \text{for $x_2 = 1$};\\
      \frac{1/24}{1/4} = 1/6 & \text{for $x_2 = 2$};\\
      \frac{1/24}{1/4} = 1/6 & \text{for $x_2 = 3$};\\
      \frac{1/24}{1/4} = 1/6 & \text{for $x_2 = 4$};\\
      \frac{1/24}{1/4} = 1/6 & \text{for $x_2 = 5$};\\
      \frac{1/24}{1/4} = 1/6 & \text{for $x_2 = 6$}.\\
   \end{cases}
$$
To determine if\ $X_1$ and\ $X_2$ are independent, *each*\ $x_1$ and\ $x_2$  pair must be considered. 
As an example, we see
\begin{align*}
   p_{X_1}(0) \times p_{X_2}(1) = 1/4 \times 1/6 = 1/24 &= p_{X_1, X_2}(0, 1);\\
   p_{X_1}(0) \times p_{X_2}(2) = 1/4 \times 1/6 = 1/24 &= p_{X_1, X_2}(0, 2);\\
   p_{X_1}(1) \times p_{X_2}(1) = 1/2 \times 1/6 = 1/12 &= p_{X_1, X_2}(1, 1);\\
   p_{X_1}(2) \times p_{X_2}(1) = 1/4 \times 1/6 = 1/24 &= p_{X_1, X_2}(2, 1).
\end{align*}
This is true for all pairs, and so\ $X_1$ and\ $X_2$ are independent random variables. 
Independence is, however, obvious from the description of the random process (Example\ \@ref(exm:BVDiscreteEG)), and is easily seen from Table\ \@ref(tab:JointPF).
:::



:::{.example #BVContinuousIndependence2 name="Bivariate continuous: Independence"}
Consider the continuous random variables\ $X_1$ and\ $X_2$ with joint pdf
$$
   f_{X_1, X_2}(x_1, x_2)  =
   \begin{cases}
      \frac{2}{7}(x_1 + 2x_2) & \text{for $0 < x_1 < 1$ and $1 < x_2 < 2$};\\
      0 & \text{elsewhere.}
   \end{cases}
$$
The marginal distribution of\ $X_1$ is
$$
   f_{X_1}(x_1)
   = \int_1^2 \frac{2}{7}(x_1 + 2x_2)\,dx_2\\
   =   \frac{2}{7}(x_1 + 3)
$$
for $0 < x_1 < 1$ (and zero elsewhere). Likewise, the marginal distribution of\ $X_2$ is
$$
   f_{X_2}(x_2)
   = \frac{2}{7}(x_1^2/2 + 2 x_1 x_2)\Big|_{x_1 = 0}^1
   = \frac{1}{7}(1 + 4x_2)
$$
for $1 < x_2 < 2$ (and zero elsewhere). 
(Both the marginal distributions must be valid density functions; verify!)
Since
$$
   f_{X_1}(x_1) \times f_{X_2}(x_2) = \frac{2}{49}(x_1 + 3)(1 + 4x_2) \ne f_{X_1, X_2}(x_1, x_2),
$$
the random variables\ $X_1$ and\ $X_2$ are *not independent*.

The conditional distribution of\ $X_1$ given $X_2 = x_2$ is
\begin{align*}
   f_{X_1 \mid X_2 = x_2}(x_1 \mid x_2)
   &= \frac{ f_{X_1, X_2}(x_1, x_2)}{ f_{X_2}(x_2)} \\
   &= \frac{ (2/7) (x_1 + 2x_2)}{ (1/7)(1 + 4x_2)}
\end{align*}
for $0 < x_1 < 1$ and any given value of $1 < x_2 < 2$. 
(Again, this conditional density must be a valid pdf.) 
So, for example,
$$
   f_{X_1 \mid X_2 = 1.5}(x_1\mid x_2 = 1.5)
   = \frac{ (2/7) (x_1 + 2\times 1.5)}{ (1/7)(1 + 4\times 1.5)}
   = \frac{2}{7}(x_1 + 3)
$$
for $0 < x_1 < 1$ and is zero elsewhere. 
And,
$$
   f_{X_1\mid X_2 = 1}(x_1 \mid 1)
   = \frac{ (2/7) (x_1 + 2\times 1)}{ (1/7)(1 + 4\times 1)}
   = \frac{2}{5}(x_1 + 2)
$$
for $0 < x_1 < 1$ and is zero elsewhere. 
Since the distribution of\ $X_1$ depends on the given value of\ $X_2$, $X_1$ and\ $X_2$ are *not* independent.
:::



:::{.example #BVContinuousIndependence3 name="Bivariate continuous: Independence"}
Consider the two continuous random variables\ $Y_1$ and\ $Y_2$ with joint probability function
$$
   f_{Y_1, Y_2}(y_1, y_2)=
   \begin{cases}
      2(y_1 + y_2) & \text{for $0 < y_1 < y_2 < 1$};\\
      0 & \text{elsewhere}.
   \end{cases}
$$
A diagram of the region over which\ $Y_1$ and\ $Y_2$ are defined is shown in Fig.\ \@ref(fig:BVRegion).
To determine if\ $Y_1$ and\ $Y_2$ are independent, the two marginal distributions are needed.
For example:
$$
   f_{Y_1}(y_1) = 1 + 2y_1 - 3y_1^2\quad\text{for $0 < y_1 < 1$}.
$$
Since the distribution of\ $Y_1$ depends on the value of\ $Y_2$, this means\ $Y_1$ and\ $Y_2$ are *not independent*.
:::


```{r BVRegion, echo=FALSE, fig.align="center", fig.width=4, fig.height=4,fig.cap="The region over which $f_{Y_1, Y_2}(y_1, y_2)$ is defined."}
plot( x = c(0, 1),
      y = c(0, 1),
      xlab = expression(italic(y)[1]),
      ylab = expression(italic(y)[2]),
      type = "n",
      main = expression(paste("The region where"~italic(f)~"is defined")),
      las = 1)
polygon( x = c(0, 1, 0, 0),
         y = c(0, 1, 1, 0),
         col = plotColour)
lines( x = c(0, 1, 0, 0),
       y = c(1, 1, 0, 1),
       lwd = 2)
```


## Mixed joint probability functions {#MixedJointPF}

So far, the bivariate distributions have included the cases of two discrete or two continuous random variables.
However, it is also possible for one variable, say\ $X$, to be continuous and the other, say\ $Y$, to be discrete.


:::{.definition #BivariateProbFunctionCont name="Mixed bivariate probability function"}
Let $(X, Y)$ be a random vector where\ $X$ is *continuous* with range space $S_X \subseteq \mathbb{R}$, and\ $Y$ is *discrete* with range space $S_Y = \{y_1, y_2, \dots\}$.
The *joint density--mass function* of $(X, Y)$ is defined on
$$
  \mathcal{R}_{X, Y} = S_X \times S_Y
$$
by
$$
  f_{X, Y}(x, y_j) 
  = f_{X \mid Y}(x \mid y_j) \cdot p_Y(y_j),
$$
where:

- $p_Y(y_j) = \Pr(Y = y_j)$ is the probability *mass* function of\ $Y$, and
- $f_{X \mid Y}(x \mid y_j)$ is the conditional *density* function of\ $X$ given\ $Y = y_j$.
:::


Like all probability functions, the joint density--mass function is non-negative:
$$
  f_{X, Y}(x, y_j) \geq 0 \quad \text{for all $(x, y_j)\in\mathcal{R}_{X, Y}$}.
$$
In addition, the total probability is one:
$$
  \sum_{j\in S_Y} \int_{S_X} f_{X,Y}(x, y_j)\, dx = 1.
$$

The probability of any event $A \subseteq S_X$ and $y_j \in S_Y$ is
$$
  \Pr(X \in A, Y = y_j) = \int_A f_{X,Y}(x, y_j)\, dx.
$$



:::{.example #MixedRV name="Mixed random variable"}
Suppose a random vector $(X, Y)$ is defined so that $Y \in \{1, 2\}$ with
$$
  p_Y(1) = 0.4, \quad p_Y(2) = 0.6.
$$
Conditional on $Y = 1$, the probability density function of\ $X$ is
$$
  f_{X \mid Y}(x \mid y = 1) = 
  \begin{cases}
    1 & 0 \le x \le 1, \\
    0 & \text{otherwise,}
  \end{cases}
$$
and conditional on $Y = 2$, the probability density function of\ $X$ is
$$
  f_{X \mid Y}(x \mid y = 2) 
  = 
  \begin{cases}
    1/2   & 0 \le x \le 2, \\
    0     & \text{otherwise.}
  \end{cases}
$$
Then the joint density--mass function is
$$
  f_{X, Y}(x, y) 
  = f_{X \mid Y}(x \mid y) \cdot p_Y(y)
$$
or, more explicitly:
$$
  f_{X, Y}(x, 1) =
  \begin{cases}
    0.4  & 0 \le x \le 1, \\
    0,   & \text{otherwise;}
\end{cases}
\quad
f_{X, Y}(x, 2) 
= 
\begin{cases}
  0.3 & 0 \le x \le 2, \\
  0   & \text{otherwise.}
\end{cases}
$$
Notice that 
$$
  \sum_{y \in \{1,2\}} \int_{-\infty}^{\infty} f_{X,Y}(x,y)\, dx
  = \int_0^1 0.4 \, dx + \int_0^2 0.3 \, dx
  = 0.4 + 0.6 = 1
$$
as required.

Then, for instance, we can find $\Pr(X \le 0.5, Y = 2)$:
$$
  \Pr(X \le 0.5, Y = 2) 
  = \int_0^{0.5} f_{X, Y}(x, 2)\, dx
  = \int_0^{0.5} 0.3\, dx
  = 0.15.
$$
:::



## Multivariate random variables* {#Multivariate}

### Random Vectors {#RandomVectors}

Up to now we have studied *univariate* random variables (i.e., a single random variable) 
and *bivariate* random variables (two jointly distributed random variables). 
These ideas can be extended to more random variables; the case of *multivariate* random variables, where several random variables are considered simultaneously.  

To do this, we use *random vectors*.
A random vector is a column vector of $n$\ random variables:
$$
  \mathbf{X} = (X_1, X_2, \dots, X_n)^T.
$$
Each\ $X_i$ is a random variable, and together they form an $n$-dimensional random vector.

In the bivariate case, for example,
\begin{equation}
   \mathbf{X} 
   = \left( 
     \begin{array}{c} 
       X_1 \\ 
       X_2 
     \end{array} 
     \right).
  \label{EQN:matrix1}
\end{equation}
The linear combination $Y = a_1 X_1 + a_2 X_2$ can be expressed
\begin{equation}
   Y 
   = a_1 X_1 + a_2 X_2 
   = (a_1, a_2)
   \left( 
   \begin{array}{c} 
      X_1 \\ 
      X_2 
   \end{array} 
   \right) 
   = \mathbf{a}^T\mathbf{X}
   (\#eq:BivarCombination)
\end{equation}
where the (column) vector $\mathbf{a} = \left( \begin{array}{c} a_1 \\ a_2 \end{array} \right)$, and the superscript\ $T$ means 'transpose'.


### Joint probability functions {#MultivariateProbFn}

The *joint probability function* of $\mathbf{X}$ describes the probability distribution for all $n$\ random variables simultaneously.


:::{.definition #JointPMF name="Joint probability function (discrete)"}
Let $\mathbf{X} = (X_1, \dots, X_n)$ be an $n$-dimensional discrete random variable. 
The random variable $X_j$ (for $j = 1, \dots, n$) takes values in some set $S_j$ (the range).  

Then, the range space of the random vector is
$$
  \mathcal{X} \subseteq S_1\times \cdots \times S_n.
$$
Then, the *joint probability mass function* is
$$
  p(x_1, \dots, x_n) = P(X_1 = x_1,\, \dots,\, X_n = x_n)
  \quad \text{for $(x_1,\dots,x_n) \in \mathcal{X}$},
$$
such that 
$$
  p(x_1,\dots,x_n) \geq 0 
  \quad \text{for all $(x_1,\dots,x_n) \in \mathcal{X}$},
$$
and
$$
  \sum_{(x_1,\dots,x_n) \in \mathcal{X}} p(x_1,\dots,x_n) = 1.
$$
:::



:::{.example #Dice3D name="Three dice"}
Consider the random vector $\mathbf{X} = (X_1, X_2, X_3)$ representing the outcome of rolling three fair six-sided dice.  

Since each $X_j \in \{1,2,3,4,5,6\}$, the range space is
$$
  \mathcal{X} = \{1,2,3,4,5,6\} \times \{1,2,3,4,5,6\} \times \{1,2,3,4,5,6\}.
$$
Assuming the dice are independent, the joint probabuility mass function is
$$
  p(x_1,x_2,x_3) = P(X_1=x_1, X_2=x_2, X_3=x_3) = \frac{1}{6^3} = \frac{1}{216}, 
  \quad \text{for $(x_1,x_2,x_3) \in \mathcal{X}$}.
$$

For instance, the probability that the sum of the three dice equals\ $10$ is
$$
  \Pr(X_1 + X_2 + X_3 = 10) 
  = \sum_{(x_1, x_2, x_3) \in \mathcal{X},\, x_1 + x_2 + x_3 = 10} p(x_1,x_2,x_3).
$$
R TO GET ANSWER?
:::


:::{.definition #JointPDF name="Joint probability function (continuous)"}
Let $\mathbf{X} = (X_1, \dots, X_n)$ be an $n$-dimensional continuous random vector.  
For each $j = 1, \dots, n$, the random variable\ $X_j$ takes values in some subset $S_j \subseteq \mathbb{R}$.  

The range space of the vector is therefore
$$
  \mathcal{X} \subseteq S_1 \times \cdots \times S_n \subseteq \mathbb{R}^n.
$$
The *joint probability density function* (pdf) of $\mathbf{X}$ is
$$
  f(x_1, \dots, x_n), \quad \text{for $(x_1,\dots,x_n) \in \mathcal{X}$},
$$
such that 
$$
  f(x_1,\dots,x_n) \geq 0 
  \quad \text{for all $(x_1,\dots,x_n) \in \mathcal{X}$},
$$
and
$$
  \int_{\mathcal{X}} f(x_1,\dots,x_n)\, dx_1 \cdots dx_n = 1.
$$
:::


:::{.example #UniformCube3D name="Three-Dimensional Uniform Example (Continuous)"}
Consider the random vector $\mathbf{X} = (X_1, X_2, X_3)$, uniformly distributed on the unit cube $[0, 1]^3$.  
The range space is
$$
  \mathcal{X} = [0, 1] \times [0, 1] \times [0, 1] \subset \mathbb{R}^3.
$$

The joint probability density function is:
$$
  f(x_1,x_2,x_3) =
  \begin{cases} 
    1 & \text{for $0 \le x_j \le 1$ for all $j = 1, 2, 3$},\\
    0 & \text{otherwise}.
  \end{cases}
$$

The marginal distributions are all uniform on $[0, 1]$:
$$
  f_{X_1}(x_1) = \int_0^1 \int_0^1 f(x_1, x_2, x_3)\, dx_2 dx_3 = 1, \quad 0\le x_1 \le 1.
$$

For instance, the probability that the sum of the three variables is less than or equal to\ $1$ is
$$
  P(X_1 + X_2 + X_3 \le 1) 
  = \text{volume of the tetrahedron } \{(x_1,x_2,x_3)\in\mathcal{X}: x_1 + x_2 + x_3 \le 1\} 
  = \frac{1}{6}.
$$
:::





### Joint distribution functions {#MultivariateDistFn}

The *multivariate distribution function (CDF)* is
$$
  F(x_1, \dots, x_n) = P(X_1 \leq x_1, \dots, X_n \leq x_n).
$$



:::{.example #Dice3D_CDF name="Three dice"}
Let $\mathbf{X} = (X_1, X_2, X_3)$ be the random vector representing the outcome of rolling three fair six-sided dice, as in Example\ \@ref(exm:Dice3D) (where the joint probability mass function is given).

The *joint cumulative distribution function* is
$$
  F(x_1, x_2, x_3) 
  = P(X_1 \le x_1, X_2 \le x_2, X_3 \le x_3)
  = \sum_{i = 1}^{\lfloor x_1 \rfloor} 
    \sum_{j = 1}^{\lfloor x_2 \rfloor} \sum_{k = 1}^{\lfloor x_3 \rfloor} p(i, j, k),
$$
where $\lfloor x \rfloor$ denotes the greatest integer less than or equal to $x$.

For instance, the probability that all dice shows less than or equal to\ $3$ is
$$
  F(3, 3, 3) 
  = \sum_{i = 1}^{3} \sum_{j = 1}^{3} \sum_{k = 1}^{3} \frac{1}{216} 
  = \frac{27}{216} = \frac{1}{8}.
$$
:::


:::{.example #UniformCube3D_CDF name="Three-Dimensional Uniform Cube: CDF"}
Let $\mathbf{X} = (X_1, X_2, X_3)$ be uniformly distributed on the unit cube $[0, 1]^3$, as in Example\ \@ref(exm:UniformCube3D) (where the joing probability density function is given)

The *joint cumulative distribution function* is
$$
  F(x_1, x_2, x_3) = P(X_1 \le x_1, X_2 \le x_2, X_3 \le x_3)
  = \int_0^{x_1}\!\! \int_0^{x_2}\!\! \int_0^{x_3} f(u_1, u_2, u_3)\, du_3\, du_2\, du_1,
$$
for $0 \le x_1, x_2, x_3 \le 1$.

For instance, the probability that each variable is less than\ $0.5$ is
$$
  F(0.5, 0.5, 0.5) = \int_0^{0.5}\!\! \int_0^{0.5}\!\! \int_0^{0.5} 1 \, du_3\, du_2\, du_1 = 0.5^3 = 0.125.
$$
:::


### Marginal and conditional distributions

Marginal distributions are obtained by summing or integrating out unwanted variables. For example,
$$
  f_{X_1}(x_1) = \int_{\mathbb{R}^{n-1}} f(x_1, x_2, \dots, x_n)\, dx_2 \cdots dx_n.
$$

Conditional distributions are defined in the natural way:
$$
  f_{X_1 \mid X_2, \dots, X_n}(x_1 \mid x_2, \dots, x_n) =
  \frac{f(x_1, x_2, \dots, x_n)}{f_{X_2,\dots,X_n}(x_2, \dots, x_n)}.
$$

EXAMPLES


### Multivariate independence {#MultivariateIndependence}

In the multivariate case, $n$\ random variables $X_1, \dots, X_n$ are *independent* if knowing the value of any subset of them gives no information about the others.
Formally, let $\mathbf{X} = (X_1, \dots, X_n)$ be an $n$-dimensional random vector with joint probability distribution. 
Then:

- **Discrete case:** The random variables are independent if and only if the joint probability mass function factors as the product of the marginal probability functions:
$$
  p(x_1, \dots, x_n) = \prod_{i=1}^{n} p_{X_i}(x_i),
  \quad (x_1, \dots, x_n) \in \mathcal{X}.
$$

- **Continuous case:** The random variables are independent if and only if the joint probability density function factors as the product of the marginal densities:
$$
  f(x_1, \dots, x_n) = \prod_{i=1}^{n} f_{X_i}(x_i),
  \quad (x_1, \dots, x_n) \in \mathcal{X}.
$$

- **Equivalently for CDFs:** Independence is also equivalent to
$$
  F(x_1, \dots, x_n) = \prod_{i=1}^{n} F_{X_i}(x_i),
$$
where $F$ is the joint cumulative distribution function.  

In practice, independence allows us to compute joint probabilities or volumes simply by multiplying the corresponding marginal probabilities or integrating products of marginal densities.  


:::{.example #Dice3D_Indep name="Three Dice Example: Independence"}
Consider three fair six-sided dice, represented by the random vector $\mathbf{X} = (X_1,X_2,X_3)$.  

Each $X_j \in \{1,2,3,4,5,6\}$, so $\mathcal{X} = \{1,\dots,6\}^3$.  
The dice are independent, so the joint pmf factors as the product of the marginals:
$$
  p(x_1, x_2, x_3) 
  = P(X_1 = x_1, X_2 = x_2, X_3 = x_3) 
  = \prod_{i = 1}^3 P(X_i = x_i) 
  = \frac{1}{6}\cdot   \frac{1}{6} \cdot \frac{1}{6} = \frac{1}{216}.
$$
Equivalently, the joint CDF factors as
$$
  F(x_1, x_2, x_3) = P(X_1\le x_1, X_2 \le x_2, X_3 \le x_3) 
  = \prod_{i = 1}^3 F_{X_i}(x_i).
$$

**Example:** Probability that all dice less than or equal to\ $3$:
$$
  F(3, 3, 3) 
  = \prod_{i = 1}^3 F_{X_i}(3) 
  = \left(\frac{3}{6}\right)^3 
  = \frac{27}{216} = \frac{1}{8}.
$$
:::


:::{.example #UniformCube3D_Indep name="Three-Dimensional Uniform Cube: Independence"}
Let $\mathbf{X} = (X_1, X_2, X_3)$ be uniformly distributed on the unit cube $[0, 1]^3$, with joint pdf
$$
  f(x_1, x_2, x_3) = 1, \quad (x_1, x_2, x_3) \in [0, 1]^3.
$$

 The variables $X_1, X_2, X_3$ are independent, so the joint pdf factors as
$$
  f(x_1, x_2, x_3) 
  = f_{X_1}(x_1) f_{X_2}(x_2) f_{X_3}(x_3) 
  = 1 \cdot 1 \cdot 1.
$$

Equivalently, the joint CDF factors as
$$
  F(x_1, x_2, x_3) 
  = P(X_1 \le x_1, X_2 \le x_2, X_3 \le x_3) 
  = \prod_{i = 1}^3 F_{X_i}(x_i).
$$

**Example:** Probability that all variables are less than\ $0.5$:
$$
  F(0.5, 0.5, 0.5) = 0.5^3 = 0.125.
$$
:::


## Simulation {#SimulationBivariate}

As with univariate distributions (Sects.\ \@ref(SimulationDiscrete) and\ \@ref(SimulationContinuous)), simulation can be used with bivariate distributions.
Random numbers from the *bivariate normal* distribution (Sect.\ \@ref(BVNormalDistribution)) are generated using the function `dmnorm()` from the library `mnorm`.
Random numbers from the *multinomial* distribution (Sect.\ \@ref(MultinomialDistribution)) are generated using the function `rmultinom()`.
More commonly, univariate distributions are combined.

Monthly rainfall, for example, is commonly modelled using gamma distributions (for example, @husak2007use).
Simulating rainfall is then used in others models (such as for cropping simulations; for example @ines2006bias).
As an example, consider a location where the monthly rainfall is well-modelled by a gamma distribution with a shape parameter $\alpha = 1.6$ and a scale parameter of $\beta = 220$ (Fig.\ \@ref(fig:RainfallSim), left panel):

```{r echo=FALSE}
set.seed(107283)
```

```{r echo=TRUE}
library("mnorm") # Need to explicitly load  mnorm  library
MRain <- rgamma( 2000,
                 shape = 1.6,
                 scale = 220)

cat("Rainfall exceeding 900mm:",
    sum(MRain > 900) / 2000 * 100, "%\n")
# Directly:
round( (1 - pgamma(900, shape = 1.6, scale = 220) ) * 100, 2)
```

The percentage of months with rainfall exceeding\ $1000\mms$ was also computed. 
However, now suppose that the shape parameter $\alpha$ also varies, with an exponential distribution with mean\ $2$ (Fig.\ \@ref(fig:RainfallSim), centre panel):

```{r echo=FALSE}
set.seed(1072823)
```
```{r echo=TRUE}
MRain2 <- rgamma( 1000,
                  shape = rexp(1000, rate = 1/2),
                  scale = 220)

cat("Rainfall exceeding 900mm:",
    sum(MRain2 > 900) / 1000 * 100, "%\n")
```

Using simulation, it is also easy to simulate the impact of the scale parameter\ $\beta$ varying also, suppose with a normal distribution mean of\ $200$ and variance of\ $16$ (Fig.\ \@ref(fig:RainfallSim), right panel):

```{r echo=FALSE}
set.seed(1072483)
```

```{r echo=TRUE}
MRain3 <- rgamma( 1000,
                  shape = rexp(1000, rate = 1/2),
                  scale = rnorm(1000, mean = 200, sd = 4))

cat("Rainfall exceeding 900:",
    sum(MRain3 > 900) / 1000 * 100, "%\n")
```

```{r echo=FALSE, RainfallSim, fig.width=6, fig.height = 3, out.width='100%', fig.align="center", fig.cap="Three simulations using the gamma distribution."}
par( mfrow = c(1, 3))

hist(MRain,
     las = 1,
     breaks = seq(0, 4500, by = 250),
     xlim = c(0, 4500),
     ylim = c(0, 1000),
     xlab = "Monthly rainfall (in mm)",
     main = "Rainfall: Sim 1")
hist(MRain2,
     las = 1,
     breaks = seq(0, 4500, by = 250),
     xlim = c(0, 4500),
     ylim = c(0, 1000),
     xlab = "Monthy rainfall (in mm)",
     main = "Rainfall: Sim 2")
hist(MRain3,
     las = 1,
     breaks = seq(0, 4500, by = 250),
     xlim = c(0, 4500),
     ylim = c(0, 1000),
     xlab = "Monthly rainfall (in mm)",
     main = "Rainfall: Sim 3")
```



```{r echo=FALSE, results=FALSE, message=FALSE, warning=FALSE}
# Estimates from here, for instance:
PCSM <- read.csv("./Data/PlaneCreekSugarMill/IDCJAC0001_033059_Data12.csv",
                 na.strings = "null")
PCSMjan <- PCSM$Jan

library(fitdistrplus)
fitdist(PCSMjan[!is.na(PCSMjan)], "gamma")
```


## Exercises {#MultivariateExercises}

Selected answers appear in Sect.\ \@ref(AnswersChapMultivariate).


:::{.exercise #DiscreteXY}
The discrete random variables\ $X$ and\ $Y$ have the joint probability function shown in Table\ \@ref(tab:TableXY).
Determine:

1. $\Pr(X = 1, Y = 2)$ 
1. $\Pr(X + Y \le 1)$.
1. $\Pr(X > Y)$. 
1. the marginal probability function of\ $X$.
1. the pf of $Y \mid X = 1$.
:::

```{r TableXY, echo=FALSE}
XYTable <- array( dim = c(2, 4))
  
XYTable[1, ] <- c("$X = 0$",
                  "$1/12$",
                  "$1/6$",
                  "$1/24$")
XYTable[2, ] <- c("$X = 1$",
                  "$1/4$",
                  "$1/4$",
                  "$5/24$")

if( knitr::is_latex_output() ) {

  knitr::kable(XYTable,
               format = "latex",
               booktabs = TRUE,
               align = "r",
               longtable = FALSE,                
               escape = FALSE,
               col.names = c("",
                             "$Y = 0$",
                             "$Y = 1$",
                             "$Y = 2$"),
               caption = "$X$ and $Y$.") %>%
#    kable_styling(font_size = 10) %>%
    row_spec(0, bold = TRUE) 
} else{

  knitr::kable(XYTable,
               format = "html",
               booktabs = TRUE,
               align = "r",
               longtable = FALSE,                
               escape = FALSE,
               col.names = c("",
                             "$Y = 0$",
                             "$Y = 1$",
                             "$Y = 2$"),
               caption = "$X$ and $Y$.") %>%
    row_spec(0, bold = TRUE) 
}
```


:::{.exercise #AandBsimple}
The random variable\ $A$ has mean\ $13$ and variance\ $5$. 
The random variable\ $B$ has mean\ $4$ and variance\ $2$. 
Assuming\ $A$ and\ $B$ are independent, find:

1. $\operatorname{E}[A + B]$.
1. $\operatorname{var}[A + B]$.
1. $\operatorname{E}[2A - 3B]$.
1. $\operatorname{var}[2A - 3B]$.
:::


:::{.exercise #AandBsimple2}
Repeat Exercise\ \@ref(exr:AandBsimple), but with $\operatorname{Cov}(A, B) = 0.2$.
:::

:::{.exercise #SampleMean}
Suppose $X_1, X_2, \dots, X_n$ are independently distributed random variables, each with mean\ $\mu$ and variance\ $\sigma^2$.
Define the sample mean as $\overline{X} = \left( \sum_{i=1}^n X_i\right)/n$.

1. Prove that $\operatorname{E}[\overline{X}] = \mu$.
1. Find the variance of\ $\overline{X}$.
:::



:::{.exercise #PoissonGammaRainfall}
The *Poisson-gamma distributions*\index{Poisson--gamma distribution} (e.g., see @mypapers:Hasan:simplePG), used for modelling rainfall, can be developed as follows:

* Let\ $N$ be the number of rainfall events in a month, where $N\sim \text{Pois}(\lambda)$.
  If no rainfall events are recorded in any month, then the monthly rainfall is $Z = 0$.
* For each rainfall event (that is, when $N > 0$), say\ $i$ for $i = 1, 2, \dots N$, the *amount* of rain in event\ $i$, say\ $Y_i$, is modelled using a gamma distribution with parameter\ $\alpha$ and\ $\beta$.
* The total monthly rainfall is then $Z = \sum_{i = 1}^N Y_i$.

Suppose the monthly rainfall station at a particular station can be modelled using $\lambda = 0.78$, $\alpha = 0.5$ and $\beta = 6$.

1. Use a simulation to produce a one-month rainfall total using this model.
1. Repeat for $1\,000$\ simulations (i.e., simulate $1\,000$\ months), and plot the distribution.
1. What type of random variable is the monthly rainfall: discrete, continuous, or mixed? 
   Explain.
1. Based on the $1\,000$\ simulations, approximately how often does a month have exactly zero rainfall?
1. Based on the $1\,000$\ simulations, what is the mean monthly rainfall?
1. Based on the $1\,000$\ simulations, what is the mean monthly rainfall in months where rain is recorded?
:::


:::{.exercise #NBfromPoisGamma}
Suppose $X \sim \text{Pois}(\lambda)$.
Show that if $\lambda\sim\text{Gam}(a, p/(1 - p) )$, then the distribution of\ $X$ has a negative binomial distribution.
:::
  
  
:::{.exercise #JointProbAbsolute}
$(X, Y)$ has joint probability function given by
$$
  \Pr(X = x, Y = y) = k |x - y|
$$
for $x = 0, 1, 2$ and $y = 1, 2, 3$.

1. Find the value\ $k$.
1. Construct a table of probabilities for this distribution.
1. Find $\Pr(X \le 1, Y = 3)$.
1. Find $\Pr(X + Y \ge 3)$.
:::

  
:::{.exercise #JointPDF}
For what value of\ $k$ is $f(x,y) = kxy$ (for $0 \le x \le 1$; $0 \le y \le 1$, a valid joint pdf?
                                           
1. Then, find $\Pr(X \le x_0, Y\le y_0)$.
1. Hence evaluate $\Pr\left(X \le (3/8), Y \le (5/8) \right)$.
:::

  
  
:::{.exercise #LinearCombinCovariance}
$X$, $Y$ and\ $Z$ are uncorrelated random variables with expected values\ $\mu_x$, $\mu_y$ and\ $\mu_z$ and standard deviations\ $\sigma_x$, $\sigma_y$ and\ $\sigma_z$. 
$U$ and\ $V$ are defined by
\begin{align*}
   U&= X - Z;\\
   V&= X - 2Y + Z.
\end{align*}

1. Find the expected values of\ $U$ and\ $V$.
1. Find the variance of\ $U$ and\ $V$.
1. Find the covariance between\ $U$ and\ $V$.
1. Under what conditions on\ $\sigma_x$, $\sigma_y$ and\ $\sigma_z$ are\ $U$ and\ $V$ uncorrelated?
:::
  

:::{.exercise #JointDiscrete11}
Suppose $(X, Y)$ has joint probability function given by
$$
  \Pr(X = x, Y = y) = \frac{|x - y|}{11}
$$
for $x = 0, 1, 2$ and $y = 1, 2, 3$.

1. Find $\operatorname{E}[X \mid Y = 2]$.
1. Find $\operatorname{E}[Y \mid X\ge 1]$.
:::
  
  
:::{.exercise #CorrContDist}
The pdf of $(X, Y)$ is given by
$$
   f_{X, Y}(x, y) = 1 - \alpha(1 - 2x)(1 - 2y), 
$$
for $0 \le x \le 1$, $0 \le y \le 1$ and $-1 \le \alpha \le 1$.

1. Find the marginal distributions of\ $X$ and\ $Y$.
1. Evaluate the correlation coefficient\ $\rho_{XY}$.
1. For what value of\ $\alpha$ are\ $X$ and\ $Y$ independent?
1. Find $\Pr(X < Y)$.
:::
  
  
:::{.exercise #Marginal2}
For the random vector $(X, Y)$, the conditional pdf of\ $Y$ given $X = x$ is
$$
  f_{Y \mid X = x}(y\mid x) = \frac{2(x + y)}{2x + 1},
$$
for $0 < y <1$.
The marginal pdf of\ $X$ is given by
$$
  g_X(x) = x + \frac{1}{2}
$$
for $0 <x < 1$.

1. Find $F_Y(y \mid x)$ and hence evaluate $\Pr(Y < 3/4 \mid  X = 1/3)$.
1. Find the joint pdf, $f_{X, Y}(x, y)$, of\ $X$ and\ $Y$.
1. Find $\Pr(Y < X)$.
:::

  
  
:::{.exercise}
The random variables $X_1$, $X_2$, and $X_3$ have y

* $X_1$: mean\ $\mu_1 = 5$ with standard deviation $\sigma_1 = 2$.
* $X_2$: mean\ $\mu_2 = 3$ with standard deviation $\sigma_2 = 3$.
* $X_3$: mean\ $\mu_3 = 6$ with standard deviation $\sigma_3 = 4$.

The correlations are: $\rho_{12} = -\frac{1}{6}$, $\rho_{13} = \frac{1}{6}$ and $\rho_{23} = \frac{1}{2}$. 

If the random variables\ $U$ and\ $V$ are defined by $U = 2X_1 + X_2 - X_3$ and $V = X_1  - 2X_2 - X_3$, find

1. $\operatorname{E}[U]$.
1. $\operatorname{var}[U]$.
1. $\operatorname{Cov}(U, V)$.
:::

  
:::{.exercise}
Let\ $X$ and\ $Y$ be the body mass index (BMI) and percentage body fat for netball players attending the AIS. 
Assume\ $X$ and\ $Y$ have a bivariate normal distribution with $\mu_X = 23$, $\mu_Y = 21$, $\sigma_X = 3$, $\sigma_Y = 6$ and $\rho_{XY} = 0.8$. 
Find

1. the expected BMI of a netball player who has a percent body fat of\ $30$.
1. the expected percentage body fat of a netball player who has a BMI of\ $19$.
:::

  
:::{.exercise}
Let\ $X$ and\ $Y$ have a bivariate normal distribution with parameters $\mu_x = 1$, $\mu_y = 4$, $\sigma^2_x = 4$, $\sigma^2_y = 9$ and $\rho = 0.6$. 
Find

1. $\Pr(-1.5 < X < 2.5)$.
1. $\Pr(-1.5 < X < 2.5 \mid Y = 3)$.
1. $\Pr(0 < Y < 8)$.
1. $\Pr(0 < Y < 8 \mid X = 0)$.
:::
  

  
:::{.exercise #FairCoinTossedTwice}
Consider a random process where a fair coin is tossed twice. 
Let\ $X$ be the number of heads observed in the two tosses, and\ $Y$ be the number of heads on the first toss of the coin.

1. Construct the table of the joint probability function for\ $X$ and\ $Y$.
1. Determine the marginal probability function for\ $X$.
1. Determine the conditional distribution of\ $X$ given one head appeared on the first toss.
1. Determine if the variables\ $X$ and\ $Y$ are independent or not, justifying your answer with necessary calculation or argument.
:::
  
  
:::{.exercise #DiceMinMaxCD}
Two fair, six-sided dice are rolled, and the numbers on the top faces observed. 
Event\ $A$ is the maximum of the two numbers, and Event\ $B$ is the minimum of the two numbers.
Then, define\ $C$ as\ $0$ if the maximum is odd, and as\ $1$ otherwise; and define\ $D$ as\ $0$ if the minimum is divisible by three, and as\ $1$ otherwise.

Construct the joint probability function for\ $C$ and\ $D$.
:::

  
  
:::{.exercise #GammaExp}
Consider\ $n$ random variables\ $Z_i$ such that $Z_i \sim \text{Exp}(\beta)$ for every $i = 1, \dots, n$. 
Show that the distribution of $Z_1 + Z_2 + \cdots + Z_n$ has a gamma distribution $\text{Gam}(n, \beta)$, and determine the parameters of this gamma distribution.
(Hint: See Theorem\ \@ref(thm:MGFIndependent).)
:::

  
  
:::{.exercise #DailyTempBivar}
@climate:wilks:statmethods (p.\ 101) states that the maximum daily temperatures measured at Ithaca ($I$) and Canandaigua ($C$) in January 1987 are both symmetrical. 
He also says that the two temperatures could be modelled with a bivariate normal
distribution with $\mu_I = 29.87$, $\mu_C = 31.77$, $\sigma_I = 7.71$, $\sigma_C = 7.86$ and  $\rho_{IC} = 0.957$.
(All measurements are in degrees Fahrenheit.)

1. Explain, in context, what a correlation coefficient of\ $0.957$ means.
1. Determine the marginal distributions of\ $C$ and of\ $I$.
1. Find the conditional distribution of $C\mid I$.
1. Plot the pdf of Canandaigua maximum temperature.
1. Plot the conditional pdf of Canandaigua maximum temperature given that the maximum temperature at Ithaca is\ $25^\circ$F.
1. Comment on the differences between the two pdfs plotted above.
1. Find $\Pr(C < 32 \mid I = 25)$.
1. Find $\Pr(C < 32)$.
1. Comment on the differences between the last two answers.
1. If temperature were measured in degrees Celsius instead of degrees Fahrenheit, how would the value of\ $\rho_{IC}$ change?
:::




:::{.exercise #DiscreteJointTable}
The discrete random variables\ $X$ and\ $Y$ have the joint probability distribution shown in the following table:

Value of $x$ | $y = 1$ | $y = 2$ | $y = 3$
--------|------|------|------
$x = 0$ | 0.20 | 0.15 | 0.05 
$x = 1$ | 0.20 | 0.25 | 0.00 
$x = 3$ | 0.10 | 0.05 | 0.00


1. Determine the marginal distribution of\ $X$.
1. Calculate $\Pr(X \ne Y)$.
1. Calculate $\Pr(X + Y = 2 \mid X = Y)$.
1. Are\ $X$ and\ $Y$ independent? 
   Justify your answer.
1. Calculate the correlation of\ $X$ and\ $Y$; i.e., compute $\text{Cor}(X,Y)$.
:::


:::{.exercise}
Suppose\ $X$ and\ $Y$ have the joint pdf
$$
   f_{X, Y}(x, y) = \frac{2 + x + y}{8} \quad\text{for $-1 < x < 1$ and $-1 < y < 1$}.
$$

1. Sketch the distribution using **R**.
1. Determine the marginal pdf of\ $X$.
1. Are\ $X$ and\ $Y$ independent?
   Give reasons.
1. Determine $\Pr(X > 0, Y > 0)$.
1. Determine $\Pr(X \ge 0, Y \ge 0, X + Y \le 1)$.
1. Determine $\operatorname{E}[XY]$.
1. Determine $\operatorname{var}[Y]$.
1. Determine $\operatorname{Cov}(X, Y)$.
:::




:::{.exercise}
Historically, final marks in a certain course are approximately normally distributed with mean of\ $64$ and standard deviation\ $8$.
Out of fifteen students completing the course, what is the probability that $2$\ obtain HDs, $3$\ Distinctions, $4$\ Credits, $5$\ Passes and $2$\ Fails?
:::


:::{.exercise #IIDcubic}
Let $X_1, X_2, X_3, \dots, X_n$ denote are independently and identically distributed with pdf
$$
   f_X(x) = 4x^3\quad \text{for $0 < x < 1$}.
$$

1. Write down an expression for the joint pdf of distribution of $X_1, X_2, X_3, \dots, X_n$.
1. Determine the probability that the first observation\ $X_1$ is less than\ $0.5$.
1. Determine the probability that *all* observations are less than\ $0.5$.
1. Use the result above to deduce then the probability that the largest observation is less than\ $0.5$.
:::


:::{.exercise}
Let\ $X$ and\ $Y$ have a bivariate normal distribution with $\operatorname{E}[X] = 5$, $\operatorname{E}[Y] = -2$, $\operatorname{var}[X] = 4$, $\operatorname{var}[Y] = 9$, and $\operatorname{Cov}(X, Y) = -3$. 
Determine the joint distribution of $U = 3X + 4Y$ and $V = 5X - 6Y$.
:::


:::{.exercise #TwoFairDie}
Two fair dice are rolled.
Let\ $X$ and\ $Y$ denote, respectively, the maximum and minimum of the numbers of spots showing on the two dice.

1. Construct a table that enumerates the sample space.
1. Determine $E(Y\mid X = 4)$ for $1\le x\le 6$.
1. Simulate this experiment using **R**.
   Compare the simulated results with the theoretical results found above.
:::



:::{.exercise}
Let\ $X$ be a random variable for which $\operatorname{E}[X] = \mu$ and $\operatorname{var}[X] = \sigma^2$, and let\ $c$ be an arbitrary constant.

1. Show that
$$
      \operatorname{E}[(X - c)^2] = (\mu - c)^2 + \sigma^2.
$$
1. What does the result above tell us about the possible size of $\operatorname{E}[(X - c)^2]$?
:::



:::{.exercise #DailyRainfallSim}
In Sect.\ \@ref(SimulationBivariate), simulation was used for monthly rainfall.
*Daily* rainfall, however, is more difficult to model as some days have exactly zero rainfall, and on some days a continuous amount of rainfalls; i.e., daily monthly is a *mixed* random variable (Sect.\ \@ref(RVsMixed)).

One way to model daily rainfall is to use a two-step process [@climate:chandler:2002].
Firstly, model whether a day records rainfall or not (using a binomial distribution): the *occurrence model*.
Then, for days on which rain falls, model the amount of rainfall using a gamma distribution: the *amounts model*.

1. Use this information to model daily rainfall over one year ($365$\ days), for a location where the probability of a wet day is\ $0.32$, and the amount of rainfall on wet days follows a gamma distribution with $\alpha = 2$ and $\beta = 20$.
   Produce a histogram of the distribution of *annual* rainfall after $1\,000$\ simulations.
1. Suppose that the probability of rainfall,\ $p$, depends on the day of the year, such that:
$$
   p =  \left\{1 + \cos[ 2\pi\times(\text{day of year})/365]\right\} / 2.2.
$$
   Plot the change in\ $p$ over the day of the year.
1. Revise the first model using this value of\ $p$.
   Produce a histogram of the distribution of *annual* rainfall after $1\,000$\ simulations.
1. Revise the *initial* model (where the probability of rain of Day\ 1 is still\ $0.32$), so that the value of\ $p$ depends on what happened the day before:
   If day\ $i$ receives rain, then the probability that the following day receives rain is $p = 0.55$; if day\ $i$ does *not* receive rain, then the probability that the following day receives rain is just $p = 0.15$.
   Produce a histogram of the distribution of *annual* rainfall after $1\,000$\ simulations.
:::



:::{.exercise #JointTriangular}
Consider the joint pdf
$$
   f_{X, Y}(x, y) = 
   \begin{cases}
      c x(y + 1)  & \text{where $x + y < 2$ with $x > 0$ and $y > 0$};\\
      0           & \text{elsewhere}.
   \end{cases}
$$

1. Draw the region over which the joint pdf is defined.
1. Compute the value of\ $c$.
1. Compute $P(Y < 1 \mid X > 1)$.
1. Compute $P(Y < 1 \mid X > 0.25)$.
1. Compute $\Pr(Y < 1)$
:::


:::{.exercise #JointQuad}
Consider the joint pdf
$$
   f_{X, Y}(x, y) = 
   \begin{cases}
      k ( 1 - x) y & \text{for the region $R$ below};\\
      0            & \text{elsewhere},
   \end{cases}
$$
where the region\ $R$ is shown in Fig.\ \@ref(fig:RegionRA) (left panel).

1. Determine the value of\ $k$.
1. Compute $\Pr(X > Y)$.
1. Compute $\Pr(X > 0.5)$.
:::


:::{.exercise #JointLinear}
Consider the joint pdf
$$
   f_{X, Y}(x, y) = 
   \begin{cases}
      k ( x + 2y) y & \text{for the region $A$ below};\\
      0             & \text{elsewhere},
   \end{cases}
$$
where the region\ $A$ is shown in Fig.\ \@ref(fig:RegionRA) (right panel).

1. Determine the value of\ $k$.
1. Compute $\Pr(X > Y)$.
1. Compute $\Pr(X > 0.5)$.


```{r RegionRA, echo=FALSE, fig.align="center", fig.cap = "The region $R$ (left) and the region $A$ (right).", out.width = '100%', fig.width=9}
par(mfrow = c(1, 2))

###

x <- seq(0, 1, 
         length = 500)
y <- x^2

plot(x = x,
     y = y,
     xlab = expression(italic(x)),
     ylab = expression(italic(y)),
     type = "l",
     las = 1,
     lwd = 3)
polygon( x = c(x, rev(x)),
         y = c(y, rep(1, length(y))) ,
         col = "grey")
text(0.4, 0.6, 
     expression(italic(R)),
     cex = 2)
text(0.8, 0.3, 
     expression( italic(y) == italic(x)^2),
     pos = 4,
     cex = 1.2)
arrows(x0 = 0.74, 
       x1 = 0.6,
       y0 = 0.30,
       y1 = 0.36,
       angle = 15,
       length = 0.2)



###

x <- seq(0, 1, 
         length = 500)
y <- 1 - x 

plot(x = x,
     y = y,
     xlab = expression(italic(x)),
     ylab = expression(italic(y)),
     type = "l",
     las = 1,
     lwd = 3)
polygon( x = c(0, 0, 1),
         y = c(0, 1, 0),
         col = "grey")
text(0.25, 0.35, 
     expression(italic(A)),
     cex = 2)
text(0.8, 0.6, 
     expression( italic(y) == 1-italic(x)),
     pos = 3,
     cex = 1.2)
arrows(x0 = 0.8, 
       x1 = 0.6,
       y0 = 0.6,
       y1 = 0.4,
       angle = 15,
       length = 0.2)


```
:::


:::{.exercise #Mixture}
A company produces a $500\gs$ packet of 'trail mix' that includes a certain weight of nuts\ $X$, dried fruit\ $Y$, and seeds\ $Z$.
The actually weights of each ingredient vary randomly, depending on seasonality and availability.

1. Explain why there are really just two variables in this problem.
2. Suppose the weight of nuts *plus* dried fruit must be between\ $300\gs$ and $400\gs$.
   Draw the sample space.
3. *In addition*, suppose the weight of nuts must be *at least*\ $100\gs$, and the weight of dried fruit must be *at least* $100\gs$.
   Draw the sample space.
4. Under the above conditions, assume the weights of ingredients used in the trail mix are uniformly distributed.
   Determine the probability function.
:::


:::{.exercise #RandomNumbers}
Suppose you choose a number at random from the continuous uniform distribution between\ $0$ and\ $1$. 
Then, you keep choosing numbers from this $\text{Unif}(0, 1)$ distribution until you obtain a number larger than the one you started with. 
You record the number of times you have to select a number before you have success (i.e., a larger number that teh initially-selected number). 

More precisely:

1. Let $X\sim\text{Unif}(0, 1)$, your initial number.
2. You then draw $Y_1, Y_2, \dots\sim\text{Unif}(0, 1)$, i.i.d., until you get a value $Y_n > X$.
3. Your score is $N$, the number of trials until this happens.

What is the expected value of\ $N$, the number of values to draw to exceed the initially-selected number?
:::



:::{.exercise #MixedBiomarker}
Suppose\ $X$ is the concentration of a biomarker (in mmol.L^$-1$^), and\ $Y$ is a disease indicator where $Y = 1$ refers to a patient with the disease and $Y = 0$ refers to a patient without the disease.  

The prevalence of the disease in the population is
$$
  \Pr(Y = 1) = 0.10, \quad \Pr(Y = 0) = 0.90.
$$
Conditional on disease status, biomarker concentrations follow different ??normal?? distributions.
If $Y = 1$:  
$$
  X \mid Y = 1 \sim \mathcal{N}(\mu_1=8, \, \sigma_1^2 = 1^2),
$$
and if $Y = 0$:
$$
  X \mid Y = 0 \sim \mathcal{N}(\mu_0=5, \, \sigma_0^2 = 1^2).
$$
1. Write down the joint density--mass function $f_{X, Y}(x, y)$.  
2. Derive the marginal density of $X$, $f_X(x)$.  
3. Compute $\Pr(Y = 1 \mid X = 7)$ (the posterior probability of disease given biomarker level $7$).  
4. Interpret this probability in plain language.
:::


:::{.exercise #MixedReliability}
A machine component can fail in one of three ways:
- $Y = 1$: failure is due to a mechanical failure, with probability\ $0.5$;
- $Y = 2$: failure is due to a electrical failure, with probability\ $0.3$;  
- $Y = 3$: failure is due to a thermal failure, with probability\ $0.2$.

The random variable\ $X$ denotes the *time to failure* (in hundreds of hours).
The time-to-failure has the distribution
$$
  f_{X\mid Y}(x\mid Y = y) 
  = \frac{1}{\lambda_y}\exp(-\lambda_y x)
  \quad\text{for $x > 0$}
$$
where the value of\ $\lambda_y$ is conditional on the failure mode:
- if failure is due to a mechanical failure, $\lambda_y = 1$;
- if failure is due to a electrical failure, $\lambda_y = 0.5$;  
- if failure is due to a thermal failure, $\lambda_y = 0.25$.

1. Write down the joint density--mass function $f_{X, Y}(x, y)$.  
2. Find the marginal density of the time to failure $f_X(x)$.  
3. Compute $\Pr(Y = 1 \mid X \leq 2)$, the probability that the failure mode was mechanical given that the component failed within $200$ hours.  
4. Interpret the result in words.
:::

