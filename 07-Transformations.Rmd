# Transformations of random variables {#Transformations}

::: {.objectivesBox .objectives data-latex="{iconmonstr-target-4-240.png}"}
On completion of this module, you should be able to:

* derive the distribution of a transformed variable, given the distribution of the original variable using either
   * the distribution function method;
   * the change of variable method; or
   * the moment generating function method as appropriate.
* find the joint distribution of two transformed variables in a bivariate situation.
:::


## Introduction

The type of problem considered in this chapter is as follows.
Given a random variable $X$ with known distribution, and a function $u$, how do we find the probability distribution of a random variable $Y = u(X)$?

Among several available techniques, three are considered here:

1. the distribution function method;
2. the change of variable method; and
3. the moment generating function method

An important concept in this context is a *1:1 transformation*.

:::{.definition #OneOneTransformation name="One-to-one transformation"}
Given random variable's $X$ and $Y$ with range spaces $R_X,\,R_Y$ respectively, the function $u$ is said to represent a 1:1 transformation (or mapping) if to each $x\in R_X$ there corresponds exactly one $y\in R_Y$.
:::


When $Y = u(X)$ is a 1:1 transformation, the inverse function is uniquely defined; that is, $X$ can be written uniquely in terms of $Y$. 
This feature is important when considering the distribution of $Y$ when the distribution of $X$ is known.


## The change of variable method {#ChangeOfVariable}

The discrete and continuous cases are considered separately. 
The method is relatively straightforward for one-to-one transformations; that is, functions that are only decreasing functions (such as $Y = 1 - X$), or only increasing functions (such as $Y = \exp(X)$.)
Considerable care needs to be exercised if the transformation is not 1--1. 
Examples are given below.


### Discrete random variables

#### Univariate case

Let $X$ be a discrete random variable with pf $p_X(x)$. 
Let $R_X$ denote the set of discrete points at each of which $p_X(x) > 0$.
Let $y = u(x)$ define a *one-to-one transformation* that maps $R_X$ onto $R_Y$, the set of discrete points at each of which the transformed variable $Y$ has a non-zero probability. 
If we solve $y = u(x)$ for $x$ in terms of $y$, say $x  =  w(y)$, then for each $y \in R_Y$, we have $x = w(y)\in R_X$.


:::{.example #Transform1 name="Transformation (1:1)"}
Given $\Pr(X = x) = x/15 = p_X(x)$, $x\in R_X = \{1, 2, 3, 4, 5\}$, find the probability function of $Y$ where $Y = 2X + 1$.

Note that $R_Y = \{3, 5, 7, 9, 11\}$ and the mapping $y = 2x + 1 = u(x)$ is 1:1. 
We wish to find a 'formula' for $\Pr(Y = y)$, $y\in R_Y$.
Now
\[ 
   \Pr(Y = y)
   = \Pr(2X + 1 = y)
   = \Pr(X = \frac{y - 1}{2})
   = \frac{y - 1}{30}. 
\]
So the probability function of $Y$ is
\[ 
   \Pr(Y = y)
   = \frac{y - 1}{30},\quad y=3, 5, 7, 9, 11. 
\]
(Note that the probabilities given by this 'formula' add to 1.)
:::

The above procedure for $Y = u(X)$ a 1:1 mapping can be stated *more generally* as follows.
\[
   \Pr(Y = y) 
   = \Pr(u(X) = y)
   = \Pr(X = u^{-1} (y))
   = p_X(u^{-1}(y)), \quad y\in R_Y
\]


:::{.example #Transform2 name="Transformation (1:1)"}
Let $X$ have a binomial distribution with pf
\[
   p_X(x) = \begin{cases}
               \binom{3}{x}(0.2)^x (0.8)^{3-x} & \text{for $x = 0, 1, 2, 3$}\\
               0 & \text{otherwise}
            \end{cases}
\]
The pf of $Y = X^2$ can be found using the change of variable methods as follows.

First note that $Y = X^2$ is *not* a one-to-one transformation in general, but it is here since $X$ has non-zero probability only for $x = 0, 1, 2, 3$.

The transformation $y = u(x) = x^2$, $R_X = \{ x \mid x = 0, 1, 2, 3 \}$ maps onto $R_Y = \{y \mid y = 0, 1, 4, 9\}$.
The inverse function becomes $x = w(y) = \sqrt{y}$, and hence the pf of $Y$ becomes
\[
   p_Y(y) = p_X(\sqrt{y})
   = \begin{cases}
               \binom{3}{\sqrt{y}}(0.2)^{\sqrt{y}} (0.8)^{3 - \sqrt{y}} & \text{for $y = 0, 1, 4, 9$}\\
               0 & \text{otherwise.}
     \end{cases}
\]
:::

We now consider the case where the function $u$ is *not* 1:1.

:::{.example #TransformNot11 name="Transformation not 1:1"}
Suppose $\Pr(X = x)$ is as in Example \@ref(exm:Transform1) and define $Y = |X - 3|$.
Then since $R_Y = \{0, 1, 2\}$ the mapping is clearly not 1:1. 
In fact, the event $Y = 0$ occurs if $X = 3$, the event $Y = 1$ occurs if $X = 2$ or 4, and the event $Y = 2$ occurs if $X = 1$ or 5.

To find the probability distribution of $Y$ means finding the probabilities associated with the values $0, 1, 2$ in $R_Y$, and we have
\begin{align*}
   \Pr(Y = 0) 
   &= \Pr(X = 3) = 3/15\\
   \Pr(Y = 1) 
   &= \Pr(X = 2 \text{ or } 4) = \frac{2}{15} + \frac{4}{15}\\
   \Pr(Y = 2) 
   &= \Pr(X = 1 \text{ or } 5) = \frac{1}{15} + \frac{5}{15}
\end{align*}
So the probability function of $Y$ can be expressed as table:

| $y$           | 0  |  1  |  2
|--------------:|:--:|:---:|:--:|
| $\Pr(Y = y)$  | $\displaystyle \frac{1}{5}$ | $\displaystyle \frac{2}{5}$ | $\displaystyle \frac{2}{5}$

:::

#### Bivariate case

The bivariate case is similar to the univariate case.
Here we have a joint pf $p_{X_1, X_2}(x_1, x_2)$ of two discrete random variables $X_1$ and $X_2$ defined on the two-dimensional set of points $R^2_X$ for which $p(x_1, x_2)>0$.
There are now two *one-to-one transformations*:
\begin{align*}
   y_1 &= u_1( x_1, x_2)\\
   y_2 &= u_2( x_1, x_2)
\end{align*}
that map $R^2_X$ onto $R^2_Y$ (the two-dimensional set of points for which $p(y_1, y_2) > 0$).

The two inverse functions are
\begin{align*}
   x_1 &= w_1( y_1, y_2)\\
   x_2 &= w_2( y_1, y_2).
\end{align*}
Then the joint pf of the new (transformed) random variables
is
\[
   p_{Y_1, Y_2}(y_1, y_2) =
   \begin{cases}
      p_{X_1, X_2}\bigl( w_1(y_1, y_2), w_2(y_1, y_2)\bigr) & \text{where $(y_1, y_2)\in R^2_Y$}\\
      0 & \text{elsewhere}
   \end{cases}
\]

:::{.example #TransformBivariate name="Transformation (bivariate)"}
Let the two discrete random variables $X_1$ and $X_2$ have the joint pf shown in Table \@ref(tab:Bivar4).



```{r Bivar4, echo=FALSE}
Bivar4Table <- array( dim = c(2, 4))

Bivar4Table[1, ] <- c("$x_1 = -1$", "$0.3$", "$0.1$", "$0.1$")
Bivar4Table[2, ] <- c("$x_1 = +1$", "$0.2$", "$0.2$", "$0.1$")


Bivar4Table.caption <- "A bivariate probability function"
if( knitr::is_latex_output() ) {
  knitr::kable(Bivar4Table,
               format = "latex",
               booktabs = TRUE,
               col.names = c("",
                             "$x_2 = 0$",
                             "$x_2 = 1$",
                             "$x_2 = 2$"),
               align = "r",
               longtable = FALSE,                
               escape = FALSE,
               caption = Bivar4Table.caption) %>%
    kable_styling(font_size = 10) %>%
    row_spec(0, bold = TRUE) 
}
if( knitr::is_html_output() ) {
  knitr::kable(Bivar4Table,
               escape = TRUE,
               format = "html",
               booktabs = TRUE,
               align = "r",
               longtable = FALSE,                
               col.names = c("",
                             "$x_2 = 0$",
                             "$x_2 = 1$",
                             "$x_2 = 2$"),
               caption = Bivar4Table.caption) %>%
    kable_styling(font_size = 10) %>%
    row_spec(0, bold = TRUE) 
}
```



Consider the two one-to-one transformations
\begin{align*}
   Y_1 &= X_1 + X_2 \\
   Y_2 &= 2 X_1.
\end{align*}
The joint pf of $Y_1$ and $Y_2$ can be found by noting where the $(x_1, x_2)$ pairs are mapped to in the $y_1, y_2$ space:

| $(x_1,x_2)$ | $\mapsto$ | $(y_1,y_2)$
|------------:|:---------:|:----------------
| $(-1, 0)$   | $\mapsto$ | $(-1, -2)$ 
| $(-1, 1)$   | $\mapsto$ | $(0, -2)$
| $(-1, 2)$   | $\mapsto$ | $(1, -2)$
| $(1, 0)$    | $\mapsto$ | $(1, 2)$ 
| $(1, 1)$    | $\mapsto$ | $(2, 2)$ 
| $(1, 2)$    | $\mapsto$ | $(3, 2)$ 


The joint pf can then be constructed as shown in Table \@ref(tab:Bivar5).


```{r Bivar5, echo=FALSE}
Bivar5Table <- array( dim = c(2, 6))

Bivar5Table[1, ] <- c("$y_2 = -2$", "$0.3$", "$0.1$", "$0.1$", "$0.0$", "$0.0$")
Bivar5Table[2, ] <- c("$y_2 = +2$", "$0.0$", "$0.0$", "$0.2$", "$0.2$", "$0.1$")


Bivar5Table.caption <- "The joint probability function for $Y_1$ and $Y_2$"
if( knitr::is_latex_output() ) {
  knitr::kable(Bivar5Table,
               format = "latex",
               booktabs = TRUE,
               col.names = c("",
                             "$y_1 = -1$",
                             "$y_2 = 0$",
                             "$y_3 = 1$",
                             "$y_4 = 2$",
                             "$y_5 = 3$"),
               align = "r",
               longtable = FALSE,                
               escape = FALSE,
               caption = Bivar5Table.caption) %>%
    kable_styling(font_size = 10) %>%
    row_spec(0, bold = TRUE) 
}
if( knitr::is_html_output() ) {
  knitr::kable(Bivar5Table,
               escape = TRUE,
               format = "html",
               booktabs = TRUE,
               align = "r",
               longtable = FALSE,                
               col.names = c("",
                             "$y_1 = -1$",
                             "$y_2 = 0$",
                             "$y_3 = 1$",
                             "$y_4 = 2$",
                             "$y_5 = 3$"),
               caption = Bivar5Table.caption) %>%
    kable_styling(font_size = 10) %>%
    row_spec(0, bold = TRUE) 
}
```
:::


Sometimes, a joint pf of two random variables is given, but only one new random variable is required.
In this case, a second (dummy) transformation is developed, usually one that is simple.
See the following example.

:::{.example #TransformBivariate2 name="Trandformation (bivariate)"}
Let $X_1$ and $X_2$ be two independent random variables with the joint pf
\[
   p_{X_1, X_2}(x_1, x_2) =
         \frac{\mu_1^{x_1} \mu_x^{x_2} \exp( -\mu_1 - \mu_2 )}{x_1! x_2!}
         \quad\text{for $x_1$ and $x_2 = 0, 1, 2, \dots$}
\]
(This is the joint pf of two independent Poisson random variables.)
Suppose we wish to find the pf of $Y_1 = X_1 + X_2$.

We can consider the two *one-to-one transformations*:
\begin{align}
   y_1 &= x_1 + x_2 = u_1(x_1, x_2) \label{EQN:transforms:bvt1}\\
   y_2 &= x_2 = u_2(x_1, x_2) \label{EQN:transforms:bvt2}
\end{align}
which maps the points in $R^2_X$ onto
\[
   R^2_Y = \left\{ (y_1, y_2)\mid y_1 = 0, 1, 2, \dots; y_2 = 0, 1, 2, \dots, y_1\right\}.
\]
Notice that $Y_2$ is a dummy transform, and it is very simple.
We could have chosen any second transform (as it is of no interest), and so chose one that is simple.

The inverse functions are given by
\begin{align*}
   x_1 &= y_1 - y_2 = w_1(y_1, y_2)\\
   x_2 &= y_2 = w_2(y_2)
\end{align*}
by rearranging Equations (\ref{EQN:transforms:bvt1}) and (\ref{EQN:transforms:bvt2}).
Then the joint pf of $Y_1$ and $Y_2$ is
\begin{align*}
   p_{Y_1, Y_2}(y_1, y_2)
   &= p_{X_1, X_2}(x_1, x_2)\bigl( w_1(y_1, y_2), w_2(y_1, y_2)\bigr) \\
   &= \frac{\mu_1^{y_1 - y_2}\mu_2^{y_2} \exp(-\mu_1 - \mu_2)}{(y_1-y_2)! y_2!}\quad
   \text{for $(y_1, y_2)\in R^2_Y$}
\end{align*}

This does not end the question, since we originally sought the pf of just $Y_1$; so we need to find the marginal pf of $p_{Y_1, Y_2}(y_1, y_2)$.

Now the marginal pf of $Y_1$ is given by
\[
   p_{Y_1}(y_1) = \sum_{y_2=0}^{y_1} p_{Y_1, Y_2}(y_1, y_2)
   = \sum_{y_2=0}^{y_1} \frac{\mu_1^{y_1 - y_2}\mu_2^{y_2} \exp(-\mu_1 - \mu_2)}{(y_1-y_2)! y_2!},
\]
which can be shown to be
\[
   p_{Y_1}(y_1) =
   \begin{cases}
      \displaystyle{\frac{(\mu_1+\mu_2)^{y_1}\exp\bigl(-(\mu_1 + \mu_2)\bigr)}{y_1!}} & \text{for $y_1 = 0, 1, 2, \dots$}\\
      0 & \text{otherwise.}
   \end{cases}
\]
(Showing this is difficult and the details are not given.)
This is the pf of a Poisson random variable with mean $\mu_1 + \mu_2$.
Thus $Y_1 \sim \text{Pois}(\lambda = \mu_1 + \mu_2)$.
:::


### Continuous random variables

#### Univariate case

:::{.theorem #ChangeOfVarCont name="Change of variable (continuous rv)"}
If $X$ has pdf $f_X(x), \ x\in R_X$ and $u$ is a strictly increasing or decreasing function for $x\in R_X$ then the random variable $Y = u(X)$ has pdf
\[
   f_Y(y) = f_X(x) \left|\frac{dx}{dy}\right|
\]
where the RHS is expressed as a function of $y$.
:::

:::{.proof}
Let the inverse function be $X = w(Y)$, that is $w(y) = u^{-1}(x)$.

**Case 1:** $y = u(x)$ is a strictly *increasing* function (for example, see Fig. \@ref(fig:Transformation), left panel).

```{r Transformation, echo=FALSE, fig.align="center", fig.cap="A monotone increasing transformation function", fig.width=7, fig.height=4, out.width='80%'}
par( mfrow = c(1, 2))

logit <- function(x) { 
  log(x / (1 - x)) 
  }

y <- seq(0.01, 1,
         length = 100)
x <- logit(y)

y0 <- 0.4
y1 <- 0.8

x0 <- logit( y0 )
x1 <- logit( y1 )

plot( x = x,
      y = y,
      lwd = 2,
      axes = FALSE,
      type = "l",
      col = plotColour1,
      main = "Increasing function",
      xlab = expression(italic(x)),
      ylab = expression(italic(y)==italic(u)(italic(x))))
axis(side = 2,
     las = 1,
     at = c(y0, y1),
     labels = c(expression(italic(a)), 
                expression(italic(b))) ) 
axis(side = 1,
     at = c(x0, x1 ),
     labels = c(expression(italic(w)*paste("(",italic(a),")")),
                expression(italic(w)*paste("(",italic(b),")")) ) )
box()

lines( x = c( min(x), x0, x0),
       y = c(y0, y0, 0),
       lty = 2)
lines( x = c( min(x), x1, x1),
       y = c(y1, y1, 0),
       lty = 2)

text(x = 3.6,
     y = 0.85,
     label = expression(italic(y)==italic(u)(italic(x))))



logit2 <- function(x) { 
  -log(x / (1 - x)) 
  }

y <- seq(0.01, 0.99,
         length = 100)
x <- logit2(y)

y0 <- 0.4
y1 <- 0.8

x0 <- logit2( y0 )
x1 <- logit2( y1 )

plot( x = x,
      y = y,
      lwd = 2,
      col = plotColour1,
      axes = FALSE,
      type = "l",
      main = "Decreasing function",
      xlab = expression(italic(x)),
      ylab = expression(italic(y)==italic(u)(italic(x))))
axis(side = 2,
     las = 1,
     at = c(y0, y1),
     labels = c(expression(italic(a)), 
                expression(italic(b))) ) 
axis(side = 1,
     at = c(x0, x1),
     labels = c(expression(italic(w)*paste("(",italic(a),")")),
                expression(italic(w)*paste("(",italic(b),")")) ) )
box()

lines( x = c( min(x), x0, x0),
       y = c(y0, y0, 0),
       lty = 2)
lines( x = c( min(x), x1, x1),
       y = c(y1, y1, 0),
       lty = 2)

text(x = 2.5,
     y = 0.7,
     label = expression(italic(y)==italic(u)(italic(x))))
```

If $a < y < b$ then $w(a) < x < w(b)$ and  $\Pr(a < Y < b) = \Pr(w(a) < X <w(b))$, so that,
\[
   {\int^b_a f_Y(y)\,dy
   =\int^{w(b)}_{w(a)}f_X(x)\,dx
   =\int^b_af\{ w(y)\}\frac{dx}{dy}\,\,dy}
\]
Therefore, $\displaystyle {f_Y(y) = f_X\left( w(y) \right)\frac{dx}{dy}}$, where $w(y) = u^{-1}(x)$.

**Case 2:** $y = u(x)$ is a strictly *decreasing* function of $x$ (for example, see Fig. \@ref(fig:Transformation), right panel).

If $a < y < b$ then $w(b) < x < w(a)$ and $\Pr(a < Y < b) = \Pr(w(b) < X < w(a))$, so that,
\begin{align*}
     \int^b_a f_Y(y)\,dy & = \int^{w(a)}_{w(b)}f_X(x)\,dx\\
     & = \int^a_bf_X(x)\frac{dx}{dy}\,\,dy\\
     & = - \int ^b_a f_X(x)\frac{dx}{dy}\,dy.
\end{align*}
Therefore $f_Y(y) = -f_X\left( w(y) \right)\displaystyle{\frac{dx}{dy}}$.
But $dx/dy$ is negative in the case of a decreasing function, so that in general,
\[ 
   f_Y(y) = f_X(x)\left|\frac{dx}{dy} \right|. 
\]
:::

The absolute  value  of  $w'(y) = dx/dy$  is  called  the *Jacobian  of  the transformation*.


:::{.example #Transform3 name="Transformation"}
Let the pdf of $X$ be given by
\[
   f_X(x) = 1 \quad\text{for $0 < x < 1$}.
\]
Consider the transformation $Y = -2\log X$.
(Here, $\log$ refers to a logarithm to base $e$, or a *natural logarithm*.)

First note that the transformation is one-to-one.
The inverse transformation is
\[
   X = \exp( -Y/2) = w(Y).
\]
The space $R_X = \{x \mid 0 < x < 1\}$ is mapped to $R_y = \{y \mid 0 < y < \infty\}$.
Then,
\[
   w'(y) = \frac{d}{dy} \exp(-y/2) = -\frac{1}{2}\exp(-y/2),
\]
and so the *Jacobian* of the transformation $|w'(y)| = \exp(-y/2)/2$.
Now the pdf of $Y = -2\log X$ becomes
\begin{align*}
   f_Y(y)
   &= f_X\{w(y)\} |w'(y)| \\
   &= f_X\{\exp(-y/2)\} \exp(-y/2)/2 \\
   &= \frac{1}{2}\exp(-y/2)\quad\text{for $y > 0$}.
\end{align*}
Note that $Y\sim\text{Exp}(2)$.
:::



:::{.example #TransformSquRt name="Square root transformation"}
Given $X$ has pdf $f_X(x) = e^{-x}$, $x \geq 0$, find the pdf of $Y = \sqrt{X}$.
For $x \geq 0$, $y = \sqrt{x}$ is a strictly increasing function (Fig. \@ref(fig:SqrtRt)).

```{r SqrtRt, echo=FALSE, fig.align="center", fig.cap="The square-root transformation", fig.width=4, fig.height=3.5}
x <- seq(0, 20,
         length = 100)
y <- sqrt(x)

plot(x = x,
     y = y,
     lwd = 2,
     col = plotColour1,
     las = 1,
     main = expression(paste("The transformation ", italic(y) == sqrt(italic(x)))),
     xlab = expression(italic(x)),
     ylab = expression( italic(y) == sqrt(italic(x))),
     type = "l")

```

The inverse relation is $x = y^2$; $dx/dy = 2y$.
So the pdf of $Y$ is
\begin{align*}
    f_Y(y)
    &= f_X(x)\left|\frac{dx}{dy}\right|\\
    &= 2y\,e^{-y^2},\quad y\in [0,\infty).
\end{align*}
:::


::: {.example #TransformationTan name="Tan transformation"}
Let random variable $X$ be uniformly distributed on $\displaystyle{\left[-\frac\pi 2,\frac \pi 2 \right]}$. 
Find the distribution of $Y = \tan X$ (Fig. \@ref(fig:TanXform)).


```{r TanXform, echo=FALSE, fig.align="center", fig.cap="The tan transformation", fig.width=4,fig.height=3.5}
x <- seq(-pi/2 + 0.1, 
         pi/2 - 0.1,
         length = 100)
y <- tan(x)

plot(x = x,
     y = y,
     col = plotColour1,
     lwd = 2,
     las = 1,
     xlim = c(-pi/2, pi/2),
     axes = FALSE,
     main = expression(paste("The transformation ", italic(y) == plain(tan)(italic(x)))),
     xlab = expression(italic(x)),
     ylab = expression( italic(y) == plain(tan)(italic(x))),
     type = "l")
axis(side = 2,
     las = 1)
axis(side = 1,
     at = c(-pi/2, 0, pi/2),
     labels = c(expression(-pi/2),
                "0",
                expression(pi/2)) )
abline(h = 0, 
       lty = 1,
       col = "grey")
abline(v = 0, 
       lty = 1,
       col = "grey")
box()

```
To find $R_Y$ consider the mapping $y = \tan x$.
Then $R_Y = \{ y\mid -\infty <y<\infty\}$ and the mapping is 1:1, which means $x = \tan^{-1}y$, and $dx/dy = 1/(1 + y^2)$.
Hence
\[
   f_Y(y)
   = f_X(x)\left|\frac{dx}{dy}\right|
   = \frac 1\pi\frac{1}{1 + y^2}
\]
This is known as the *Cauchy distribution*.
:::


A case where the function $u$ is not 1:1 is considered by an example. 
A modification of Theorem \@ref(thm:ChangeOfVarCont) is used.


:::{.example #TransformationNon11 name="Transformation (not 1:1)"}
Given a random variable $Z$ which is distributed $N(0, 1)$, find the probability distribution of $Y = \frac{1}{2}Z^2$.


```{r Non11Xform, echo=FALSE, fig.align="center", fig.cap="A transformation that is not 1:1", fig.height=3.5, fig.width=4}
x <- seq(-5, 5,
         length = 100)
square <- function(x) { x^2 }

y <- square(x)


x1 <- -4
x2 <- -2
x3 <- 2
x4 <- 4

y1 <- square(x1)
y2 <- square(x2)
y3 <- square(x3)
y4 <- square(x4)

plot(x = x,
     y = y,
     lwd = 2,
     col = plotColour1,
     las = 1,
     axes = FALSE,
     main = "A non-1:1 transformation",
     xlab = expression(italic(x)),
     ylab = expression(italic(y) == italic(x)^2),
     type = "l")
axis(side = 1,
     at = c(x1, x2, x3, x4),
     labels = c( expression(-sqrt(2)),
                 expression(-sqrt(-2*italic(a))),
                 expression(sqrt(2*italic(a))),
                 expression(sqrt(2)) ) )
axis(side = 2,
     las = 1,
     at = c(y1, y2),
     label = c(expression(italic(b)),
               expression(italic(a))))
box()


lines( x = c(x1, x1, x4, x4),
       y = c(0, y1, y4, 0),
       lty = 2)
lines( x = c(x2, x2, x3, x3),
       y = c(0, y2, y3, 0),
       lty = 2)

abline(h = 0,
       col = "grey")
abline(v = 0,
       col = "grey")

```

The relationship $y = u(z) = \frac{1}{2}z^2$ is not strictly increasing or strictly decreasing in $(-\infty, \infty )$ so we can't apply Theorem \@ref(thm:ChangeOfVarCont) directly.
However, we can subdivide the range of $z$ and $y$ so that in each portion the relationship is monotonic. 
Now
\[
   {f_Z(z)=\frac{1}{\sqrt{2\pi}}\,e^{-\frac{1}{2} z^2},\quad-\infty <z<\infty}
\]
The inverse relation, $z = u^{-1}(y)$ is $z = \pm \sqrt{2y}$. 
For a given value of $y$ there are 2 values of $z$. 
However, in the range $-\infty < z < 0$, $y$ and $z$ are monotonically related. 
Similarly, for $0 < z <\infty$, $y$ and $z$ are monotonically related. 
Thus (see Fig. \@ref(fig:Non11Xform)),
\[
   \Pr(a < Y <b) = \Pr(-\sqrt{2b} < Z < -\sqrt{2a}) + \Pr(\sqrt{2a} < Z < \sqrt{2b}).
\]
The two terms on the right are equal because the distribution of $Z$ is symmetrical about $0$.

Thus $\Pr(a < Y < b) = 2\Pr(\sqrt{2a} < Z < \sqrt{2b})$, and
\begin{align*}
     f_Y(y)
     &= 2f_Z(z)\left| \frac{dz}{dy}\right|\\
     &= 2\frac{1}{\sqrt{2\pi}}e^{-y}\frac{1}{\sqrt{2y}};
\end{align*}
that is,
\[
   f_Y(y)
   = e^{-y}y^{-\frac 12} / \sqrt{\pi},\quad0<y<\infty.
\]
This pdf is that of a gamma distribution with parameters $\alpha = 1/2$ and $\beta = 1$.
It follows from this result that if $X$ is $N(\mu,\sigma^2)$, the pdf of $Y = \frac 12 (X - \mu )^2 / \sigma^2$ is also $\text{Gamma}(\alpha = 1/2,\beta = 1)$ since then $\frac{X - \mu}{\sigma}$ is distributed $N(0, 1)$.
:::

Note that the probability can only be doubled as in Example \@ref(exm:TransformationNon11) if both $Y = u(Z)$ and the pdf of $Z$ are symmetrical about the same point.



## The distribution function method {#DistributonFunctionMethod}

::: {.importantBox .important data-latex="{iconmonstr-warning-8-240.png}"}
This method only works for continuous random variables.
:::

There are two basic steps:

1. Find the *distribution function* of the transformed variable. 
2. Differentiate to find the *probability density function*.

The procedure is best demonstrated using an example.

:::{.example #DFMethod name="Distribution function method"}
Consider the random variable $X$ with pdf
\[
   f_X(x) = \begin{cases}
               x/4 & \text{for $1 < x < 3$}\\
               0 & \text{elsewhere.}
            \end{cases}
\]
Suppose we seek the pdf of the random variable $Y$ where $Y = X^2$.
Note that $1 < y < 9$.

The distribution function for $Y$ is
\begin{align*}
   F_Y(y)
   &= \Pr(Y\le y) \qquad\text{(by definition)}\\
   &= \Pr(X^2 \le y) \qquad\text{(since $Y = X^2$)}\\
   &= \Pr(X\le \sqrt{y} ).
\end{align*}
This last step is not trivial, but it is critical. 
(The next example shows a situation where more care is needed.) 
In this case, there is a one-to-one relationship between $X$ and $Y$ over the region of which $X$ is defined (i.e., has a positive probability); see Fig. \@ref(fig:SquaringXform).


```{r SquaringXform, echo=FALSE, fig.align="center", fig.cap="The transformation $Y = X^2$ when $X$ is defined from $1$ to $3$. The thicker line corresponds to the region where the transformation applies. Note that for $Y < y$, $2 - \\sqrt{y - 1} < X < 2 + \\sqrt{y - 1}$.", fig.width=4.5, fig.height=4}

x <- seq(-1, 4,
         length = 100)
y <- x^2

plot(x = x,
     y = y,
     type = "l",
     col = plotColour1,
     lwd = 1,
     axes = FALSE,
     xlab = expression(italic(x)),
     ylab = expression(italic(y) == italic(x)^2),
     main = "The squaring transformation",
     las = 1)

Thicker <- ( x >= 1 ) & ( x <= 3 )
lines(x = x[ Thicker],
      y = y[Thicker],
      lwd = 5)
lines( x = c( min(x), sqrt(3), sqrt(3)),
       y = c( 3, 3, 0),
       lty = 2)
axis(side = 2,
     at = c(0, 3, 6, 8, 10, 12, 14, 16),
     las = 1,
     labels = c(0, "y = 3", 6, 8, 10, 12, 14, 16))
axis(side = 1,
     at = c(-1, 0, 1, sqrt(3), 3, 4),
     labels = c(-1, 0, 1, 
                expression(italic(x) == sqrt(3) ),
                3, 4))
box()
```

Then continue as follows:
\begin{align*}
   F_Y(y)
   &=\Pr( X\le \sqrt{y}) \\
   &= F_X\bigl(\sqrt{y}\bigr) \qquad\text{(by definition of $F_X(x)$)} \\
   &= \int_1^{\sqrt{y}} (x/4) \,dx \\
   &= (y - 1)/8
\end{align*}
for $1 < y < 9$, and is zero elsewhere. 
Now recall this has found the *distribution function* of $Y$.
To find the pdf we must differentiate:
\[
   f_Y(y)
   = \frac{d}{dy} (y-1)/8
   = \begin{cases}
        1/8 & \text{for $1 < y < 9$}\\
        0 & \text{elsewhere.}
     \end{cases}
\]
Note the range for which $Y$ is defined; since $1 < x < 3$, then $1 < y < 9$.
:::



:::{.example #TransformA name="Transformation"}
Consider the same random variable $X$ as in the previous example, but the transformation $Y = (X - 2)^2 + 1$ (Fig. \@ref(fig:SquaringXform2)).




```{r SquaringXform2, echo=FALSE, fig.align="center", fig.cap="The transformation $Y = (X - 2)^2 + 1$ when $X$ is defined from $1$ to $3$. The thicker line corresponds to the region where the transformation applies. Note that for $Y < y$, $2 - \\sqrt{y - 1} < X < 2 + \\sqrt{y - 1}$.", fig.width=5, fig.height=4}

x <- seq(0, 4,
         length = 100)
xf <- function(x) { (x - 2)^2 + 1}

y <- xf(x)

plot(x = x,
     y = y,
     type = "l",
     lwd = 1,
     col = plotColour1,
     ylim = c(0, 2.25),
     axes = FALSE,
     xlab = expression(italic(x)),
     ylab = expression(italic(y)==group("(", italic(x)-2,")")^2+1),
     main = "Another transformation",
     las = 1)

x0 <- 1.25
x1 <- 2.75
y0 <- xf( x0 )
y1 <- xf( x1 )


Thicker <- ( x >= 1 ) & ( x <= 3 )
lines(x = x[ Thicker],
      y = y[Thicker],
      lwd = 5)

lines( x = c( min(x), x0, x0),
       y = c(y0, y0, 0),
       lty = 2)
lines( x = c( min(x), x1, x1),
       y = c(y1, y1, 0),
       lty = 2)
axis(side = 2,
     at = c(0, 1, y0, 2),
     las = 1,
     labels = c(0, 1, expression(italic(y)), 2))
axis(side = 1,
     at = c(x0, 2, x1),
     labels = c(expression(2-sqrt(italic(y)-1)),
                2, 
                expression(2+sqrt(italic(y)-1)) ))
box()
```

In this case, the transformation is **not** a one-to-one transform.
Proceed as before to find the distribution function of $Y$:
\begin{align*}
   F_Y(y)
   &= \Pr(Y\le y) \qquad\text{(by definition)}\\
   &= \Pr\big( (X - 2)^2 + 1  \le y\big) \qquad\text{(since $Y = (X - 2)^2 + 1$)}.
\end{align*}
At this point care is needed. 
From Fig. \@ref(fig:SquaringXform2)), whenever $(X - 2)^2 + 1 < y$ for some value $y$, then $X$ must be in the range $2 - \sqrt{y - 1}$ to $2 + \sqrt{y - 1}$. 
So:
\begin{align*}
   F_Y(y)
   &= \Pr\big( (X - 2)^2 + 1 \le y\big) \\
   &= \Pr\left( 2 - \sqrt{y - 1} < X < 2 + \sqrt{y - 1} \right)\\
   &= \int_{2-\sqrt{y - 1}}^{2 + \sqrt{y - 1}} x/4\,dx \\
   &= \frac{1}{8} x^2\Big|_{2 - \sqrt{y - 1}}^{2 + \sqrt{y - 1}} \\
   &= \frac{1}{8} \left[ \left(2 + \sqrt{y - 1}\right)^2 - \left(2 - \sqrt{y - 1}\right)^2\right] \\
   &=  \sqrt{y - 1}.
\end{align*}
Again, this is the distribution function; so
\[
   f_Y(y) = \begin{cases}
               \frac{1}{2\sqrt{y - 1}} & \text{for $1 < y < 2$}\\
               0 & \text{elsewhere.}
            \end{cases}
\]
:::



:::{.example #TransformB name="Transformation"}
Example \@ref(exm:TransformationNon11) is repeated here using the df method.
Given $Z$ is distributed $N(0, 1)$ we seek the probability distribution of $Y = \frac{1}{2} Z^2$.
\[ 
   f_Z(z) 
   = (2\pi )^{-\frac 12}\,e^{-z^2/2},\quad z\in (-\infty ,\,\infty ).
\]
Let $Y$ have pdf $f_Y(y)$ and df $F_Y(y)$. 
Then
\begin{align*}
     F_Y(y) 
     & = \Pr(Y\leq y) = \Pr\left(\frac{1}{2}Z^2\leq y\right) 
     &= \Pr(Z^2\leq 2y)\\
     & = \Pr(-\sqrt{2y}\leq Z\leq \sqrt{2y})\\
     & = F_Z(\sqrt{2y}) - F_Z(-\sqrt{2y})
\end{align*}
where $F_Z$ is the df of $Z$.
Hence
\begin{align*}
     f_Y(y) & = F_Y'(y)=F_Z'(\sqrt{2y})-F_Z'(-\sqrt{2y})\\
     & = \frac{\sqrt{2}}{2\sqrt{y}}f_Z(\sqrt{2y})-\frac{\sqrt{2}}{-
2\sqrt{y}}f_Z(-\sqrt{2y})\\[2mm]
     & = \frac{1}{\sqrt{2y}}[f_Z(\sqrt{2y}) + f_Z(-\sqrt{2y})]\\
     & = \frac{1}{2y} \left[ \frac{1}{\sqrt{2\pi}}\,e^{-y}+\frac{1}{\sqrt{2\pi}}\,e^{-y}\right]\\
     & = \frac{e^{-y}y^{-\frac{1}{2}}}{\sqrt{\pi}}
\end{align*}
as before.
:::

Obviously, care is needed to ensure the steps are followed logically. 
Drawing diagrams like Fig. \@ref(fig:SquaringXform) and \@ref(fig:SquaringXform2) are encouraged. 
Note that the functions you finish with should be pdfs; you should check that they actually are.

This method can also be used when there is more than one variable of interest. 
**See the texts for examples.**


## The moment generating function method {#TransformationMoments}

The moment generating function (mgf) of a random variable, if it exists, completely specifies the distribution of the random variable.
Thus, if two random variables have the same mgf, then they must have identical distributions.
The mgf method used here is very useful to find the distribution of a linear combination of $n$ independent random variables.
The method essentially involves the computation of the mgf of the transformed variable $Y = u(X_1, X_2, \dots, X_n)$ when the joint distribution of independent $X_1, X_2, \dots, X_n$ is given.

We construct the method below for the transformation $Y = X_1 + X_2 + \cdots X_n$, but the same principles can be applied for other linear combinations also.

Consider $n$ independent random variables $X_1, X_2, \dots, X_n$ with mgfs $M_{X_1}(t)$, $M_{X_2}(t)$, $\dots$, $M_{X_n}(t)$.
We consider the transformation $Y = X_1 + X_2 + \cdots X_n$.
Since the $X_i$ are independent, $f_{X_1,X_2\dots X_n}(x_1, x_2, \dots, x_n) = f_{X_1}(x_1).f_{X_2}(x_2)\dots f_{X_n}(x_n)$.
So, by definition of the mgf,
\begin{align*}
   M_Y(t)
   &= \text{E}(\exp(tY)) \\
   &= \text{E}(\exp[t(X_1 + X_2 + \cdots X_n)]) \\
   &= \int\!\!\!\int\!\!\!\cdots\!\!\!\int \exp[t(x_1 + x_2 + \cdots x_n)] f(x_1, x_2, \dots x_n)\,dx_n\dots dx_2 dx_1 \\
   &= \int\!\!\!\int\!\!\!\cdots\!\!\!\int \exp(tx_1) f(x_1) \exp(t{x_2}) f(x_2)\dots \exp(t{x_n})f(x_n) \,dx_n\dots dx_2 dx_1 \\
   &= \int \exp(t x_1) f(x_1)\,dx_1 \int \exp(t{x_2}) f(x_2)\,dx_2 \dots \int \exp(t{x_n})f(x_n)\,dx_n \\
   &= M_{X_1}(t) M_{X_2}(t)\dots M_{X_n}(t) \\
   &= \prod_{i=1}^n M_{X_i}(t)
\end{align*}

Hence we get the following result:
If $X_1, X_2, \dots, X_n$ are independent random variables and $Y  =  X_1 + X_2 + \dots + X_n$, then the mgf of $Y$,
$\displaystyle M_Y(t)  =  \prod_{i = 1}^n M_{X_i}(t)$ where $M_{X_i}(t)$ is the value of the mgf of $X_i$ at $t$ for $i = 1, 2, \dots, n$.

Note that $\prod_{i = 1}^n a_i = a_1 a_2\dots a_n$; $\prod$ is the symbol for a product in the same way that $\sum$ is the symbol for a summation.

The above result also holds for discrete variables; just replace the integrations by summations.


:::{.example #MGFLinearX name="MGF method for transformations"}
Suppose that $X_i \sim \text{Pois}(\lambda_i)$ for $i  =  1, 2, \dots, n$.
What is the distribution of $Y  =  X_1  +  X_2  + \dots  +  X_n$?

Since $X_i$ has a Poisson distribution with parameter $\lambda_i$, the mgf of $X_i$ is
\[
   M_{X_i}(t) = \exp[ \lambda_i(e^t - 1)].
\]
The mgf of
$Y  = X_1 + X_2 + \cdots X_n$ is
\begin{align*}
   M_Y(t)
   &= \prod_{i = 1}^n \exp[ \lambda_i(e^t - 1)] \\
   &= \exp[ \lambda_1(e^t - 1)] \exp[ \lambda_2(e^t - 1)] \dots \exp[ \lambda_n(e^t - 1)] \\
   &= \exp\left[ (e^t - 1)\sum_{i = 1}^n \lambda_i\right].
\end{align*}
Using $\Lambda = \sum_{i = 1}^n \lambda_i$, the mgf of $Y$ is
\[
   M_Y(t) = \exp\left[ (e^t - 1)\Lambda \right],
\]
which is the mgf of a Poisson distribution with mean $\Lambda = \sum_{i = 1}^n \lambda_i$.
This means that the sum of $n$ independent Poisson distribution is also a Poisson distribution, whose mean is the sum of the individual Poisson means.
:::


## The chi-squared distribution

Examples \@ref(exm:TransformationNon11) and \@ref(exm:TransformB) give rise to the chi-square distribution, which is an important model in statistical theory.


:::{.definition #ChiSquaredDistribution name="Chi-squared distribution"}
A continuous random variable $X$ with probability density function
\begin{equation}
   f_X(x)
   = \frac{x^{(\nu/2) - 1}e^{-x/2}}{2^{\nu/2}\Gamma(\nu/2)},\quad x > 0
\end{equation}
is said to have a *chi-square distribution* with parameter $\nu > 0$).
The parameter is known as the *degrees of freedom*. 
We write $X \sim \chi^2(\nu)$.
:::

The chi-square distribution is a special case of the gamma distribution.
Comparison of Definitions \@ref(def:ChiSquaredDistribution) and \@ref(def:GammaDistribution) reveal that $X$ has a $\chi^2(\gamma)$ distribution if, and only if, $X$ has a gamma distribution with parameters $\beta = 2$ and $\alpha = \nu/2$ (EXERCISE?)
Some plots of $\chi^2$-distributions are shown in Fig. \@ref(fig:ChisqPlots).






```{r ChisqPlots, echo=FALSE, out.width='100%', fig.height=3, fig.width=6.5, fig.cap="Some $\\chi^2$-distributions"}
par( mfrow = c(1, 3))

xx <- seq(0.001, 6,
          length = 100)

plot( x = xx,
      y = dchisq(xx, df = 1),
#      ylim = c(0, 0.4),
      lwd = 2,
      col = plotColour1,
      type = "l",
      xlab = expression(italic(x)),
      ylab = "Density",
      las = 1,
      main = expression( paste("The"~chi^2*"-distribution: df"==1)))

xx <- seq(0.001, 15,
          length = 100)
plot( x = xx,
      y = dchisq(xx, df = 5),
      lwd = 2,
      type = "l",
      col = plotColour1,
      xlab = expression(italic(x)),
      ylab = "Density",
      las = 1,
      main = expression( paste("The"~chi^2*"-distribution: df"==6)))


xx <- seq(0.001, 50,
          length = 100)
plot( x = xx,
      y = dchisq(xx, df = 20),
      lwd = 2,
      type = "l",
      col = plotColour1,
      xlab = expression(italic(x)),
      ylab = "Density",
      las = 1,
      main = expression( paste("The"~chi^2*"-distribution: df"==20)))
```

The basic properties of the chi-square follow directly from those of the gamma distribution.


:::{.theorem #ChisqProperties name="Properties of chi-squared distribution"}
If $X\sim\chi^2(\nu)$ then

* $\text{E}(X) = \nu$.
* $\text{var}(X) = 2\nu$.
* $M_X(t) = (1 - 2t)^{-\nu/2}$
:::

:::{.proof}
See Theorem \@ref(thm:GammaProperties).
:::

The importance of the chi-square distribution is hinted at in Examples \@ref(exm:TransformationNon11) and \@ref(exm:TransformB), which essentially prove the following theorem.


:::{.theorem #ChiSqDistribution name="Chi-square distribution with 1 df"}
If $Z\sim N(0, 1)$ then $Z^2$ has a chi-square distribution with one degree of freedom.
:::

:::{.proof}
Exercise---see Example \@ref(exm:TransformationNon11).
:::


A useful property of the chi-square distribution is that the sum of independent random variables, each with a chi-square distribution, also has a chi-square distribution.
This property is exemplified in the following theorem, which will be made use of later.



:::{.theorem #SumSquaredNormals name="Chi-squared distribution"}
If $Z_1, Z_2,\dots, Z_n$ are independently and identically distributed as $N(0, 1)$, then the sum of square $S = \sum_i Z_i^2$ has a $\chi^2(n)$ distribution.
:::

:::{.proof}
The elegant way to prove this result is using mgf's. 
Since $Z_i \sim \chi^2(1)$, from Theorem \@ref(thm:ChisqProperties)
\[
   M_{Z_i}(t)
   = (1 - 2t)^{-1/2}.
\]
It follows then from Theorem \@ref(thm:MGFIndependent) that $S = \sum_{i=1}^n Z_i^2$ has mgf
\begin{align*}
   M_{S}(t)
   &= \prod_{i=1}^n (1-2t)^{-1/2}\\
   &= \left[(1-2t)^{-1/2}\right]^n = (1 - 2t)^{-n/2}
\end{align*}
which is the mgf of $\chi^2(n)$.
:::

Chi-square probabilities cannot in general be calculated without resorting to numerical integration. 
Key chi-square quantiles (i.e., values of the variable corresponding to particular values of the distribution function) are tabulated for various values of the degrees of freedom in most statistics texts including WMS and DGS. 
These tables are of value in statistical inference.

:::{.example #ChisqProb name="Chi-squared distributions"}
The variable $X$ has a chi-square distribution with 12 df. 
Determine the value of $X$ below which lies 90% of the distribution.

We seek a value $c$ such that $\Pr(X < c) = F_X(c) = 0.90$ where $X\sim\chi^2(12)$.

In the table on p774 of DGS we note that $p = 0.90$ and $n = 12$ give the required value as $c = 18.55$.

In Table~6 in WMS we note that $\chi^2_\alpha$ denotes the quantile *above* which the proportion $\alpha$ of the distribution lies.
Interest therefore is in $\chi^2_{0.10}$, which for df = 12 gives $c = 18.5494$.

In **R**:
```{r, echo=TRUE}
qchisq(0.90, df = 12)
```
:::


