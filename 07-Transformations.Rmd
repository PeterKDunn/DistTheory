# Transformations of random variables {#Transformations}

::: {.objectivesBox .objectives data-latex="{iconmonstr-target-4-240.png}"}
On completion of this module, you should be able to:

* derive the distribution of a transformed variable, given the distribution of the original variable, using the distribution function method, the change of variable method, and the moment generating function method as appropriate.
* find the joint distribution of two transformed variables in a bivariate situation.
:::


## Introduction

In this chapter, we consider the distribution of a random variable $Y = u(X)$, given a random variable $X$ with known distribution, and a function $u(\cdot)$.
Among several available techniques, three are considered:


1. the change of variable method (Sect. \@ref(ChangeOfVariable));
2. the distribution function method for continuous random variable only (Sect. \@ref(DistributonFunctionMethod));
3. the moment generating function method (Sect. \ref(TransformationMoments)).

An important concept in this context is a *one-to-one transformation*.


:::{.definition #OneOneTransformation name="One-to-one transformation"}
Given random variables $X$ and $Y$ with range spaces $R_X$ and $R_Y$ respectively, the function $u$ is a one-to-one transformation (or mapping) if for each $x\in R_X$ there corresponds exactly one $y\in R_Y$.
:::


When $Y = u(X)$ is a one-to-one transformation, the inverse function is uniquely defined; that is, $X$ can be written uniquely in terms of $Y$. 
This is important when considering the distribution of $Y$ when the distribution of $X$ is known.


## The change of variable method {#ChangeOfVariable}

The method is relatively straightforward for one-to-one transformations (such as $Y = 1 - X$ or $Y = \exp(X)$).
Considerable care needs to be exercised if the transformation is not one-to-one. 
Examples are given below.
The discrete and continuous cases are considered separately. 


### Discrete random variables

#### Univariate case

Let $X$ be a discrete random variable with pf $p_X(x)$. 
Let $R_X$ denote the set of discrete points for which $p_X(x) > 0$.
Let $y = u(x)$ define a *one-to-one transformation* that maps $R_X$ onto $R_Y$, the set of discrete points at each of which the transformed variable $Y$ has a non-zero probability. 
If we solve $y = u(x)$ for $x$ in terms of $y$, say $x = w(y)$, then for each $y \in R_Y$, we have $x = w(y)\in R_X$.


:::{.example #Transform1 name="Transformation (1:1)"}
Given
\[
   p_X(x) = 
   \begin{cases}
      x/15 & \text{for $x = 1, 2, 3, 4, 5$};\\
      0    & \text{elsewhere}.
   \end{cases}
\]
To find the probability function of $Y$ where $Y = 2X + 1$, first see that $R_X = \{1, 2, 3, 4, 5\}$.
Hence $R_Y = \{3, 5, 7, 9, 11\}$ and the mapping $y = 2x + 1 = u(x)$ is one-to-one. 
Now
\[ 
   \Pr(Y = y)
   = \Pr(2X + 1 = y)
   = \Pr\left(X = \frac{y - 1}{2}\right)
   = \frac{y - 1}{30}. 
\]
So the probability function of $Y$ is
\[ 
   \Pr(Y = y)
   = \begin{cases}
      (y - 1)/30 & \text{for $y = 3, 5, 7, 9, 11$};\\
      0          & \text{elsewhere}.
    \end{cases}
\]
(Note: The probabilities in this pf add to 1.)
:::


The above procedure when $Y = u(X)$ is a one-to-one mapping can be stated generally as
\[
   \Pr(Y = y) 
   = \Pr\big(u(X) = y\big)
   = \Pr\big(X = u^{-1} (y)\big)
   = p_X\big(u^{-1}(y)\big), \quad\text{for $y\in R_Y$}.
\]


:::{.example #Transform2 name="Transformation (1:1)"}
Let $X$ have a binomial distribution with pf
\[
   p_X(x) = \begin{cases}
               \binom{3}{x}(0.2)^x (0.8)^{3 - x} & \text{for $x = 0, 1, 2, 3$};\\
               0 & \text{otherwise}.
            \end{cases}
\]
To find the pf of $Y = X^2$, first note that $Y = X^2$ is *not* a one-to-one transformation in general, but is here since $X$ has non-zero probability only for $x = 0, 1, 2, 3$.

The transformation $y = u(x) = x^2$, $R_X = \{ x \mid x = 0, 1, 2, 3 \}$ maps onto $R_Y = \{y \mid y = 0, 1, 4, 9\}$.
The inverse function is $x = w(y) = \sqrt{y}$, and hence the pf of $Y$ is
\[
   p_Y(y) = p_X(\sqrt{y})
   = \begin{cases}
               \binom{3}{\sqrt{y}}(0.2)^{\sqrt{y}} (0.8)^{3 - \sqrt{y}} & \text{for $y = 0, 1, 4, 9$}\\
               0 & \text{otherwise}.
     \end{cases}
\]
:::

Now consider the case where the function $u$ is *not* 1:1.


:::{.example #TransformNot11 name="Transformation not 1:1"}
Suppose $\Pr(X = x)$ is as in Example \@ref(exm:Transform1), and define $Y = |X - 3|$.
Since $R_Y = \{0, 1, 2\}$ the mapping is not one-to-one: the event $Y = 0$ occurs if $X = 3$, the event $Y = 1$ occurs if $X = 2$ or $X = 4$, and the event $Y = 2$ occurs if $X = 1$ or $X = 5$.
Hence, $R_Y  \{ 0, 1, 2\}$.

To find the probability distribution of $Y$:
\begin{align*}
   \Pr(Y = 0) 
   &= \Pr(X = 3) = 3/15 = \frac{1}{5};\\
   \Pr(Y = 1) 
   &= \Pr(X = 2 \text{ or } 4) = \frac{2}{15} + \frac{4}{15} = \frac{2}{5};\\
   \Pr(Y = 2) 
   &= \Pr(X = 1 \text{ or } 5) = \frac{1}{15} + \frac{5}{15} = \frac{2}{5}.
\end{align*}
The probability function of $Y$ is
\[
   p_Y(y) = 
   \begin{cases}
       1/5 & \text{for $y = 0$};\\
       2/5 & \text{for $y = 1$};\\
       2/5 & \text{for $y = 2$};\\
       0   & \text{elsewhere}.
   \end{cases}
\]
:::


#### Bivariate case

The bivariate case is similar to the univariate case.
We have a joint pf $p_{X_1, X_2}(x_1, x_2)$ of two discrete random variables $X_1$ and $X_2$ defined on the two-dimensional set of points $R^2_X$ for which $p(x_1, x_2) > 0$.
There are now two *one-to-one transformations*:
\[
   y_1 = u_1( x_1, x_2)\qquad\text{and}\qquad y_2 = u_2( x_1, x_2)
\]
that map $R^2_X$ onto $R^2_Y$ (the two-dimensional set of points for which $p(y_1, y_2) > 0$).
The two inverse functions are
\[
   x_1 = w_1( y_1, y_2)\qquad\text{and}\qquad x_2 = w_2( y_1, y_2).
\]
Then the joint pf of the new (transformed) random variables
is
\[
   p_{Y_1, Y_2}(y_1, y_2) =
   \begin{cases}
      p_{X_1, X_2}\big( w_1(y_1, y_2), w_2(y_1, y_2)\big) & \text{where $(y_1, y_2)\in R^2_Y$};\\
      0 & \text{elsewhere}.
   \end{cases}
\]


:::{.example #TransformBivariate name="Transformation (bivariate)"}
Let the two discrete random variables $X_1$ and $X_2$ have the joint pf shown in Table \@ref(tab:Bivar4).


```{r Bivar4, echo=FALSE}
Bivar4Table <- array( dim = c(2, 4))

Bivar4Table[1, ] <- c("$x_1 = -1$", "$0.3$", "$0.1$", "$0.1$")
Bivar4Table[2, ] <- c("$x_1 = +1$", "$0.2$", "$0.2$", "$0.1$")


Bivar4Table.caption <- "A bivariate probability function"
if( knitr::is_latex_output() ) {
  knitr::kable(Bivar4Table,
               format = "latex",
               booktabs = TRUE,
               col.names = c("",
                             "$x_2 = 0$",
                             "$x_2 = 1$",
                             "$x_2 = 2$"),
               align = "r",
               longtable = FALSE,                
               escape = FALSE,
               caption = Bivar4Table.caption) %>%
    kable_styling(font_size = 10) %>%
    row_spec(0, bold = TRUE) 
}
if( knitr::is_html_output() ) {
  knitr::kable(Bivar4Table,
               escape = TRUE,
               format = "html",
               booktabs = TRUE,
               align = "r",
               longtable = FALSE,                
               col.names = c("",
                             "$x_2 = 0$",
                             "$x_2 = 1$",
                             "$x_2 = 2$"),
               caption = Bivar4Table.caption) %>%
    row_spec(0, bold = TRUE) 
}
```



Consider the two one-to-one transformations
\[
   Y_1 = X_1 + X_2 \qquad\text{and}\qquad Y_2 = 2 X_1.
\]
The joint pf of $Y_1$ and $Y_2$ can be found by noting where the $(x_1, x_2)$ pairs are mapped to in the $y_1, y_2$ space:

| $(x_1,x_2)$ | $\mapsto$ | $(y_1,y_2)$
|:-----------:|:---------:|:----------------:
| $(-1, 0)$   | $\mapsto$ | $(-1, -2)$ 
| $(-1, 1)$   | $\mapsto$ | $(0, -2)$
| $(-1, 2)$   | $\mapsto$ | $(1, -2)$
| $(1, 0)$    | $\mapsto$ | $(1, 2)$ 
| $(1, 1)$    | $\mapsto$ | $(2, 2)$ 
| $(1, 2)$    | $\mapsto$ | $(3, 2)$ 


The joint pf can then be constructed as shown in Table \@ref(tab:Bivar5).


```{r Bivar5, echo=FALSE}
Bivar5Table <- array( dim = c(2, 6))

Bivar5Table[1, ] <- c("$y_2 = -2$", "$0.3$", "$0.1$", "$0.1$", "$0.0$", "$0.0$")
Bivar5Table[2, ] <- c("$y_2 = +2$", "$0.0$", "$0.0$", "$0.2$", "$0.2$", "$0.1$")


Bivar5Table.caption <- "The joint probability function for $Y_1$ and $Y_2$"
if( knitr::is_latex_output() ) {
  knitr::kable(Bivar5Table,
               format = "latex",
               booktabs = TRUE,
               col.names = c("",
                             "$y_1 = -1$",
                             "$y_2 = 0$",
                             "$y_3 = 1$",
                             "$y_4 = 2$",
                             "$y_5 = 3$"),
               align = "r",
               longtable = FALSE,                
               escape = FALSE,
               caption = Bivar5Table.caption) %>%
    kable_styling(font_size = 10) %>%
    row_spec(0, bold = TRUE) 
}
if( knitr::is_html_output() ) {
  knitr::kable(Bivar5Table,
               escape = TRUE,
               format = "html",
               booktabs = TRUE,
               align = "r",
               longtable = FALSE,                
               col.names = c("",
                             "$y_1 = -1$",
                             "$y_2 = 0$",
                             "$y_3 = 1$",
                             "$y_4 = 2$",
                             "$y_5 = 3$"),
               caption = Bivar5Table.caption) %>%
    row_spec(0, bold = TRUE) 
}
```
:::


Sometimes, a joint pf of two random variables is given, but only *one* new random variable is required.
In this case, a second (dummy) transformation is used, usually very simple.


:::{.example #TransformBivariate2 name="Transformation (bivariate)"}
Let $X_1$ and $X_2$ be two independent random variables with the joint pf
\[
   p_{X_1, X_2}(x_1, x_2) =
         \frac{\mu_1^{x_1} \mu_x^{x_2} \exp( -\mu_1 - \mu_2 )}{x_1!\, x_2!}
         \quad\text{for $x_1$ and $x_2 = 0, 1, 2, \dots$}
\]
This is the joint pf of two independent Poisson random variables.
Suppose we wish to find the pf of $Y_1 = X_1 + X_2$.

We can consider the two *one-to-one transformations*:
\begin{align}
   y_1 &= x_1 + x_2             = u_1(x_1, x_2)\\
   y_2 &= x_2\phantom{{} + x_2} = u_2(x_1, x_2)
\end{align}
which maps the points in $R^2_X$ onto
\[
   R^2_Y = \left\{ (y_1, y_2)\mid y_1 = 0, 1, 2, \dots; y_2 = 0, 1, 2, \dots, y_1\right\}.
\]
$Y_2$ is a dummy transform, and it is very simple.
Any second transform could be chosen (as it is not of direct interest), and so chose one that is simple.

The inverse functions are
\begin{align*}
   x_1 &= y_1 - y_2              = w_1(y_1, y_2)\\
   x_2 &= y_2 \phantom{{} - y_2} = w_2(y_2)
\end{align*}
by rearranging the original transfosrmations.
Then the *joint* pf of $Y_1$ and $Y_2$ is
\begin{align*}
   p_{Y_1, Y_2}(y_1, y_2)
   &= p_{X_1, X_2}(x_1, x_2)\big( w_1(y_1, y_2), w_2(y_1, y_2)\big) \\
   &= \frac{\mu_1^{y_1 - y_2}\mu_2^{y_2} \exp(-\mu_1 - \mu_2)}{(y_1 - y_2)! y_2!}\quad
   \text{for $(y_1, y_2)\in R^2_Y$}.
\end{align*}
Recall that we seek the pf of just $Y_1$, so we need to find the marginal pf of $p_{Y_1, Y_2}(y_1, y_2)$.
The marginal pf of $Y_1$ is
\[
   p_{Y_1}(y_1) = \sum_{y_2 = 0}^{y_1} p_{Y_1, Y_2}(y_1, y_2)
   = \sum_{y_2 = 0}^{y_1} \frac{\mu_1^{y_1 - y_2}\mu_2^{y_2} \exp(-\mu_1 - \mu_2)}{(y_1 - y_2)!\, y_2!},
\]
which is equivalent to
\[
   p_{Y_1}(y_1) =
   \begin{cases}
      \displaystyle{\frac{(\mu_1 + \mu_2)^{y_1}\exp\big[-(\mu_1 + \mu_2)\big]}{y_1!}} & \text{for $y_1 = 0, 1, 2, \dots$}\\
      0 & \text{otherwise}.
   \end{cases}
\]
This is the pf of a Poisson random variable with mean $\mu_1 + \mu_2$.
Thus $Y_1 \sim \text{Pois}(\lambda = \mu_1 + \mu_2)$.
:::


### Continuous random variables

#### Univariate case

:::{.theorem #ChangeOfVarCont name="Change of variable (continuous rv)"}
If $X$ has pdf $f_X(x)$ for $x\in R_X$ and $u$ is a one-to-one function for $x\in R_X$, then the random variable $Y = u(X)$ has pdf
\[
   f_Y(y) = f_X(x) \left|\frac{dx}{dy}\right|
\]
where the RHS is expressed as a function of $y$.
The term $\left|dx/dy\right|$ is called the *Jacobian of the transformation*.
:::

:::{.proof}
Let the inverse function be $X = w(Y)$ so that $w(y) = u^{-1}(x)$.

**Case 1:** $y = u(x)$ is a strictly *increasing* function (Fig. \@ref(fig:Transformation), left panel).
If $a < y < b$ then $w(a) < x < w(b)$ and  $\Pr(a < Y < b) = \Pr\big(w(a) < X <w(b)\big)$, so
\[
   {\int^b_a f_Y(y)\,dy
   =\int^{w(b)}_{w(a)}f_X(x)\,dx
   =\int^b_af\big( w(y)\big)\frac{dx}{dy}\,\,dy}.
\]
Therefore, $\displaystyle {f_Y(y) = f_X\big( w(y) \big)\frac{dx}{dy}}$, where $w(y) = u^{-1}(x)$.


```{r Transformation, echo=FALSE, fig.align="center", fig.cap="A monotone increasing transformation function (left panel) and decreasing function (right panel).", fig.width=7, fig.height=4, out.width='100%'}
par( mfrow = c(1, 2))

logit <- function(x) { 
  log(x / (1 - x)) 
  }

y <- seq(0.01, 1,
         length = 100)
x <- logit(y)

y0 <- 0.4
y1 <- 0.8

x0 <- logit( y0 )
x1 <- logit( y1 )

plot( x = x,
      y = y,
      lwd = 2,
      axes = FALSE,
      type = "l",
      main = "Increasing function",
      xlab = expression(italic(x)),
      ylab = expression(italic(y)==italic(u)(italic(x))))
axis(side = 2,
     las = 1,
     at = c(y0, y1),
     labels = c(expression(italic(a)), 
                expression(italic(b))) ) 
axis(side = 1,
     at = c(x0, x1 ),
     labels = c(expression(italic(w)*paste("(",italic(a),")")),
                expression(italic(w)*paste("(",italic(b),")")) ) )
box()

lines( x = c( min(x), x0, x0),
       y = c(y0, y0, 0),
       lty = 2)
lines( x = c( min(x), x1, x1),
       y = c(y1, y1, 0),
       lty = 2)

text(x = 3.6,
     y = 0.85,
     label = expression(italic(y)==italic(u)(italic(x))))



logit2 <- function(x) { 
  -log(x / (1 - x)) 
  }

y <- seq(0.01, 0.99,
         length = 100)
x <- logit2(y)

y0 <- 0.4
y1 <- 0.8

x0 <- logit2( y0 )
x1 <- logit2( y1 )

plot( x = x,
      y = y,
      lwd = 2,
      axes = FALSE,
      type = "l",
      main = "Decreasing function",
      xlab = expression(italic(x)),
      ylab = expression(italic(y)==italic(u)(italic(x))))
axis(side = 2,
     las = 1,
     at = c(y0, y1),
     labels = c(expression(italic(a)), 
                expression(italic(b))) ) 
axis(side = 1,
     at = c(x0, x1),
     labels = c(expression(italic(w)*paste("(",italic(a),")")),
                expression(italic(w)*paste("(",italic(b),")")) ) )
box()

lines( x = c( min(x), x0, x0),
       y = c(y0, y0, 0),
       lty = 2)
lines( x = c( min(x), x1, x1),
       y = c(y1, y1, 0),
       lty = 2)

text(x = 2.5,
     y = 0.7,
     label = expression(italic(y)==italic(u)(italic(x))))
```



**Case 2:** $y = u(x)$ is a strictly *decreasing* function of $x$ (Fig. \@ref(fig:Transformation), right panel).
If $a < y < b$ then $w(b) < x < w(a)$ and $\Pr(a < Y < b) = \Pr\big(w(b) < X < w(a)\big)$, so that,
\begin{align*}
     \int^b_a f_Y(y)\,dy & = \int^{w(a)}_{w(b)}f_X(x)\,dx\\
     & = \int^a_bf_X(x)\frac{dx}{dy}\,\,dy\\
     & = - \int ^b_a f_X(x)\frac{dx}{dy}\,dy.
\end{align*}
Therefore $f_Y(y) = -f_X\left( w(y) \right)\displaystyle{\frac{dx}{dy}}$.
But $dx/dy$ is negative in the case of a decreasing function, so in general
\[ 
   f_Y(y) = f_X(x)\left|\frac{dx}{dy} \right|. 
\]
:::

The absolute  value  of  $w'(y) = dx/dy$  is  called  the *Jacobian  of  the transformation*.


:::{.example #Transform3 name="Transformation"}
Let the pdf of $X$ be given by
\[
   f_X(x) = 
   \begin{cases}
      1 & \text{for $0 < x < 1$};\\
      0 & \text{elsewhere}.
   \end{cases}
\]
Consider the transformation $Y = -2\log X$ (where $\log$ refers to logarithms to base $e$, or *natural logarithms*).

The transformation is one-to-one, and the inverse transformation is
\[
   X = \exp( -Y/2) = w(Y).
\]
The space $R_X = \{x \mid 0 < x < 1\}$ is mapped to $R_y = \{y \mid 0 < y < \infty\}$.
Then,
\[
   w'(y) = \frac{d}{dy} \exp(-y/2) = -\frac{1}{2}\exp(-y/2),
\]
and so the *Jacobian* of the transformation $|w'(y)| = \exp(-y/2)/2$.
The pdf of $Y = -2\log X$ is
\begin{align*}
   f_Y(y)
   &= f_X\{w(y)\} |w'(y)| \\
   &= f_X\{\exp(-y/2)\} \exp(-y/2)/2 \\
   &= \frac{1}{2}\exp(-y/2)\quad\text{for $y > 0$}.
\end{align*}
That is, $Y \sim \text{Exp}(2)$.
:::


:::{.example #TransformSquRt name="Square root transformation"}
Consider the random variable $X$ with pdf $f_X(x) = e^{-x}$ for $x \geq 0$.
To find the pdf of $Y = \sqrt{X}$, first see that $y = \sqrt{x}$ is a strictly increasing function for $x \geq 0$ (Fig. \@ref(fig:SqrtRt)).


```{r SqrtRt, echo=FALSE, fig.align="center", fig.cap="The square-root transformation", fig.width=4, fig.height=3.5}
x <- seq(0, 20,
         length = 100)
y <- sqrt(x)

plot(x = x,
     y = y,
     lwd = 2,
     las = 1,
     main = expression(paste("The transformation ", italic(y) == sqrt(italic(x)))),
     xlab = expression(italic(x)),
     ylab = expression( italic(y) == sqrt(italic(x))),
     type = "l")

```

The inverse relation is $x = y^2$, and $dx/dy = |2y| = 2y$.
The pdf of $Y$ is
\begin{align*}
    f_Y(y)
    &= f_X(x)\left|\frac{dx}{dy}\right|\\
    &= 2y e^{-y^2}\quad \text{for $y\geq0$}.
\end{align*}
:::


::: {.example #TransformationTan name="Tan transformation"}
Let random variable $X$ be uniformly distributed on $[-\pi/2, \pi/2]$. 
Find the distribution of $Y = \tan X$ (Fig. \@ref(fig:TanXform)).


```{r TanXform, echo=FALSE, fig.align="center", fig.cap="The tan transformation", fig.width=4,fig.height=3.5}
x <- seq(-pi/2 + 0.1, 
         pi/2 - 0.1,
         length = 100)
y <- tan(x)

plot(x = x,
     y = y,
     lwd = 2,
     las = 1,
     xlim = c(-pi/2, pi/2),
     axes = FALSE,
     main = expression(paste("The transformation ", italic(y) == plain(tan)(italic(x)))),
     xlab = expression(italic(x)),
     ylab = expression( italic(y) == plain(tan)(italic(x))),
     type = "l")
axis(side = 2,
     las = 1)
axis(side = 1,
     at = c(-pi/2, 0, pi/2),
     labels = c(expression(-pi/2),
                "0",
                expression(pi/2)) )
abline(h = 0, 
       lty = 1,
       col = "grey")
abline(v = 0, 
       lty = 1,
       col = "grey")
box()

```
For the mapping $y = \tan x$, we see that $R_Y = \{ y\mid -\infty <y<\infty\}$.
The mapping is one-to-one, and so $x = \tan^{-1}y$, and $dx/dy = 1/(1 + y^2)$.
Hence
\[
   f_Y(y)
   = f_X(x)\left|\frac{dx}{dy}\right|
   = \frac{1}{\pi(1 + y^2)}.
\]
This is the [*Cauchy distribution*](exr:C3Cauchy).
:::


A case where the function $u$ is not one-to-one is considered by an example, using a modification of Theorem \@ref(thm:ChangeOfVarCont).


:::{.example #TransformationNon11 name="Transformation (not 1:1)"}
Given a random variable $Z$ which follows a $N(0, 1)$ distribution, find the probability distribution of $Y = \frac{1}{2}Z^2$.


```{r Non11Xform, echo=FALSE, fig.align="center", fig.cap="A transformation not 1:1", fig.height=3.5, fig.width=4}
x <- seq(-5, 5,
         length = 100)
square <- function(x) { x^2 }

y <- square(x)

x1 <- -4
x2 <- -2
x3 <- 2
x4 <- 4

y1 <- square(x1)
y2 <- square(x2)
y3 <- square(x3)
y4 <- square(x4)

plot(x = x,
     y = y,
     lwd = 2,
     las = 1,
     axes = FALSE,
     main = "A non-1:1 transformation",
     xlab = expression(italic(x)),
     ylab = expression(italic(y) == italic(x)^2),
     type = "l")
axis(side = 1,
     at = c(x1, x2, x3, x4),
     labels = c( expression(-sqrt(2)),
                 expression(-sqrt(-2*italic(a))),
                 expression(sqrt(2*italic(a))),
                 expression(sqrt(2)) ) )
axis(side = 2,
     las = 1,
     at = c(y1, y2),
     label = c(expression(italic(b)),
               expression(italic(a))))
box()


lines( x = c(x1, x1, x4, x4),
       y = c(0, y1, y4, 0),
       lty = 2)
lines( x = c(x2, x2, x3, x3),
       y = c(0, y2, y3, 0),
       lty = 2)

abline(h = 0,
       col = "grey")
abline(v = 0,
       col = "grey")

```

The relationship $y = u(z) = \frac{1}{2}z^2$ is not increasing or strictly decreasing in $(-\infty, \infty )$ so Theorem \@ref(thm:ChangeOfVarCont) cannot be applied directly.
Instead, subdivide the range of $z$ and $y$ so that in each portion the relationship *is* monotonic.
Then:
\[
   f_Z(z) =
   \frac{1}{\sqrt{2\pi}}\,e^{-\frac{1}{2} z^2}\quad\text{for $-\infty < z < \infty$}.
\]
The inverse relation, $z = u^{-1}(y)$ is $z = \pm \sqrt{2y}$. 
For a given value of $y$, two values of $z$ are possible. 
In the range $-\infty < z < 0$, and then $y$ and $z$ are monotonically related. 
Similarly, for $0 < z <\infty$, $y$ and $z$ are monotonically related. 
Thus (see Fig. \@ref(fig:Non11Xform)),
\[
   \Pr(a < Y <b) = \Pr(-\sqrt{2b} < Z < -\sqrt{2a}\,) + \Pr(\sqrt{2a} < Z < \sqrt{2b}\,).
\]
The two terms on the right are equal because the distribution of $Z$ is symmetrical about $0$.
Thus $\Pr(a < Y < b) = 2\Pr(\sqrt{2a} < Z < \sqrt{2b}\,)$, and
\begin{align*}
     f_Y(y)
     &= 2f_Z(z)\left| \frac{dz}{dy}\right|\\
     &= 2\frac{1}{\sqrt{2\pi}}e^{-y}\frac{1}{\sqrt{2y}};
\end{align*}
that is,
\[
   f_Y(y)
   = e^{-y}y^{-\frac{1}{2}} / \sqrt{\pi}\quad\text{for $0 < y < \infty$}.
\]
This pdf is a [gamma distribution](GammaDistribution) with parameters $\alpha = 1/2$ and $\beta = 1$.
It follows that if $X$ is $N(\mu,\sigma^2)$, the pdf of $Y = \frac{1}{2} (X - \mu )^2 / \sigma^2$ is also $\text{Gamma}(\alpha = 1/2,\beta = 1)$ since then $(X - \mu)\sigma$ is distributed $N(0, 1)$.
:::

Note that the probability can only be doubled as in Example \@ref(exm:TransformationNon11) if both $Y = u(Z)$ and the pdf of $Z$ are symmetrical about the same point.



## The distribution function method {#DistributonFunctionMethod}

::: {.importantBox .important data-latex="{iconmonstr-warning-8-240.png}"}
This method only works for continuous random variables.
:::

There are two basic steps:

1. Find the *distribution function* of the transformed variable. 
2. Differentiate to find the *probability density function*.

The procedure is best demonstrated using an example.

:::{.example #DFMethod name="Distribution function method"}
Consider the random variable $X$ with pdf
\[
   f_X(x) = \begin{cases}
               x/4 & \text{for $1 < x < 3$};\\
               0 & \text{elsewhere}.
            \end{cases}
\]
To find the pdf of the random variable $Y$ where $Y = X^2$, first see that $1 < y < 9$.
The distribution function for $Y$ is
\begin{align*}
   F_Y(y)
   &= \Pr(Y\le y) \qquad\text{(by definition)}\\
   &= \Pr(X^2 \le y) \qquad\text{(since $Y = X^2$)}\\
   &= \Pr(X\le \sqrt{y}\,).
\end{align*}
This last step is not trivial, but is critical. 
Sometimes, more care is needed (as in the next example). 
In this case, there is a one-to-one relationship between $X$ and $Y$ over the region of which $X$ is defined (i.e., has a positive probability); see Fig. \@ref(fig:SquaringXform).


```{r SquaringXform, echo=FALSE, fig.align="center", fig.cap="The transformation $Y = X^2$ when $X$ is defined from $1$ to $3$. The thicker line corresponds to the region where the transformation applies. Note that if $Y < y$, then $2 - \\sqrt{y - 1} < X < 2 + \\sqrt{y - 1}$.", fig.width=4.5, fig.height=4}

x <- seq(-1, 4,
         length = 100)
y <- x^2

plot(x = x,
     y = y,
     type = "l",
     col = plotColour1,
     lwd = 1,
     axes = FALSE,
     xlab = expression(italic(x)),
     ylab = expression(italic(y) == italic(x)^2),
     main = "The squaring transformation",
     las = 1)

Thicker <- ( x >= 1 ) & ( x <= 3 )
lines(x = x[ Thicker],
      y = y[Thicker],
      lwd = 5)
lines( x = c( min(x), sqrt(3), sqrt(3)),
       y = c( 3, 3, 0),
       lty = 2)
axis(side = 2,
     at = c(0, 3, 6, 8, 10, 12, 14, 16),
     las = 1,
     labels = c(0, "y = 3", 6, 8, 10, 12, 14, 16))
axis(side = 1,
     at = c(-1, 0, 1, sqrt(3), 3, 4),
     labels = c(-1, 0, 1, 
                expression(italic(x) == sqrt(3) ),
                3, 4))
box()
```

Then continue as follows:
\begin{align*}
   F_Y(y)
   &=\Pr( X\le \sqrt{y}\,) \\
   &= F_X\big(\sqrt{y}\,\big) \qquad\text{(by definition of $F_X(x)$)} \\
   &= \int_1^{\sqrt{y}} (x/4) \,dx \\
   &= (y - 1)/8
\end{align*}
for $1 < y < 9$, and is zero elsewhere. 
This is the *distribution function* of $Y$; to find the pdf:
\[
   f_Y(y)
   = \frac{d}{dy} (y - 1)/8
   = \begin{cases}
        1/8 & \text{for $1 < y < 9$};\\
        0 & \text{elsewhere}.
     \end{cases}
\]
Note the range for which $Y$ is defined; since $1 < x < 3$, then $1 < y < 9$.
:::



:::{.example #TransformA name="Transformation"}
Consider the same random variable $X$ as in the previous example, but the transformation $Y = (X - 2)^2 + 1$ (Fig. \@ref(fig:SquaringXform2)).




```{r SquaringXform2, echo=FALSE, fig.align="center", fig.cap="The transformation $Y = (X - 2)^2 + 1$ when $X$ is defined from $1$ to $3$. The thicker line corresponds to the region where the transformation applies. Note that if $Y < y$, then $2 - \\sqrt{y - 1} < X < 2 + \\sqrt{y - 1}$.", fig.width=5, fig.height=4}

x <- seq(0, 4,
         length = 100)
xf <- function(x) { (x - 2)^2 + 1}

y <- xf(x)

plot(x = x,
     y = y,
     type = "l",
     lwd = 1,
     col = plotColour1,
     ylim = c(0, 2.25),
     axes = FALSE,
     xlab = expression(italic(x)),
     ylab = expression(italic(y)==group("(", italic(x)-2,")")^2+1),
     main = "Another transformation",
     las = 1)

x0 <- 1.25
x1 <- 2.75
y0 <- xf( x0 )
y1 <- xf( x1 )


Thicker <- ( x >= 1 ) & ( x <= 3 )
lines(x = x[ Thicker],
      y = y[Thicker],
      lwd = 5)

lines( x = c( min(x), x0, x0),
       y = c(y0, y0, 0),
       lty = 2)
lines( x = c( min(x), x1, x1),
       y = c(y1, y1, 0),
       lty = 2)
axis(side = 2,
     at = c(0, 1, y0, 2),
     las = 1,
     labels = c(0, 1, expression(italic(y)), 2))
axis(side = 1,
     at = c(x0, 2, x1),
     labels = c(expression(2-sqrt(italic(y)-1)),
                2, 
                expression(2+sqrt(italic(y)-1)) ))
box()
```

In this case, the transformation is **not** a one-to-one transform.
Proceed as before to find the distribution function of $Y$:
\begin{align*}
   F_Y(y)
   &= \Pr(Y\le y) \qquad\text{(by definition)}\\
   &= \Pr\big( (X - 2)^2 + 1  \le y\big)
\end{align*}
since $Y = (X - 2)^2 + 1$.
From Fig. \@ref(fig:SquaringXform2), whenever $(X - 2)^2 + 1 < y$ for some value $y$, then $X$ must be in the range $2 - \sqrt{y - 1}$ to $2 + \sqrt{y - 1}$. 
So:
\begin{align*}
   F_Y(y)
   &= \Pr\big( (X - 2)^2 + 1 \le y\big) \\
   &= \Pr\left( 2 - \sqrt{y - 1} < X < 2 + \sqrt{y - 1} \right)\\
   &= \int_{2-\sqrt{y - 1}}^{2 + \sqrt{y - 1}} x/4\,dx \\
   &= \left.\frac{1}{8} x^2\right|_{2 - \sqrt{y - 1}}^{2 + \sqrt{y - 1}} \\
   &= \frac{1}{8} \left[ \left(2 + \sqrt{y - 1}\right)^2 - \left(2 - \sqrt{y - 1}\right)^2\right] \\
   &=  \sqrt{y - 1}.
\end{align*}
Again, this is the distribution function; so
\[
   f_Y(y) = \begin{cases}
               \frac{1}{2\sqrt{y - 1}} & \text{for $1 < y < 2$};\\
               0 & \text{elsewhere}.
            \end{cases}
\]
:::



:::{.example #TransformB name="Transformation"}
Example \@ref(exm:TransformationNon11) is repeated here using the distribution function method.
Given $Z$ is distributed $N(0, 1)$ we seek the probability distribution of $Y = \frac{1}{2} Z^2$.
First,
\[ 
   f_Z(z) 
   = (2\pi )^{-\frac 12}\,e^{-z^2/2}\quad\text{for $z\in (-\infty ,\,\infty )$}.
\]
Let $Y$ have pdf $f_Y(y)$ and df $F_Y(y)$. 
Then
\begin{align*}
     F_Y(y) 
      = \Pr(Y\leq y) 
     &= \Pr\left(\frac{1}{2}Z^2\leq y\right)\\
     &= \Pr(Z^2\leq 2y)\\
     & = \Pr(-\sqrt{2y}\leq Z\leq \sqrt{2y}\,)\\
     & = F_Z(\sqrt{2y}\,) - F_Z(-\sqrt{2y}\,)
\end{align*}
where $F_Z$ is the df of $Z$.
Hence
\begin{align*}
     f_Y(y) 
       = F_Y'(y)
     &= F_Z'(\sqrt{2y}\,)-F_Z'(-\sqrt{2y}\,)\\
     &= \frac{\sqrt{2}}{2\sqrt{y}}f_Z(\sqrt{2y}\,) - \frac{\sqrt{2}}{-
2\sqrt{y}}f_Z(-\sqrt{2y}\,)\\[2mm]
     &= \frac{1}{\sqrt{2y}}[f_Z(\sqrt{2y}\,) + f_Z(-\sqrt{2y}\,)]\\
     &= \frac{1}{2y} \left[ \frac{1}{\sqrt{2\pi}}\,e^{-y}+\frac{1}{\sqrt{2\pi}}\,e^{-y}\right]\\
     &= \frac{e^{-y}y^{-\frac{1}{2}}}{\sqrt{\pi}}
\end{align*}
as before.
:::

Care is needed to ensure the steps are followed logically. 
Diagrams like Fig. \@ref(fig:SquaringXform) and \@ref(fig:SquaringXform2) are encouraged. 

::: {.importantBox .important data-latex="{iconmonstr-warning-8-240.png}"}
The functions that are produced should be pdfs; check that this is the case.
:::


This method can also be used when there is more than one variable of interest. 

**YET TO ADD**


## The moment generating function method {#TransformationMoments}


The moment generating function (mgf) method is useful for finding the distribution of a linear combination of $n$ independent random variables.
The method essentially involves the computation of the mgf of the transformed variable $Y = u(X_1, X_2, \dots, X_n)$ when the joint distribution of independent $X_1, X_2, \dots, X_n$ is given.

The mgf method relies on this observation: Since the mgf of a random variable (if it exists) completely specifies the distribution of the random variable, then if two random variables have the same mgf they must have identical distributions.

Below, the transformation $Y = X_1 + X_2 + \cdots X_n$ is demonstrated, but the same principles can be applied for other linear combinations also.

Consider $n$ independent random variables $X_1, X_2, \dots, X_n$ with mgfs $M_{X_1}(t)$, $M_{X_2}(t)$, $\dots$, $M_{X_n}(t)$, and consider the transformation $Y = X_1 + X_2 + \cdots X_n$.
Since the $X_i$ are independent, $f_{X_1,X_2\dots X_n}(x_1, x_2, \dots, x_n) = f_{X_1}(x_1).f_{X_2}(x_2)\dots f_{X_n}(x_n)$.
So, by definition of the mgf,
\begin{align*}
   M_Y(t)
   &= \text{E}(\exp(tY)) \\
   &= \text{E}(\exp[t(X_1 + X_2 + \cdots X_n)]) \\
   &= \int\!\!\!\int\!\!\!\cdots\!\!\!\int \exp[t(x_1 + x_2 + \cdots x_n)] f(x_1, x_2, \dots x_n)\,dx_n\dots dx_2\, dx_1 \\
   &= \int\!\!\!\int\!\!\!\cdots\!\!\!\int \exp(tx_1) f(x_1) \exp(t{x_2}) f(x_2)\dots \exp(t{x_n})f(x_n) \,dx_n\dots dx_2\, dx_1 \\
   &= \int \exp(t x_1) f(x_1)\,dx_1 \int \exp(t{x_2}) f(x_2)\,dx_2 \dots \int \exp(t{x_n})f(x_n)\,dx_n \\
   &= M_{X_1}(t) M_{X_2}(t)\dots M_{X_n}(t) \\
   &= \prod_{i = 1}^n M_{X_i}(t).
\end{align*}
($\prod$ is the symbol for a product of terms, in the same way that $\sum$ is the symbol for a summation of terms.)
The above result also holds for discrete variables, where summations replace integrations.

This result follows:
If $X_1, X_2, \dots, X_n$ are independent random variables and $Y  =  X_1 + X_2 + \dots + X_n$, then the mgf of $Y$ is
\[
   M_Y(t)  =  \prod_{i = 1}^n M_{X_i}(t)
\]
where $M_{X_i}(t)$ is the mgf of $X_i$ at $t$ for $i = 1, 2, \dots, n$.


:::{.example #MGFLinearX name="Mgf method for transformations"}
Suppose that $X_i \sim \text{Pois}(\lambda_i)$ for $i  =  1, 2, \dots, n$.
What is the distribution of $Y  =  X_1  +  X_2  + \dots  +  X_n$?

Since $X_i$ has a Poisson distribution with parameter $\lambda_i$, the mgf of $X_i$ is
\[
   M_{X_i}(t) = \exp[ \lambda_i(e^t - 1)].
\]
The mgf of
$Y  = X_1 + X_2 + \cdots X_n$ is
\begin{align*}
   M_Y(t)
   &= \prod_{i = 1}^n \exp[ \lambda_i(e^t - 1)] \\
   &= \exp[ \lambda_1(e^t - 1)] \exp[ \lambda_2(e^t - 1)] \dots \exp[ \lambda_n(e^t - 1)] \\
   &= \exp\left[ (e^t - 1)\sum_{i = 1}^n \lambda_i\right].
\end{align*}
Using $\Lambda = \sum_{i = 1}^n \lambda_i$, the mgf of $Y$ is
\[
   M_Y(t) = \exp\left[ (e^t - 1)\Lambda \right],
\]
which is the mgf of a Poisson distribution with mean $\Lambda = \sum_{i = 1}^n \lambda_i$.
This means that the sum of $n$ independent Poisson distribution is also a Poisson distribution, whose mean is the sum of the individual Poisson means.
:::


## The chi-squared distribution

Examples \@ref(exm:TransformationNon11) and \@ref(exm:TransformB) produce the chi-square distribution, which is an important model in statistical theory (Theorem \@ref(thm:ChiSquare)).


:::{.definition #ChiSquaredDistribution name="Chi-squared distribution"}
A continuous random variable $X$ with probability density function
\begin{equation}
   f_X(x)
   = \frac{x^{(\nu/2) - 1}e^{-x/2}}{2^{\nu/2}\Gamma(\nu/2)}\quad\text{for $x > 0$}
\end{equation}
is said to have a *chi-square distribution* with parameter $\nu > 0$.
The parameter $\nu$ is called the *degrees of freedom*. 
We write $X \sim \chi^2(\nu)$.
:::


Some plots of $\chi^2$-distributions are shown in Fig. \@ref(fig:ChisqPlots).


::: {.linkBox .link data-latex="{iconmonstr-link-1-240.png}"}
The chi-squared is a special case of the gamma distribution with $\alpha = \nu/2$ and $\beta = 2$.
This means that properties of the chi-squared distribution can be obtained from those for the gamma distribution.
:::




```{r ChisqPlots, echo=FALSE, out.width='100%', fig.height=3, fig.width=6.5, fig.cap="Some $\\chi^2$-distributions"}
par( mfrow = c(1, 3))

xx <- seq(0.01, 4,
          length = 100)

plot( x = xx,
      y = dchisq(xx, df = 1),
#      ylim = c(0, 0.4),
      lwd = 2,
      type = "l",
      xlab = expression(italic(x)),
      ylab = "Density",
      las = 1,
      main = expression( paste("The"~chi^2*"-distribution: df"==1)))

xx <- seq(0.001, 15,
          length = 100)
plot( x = xx,
      y = dchisq(xx, df = 5),
      lwd = 2,
      type = "l",
      xlab = expression(italic(x)),
      ylab = "Density",
      las = 1,
      main = expression( paste("The"~chi^2*"-distribution: df"==6)))


xx <- seq(0.001, 50,
          length = 100)
plot( x = xx,
      y = dchisq(xx, df = 20),
      lwd = 2,
      type = "l",
      xlab = expression(italic(x)),
      ylab = "Density",
      las = 1,
      main = expression( paste("The"~chi^2*"-distribution: df"==20)))
```

The basic properties of the chi-square follow directly from those of the [gamma distribution](thm:GammaProperties).


:::{.theorem #ChisqProperties name="Properties of chi-squared distribution"}
If $X\sim\chi^2(\nu)$ then

* $\text{E}(X) = \nu$.
* $\text{var}(X) = 2\nu$.
* $M_X(t) = (1 - 2t)^{-\nu/2}$.
:::

:::{.proof}
See Theorem \@ref(thm:GammaProperties).
:::


The importance of the chi-square distribution is hinted at in Examples \@ref(exm:TransformationNon11) and \@ref(exm:TransformB), which essentially prove the following theorem.


:::{.theorem #ChiSqDistribution name="Chi-square distribution with 1 df"}
If $Z\sim N(0, 1)$ then $Z^2$ has a chi-square distribution with one degree of freedom.
:::

:::{.proof}
Exercise; see Example \@ref(exm:TransformationNon11).
:::


A useful property of the chi-square distribution is that the sum of independent random variables, each with a chi-square distribution, also has a chi-square distribution.
This property is given in the following theorem, which will be used later.


:::{.theorem #SumSquaredNormals name="Chi-squared distribution"}
If $Z_1, Z_2,\dots, Z_n$ are independently and identically distributed (iid) as $N(0, 1)$, then the sum of squares $S = \sum_i Z_i^2$ has a $\chi^2(n)$ distribution.
:::

:::{.proof}
Since $S$ is a linear combination of known distributions, the mgf method is appropriate.
Since $Z_i \sim \chi^2(1)$, from Theorem \@ref(thm:ChisqProperties)
\[
   M_{Z_i}(t)
   = (1 - 2t)^{-1/2}.
\]
From Theorem \@ref(thm:MGFIndependent) then, $S = \sum_{i = 1}^n Z_i^2$ has mgf
\begin{align*}
   M_{S}(t)
   &= \prod_{i = 1}^n (1 - 2t)^{-1/2}\\
   &= \left[(1 - 2t)^{-1/2}\right]^n 
    = (1 - 2t)^{-n/2},
\end{align*}
which is the mgf of $\chi^2(n)$.
:::


Chi-square probabilities cannot in general be calculated without computers or tables.


::: {.softwareBox .software data-latex="{iconmonstr-laptop-4-240.png}"}
In **R**, the chi-squared distribution functions have the form `[dpqr]chisq(df)`, where `df`${} = \nu$ refers to the degrees of freedom.
:::


:::{.example #ChisqProb name="Chi-squared distributions"}
The variable $X$ has a chi-square distribution with 12 df. 
Determine the value of $X$ below which lies 90% of the distribution.

We seek a value $c$ such that $\Pr(X < c) = F_X(c) = 0.90$ where $X\sim\chi^2(12)$.
In **R**:

```{r}
qchisq(0.9, df = 12)
```

That is, about 90% of the distribution lies below `r round(qchisq(0.9, df=12), 3)`.
:::


## Exercises

